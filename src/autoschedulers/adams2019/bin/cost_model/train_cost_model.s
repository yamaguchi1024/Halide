	.text
	.file	"halide_buffer_t.cpp"
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function train_cost_model
.LCPI0_0:
	.quad	-9223372036854775808            # 0x8000000000000000
	.quad	0                               # 0x0
.LCPI0_8:
	.long	0                               # 0x0
	.long	32                              # 0x20
	.long	1                               # 0x1
	.long	0                               # 0x0
.LCPI0_9:
	.long	0                               # 0x0
	.long	32                              # 0x20
	.long	32                              # 0x20
	.long	0                               # 0x0
.LCPI0_10:
	.long	0                               # 0x0
	.long	8                               # 0x8
	.long	1                               # 0x1
	.long	0                               # 0x0
.LCPI0_11:
	.long	0                               # 0x0
	.long	40                              # 0x28
	.long	8                               # 0x8
	.long	0                               # 0x0
.LCPI0_12:
	.long	0                               # 0x0
	.long	7                               # 0x7
	.long	320                             # 0x140
	.long	0                               # 0x0
.LCPI0_13:
	.long	0                               # 0x0
	.long	24                              # 0x18
	.long	1                               # 0x1
	.long	0                               # 0x0
.LCPI0_14:
	.long	0                               # 0x0
	.long	39                              # 0x27
	.long	24                              # 0x18
	.long	0                               # 0x0
.LCPI0_15:
	.long	0                               # 0x0
	.long	40                              # 0x28
	.long	1                               # 0x1
	.long	0                               # 0x0
.LCPI0_16:
	.long	0                               # 0x0
	.long	7                               # 0x7
	.long	40                              # 0x28
	.long	0                               # 0x0
.LCPI0_17:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	32                              # 0x20
	.long	0                               # 0x0
.LCPI0_18:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	1024                            # 0x400
	.long	0                               # 0x0
.LCPI0_19:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	8                               # 0x8
	.long	0                               # 0x0
.LCPI0_22:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	24                              # 0x18
	.long	0                               # 0x0
.LCPI0_23:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	936                             # 0x3a8
	.long	0                               # 0x0
.LCPI0_44:
	.long	24                              # 0x18
	.long	4                               # 0x4
	.long	24                              # 0x18
	.long	39                              # 0x27
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5
.LCPI0_1:
	.quad	-9223372036854774784            # 0x8000000000000400
	.quad	256                             # 0x100
	.quad	32768                           # 0x8000
	.quad	65536                           # 0x10000
.LCPI0_2:
	.quad	64                              # 0x40
	.quad	4                               # 0x4
	.quad	8                               # 0x8
	.quad	2                               # 0x2
.LCPI0_3:
	.quad	16                              # 0x10
	.quad	32                              # 0x20
	.quad	512                             # 0x200
	.quad	2048                            # 0x800
.LCPI0_4:
	.quad	8192                            # 0x2000
	.quad	16384                           # 0x4000
	.quad	131072                          # 0x20000
	.quad	524288                          # 0x80000
.LCPI0_20:
	.long	0                               # 0x0
	.long	8                               # 0x8
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	0                               # 0x0
	.long	40                              # 0x28
	.long	8                               # 0x8
	.long	0                               # 0x0
.LCPI0_21:
	.long	0                               # 0x0
	.long	7                               # 0x7
	.long	320                             # 0x140
	.long	0                               # 0x0
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	2240                            # 0x8c0
	.long	0                               # 0x0
.LCPI0_24:
	.long	73730                           # 0x12002
	.long	3                               # 0x3
	.long	73730                           # 0x12002
	.long	1                               # 0x1
	.long	73730                           # 0x12002
	.long	1                               # 0x1
	.long	73730                           # 0x12002
	.long	2                               # 0x2
.LCPI0_25:
	.long	73730                           # 0x12002
	.long	3                               # 0x3
	.long	73730                           # 0x12002
	.long	2                               # 0x2
	.long	73730                           # 0x12002
	.long	4                               # 0x4
	.long	73730                           # 0x12002
	.long	2                               # 0x2
.LCPI0_26:
	.quad	1073741824                      # 0x40000000
	.quad	2147483648                      # 0x80000000
	.quad	4294967296                      # 0x100000000
	.quad	8589934592                      # 0x200000000
.LCPI0_27:
	.quad	4194304                         # 0x400000
	.quad	8388608                         # 0x800000
	.quad	16777216                        # 0x1000000
	.quad	33554432                        # 0x2000000
.LCPI0_28:
	.quad	17179869184                     # 0x400000000
	.quad	34359738368                     # 0x800000000
	.quad	68719476736                     # 0x1000000000
	.quad	137438953472                    # 0x2000000000
.LCPI0_29:
	.quad	67108864                        # 0x4000000
	.quad	134217728                       # 0x8000000
	.quad	268435456                       # 0x10000000
	.quad	536870912                       # 0x20000000
.LCPI0_30:
	.quad	0                               # 0x0
	.quad	0                               # 0x0
	.quad	36028797018963968               # 0x80000000000000
	.quad	0                               # 0x0
.LCPI0_31:
	.quad	1024                            # 0x400
	.quad	2048                            # 0x800
	.quad	0                               # 0x0
	.quad	4096                            # 0x1000
.LCPI0_32:
	.quad	562949953421312                 # 0x2000000000000
	.quad	35184372088832                  # 0x200000000000
	.quad	2251799813685248                # 0x8000000000000
	.quad	9007199254740992                # 0x20000000000000
.LCPI0_33:
	.quad	0                               # 0x0
	.quad	0                               # 0x0
	.quad	8796093022208                   # 0x80000000000
	.quad	0                               # 0x0
.LCPI0_34:
	.quad	16                              # 0x10
	.quad	32                              # 0x20
	.quad	0                               # 0x0
	.quad	64                              # 0x40
.LCPI0_35:
	.quad	0                               # 0x0
	.quad	144115188075855872              # 0x200000000000000
	.quad	576460752303423488              # 0x800000000000000
	.quad	0                               # 0x0
.LCPI0_36:
	.quad	8192                            # 0x2000
	.quad	0                               # 0x0
	.quad	0                               # 0x0
	.quad	16384                           # 0x4000
.LCPI0_37:
	.quad	0                               # 0x0
	.quad	1099511627776                   # 0x10000000000
	.quad	0                               # 0x0
	.quad	0                               # 0x0
.LCPI0_38:
	.quad	2                               # 0x2
	.quad	0                               # 0x0
	.quad	4                               # 0x4
	.quad	8                               # 0x8
.LCPI0_39:
	.quad	0                               # 0x0
	.quad	140737488355328                 # 0x800000000000
	.quad	0                               # 0x0
	.quad	0                               # 0x0
.LCPI0_40:
	.quad	128                             # 0x80
	.quad	0                               # 0x0
	.quad	256                             # 0x100
	.quad	512                             # 0x200
.LCPI0_41:
	.quad	32768                           # 0x8000
	.quad	65536                           # 0x10000
	.quad	131072                          # 0x20000
	.quad	262144                          # 0x40000
.LCPI0_42:
	.quad	0                               # 0x0
	.quad	4611686018427387904             # 0x4000000000000000
	.quad	0                               # 0x0
	.quad	0                               # 0x0
.LCPI0_43:
	.quad	524288                          # 0x80000
	.quad	0                               # 0x0
	.quad	1048576                         # 0x100000
	.quad	2097152                         # 0x200000
.LCPI0_45:
	.quad	274877906944                    # 0x4000000000
	.quad	1099511627776                   # 0x10000000000
	.quad	4398046511104                   # 0x40000000000
	.quad	17592186044416                  # 0x100000000000
.LCPI0_46:
	.quad	65536                           # 0x10000
	.quad	262144                          # 0x40000
	.quad	128                             # 0x80
	.quad	1048576                         # 0x100000
.LCPI0_47:
	.quad	8                               # 0x8
	.quad	512                             # 0x200
	.quad	2048                            # 0x800
	.quad	16384                           # 0x4000
.LCPI0_48:
	.quad	4194304                         # 0x400000
	.quad	16777216                        # 0x1000000
	.quad	67108864                        # 0x4000000
	.quad	268435456                       # 0x10000000
.LCPI0_49:
	.quad	0                               # 0x0
	.quad	0                               # 0x0
	.quad	0                               # 0x0
	.quad	2                               # 0x2
.LCPI0_50:
	.quad	2305843009213693952             # 0x2000000000000000
	.quad	4611686018427387904             # 0x4000000000000000
	.quad	1152921504606846976             # 0x1000000000000000
	.quad	0                               # 0x0
.LCPI0_51:
	.quad	1073741824                      # 0x40000000
	.quad	4294967296                      # 0x100000000
	.quad	17179869184                     # 0x400000000
	.quad	68719476736                     # 0x1000000000
.LCPI0_52:
	.quad	72057594037927936               # 0x100000000000000
	.quad	288230376151711744              # 0x400000000000000
	.quad	576460752303423488              # 0x800000000000000
	.quad	144115188075855872              # 0x200000000000000
.LCPI0_53:
	.quad	9007199254740992                # 0x20000000000000
	.quad	18014398509481984               # 0x40000000000000
	.quad	4503599627370496                # 0x10000000000000
	.quad	36028797018963968               # 0x80000000000000
.LCPI0_54:
	.quad	-9222809086901354496            # 0x8002000000000000
	.quad	1125899906842624                # 0x4000000000000
	.quad	281474976710656                 # 0x1000000000000
	.quad	2251799813685248                # 0x8000000000000
.LCPI0_55:
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	39                              # 0x27
	.long	1                               # 0x1
	.long	1                               # 0x1
	.long	1                               # 0x1
	.long	1                               # 0x1
	.long	1                               # 0x1
.LCPI0_56:
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	32                              # 0x20
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	0                               # 0x0
	.long	8                               # 0x8
	.long	1                               # 0x1
.LCPI0_57:
	.long	40                              # 0x28
	.long	0                               # 0x0
	.long	7                               # 0x7
	.long	0                               # 0x0
	.long	24                              # 0x18
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	24                              # 0x18
.LCPI0_58:
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	32                              # 0x20
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	0                               # 0x0
	.long	32                              # 0x20
.LCPI0_59:
	.quad	2097152                         # 0x200000
	.quad	4194304                         # 0x400000
	.quad	16777216                        # 0x1000000
	.quad	33554432                        # 0x2000000
.LCPI0_60:
	.quad	64                              # 0x40
	.quad	16                              # 0x10
	.quad	256                             # 0x100
	.quad	512                             # 0x200
.LCPI0_61:
	.quad	536870912                       # 0x20000000
	.quad	2147483648                      # 0x80000000
	.quad	4294967296                      # 0x100000000
	.quad	1073741824                      # 0x40000000
.LCPI0_62:
	.quad	8192                            # 0x2000
	.quad	16384                           # 0x4000
	.quad	32768                           # 0x8000
	.quad	65536                           # 0x10000
.LCPI0_63:
	.quad	131072                          # 0x20000
	.quad	524288                          # 0x80000
	.quad	1048576                         # 0x100000
	.quad	262144                          # 0x40000
.LCPI0_64:
	.quad	2                               # 0x2
	.quad	4                               # 0x4
	.quad	8                               # 0x8
	.quad	32                              # 0x20
.LCPI0_65:
	.quad	8388608                         # 0x800000
	.quad	67108864                        # 0x4000000
	.quad	134217728                       # 0x8000000
	.quad	268435456                       # 0x10000000
.LCPI0_66:
	.quad	128                             # 0x80
	.quad	1024                            # 0x400
	.quad	2048                            # 0x800
	.quad	4096                            # 0x1000
.LCPI0_67:
	.long	7                               # 0x7
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	0                               # 0x0
	.long	24                              # 0x18
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	4                               # 0x4
.LCPI0_68:
	.long	0                               # 0x0
	.long	4                               # 0x4
	.long	0                               # 0x0
	.long	8                               # 0x8
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	40                              # 0x28
	.long	0                               # 0x0
.LCPI0_69:
	.quad	8589934592                      # 0x200000000
	.quad	17179869184                     # 0x400000000
	.quad	68719476736                     # 0x1000000000
	.quad	137438953472                    # 0x2000000000
.LCPI0_70:
	.quad	2199023255552                   # 0x20000000000
	.quad	4398046511104                   # 0x40000000000
	.quad	8796093022208                   # 0x80000000000
	.quad	35184372088832                  # 0x200000000000
.LCPI0_71:
	.quad	34359738368                     # 0x800000000
	.quad	274877906944                    # 0x4000000000
	.quad	549755813888                    # 0x8000000000
	.quad	1099511627776                   # 0x10000000000
.LCPI0_72:
	.quad	70368744177664                  # 0x400000000000
	.quad	17592186044416                  # 0x100000000000
	.quad	140737488355328                 # 0x800000000000
	.quad	281474976710656                 # 0x1000000000000
.LCPI0_73:
	.quad	256                             # 0x100
	.quad	128                             # 0x80
	.quad	1024                            # 0x400
	.quad	576460752303423488              # 0x800000000000000
.LCPI0_74:
	.quad	2                               # 0x2
	.quad	9007199254740992                # 0x20000000000000
	.quad	4                               # 0x4
	.quad	18014398509481984               # 0x40000000000000
.LCPI0_75:
	.quad	512                             # 0x200
	.quad	2048                            # 0x800
	.quad	1152921504606846976             # 0x1000000000000000
	.quad	4096                            # 0x1000
.LCPI0_76:
	.quad	32                              # 0x20
	.quad	144115188075855872              # 0x200000000000000
	.quad	288230376151711744              # 0x400000000000000
	.quad	64                              # 0x40
.LCPI0_77:
	.quad	32768                           # 0x8000
	.quad	131072                          # 0x20000
	.quad	4611686018427387904             # 0x4000000000000000
	.quad	262144                          # 0x40000
.LCPI0_78:
	.quad	16384                           # 0x4000
	.quad	8192                            # 0x2000
	.quad	65536                           # 0x10000
	.quad	2305843009213693952             # 0x2000000000000000
.LCPI0_79:
	.quad	8                               # 0x8
	.quad	36028797018963968               # 0x80000000000000
	.quad	72057594037927936               # 0x100000000000000
	.quad	16                              # 0x10
.LCPI0_80:
	.quad	-9222809086901354496            # 0x8002000000000000
	.quad	1125899906842624                # 0x4000000000000
	.quad	2251799813685248                # 0x8000000000000
	.quad	4503599627370496                # 0x10000000000000
.LCPI0_102:
	.long	0                               # 0x0
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	1                               # 0x1
	.long	3                               # 0x3
	.long	1                               # 0x1
	.long	3                               # 0x3
.LCPI0_105:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI0_5:
	.long	1                               # 0x1
.LCPI0_6:
	.long	4294967288                      # 0xfffffff8
.LCPI0_7:
	.long	8                               # 0x8
.LCPI0_81:
	.long	0xbabd3069                      # float -0.0014433983
.LCPI0_82:
	.long	0xbf317218                      # float -0.693147182
.LCPI0_83:
	.long	0xba8322ca                      # float -0.00100048748
.LCPI0_84:
	.long	0xb9a797f3                      # float -3.19659332E-4
.LCPI0_85:
	.long	0xbc0b192a                      # float -0.00848988629
.LCPI0_86:
	.long	0xbe2aae1f                      # float -0.166679844
.LCPI0_87:
	.long	0xbf800000                      # float -1
.LCPI0_88:
	.long	0xba9c2e66                      # float -0.00119156833
.LCPI0_89:
	.long	0xbd2a66bc                      # float -0.0416018814
.LCPI0_90:
	.long	0xbeffffde                      # float -0.499998987
.LCPI0_91:
	.long	0x3f800000                      # float 1
.LCPI0_92:
	.long	0x80000000                      # float -0
.LCPI0_93:
	.long	0xbe1ba6b6                      # float -0.152003139
.LCPI0_94:
	.long	0xbdd7c745                      # float -0.105360545
.LCPI0_95:
	.long	0x3f666666                      # float 0.899999976
.LCPI0_96:
	.long	0x3dcccccd                      # float 0.100000001
.LCPI0_97:
	.long	0x3f7fbe77                      # float 0.999000012
.LCPI0_98:
	.long	0x3a83126f                      # float 0.00100000005
.LCPI0_99:
	.long	0xb727c5ac                      # float -9.99999974E-6
.LCPI0_100:
	.long	0x2edbe6ff                      # float 1.00000001E-10
.LCPI0_101:
	.long	0x3727c5ac                      # float 9.99999974E-6
.LCPI0_103:
	.long	0x45800000                      # float 4096
.LCPI0_104:
	.long	0x40000000                      # float 2
.LCPI0_106:
	.long	16                              # 0x10
.LCPI0_107:
	.long	24                              # 0x18
.LCPI0_108:
	.long	32                              # 0x20
	.section	.text.train_cost_model,"ax",@progbits
	.globl	train_cost_model
	.p2align	4, 0x90
	.type	train_cost_model,@function
train_cost_model:                       # @train_cost_model
	.cfi_startproc
# %bb.0:                                # %entry
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$16320, %rsp                    # imm = 0x3FC0
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	vmovss	%xmm0, 1952(%rsp)               # 4-byte Spill
	movl	%edx, 1948(%rsp)                # 4-byte Spill
	movl	%esi, %r15d
                                        # kill: def $edi killed $edi def $rdi
	movq	%rdi, 40(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	cmpq	$0, 96(%rbp)
	sete	%al
	xorl	%edx, %edx
	cmpq	$0, 152(%rbp)
	sete	%dl
	shlq	$7, %rdx
	xorl	%edi, %edi
	cmpq	$0, 144(%rbp)
	sete	%dil
	shlq	$12, %rdi
	xorl	%esi, %esi
	cmpq	$0, 160(%rbp)
	sete	%sil
	orq	%rdx, %rdi
	shlq	$18, %rsi
	orq	%rsi, %rdi
	movq	16(%rbp), %r12
	vmovq	%r12, %xmm0
	vmovq	%r9, %xmm1
	vpunpcklqdq	%xmm0, %xmm1, %xmm0     # xmm0 = xmm1[0],xmm0[0]
	movq	%r8, 320(%rsp)                  # 8-byte Spill
	vmovq	%r8, %xmm1
	movq	%rcx, 440(%rsp)                 # 8-byte Spill
	vmovq	%rcx, %xmm2
	vpunpcklqdq	%xmm1, %xmm2, %xmm1     # xmm1 = xmm2[0],xmm1[0]
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vmovdqu	72(%rbp), %xmm1
	vmovq	104(%rbp), %xmm2                # xmm2 = mem[0],zero
	vmovq	88(%rbp), %xmm3                 # xmm3 = mem[0],zero
	vpunpcklqdq	%xmm2, %xmm3, %xmm2     # xmm2 = xmm3[0],xmm2[0]
	vinserti128	$1, %xmm2, %ymm1, %ymm1
	vpxor	%xmm2, %xmm2, %xmm2
	vpcmpeqq	%ymm2, %ymm1, %ymm1
	vpcmpeqq	%ymm2, %ymm0, %ymm0
	vpcmpeqq	24(%rbp), %ymm2, %ymm3
	vmovapd	.LCPI0_0(%rip), %xmm4           # xmm4 = [9223372036854775808,0]
	vblendvpd	%ymm0, .LCPI0_1(%rip), %ymm4, %ymm0
	vpcmpeqq	112(%rbp), %ymm2, %ymm2
	vpand	.LCPI0_2(%rip), %ymm1, %ymm1
	vpand	.LCPI0_3(%rip), %ymm2, %ymm2
	vpand	.LCPI0_4(%rip), %ymm3, %ymm3
	orq	%rax, %rdi
	vpor	%ymm2, %ymm3, %ymm2
	vorpd	%ymm2, %ymm0, %ymm0
	vpor	%ymm0, %ymm1, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpor	%xmm1, %xmm0, %xmm0
	vpshufd	$78, %xmm0, %xmm1               # xmm1 = xmm0[2,3,0,1]
	vpor	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rax
	orq	%rax, %rdi
	xorl	%eax, %eax
	tzcntq	%rdi, %rax
	cmpl	$19, %eax
	jbe	.LBB0_319
# %bb.1:                                # %no_errors_bb
	movq	%r9, %r13
	movq	48(%rbp), %rbx
	movq	32(%rbp), %r14
	movq	16(%rbx), %rax
	movq	%rax, 704(%rsp)                 # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	vzeroupper
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1800(%rsp)                # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 2208(%rsp)                # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 788(%rsp)                 # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 408(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 832(%rsp)                 # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 1520(%rsp)                # 4-byte Spill
	movq	160(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 2072(%rsp)                # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1792(%rsp)                # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 2200(%rsp)                # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 1784(%rsp)                # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1816(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1320(%rsp)                # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 1512(%rsp)                # 4-byte Spill
	movq	40(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 560(%rsp)                 # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1776(%rsp)                # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 2192(%rsp)                # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 1040(%rsp)                # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1272(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1232(%rsp)                # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1504(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 1264(%rsp)                # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	movl	24(%rax), %eax
	movl	%eax, 432(%rsp)                 # 4-byte Spill
	movq	16(%r12), %rax
	movq	%rax, 1432(%rsp)                # 8-byte Spill
	leaq	32(%r12), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1768(%rsp)                # 4-byte Spill
	movq	24(%r12), %rax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movl	36(%r12), %eax
	movl	%eax, 1760(%rsp)                # 4-byte Spill
	movq	40(%r12), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1200(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1184(%rsp)                # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 1500(%rsp)                # 4-byte Spill
	movq	16(%r13), %rax
	movq	%rax, 552(%rsp)                 # 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1752(%rsp)                # 4-byte Spill
	movq	24(%r13), %rax
	movq	%rax, 576(%rsp)                 # 8-byte Spill
	movl	36(%r13), %eax
	movl	%eax, 1744(%rsp)                # 4-byte Spill
	movq	%r13, 648(%rsp)                 # 8-byte Spill
	movq	40(%r13), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1168(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 288(%rsp)                 # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1492(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 216(%rsp)                 # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 488(%rsp)                 # 8-byte Spill
	movl	24(%rax), %ecx
	movl	%ecx, 512(%rsp)                 # 4-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 144(%rsp)                 # 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 480(%rsp)                 # 8-byte Spill
	movl	40(%rax), %eax
	movl	%eax, 416(%rsp)                 # 4-byte Spill
	movq	16(%r14), %rax
	movq	%rax, 1456(%rsp)                # 8-byte Spill
	leaq	32(%r14), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1736(%rsp)                # 4-byte Spill
	movq	24(%r14), %rax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movl	36(%r14), %eax
	movl	%eax, 1728(%rsp)                # 4-byte Spill
	movq	40(%r14), %rax
	movl	(%rax), %ecx
	movq	%rcx, 664(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 872(%rsp)                 # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 1552(%rsp)                # 4-byte Spill
	movq	24(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 1448(%rsp)                # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1720(%rsp)                # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 1712(%rsp)                # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 864(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 848(%rsp)                 # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1548(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 856(%rsp)                 # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 840(%rsp)                 # 8-byte Spill
	movl	24(%rax), %eax
	movl	%eax, 1440(%rsp)                # 4-byte Spill
	movq	144(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 992(%rsp)                 # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1704(%rsp)                # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 896(%rsp)                 # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 1696(%rsp)                # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 656(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 424(%rsp)                 # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 1544(%rsp)                # 4-byte Spill
	movq	136(%rbp), %rcx
	movq	16(%rcx), %rax
	movq	%rax, 1304(%rsp)                # 8-byte Spill
	leaq	32(%rcx), %rsi
	movq	%rcx, %rbx
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1688(%rsp)                # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 1680(%rsp)                # 4-byte Spill
	movq	440(%rsp), %rbx                 # 8-byte Reload
	movq	16(%rbx), %rax
	movq	%rax, 1408(%rsp)                # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1672(%rsp)                # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 1664(%rsp)                # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1328(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 152(%rsp)                 # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1540(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 504(%rsp)                 # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 968(%rsp)                 # 8-byte Spill
	movl	24(%rax), %ecx
	movq	%rcx, 1400(%rsp)                # 8-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 544(%rsp)                 # 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 976(%rsp)                 # 8-byte Spill
	movl	40(%rax), %eax
	movl	%eax, 1392(%rsp)                # 4-byte Spill
	movq	128(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 1808(%rsp)                # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1656(%rsp)                # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 200(%rsp)                 # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 1648(%rsp)                # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %r14d
	movl	4(%rax), %r12d
	movl	8(%rax), %eax
	movl	%eax, 1536(%rsp)                # 4-byte Spill
	movq	320(%rsp), %rbx                 # 8-byte Reload
	movq	16(%rbx), %rax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 1640(%rsp)                # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 496(%rsp)                 # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 784(%rsp)                 # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 272(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 88(%rsp)                  # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1532(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 48(%rsp)                  # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 952(%rsp)                 # 8-byte Spill
	movl	24(%rax), %ecx
	movq	%rcx, 1344(%rsp)                # 8-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 208(%rsp)                 # 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 960(%rsp)                 # 8-byte Spill
	movl	40(%rax), %eax
	movl	%eax, 36(%rsp)                  # 4-byte Spill
	movq	152(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 1472(%rsp)                # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 780(%rsp)                 # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 536(%rsp)                 # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 776(%rsp)                 # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %r13d
	movl	4(%rax), %ecx
	movq	%rcx, 1312(%rsp)                # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 1528(%rsp)                # 4-byte Spill
	movq	72(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 712(%rsp)                 # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 772(%rsp)                 # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 888(%rsp)                 # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 768(%rsp)                 # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 640(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 72(%rsp)                  # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 1524(%rsp)                # 4-byte Spill
	movq	120(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 2064(%rsp)                # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 764(%rsp)                 # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 624(%rsp)                 # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 760(%rsp)                 # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 824(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1296(%rsp)                # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1516(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 528(%rsp)                 # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 1288(%rsp)                # 8-byte Spill
	movl	24(%rax), %eax
	movq	%rax, 2056(%rsp)                # 8-byte Spill
	movq	112(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 2048(%rsp)                # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 756(%rsp)                 # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 880(%rsp)                 # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 752(%rsp)                 # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1280(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1248(%rsp)                # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1508(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 1256(%rsp)                # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 1224(%rsp)                # 8-byte Spill
	movl	24(%rax), %ecx
	movl	%ecx, 1560(%rsp)                # 4-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 1240(%rsp)                # 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 1208(%rsp)                # 8-byte Spill
	movl	40(%rax), %eax
	movq	%rax, 2040(%rsp)                # 8-byte Spill
	movq	88(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 2008(%rsp)                # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 748(%rsp)                 # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 696(%rsp)                 # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 744(%rsp)                 # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1192(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1176(%rsp)                # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1496(%rsp)                # 4-byte Spill
	vmovsd	16(%rax), %xmm0                 # xmm0 = mem[0],zero
	vmovaps	%xmm0, 2080(%rsp)               # 16-byte Spill
	movslq	24(%rax), %rax
	movq	%rax, 2000(%rsp)                # 8-byte Spill
	movq	80(%rbp), %rcx
	movq	16(%rcx), %rax
	movq	%rax, 1416(%rsp)                # 8-byte Spill
	leaq	32(%rcx), %rsi
	movq	%rcx, %rbx
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 740(%rsp)                 # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1160(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1144(%rsp)                # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1488(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 1152(%rsp)                # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 1128(%rsp)                # 8-byte Spill
	movl	24(%rax), %ecx
	movl	%ecx, 520(%rsp)                 # 4-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 1136(%rsp)                # 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 1112(%rsp)                # 8-byte Spill
	movl	40(%rax), %ecx
	movq	%rcx, 1992(%rsp)                # 8-byte Spill
	movl	48(%rax), %ecx
	movq	%rcx, 1120(%rsp)                # 8-byte Spill
	movl	52(%rax), %ecx
	movq	%rcx, 1104(%rsp)                # 8-byte Spill
	movl	56(%rax), %eax
	movq	%rax, 1336(%rsp)                # 8-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 2184(%rsp)                # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 736(%rsp)                 # 4-byte Spill
	movq	104(%rbp), %rbx
	movq	16(%rbx), %rax
	movq	%rax, 2032(%rsp)                # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	20(%rsp), %eax
	movl	%eax, 732(%rsp)                 # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 688(%rsp)                 # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 728(%rsp)                 # 4-byte Spill
	movq	40(%rbx), %rax
	movl	(%rax), %ecx
	movl	%ecx, 1028(%rsp)                # 4-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1096(%rsp)                # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1484(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movl	%ecx, 1024(%rsp)                # 4-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 1088(%rsp)                # 8-byte Spill
	movl	24(%rax), %eax
	movq	%rax, 2024(%rsp)                # 8-byte Spill
	movq	96(%rbp), %rcx
	movq	16(%rcx), %rax
	movq	%rax, 2016(%rsp)                # 8-byte Spill
	leaq	32(%rcx), %rsi
	leaq	20(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	leal	(%r12,%r14), %ecx
	cmpl	%r15d, %ecx
	movl	%r15d, %eax
	movl	%ecx, 1940(%rsp)                # 4-byte Spill
	cmovgel	%ecx, %eax
	movq	656(%rsp), %rdi                 # 8-byte Reload
	movq	424(%rsp), %rcx                 # 8-byte Reload
	leal	(%rcx,%rdi), %r8d
	cmpl	%r15d, %r8d
	movl	%r15d, %edx
	cmovgel	%r8d, %edx
	movq	1312(%rsp), %rcx                # 8-byte Reload
	leal	(%rcx,%r13), %r9d
	cmpl	%r9d, %edx
	cmovll	%r9d, %edx
	movq	1816(%rsp), %rcx                # 8-byte Reload
	movq	1320(%rsp), %rsi                # 8-byte Reload
	leal	(%rsi,%rcx), %r10d
	cmpl	%r10d, %edx
	cmovll	%r10d, %edx
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	movq	%rdi, %rsi
	cmpl	%esi, %r13d
	movl	%r13d, %r11d
	cmovgl	%esi, %r11d
	cmpl	%r11d, %ecx
	cmovlel	%ecx, %r11d
	cmpl	%r11d, %r14d
	cmovlel	%r14d, %r11d
	cmpl	%r15d, %edx
	movl	%edx, 1944(%rsp)                # 4-byte Spill
	cmovll	%r15d, %edx
	movq	288(%rsp), %rsi                 # 8-byte Reload
	cmpl	$7, %esi
	movl	$8, %ecx
	cmovgl	%esi, %ecx
	movq	%rcx, 1384(%rsp)                # 8-byte Spill
	movl	%esi, %ecx
	sarl	$31, %ecx
	andl	%esi, %ecx
	movl	%ecx, 572(%rsp)                 # 4-byte Spill
	cmpl	$9, %r12d
	movl	$8, %esi
	cmovll	%r12d, %esi
	leal	-1(%r12), %edi
	andl	$-8, %edi
	addl	%esi, %edi
	cmpl	%edi, %r12d
	movq	%r12, 632(%rsp)                 # 8-byte Spill
	cmovlel	%r12d, %edi
	movq	%rdi, 1856(%rsp)                # 8-byte Spill
	vmovd	%edx, %xmm0
	vpinsrd	$1, %eax, %xmm0, %xmm0
	vpbroadcastd	.LCPI0_5(%rip), %xmm1   # xmm1 = [1,1,1,1]
	vpmaxsd	%xmm1, %xmm0, %xmm13
	vmovd	%r11d, %xmm0
	movq	%r14, 672(%rsp)                 # 8-byte Spill
	vpinsrd	$1, %r14d, %xmm0, %xmm0
	vpxor	%xmm1, %xmm1, %xmm1
	vpminsd	%xmm1, %xmm0, %xmm15
	vpsubd	%xmm15, %xmm13, %xmm14
	vpcmpeqd	%xmm0, %xmm0, %xmm0
	vpaddd	%xmm0, %xmm14, %xmm3
	vpbroadcastd	.LCPI0_6(%rip), %xmm1   # xmm1 = [4294967288,4294967288,4294967288,4294967288]
	vpand	%xmm1, %xmm3, %xmm1
	vpaddd	%xmm1, %xmm15, %xmm1
	vpaddd	%xmm0, %xmm13, %xmm2
	vpminsd	%xmm1, %xmm2, %xmm0
	vpbroadcastd	.LCPI0_7(%rip), %xmm1   # xmm1 = [8,8,8,8]
	vpminsd	%xmm1, %xmm14, %xmm1
	vpaddd	%xmm1, %xmm0, %xmm0
	vpminsd	%xmm13, %xmm0, %xmm0
	vmovd	%xmm0, %eax
	vpextrd	$1, %xmm0, %edx
	cmpl	%edx, %eax
	movl	%edx, 1616(%rsp)                # 4-byte Spill
	movl	%eax, 24(%rsp)                  # 4-byte Spill
	cmovgel	%eax, %edx
	cmpl	$9, %r15d
	movl	$8, %edi
	cmovll	%r15d, %edi
	movl	$-8, %eax
	subl	%r15d, %eax
	cmpl	%r15d, %eax
	leal	-1(%r15), %ebx
	cmovll	%ebx, %eax
	movl	%eax, %esi
	sarl	$3, %esi
	sarl	$31, %eax
	andnl	%esi, %eax, %eax
	movl	%ebx, %esi
	sarl	$3, %esi
	cmpl	%eax, %esi
	cmovgl	%eax, %esi
	movq	%rdi, 448(%rsp)                 # 8-byte Spill
	leal	(%rdi,%rsi,8), %esi
	cmpl	%r15d, %esi
	cmovgl	%r15d, %esi
	cmpl	%esi, %edx
	movl	%esi, 224(%rsp)                 # 4-byte Spill
	cmovll	%esi, %edx
	cmpl	%r8d, %edx
	movl	%r8d, 984(%rsp)                 # 4-byte Spill
	cmovll	%r8d, %edx
	cmpl	%r9d, %edx
	movl	%r9d, 1464(%rsp)                # 4-byte Spill
	cmovll	%r9d, %edx
	cmpl	%r10d, %edx
	movl	%r10d, 1564(%rsp)               # 4-byte Spill
	cmovll	%r10d, %edx
	testl	%edx, %edx
	movl	$1, %r9d
	cmovgl	%edx, %r9d
	movq	656(%rsp), %rcx                 # 8-byte Reload
	cmpl	%r11d, %ecx
	cmovlel	%ecx, %r11d
	cmpl	%r11d, %r13d
	movq	%r13, 1984(%rsp)                # 8-byte Spill
	cmovlel	%r13d, %r11d
	movq	1816(%rsp), %rcx                # 8-byte Reload
	cmpl	%r11d, %ecx
	cmovlel	%ecx, %r11d
	leal	8(,%rax,8), %edx
	cmpl	%r15d, %edx
	cmovgl	%r15d, %edx
	leal	-1(%rdx), %esi
	sarl	$3, %esi
	cmpl	%eax, %esi
	cmovlel	%esi, %eax
	leal	8(,%rax,8), %ecx
	cmpl	%r15d, %ecx
	cmovgl	%r15d, %ecx
	cmpl	$9, %edx
	movl	$8, %edi
	cmovgel	%edi, %edx
	addl	%esi, %eax
	leal	(%rdx,%rax,8), %eax
	cmpl	%ecx, %eax
	cmovlel	%eax, %ecx
	leal	-1(%rcx), %eax
	movl	64(%rbp), %edx
	cmpl	%edx, %eax
	cmovll	%edx, %eax
	cmpl	%eax, %ebx
	movl	%ebx, 1824(%rsp)                # 4-byte Spill
	cmovlel	%ebx, %eax
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %eax
	cmpl	%r15d, %eax
	leal	1(%rax), %ebx
	cmovll	%r15d, %ebx
	movq	40(%rbp), %rax
	cmpq	$0, 16(%rax)
	movq	96(%rbp), %rsi
	movq	40(%rsi), %rax
	movl	(%rax), %edx
	movl	%edx, 1020(%rsp)                # 4-byte Spill
	movl	4(%rax), %edx
	movq	%rdx, 1072(%rsp)                # 8-byte Spill
	movl	8(%rax), %r8d
	movl	16(%rax), %edx
	movl	%edx, 1016(%rsp)                # 4-byte Spill
	movl	20(%rax), %edx
	movq	%rdx, 1064(%rsp)                # 8-byte Spill
	movl	24(%rax), %edx
	movl	%edx, 1556(%rsp)                # 4-byte Spill
	movl	32(%rax), %r10d
	movl	36(%rax), %r13d
	movl	40(%rax), %r12d
	movl	20(%rsp), %eax
	movq	24(%rsi), %rdi
	movl	36(%rsi), %esi
	jne	.LBB0_4
# %bb.2:                                # %_halide_buffer_is_bounds_query.exit
	movq	288(%rsp), %rdx                 # 8-byte Reload
	testl	%edx, %edx
	sets	%r14b
	cmpl	$8, %edx
	setg	%dl
	cmpb	%r14b, %dl
	je	.LBB0_4
# %bb.3:                                # %_halide_buffer_is_bounds_query.exit
	movq	40(%rbp), %rdx
	cmpq	$0, (%rdx)
	je	.LBB0_406
.LBB0_4:                                # %"assert succeeded"
	movq	%r15, 352(%rsp)                 # 8-byte Spill
	movl	%esi, 720(%rsp)                 # 4-byte Spill
	movl	%eax, 724(%rsp)                 # 4-byte Spill
	movl	%ebx, 1032(%rsp)                # 4-byte Spill
	movl	%r9d, 1036(%rsp)                # 4-byte Spill
	movl	%r8d, 1480(%rsp)                # 4-byte Spill
	movq	%r13, 1056(%rsp)                # 8-byte Spill
	movq	%r10, 1080(%rsp)                # 8-byte Spill
	movq	%rdi, 680(%rsp)                 # 8-byte Spill
	movq	%rcx, 2216(%rsp)                # 8-byte Spill
	movq	48(%rbp), %r14
	cmpq	$0, 16(%r14)
	jne	.LBB0_7
# %bb.5:                                # %_halide_buffer_is_bounds_query.exit799
	cmpq	$0, (%r14)
	jne	.LBB0_7
# %bb.6:                                # %true_bb
	movq	40(%r14), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%r14)
	movq	$0, 16(%r14)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%r14)
	vmovdqa	.LCPI0_8(%rip), %xmm0           # xmm0 = [0,32,1,0]
	vmovdqu	%xmm0, (%rax)
	movq	$0, 24(%r14)
.LBB0_7:                                # %after_bb
	movq	160(%rbp), %rsi
	cmpq	$0, 16(%rsi)
	movq	120(%rbp), %r9
	movq	112(%rbp), %r10
	movq	104(%rbp), %r8
	movq	32(%rbp), %r15
	movq	648(%rsp), %r14                 # 8-byte Reload
	movq	80(%rbp), %r13
	movq	16(%rbp), %rcx
	movq	40(%rbp), %rdi
	jne	.LBB0_9
# %bb.8:                                # %_halide_buffer_is_bounds_query.exit813
	cmpq	$0, (%rsi)
	je	.LBB0_12
.LBB0_9:                                # %after_bb22
	cmpq	$0, 16(%rdi)
	movq	136(%rbp), %rbx
	jne	.LBB0_13
.LBB0_10:                               # %_halide_buffer_is_bounds_query.exit815
	cmpq	$0, (%rdi)
	jne	.LBB0_13
# %bb.11:                               # %true_bb23
	movq	40(%rdi), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%rdi)
	movq	$0, 16(%rdi)
	movabsq	$8590008322, %rdx               # imm = 0x200012002
	movq	%rdx, 32(%rdi)
	vmovaps	.LCPI0_8(%rip), %xmm0           # xmm0 = [0,32,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%rdi), %rax
	vmovdqa	.LCPI0_9(%rip), %xmm0           # xmm0 = [0,32,32,0]
	vmovdqu	%xmm0, 16(%rax)
	movq	$0, 24(%rdi)
	jmp	.LBB0_13
.LBB0_12:                               # %true_bb20
	movq	40(%rsi), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%rsi)
	movq	$0, 16(%rsi)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%rsi)
	movq	1816(%rsp), %rdx                # 8-byte Reload
	movl	%edx, (%rax)
	movq	1320(%rsp), %rdx                # 8-byte Reload
	movl	%edx, 4(%rax)
	movq	$1, 8(%rax)
	movq	$0, 24(%rsi)
	cmpq	$0, 16(%rdi)
	movq	136(%rbp), %rbx
	je	.LBB0_10
.LBB0_13:                               # %after_bb25
	cmpq	$0, 16(%rcx)
	jne	.LBB0_15
# %bb.14:                               # %_halide_buffer_is_bounds_query.exit817
	cmpq	$0, (%rcx)
	je	.LBB0_18
.LBB0_15:                               # %after_bb28
	cmpq	$0, 16(%r14)
	movq	128(%rbp), %rdi
	jne	.LBB0_19
.LBB0_16:                               # %_halide_buffer_is_bounds_query.exit819
	cmpq	$0, (%r14)
	jne	.LBB0_19
# %bb.17:                               # %true_bb29
	movq	40(%r14), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%r14)
	movq	$0, 16(%r14)
	movabsq	$12884975618, %rdx              # imm = 0x300012002
	movq	%rdx, 32(%r14)
	vmovaps	.LCPI0_10(%rip), %xmm0          # xmm0 = [0,8,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%r14), %rax
	vmovaps	.LCPI0_11(%rip), %xmm0          # xmm0 = [0,40,8,0]
	vmovups	%xmm0, 16(%rax)
	movq	40(%r14), %rax
	vmovdqa	.LCPI0_12(%rip), %xmm0          # xmm0 = [0,7,320,0]
	vmovdqu	%xmm0, 32(%rax)
	movq	$0, 24(%r14)
	jmp	.LBB0_19
.LBB0_18:                               # %true_bb26
	movq	40(%rcx), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%rcx)
	movq	$0, 16(%rcx)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%rcx)
	vmovdqa	.LCPI0_10(%rip), %xmm0          # xmm0 = [0,8,1,0]
	vmovdqu	%xmm0, (%rax)
	movq	$0, 24(%rcx)
	cmpq	$0, 16(%r14)
	movq	128(%rbp), %rdi
	je	.LBB0_16
.LBB0_19:                               # %after_bb31
	movq	%r12, 1424(%rsp)                # 8-byte Spill
	cmpq	$0, 16(%r15)
	jne	.LBB0_21
# %bb.20:                               # %_halide_buffer_is_bounds_query.exit822
	cmpq	$0, (%r15)
	je	.LBB0_24
.LBB0_21:                               # %after_bb34
	movq	24(%rbp), %rsi
	cmpq	$0, 16(%rsi)
	movq	144(%rbp), %r12
	jne	.LBB0_25
.LBB0_22:                               # %_halide_buffer_is_bounds_query.exit824
	cmpq	$0, (%rsi)
	jne	.LBB0_25
# %bb.23:                               # %true_bb35
	movq	40(%rsi), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%rsi)
	movq	$0, 16(%rsi)
	movabsq	$8590008322, %rdx               # imm = 0x200012002
	movq	%rdx, 32(%rsi)
	vmovaps	.LCPI0_13(%rip), %xmm0          # xmm0 = [0,24,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%rsi), %rax
	vmovdqa	.LCPI0_14(%rip), %xmm0          # xmm0 = [0,39,24,0]
	vmovdqu	%xmm0, 16(%rax)
	movq	$0, 24(%rsi)
	jmp	.LBB0_25
.LBB0_24:                               # %true_bb32
	movq	40(%r15), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%r15)
	movq	$0, 16(%r15)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%r15)
	vmovdqa	.LCPI0_13(%rip), %xmm0          # xmm0 = [0,24,1,0]
	vmovdqu	%xmm0, (%rax)
	movq	$0, 24(%r15)
	movq	24(%rbp), %rsi
	cmpq	$0, 16(%rsi)
	movq	144(%rbp), %r12
	je	.LBB0_22
.LBB0_25:                               # %after_bb37
	cmpq	$0, 16(%r12)
	jne	.LBB0_27
# %bb.26:                               # %_halide_buffer_is_bounds_query.exit827
	cmpq	$0, (%r12)
	je	.LBB0_30
.LBB0_27:                               # %after_bb40
	cmpq	$0, 16(%rbx)
	jne	.LBB0_31
.LBB0_28:                               # %_halide_buffer_is_bounds_query.exit829
	cmpq	$0, (%rbx)
	jne	.LBB0_31
# %bb.29:                               # %true_bb41
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, 16(%rbx)
	vmovdqu	%xmm0, (%rbx)
	movq	$73730, 32(%rbx)                # imm = 0x12002
	jmp	.LBB0_31
.LBB0_30:                               # %true_bb38
	movq	40(%r12), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%r12)
	movq	$0, 16(%r12)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%r12)
	movq	656(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, (%rax)
	movq	424(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, 4(%rax)
	movq	$1, 8(%rax)
	movq	$0, 24(%r12)
	cmpq	$0, 16(%rbx)
	je	.LBB0_28
.LBB0_31:                               # %after_bb43
	movq	440(%rsp), %r15                 # 8-byte Reload
	cmpq	$0, 16(%r15)
	jne	.LBB0_34
# %bb.32:                               # %_halide_buffer_is_bounds_query.exit830
	cmpq	$0, (%r15)
	jne	.LBB0_34
# %bb.33:                               # %true_bb44
	movq	40(%rsp), %rdx                  # 8-byte Reload
	testl	%edx, %edx
	movl	$1, %eax
	cmovgl	%edx, %eax
	movq	40(%r15), %rdx
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%r15)
	movq	$0, 16(%r15)
	movabsq	$12884975618, %rsi              # imm = 0x300012002
	movq	%rsi, 32(%r15)
	vmovaps	.LCPI0_15(%rip), %xmm0          # xmm0 = [0,40,1,0]
	vmovups	%xmm0, (%rdx)
	movq	40(%r15), %rdx
	vmovdqa	.LCPI0_16(%rip), %xmm0          # xmm0 = [0,7,40,0]
	vmovdqu	%xmm0, 16(%rdx)
	movq	40(%r15), %rdx
	movl	$0, 32(%rdx)
	movl	%eax, 36(%rdx)
	movq	$280, 40(%rdx)                  # imm = 0x118
	movq	$0, 24(%r15)
.LBB0_34:                               # %after_bb46
	movl	%r11d, %eax
	sarl	$31, %eax
	movl	%eax, 1568(%rsp)                # 4-byte Spill
	cmpq	$0, 16(%rdi)
	jne	.LBB0_37
# %bb.35:                               # %_halide_buffer_is_bounds_query.exit833
	cmpq	$0, (%rdi)
	jne	.LBB0_37
# %bb.36:                               # %true_bb47
	movq	40(%rdi), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%rdi)
	movq	$0, 16(%rdi)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%rdi)
	movq	672(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, (%rax)
	movq	1856(%rsp), %rdx                # 8-byte Reload
	movl	%edx, 4(%rax)
	movq	$1, 8(%rax)
	movq	$0, 24(%rdi)
.LBB0_37:                               # %after_bb49
	andl	%r11d, 1568(%rsp)               # 4-byte Folded Spill
	movq	320(%rsp), %rbx                 # 8-byte Reload
	cmpq	$0, 16(%rbx)
	movq	72(%rbp), %r15
	jne	.LBB0_40
# %bb.38:                               # %_halide_buffer_is_bounds_query.exit835
	cmpq	$0, (%rbx)
	jne	.LBB0_40
# %bb.39:                               # %true_bb50
	movl	1036(%rsp), %eax                # 4-byte Reload
	movl	1568(%rsp), %r14d               # 4-byte Reload
	subl	%r14d, %eax
	movq	40(%rbx), %rcx
	movq	40(%rsp), %rsi                  # 8-byte Reload
	testl	%esi, %esi
	movl	$1, %edx
	cmovgl	%esi, %edx
	imull	$39, %eax, %esi
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%rbx)
	movq	$0, 16(%rbx)
	movabsq	$12884975618, %rdi              # imm = 0x300012002
	movq	%rdi, 32(%rbx)
	movl	%r14d, (%rcx)
	movq	648(%rsp), %r14                 # 8-byte Reload
	movl	%eax, 4(%rcx)
	movq	$1, 8(%rcx)
	movq	40(%rbx), %rcx
	movabsq	$167503724544, %rdi             # imm = 0x2700000000
	movq	%rdi, 16(%rcx)
	movq	80(%rbp), %r13
	movq	128(%rbp), %rdi
	movl	%eax, 24(%rcx)
	movl	$0, 28(%rcx)
	movq	40(%rbx), %rax
	movl	$0, 32(%rax)
	movl	%edx, 36(%rax)
	movl	%esi, 40(%rax)
	movl	$0, 44(%rax)
	movq	$0, 24(%rbx)
.LBB0_40:                               # %after_bb52
	movq	152(%rbp), %r11
	cmpq	$0, 16(%r11)
	movq	88(%rbp), %rsi
	movq	96(%rbp), %rdx
	jne	.LBB0_42
# %bb.41:                               # %_halide_buffer_is_bounds_query.exit838
	cmpq	$0, (%r11)
	je	.LBB0_45
.LBB0_42:                               # %after_bb55
	cmpq	$0, 16(%r15)
	jne	.LBB0_46
.LBB0_43:                               # %_halide_buffer_is_bounds_query.exit840
	cmpq	$0, (%r15)
	jne	.LBB0_46
# %bb.44:                               # %true_bb56
	movq	40(%r15), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%r15)
	movq	$0, 16(%r15)
	movabsq	$4295041026, %rcx               # imm = 0x100012002
	movq	%rcx, 32(%r15)
	movl	$0, (%rax)
	movl	1032(%rsp), %ecx                # 4-byte Reload
	movl	%ecx, 4(%rax)
	movq	$1, 8(%rax)
	movq	$0, 24(%r15)
	jmp	.LBB0_46
.LBB0_45:                               # %true_bb53
	movq	40(%r11), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%r11)
	movq	$0, 16(%r11)
	movabsq	$4295041026, %rcx               # imm = 0x100012002
	movq	%rcx, 32(%r11)
	movq	1984(%rsp), %rcx                # 8-byte Reload
	movl	%ecx, (%rax)
	movq	1312(%rsp), %rcx                # 8-byte Reload
	movl	%ecx, 4(%rax)
	movq	$1, 8(%rax)
	movq	$0, 24(%r11)
	cmpq	$0, 16(%r15)
	je	.LBB0_43
.LBB0_46:                               # %after_bb58
	cmpq	$0, 16(%r9)
	jne	.LBB0_48
# %bb.47:                               # %_halide_buffer_is_bounds_query.exit842
	cmpq	$0, (%r9)
	je	.LBB0_51
.LBB0_48:                               # %after_bb61
	cmpq	$0, 16(%r10)
	jne	.LBB0_52
.LBB0_49:                               # %_halide_buffer_is_bounds_query.exit845
	cmpq	$0, (%r10)
	jne	.LBB0_52
# %bb.50:                               # %true_bb62
	movq	40(%r10), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%r10)
	movq	$0, 16(%r10)
	movabsq	$12884975618, %rcx              # imm = 0x300012002
	movq	%rcx, 32(%r10)
	vmovaps	.LCPI0_8(%rip), %xmm0           # xmm0 = [0,32,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%r10), %rax
	vmovaps	.LCPI0_9(%rip), %xmm0           # xmm0 = [0,32,32,0]
	vmovups	%xmm0, 16(%rax)
	movq	40(%r10), %rax
	vmovdqa	.LCPI0_18(%rip), %xmm0          # xmm0 = [0,4,1024,0]
	vmovdqu	%xmm0, 32(%rax)
	movq	$0, 24(%r10)
	jmp	.LBB0_52
.LBB0_51:                               # %true_bb59
	movq	40(%r9), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%r9)
	movq	$0, 16(%r9)
	movabsq	$8590008322, %rcx               # imm = 0x200012002
	movq	%rcx, 32(%r9)
	vmovaps	.LCPI0_8(%rip), %xmm0           # xmm0 = [0,32,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%r9), %rax
	vmovdqa	.LCPI0_17(%rip), %xmm0          # xmm0 = [0,4,32,0]
	vmovdqu	%xmm0, 16(%rax)
	movq	$0, 24(%r9)
	cmpq	$0, 16(%r10)
	je	.LBB0_49
.LBB0_52:                               # %after_bb64
	cmpq	$0, 16(%rsi)
	jne	.LBB0_54
# %bb.53:                               # %_halide_buffer_is_bounds_query.exit848
	cmpq	$0, (%rsi)
	je	.LBB0_57
.LBB0_54:                               # %after_bb67
	cmpq	$0, 16(%r13)
	jne	.LBB0_58
.LBB0_55:                               # %_halide_buffer_is_bounds_query.exit851
	cmpq	$0, (%r13)
	jne	.LBB0_58
# %bb.56:                               # %true_bb68
	vmovaps	.LCPI0_20(%rip), %ymm0          # ymm0 = [0,8,1,0,0,40,8,0]
	vmovups	%ymm0, 7264(%rsp)
	vmovaps	.LCPI0_21(%rip), %ymm0          # ymm0 = [0,7,320,0,0,4,2240,0]
	vmovups	%ymm0, 7296(%rsp)
	movq	40(%r13), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r13)
	movq	$0, 16(%r13)
	movabsq	$17179942914, %rcx              # imm = 0x400012002
	movq	%rcx, 32(%r13)
	vmovups	7264(%rsp), %xmm0
	vmovups	%xmm0, (%rax)
	movq	40(%r13), %rax
	vmovups	7280(%rsp), %xmm0
	vmovups	%xmm0, 16(%rax)
	movq	40(%r13), %rax
	vmovups	7296(%rsp), %xmm0
	vmovups	%xmm0, 32(%rax)
	movq	40(%r13), %rax
	vmovdqu	7312(%rsp), %xmm0
	vmovdqu	%xmm0, 48(%rax)
	movq	$0, 24(%r13)
	jmp	.LBB0_58
.LBB0_57:                               # %true_bb65
	movq	40(%rsi), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%rsi)
	movq	$0, 16(%rsi)
	movabsq	$8590008322, %rcx               # imm = 0x200012002
	movq	%rcx, 32(%rsi)
	vmovaps	.LCPI0_10(%rip), %xmm0          # xmm0 = [0,8,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%rsi), %rax
	vmovdqa	.LCPI0_19(%rip), %xmm0          # xmm0 = [0,4,8,0]
	vmovdqu	%xmm0, 16(%rax)
	movq	$0, 24(%rsi)
	cmpq	$0, 16(%r13)
	je	.LBB0_55
.LBB0_58:                               # %after_bb70
	cmpq	$0, 16(%r8)
	jne	.LBB0_60
# %bb.59:                               # %_halide_buffer_is_bounds_query.exit854
	cmpq	$0, (%r8)
	je	.LBB0_62
.LBB0_60:                               # %after_bb73
	cmpq	$0, 16(%rdx)
	je	.LBB0_63
.LBB0_61:
	xorl	%r12d, %r12d
	cmpq	$0, 16(%r8)
	je	.LBB0_78
.LBB0_65:
	movl	$0, 160(%rsp)                   # 4-byte Folded Spill
	cmpq	$0, 16(%r13)
	je	.LBB0_79
.LBB0_66:
	movl	$0, 2128(%rsp)                  # 4-byte Folded Spill
	cmpq	$0, 16(%rsi)
	je	.LBB0_80
.LBB0_67:
	movl	$0, 2112(%rsp)                  # 4-byte Folded Spill
	cmpq	$0, 16(%r10)
	je	.LBB0_81
.LBB0_68:
	movl	$0, 2096(%rsp)                  # 4-byte Folded Spill
	cmpq	$0, 16(%r9)
	je	.LBB0_82
.LBB0_69:
	movl	$0, 816(%rsp)                   # 4-byte Folded Spill
	movq	440(%rsp), %rax                 # 8-byte Reload
	cmpq	$0, 16(%r15)
	je	.LBB0_83
.LBB0_70:
	movl	$0, 808(%rsp)                   # 4-byte Folded Spill
	cmpq	$0, 16(%r11)
	je	.LBB0_84
.LBB0_71:
	movl	$0, 800(%rsp)                   # 4-byte Folded Spill
	cmpq	$0, 16(%rbx)
	je	.LBB0_85
.LBB0_72:
	movl	$0, 792(%rsp)                   # 4-byte Folded Spill
	movq	136(%rbp), %rdx
	cmpq	$0, 16(%rdi)
	je	.LBB0_86
.LBB0_73:
	movl	$0, 1048(%rsp)                  # 4-byte Folded Spill
	movq	24(%rbp), %rcx
	cmpq	$0, 16(%rax)
	je	.LBB0_87
.LBB0_74:
	xorl	%r13d, %r13d
	cmpq	$0, 16(%rdx)
	je	.LBB0_88
.LBB0_75:
	xorl	%ebx, %ebx
	jmp	.LBB0_89
.LBB0_62:                               # %true_bb71
	movq	40(%r8), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%r8)
	movq	$0, 16(%r8)
	movabsq	$8590008322, %rcx               # imm = 0x200012002
	movq	%rcx, 32(%r8)
	vmovaps	.LCPI0_13(%rip), %xmm0          # xmm0 = [0,24,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%r8), %rax
	vmovdqa	.LCPI0_22(%rip), %xmm0          # xmm0 = [0,4,24,0]
	vmovdqu	%xmm0, 16(%rax)
	movq	$0, 24(%r8)
	cmpq	$0, 16(%rdx)
	jne	.LBB0_61
.LBB0_63:                               # %_halide_buffer_is_bounds_query.exit857
	cmpq	$0, (%rdx)
	je	.LBB0_76
.LBB0_64:                               # %after_bb76.thread
	cmpq	$0, (%rdx)
	sete	%r12b
	cmpq	$0, 16(%r8)
	jne	.LBB0_65
.LBB0_78:
	cmpq	$0, (%r8)
	sete	%al
	movl	%eax, 160(%rsp)                 # 4-byte Spill
	cmpq	$0, 16(%r13)
	jne	.LBB0_66
.LBB0_79:
	cmpq	$0, (%r13)
	sete	%al
	movl	%eax, 2128(%rsp)                # 4-byte Spill
	cmpq	$0, 16(%rsi)
	jne	.LBB0_67
.LBB0_80:
	cmpq	$0, (%rsi)
	sete	%al
	movl	%eax, 2112(%rsp)                # 4-byte Spill
	cmpq	$0, 16(%r10)
	jne	.LBB0_68
.LBB0_81:
	cmpq	$0, (%r10)
	sete	%al
	movl	%eax, 2096(%rsp)                # 4-byte Spill
	cmpq	$0, 16(%r9)
	jne	.LBB0_69
.LBB0_82:
	cmpq	$0, (%r9)
	sete	%al
	movl	%eax, 816(%rsp)                 # 4-byte Spill
	movq	440(%rsp), %rax                 # 8-byte Reload
	cmpq	$0, 16(%r15)
	jne	.LBB0_70
.LBB0_83:
	cmpq	$0, (%r15)
	sete	%cl
	movl	%ecx, 808(%rsp)                 # 4-byte Spill
	cmpq	$0, 16(%r11)
	jne	.LBB0_71
.LBB0_84:
	cmpq	$0, (%r11)
	sete	%cl
	movl	%ecx, 800(%rsp)                 # 4-byte Spill
	cmpq	$0, 16(%rbx)
	jne	.LBB0_72
.LBB0_85:
	cmpq	$0, (%rbx)
	sete	%cl
	movl	%ecx, 792(%rsp)                 # 4-byte Spill
	movq	136(%rbp), %rdx
	cmpq	$0, 16(%rdi)
	jne	.LBB0_73
.LBB0_86:
	cmpq	$0, (%rdi)
	sete	%cl
	movl	%ecx, 1048(%rsp)                # 4-byte Spill
	movq	24(%rbp), %rcx
	cmpq	$0, 16(%rax)
	jne	.LBB0_74
.LBB0_87:
	cmpq	$0, (%rax)
	sete	%r13b
	cmpq	$0, 16(%rdx)
	jne	.LBB0_75
.LBB0_88:
	cmpq	$0, (%rdx)
	sete	%bl
.LBB0_89:                               # %_halide_buffer_is_bounds_query.exit871
	movq	16(%rbp), %rdx
	movq	40(%rbp), %rsi
	movq	32(%rbp), %rdi
	movq	144(%rbp), %rax
	cmpq	$0, 16(%rax)
	je	.LBB0_97
# %bb.90:
	xorl	%eax, %eax
	cmpq	$0, 16(%rcx)
	je	.LBB0_98
.LBB0_91:
	xorl	%r8d, %r8d
	cmpq	$0, 16(%rdi)
	je	.LBB0_99
.LBB0_92:
	xorl	%r9d, %r9d
	cmpq	$0, 16(%r14)
	je	.LBB0_100
.LBB0_93:
	xorl	%r10d, %r10d
	movq	48(%rbp), %rcx
	cmpq	$0, 16(%rdx)
	je	.LBB0_101
.LBB0_94:
	xorl	%r11d, %r11d
	movq	408(%rsp), %rdi                 # 8-byte Reload
	cmpq	$0, 16(%rsi)
	je	.LBB0_102
.LBB0_95:
	xorl	%r15d, %r15d
	movl	%r12d, %esi
	cmpq	$0, 16(%rcx)
	je	.LBB0_103
.LBB0_96:
	xorl	%r14d, %r14d
	jmp	.LBB0_104
.LBB0_97:
	cmpq	$0, (%rax)
	sete	%al
	cmpq	$0, 16(%rcx)
	jne	.LBB0_91
.LBB0_98:
	cmpq	$0, (%rcx)
	sete	%r8b
	cmpq	$0, 16(%rdi)
	jne	.LBB0_92
.LBB0_99:
	cmpq	$0, (%rdi)
	sete	%r9b
	cmpq	$0, 16(%r14)
	jne	.LBB0_93
.LBB0_100:
	cmpq	$0, (%r14)
	sete	%r10b
	movq	48(%rbp), %rcx
	cmpq	$0, 16(%rdx)
	jne	.LBB0_94
.LBB0_101:
	cmpq	$0, (%rdx)
	sete	%r11b
	movq	408(%rsp), %rdi                 # 8-byte Reload
	cmpq	$0, 16(%rsi)
	jne	.LBB0_95
.LBB0_102:
	cmpq	$0, (%rsi)
	sete	%r15b
	movl	%r12d, %esi
	cmpq	$0, 16(%rcx)
	jne	.LBB0_96
.LBB0_103:
	cmpq	$0, (%rcx)
	sete	%r14b
.LBB0_104:                              # %_halide_buffer_is_bounds_query.exit878
	movl	%r13d, %ecx
	xorl	%r12d, %r12d
	movq	160(%rbp), %rdx
	cmpq	$0, 16(%rdx)
	movl	$0, %r13d
	jne	.LBB0_106
# %bb.105:
	cmpq	$0, (%rdx)
	sete	%r13b
.LBB0_106:                              # %_halide_buffer_is_bounds_query.exit879
	orb	%r13b, %r14b
	orb	%r14b, %r15b
	orb	%r15b, %r11b
	orb	%r11b, %r10b
	orb	%r10b, %r9b
	orb	%r9b, %r8b
	orb	%r8b, %al
	orb	%al, %bl
	orb	%bl, %cl
	movl	1048(%rsp), %eax                # 4-byte Reload
	orb	%cl, %al
	movl	792(%rsp), %ecx                 # 4-byte Reload
	orb	%al, %cl
	movl	800(%rsp), %eax                 # 4-byte Reload
	orb	%cl, %al
	movl	808(%rsp), %ecx                 # 4-byte Reload
	orb	%al, %cl
	movl	816(%rsp), %eax                 # 4-byte Reload
	orb	%cl, %al
	movl	2096(%rsp), %ecx                # 4-byte Reload
	orb	%al, %cl
	movl	2112(%rsp), %eax                # 4-byte Reload
	orb	%cl, %al
	movl	2128(%rsp), %ecx                # 4-byte Reload
	orb	%al, %cl
	movl	160(%rsp), %eax                 # 4-byte Reload
	orb	%cl, %al
	orb	%al, %sil
	testb	$1, %sil
	je	.LBB0_109
# %bb.107:
	xorl	%r15d, %r15d
.LBB0_108:                              # %call_destructor.exit800.thread1030
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 896(%rsp)                 # 8-byte Spill
	jmp	.LBB0_286
.LBB0_109:                              # %true_bb77
	vmovdqa	%xmm3, 2096(%rsp)               # 16-byte Spill
	vmovdqa	%xmm2, 2112(%rsp)               # 16-byte Spill
	vmovdqa	%xmm1, 2128(%rsp)               # 16-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1800(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	setne	%al
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 788(%rsp)                   # 4-byte Folded Reload
	sete	%al
	movl	%eax, 816(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1792(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 808(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 1784(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 800(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1776(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 1000(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$2, 1040(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1916(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1768(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 1912(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 1760(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1008(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1752(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 1048(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$3, 1744(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1012(%rsp)                # 4-byte Spill
	xorl	%r11d, %r11d
	cmpl	$73730, 1736(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%r11b
	xorl	%r10d, %r10d
	cmpl	$1, 1728(%rsp)                  # 4-byte Folded Reload
	sete	%r10b
	xorl	%eax, %eax
	cmpl	$73730, 1720(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 1900(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$2, 1712(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1904(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1704(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 1908(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 1696(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1932(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1688(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 1928(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$0, 1680(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1924(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1672(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 1920(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$3, 1664(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1936(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 1656(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 792(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 1648(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1004(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 724(%rsp)               # 4-byte Folded Reload
                                        # imm = 0x12002
	setne	%al
	shlq	$38, %rax
	movq	%rax, 2176(%rsp)                # 8-byte Spill
	xorl	%esi, %esi
	cmpl	$3, 720(%rsp)                   # 4-byte Folded Reload
	setne	%sil
	shlq	$39, %rsi
	testl	%edi, %edi
	setg	%al
	movq	832(%rsp), %rdx                 # 8-byte Reload
	leal	(%rdx,%rdi), %ecx
	movl	%ecx, 1896(%rsp)                # 4-byte Spill
	cmpl	$32, %ecx
	setl	%cl
	orb	%al, %cl
	movb	%cl, 287(%rsp)                  # 1-byte Spill
	movl	%edx, %eax
	andl	$-2147483648, %eax              # imm = 0x80000000
	shlq	$10, %rax
	movq	%rax, 2168(%rsp)                # 8-byte Spill
	movq	1320(%rsp), %rax                # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-2147483648, %eax              # imm = 0x80000000
	shlq	$11, %rax
	movq	%rax, 2160(%rsp)                # 8-byte Spill
	movq	1272(%rsp), %rdx                # 8-byte Reload
	testl	%edx, %edx
	setg	%al
	movq	1232(%rsp), %rdi                # 8-byte Reload
	leal	(%rdi,%rdx), %ecx
	movl	%ecx, 1892(%rsp)                # 4-byte Spill
	cmpl	$32, %ecx
	setl	%r15b
	orb	%al, %r15b
                                        # kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              # imm = 0x80000000
	shlq	$13, %rdi
	movq	%rdi, 2152(%rsp)                # 8-byte Spill
	movq	1264(%rsp), %rdx                # 8-byte Reload
	cmpl	572(%rsp), %edx                 # 4-byte Folded Reload
	setg	%al
	movq	1384(%rsp), %rcx                # 8-byte Reload
	leal	24(%rcx), %ecx
	movq	1216(%rsp), %rdi                # 8-byte Reload
	addl	%edi, %edx
	movl	%edx, 1888(%rsp)                # 4-byte Spill
	cmpl	%edx, %ecx
	vmovd	%r11d, %xmm0
	vpinsrb	$4, %r10d, %xmm0, %xmm0
	setg	%r9b
	orb	%al, %r9b
                                        # kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              # imm = 0x80000000
	shlq	$15, %rdi
	movq	%rdi, 2144(%rsp)                # 8-byte Spill
	movq	1200(%rsp), %rax                # 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	1184(%rsp), %r13                # 8-byte Reload
	addl	%r13d, %eax
	movl	%eax, 1884(%rsp)                # 4-byte Spill
	cmpl	$8, %eax
	setl	%bl
	orb	%cl, %bl
                                        # kill: def $r13d killed $r13d killed $r13 def $r13
	andl	$-2147483648, %r13d             # imm = 0x80000000
	shlq	$17, %r13
	movq	1168(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	288(%rsp), %r14                 # 8-byte Reload
	addl	%r14d, %ecx
	movl	%ecx, 1880(%rsp)                # 4-byte Spill
	cmpl	$8, %ecx
	setl	%dl
	orb	%al, %dl
                                        # kill: def $r14d killed $r14d killed $r14 def $r14
	andl	$-2147483648, %r14d             # imm = 0x80000000
	shlq	$19, %r14
	movq	216(%rsp), %rax                 # 8-byte Reload
	testl	%eax, %eax
	setg	%r8b
	movq	488(%rsp), %r10                 # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, 1876(%rsp)                # 4-byte Spill
	cmpl	$40, %eax
	setl	%cl
	orb	%r8b, %cl
	movl	%r10d, %r11d
	andl	$-2147483648, %r11d             # imm = 0x80000000
	shlq	$21, %r11
	movq	144(%rsp), %rax                 # 8-byte Reload
	testl	%eax, %eax
	setg	%r8b
	movq	480(%rsp), %rdi                 # 8-byte Reload
	addl	%edi, %eax
	movl	%eax, 1872(%rsp)                # 4-byte Spill
	cmpl	$7, %eax
	setl	%al
	orb	%r8b, %al
	movl	%edi, %r10d
	andl	$-2147483648, %r10d             # imm = 0x80000000
	shlq	$23, %r10
	movq	664(%rsp), %r12                 # 8-byte Reload
	testl	%r12d, %r12d
	setg	%dil
	movq	872(%rsp), %r8                  # 8-byte Reload
	addl	%r8d, %r12d
	movl	%r12d, 1868(%rsp)               # 4-byte Spill
	cmpl	$24, %r12d
	setl	%r12b
	orb	%dil, %r12b
	movzbl	%r12b, %edi
	vpinsrb	$8, %edi, %xmm0, %xmm0
	vpinsrb	$12, 1900(%rsp), %xmm0, %xmm9   # 4-byte Folded Reload
	movzbl	%dl, %edx
	vmovd	%edx, %xmm0
	movzbl	%r9b, %edx
	vpinsrb	$4, %edx, %xmm0, %xmm0
	movzbl	%cl, %ecx
	vpinsrb	$8, %ecx, %xmm0, %xmm0
	movzbl	%al, %eax
	vpinsrb	$12, %eax, %xmm0, %xmm8
	vmovd	1000(%rsp), %xmm2               # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrb	$4, 1916(%rsp), %xmm2, %xmm2    # 4-byte Folded Reload
	movzbl	%r15b, %eax
	vpinsrb	$8, %eax, %xmm2, %xmm2
	vpinsrb	$12, 1912(%rsp), %xmm2, %xmm10  # 4-byte Folded Reload
	vmovd	1904(%rsp), %xmm3               # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	movl	%r8d, %edi
	andl	$-2147483648, %edi              # imm = 0x80000000
	shlq	$25, %rdi
	movq	864(%rsp), %rcx                 # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	848(%rsp), %r12                 # 8-byte Reload
	addl	%r12d, %ecx
	movl	%ecx, 1000(%rsp)                # 4-byte Spill
	cmpl	$24, %ecx
	setl	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
	vpinsrb	$4, %eax, %xmm3, %xmm3
                                        # kill: def $r12d killed $r12d killed $r12 def $r12
	andl	$-2147483648, %r12d             # imm = 0x80000000
	shlq	$27, %r12
	movq	856(%rsp), %rcx                 # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	840(%rsp), %rdx                 # 8-byte Reload
	leal	(%rdx,%rcx), %r8d
	cmpl	$39, %r8d
	setl	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
	vpinsrb	$8, %eax, %xmm3, %xmm3
	vpinsrb	$12, 1908(%rsp), %xmm3, %xmm12  # 4-byte Folded Reload
	vmovd	816(%rsp), %xmm3                # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	movzbl	287(%rsp), %eax                 # 1-byte Folded Reload
	vpinsrb	$4, %eax, %xmm3, %xmm3
	vpinsrb	$8, 808(%rsp), %xmm3, %xmm3     # 4-byte Folded Reload
	vpinsrb	$12, 800(%rsp), %xmm3, %xmm11   # 4-byte Folded Reload
	vmovd	1008(%rsp), %xmm5               # 4-byte Folded Reload
                                        # xmm5 = mem[0],zero,zero,zero
	movzbl	%bl, %eax
	vpinsrb	$4, %eax, %xmm5, %xmm5
	vpinsrb	$8, 1048(%rsp), %xmm5, %xmm5    # 4-byte Folded Reload
	vpinsrb	$12, 1012(%rsp), %xmm5, %xmm5   # 4-byte Folded Reload
	vmovd	1932(%rsp), %xmm6               # 4-byte Folded Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vpinsrb	$4, 1928(%rsp), %xmm6, %xmm6    # 4-byte Folded Reload
	vpinsrb	$8, 1924(%rsp), %xmm6, %xmm6    # 4-byte Folded Reload
	vpinsrb	$12, 1920(%rsp), %xmm6, %xmm6   # 4-byte Folded Reload
	vmovd	1936(%rsp), %xmm7               # 4-byte Folded Reload
                                        # xmm7 = mem[0],zero,zero,zero
	movl	%edx, %ebx
	andl	$-2147483648, %ebx              # imm = 0x80000000
	shlq	$29, %rbx
	movq	424(%rsp), %r15                 # 8-byte Reload
                                        # kill: def $r15d killed $r15d killed $r15 def $r15
	andl	$-2147483648, %r15d             # imm = 0x80000000
	shlq	$30, %r15
	movq	1328(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	152(%rsp), %r9                  # 8-byte Reload
	addl	%r9d, %ecx
	cmpl	$40, %ecx
	setl	%dl
	orb	%al, %dl
	movzbl	%dl, %eax
	vpinsrb	$4, %eax, %xmm7, %xmm7
	vpinsrb	$8, 792(%rsp), %xmm7, %xmm7     # 4-byte Folded Reload
	vpinsrb	$12, 1004(%rsp), %xmm7, %xmm7   # 4-byte Folded Reload
	vmovd	740(%rsp), %xmm0                # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 736(%rsp), %xmm0, %xmm0     # 4-byte Folded Reload
	vpinsrd	$2, 732(%rsp), %xmm0, %xmm0     # 4-byte Folded Reload
	vpinsrd	$3, 728(%rsp), %xmm0, %xmm0     # 4-byte Folded Reload
	vmovd	756(%rsp), %xmm1                # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 752(%rsp), %xmm1, %xmm1     # 4-byte Folded Reload
	vpinsrd	$2, 748(%rsp), %xmm1, %xmm1     # 4-byte Folded Reload
	vpinsrd	$3, 744(%rsp), %xmm1, %xmm1     # 4-byte Folded Reload
	vmovd	772(%rsp), %xmm2                # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 768(%rsp), %xmm2, %xmm2     # 4-byte Folded Reload
	vpinsrd	$2, 764(%rsp), %xmm2, %xmm2     # 4-byte Folded Reload
	vpinsrd	$3, 760(%rsp), %xmm2, %xmm2     # 4-byte Folded Reload
	vmovd	1640(%rsp), %xmm3               # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vpinsrd	$1, 784(%rsp), %xmm3, %xmm3     # 4-byte Folded Reload
	vpinsrd	$2, 780(%rsp), %xmm3, %xmm3     # 4-byte Folded Reload
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vpinsrd	$3, 776(%rsp), %xmm3, %xmm1     # 4-byte Folded Reload
	vinserti128	$1, %xmm2, %ymm1, %ymm1
	vpcmpeqd	.LCPI0_24(%rip), %ymm1, %ymm1
	vpcmpeqd	.LCPI0_25(%rip), %ymm0, %ymm0
	vpmovsxdq	%xmm1, %ymm2
	vpslld	$31, %xmm9, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vmovapd	.LCPI0_31(%rip), %ymm9          # ymm9 = [1024,2048,0,4096]
	vblendvpd	%ymm3, .LCPI0_30(%rip), %ymm9, %ymm3
	vpmovsxdq	%xmm0, %ymm9
	vpandn	.LCPI0_26(%rip), %ymm9, %ymm9
	vpandn	.LCPI0_27(%rip), %ymm2, %ymm2
	vpor	%ymm2, %ymm9, %ymm2
	vpslld	$31, %xmm10, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vmovapd	.LCPI0_34(%rip), %ymm9          # ymm9 = [16,32,0,64]
	vblendvpd	%ymm4, .LCPI0_33(%rip), %ymm9, %ymm4
	vextracti128	$1, %ymm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vextracti128	$1, %ymm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vpandn	.LCPI0_28(%rip), %ymm0, %ymm0
	vpandn	.LCPI0_29(%rip), %ymm1, %ymm1
	vpor	%ymm0, %ymm1, %ymm0
	vpor	%ymm0, %ymm2, %ymm0
	vpslld	$31, %xmm12, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vmovapd	.LCPI0_36(%rip), %ymm2          # ymm2 = [8192,0,0,16384]
	vblendvpd	%ymm1, .LCPI0_35(%rip), %ymm2, %ymm1
	vextracti128	$1, %ymm0, %xmm2
	vpor	%xmm2, %xmm0, %xmm0
	vpshufd	$78, %xmm0, %xmm2               # xmm2 = xmm0[2,3,0,1]
	vpor	%xmm2, %xmm0, %xmm0
	vmovq	%xmm0, %rax
	orq	2176(%rsp), %rax                # 8-byte Folded Reload
	vpslld	$31, %xmm11, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vmovapd	.LCPI0_38(%rip), %ymm2          # ymm2 = [2,0,4,8]
	vblendvpd	%ymm0, .LCPI0_37(%rip), %ymm2, %ymm0
	vorpd	%ymm1, %ymm4, %ymm1
	vpslld	$31, %xmm5, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vmovapd	.LCPI0_40(%rip), %ymm4          # ymm4 = [128,0,256,512]
	vblendvpd	%ymm2, .LCPI0_39(%rip), %ymm4, %ymm2
	vpslld	$31, %xmm8, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vpand	.LCPI0_32(%rip), %ymm4, %ymm4
	vpslld	$31, %xmm6, %xmm5
	vpsrad	$31, %xmm5, %xmm5
	vpmovsxdq	%xmm5, %ymm5
	vpandn	.LCPI0_41(%rip), %ymm5, %ymm5
	vorpd	%ymm3, %ymm0, %ymm0
	vorpd	%ymm5, %ymm2, %ymm2
	vpslld	$31, %xmm7, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vmovapd	.LCPI0_43(%rip), %ymm5          # ymm5 = [524288,0,1048576,2097152]
	vblendvpd	%ymm3, .LCPI0_42(%rip), %ymm5, %ymm3
	vorpd	%ymm2, %ymm0, %ymm0
	vpor	%ymm3, %ymm4, %ymm2
	vorpd	%ymm2, %ymm1, %ymm1
	vorpd	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm1
	vorpd	%xmm1, %xmm0, %xmm0
	vpermilps	$78, %xmm0, %xmm1       # xmm1 = xmm0[2,3,0,1]
	vorpd	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rdx
	orq	%rdx, %rax
	orq	160(%rsp), %rsi                 # 8-byte Folded Reload
	orq	2168(%rsp), %rsi                # 8-byte Folded Reload
	orq	2160(%rsp), %rsi                # 8-byte Folded Reload
	orq	2152(%rsp), %rsi                # 8-byte Folded Reload
	orq	2144(%rsp), %rsi                # 8-byte Folded Reload
	orq	%r13, %rsi
	orq	%r14, %rsi
	orq	%r11, %rsi
	orq	%r10, %rsi
	orq	%rdi, %rsi
	orq	%r12, %rsi
	orq	%rbx, %rsi
	orq	%r15, %rsi
	movabsq	$-9223372036854775808, %rdx     # imm = 0x8000000000000000
	orq	%rdx, %rsi
	orq	%rax, %rsi
	xorl	%eax, %eax
	tzcntq	%rsi, %rax
	cmpl	$62, %eax
	jbe	.LBB0_334
# %bb.110:                              # %no_errors_bb80
	shrq	$31, %r9
	movq	%r9, 816(%rsp)                  # 8-byte Spill
	movq	504(%rsp), %rcx                 # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	968(%rsp), %rdx                 # 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 768(%rsp)                 # 4-byte Spill
	cmpl	$7, %ecx
	setl	%cl
	orb	%al, %cl
	movb	%cl, 788(%rsp)                  # 1-byte Spill
	shrq	$29, %rdx
	andl	$-4, %edx
	movq	%rdx, 808(%rsp)                 # 8-byte Spill
	movq	544(%rsp), %rdx                 # 8-byte Reload
	testl	%edx, %edx
	setg	%al
	movq	40(%rsp), %rcx                  # 8-byte Reload
	testl	%ecx, %ecx
	movl	$1, %edi
	cmovgl	%ecx, %edi
	movq	976(%rsp), %rcx                 # 8-byte Reload
	addl	%ecx, %edx
	movl	%edx, 764(%rsp)                 # 4-byte Spill
	cmpl	%edx, %edi
	setg	%r9b
	orb	%al, %r9b
	shrq	$27, %rcx
	andl	$-16, %ecx
	movq	%rcx, 800(%rsp)                 # 8-byte Spill
	movq	632(%rsp), %rax                 # 8-byte Reload
	shrq	$25, %rax
	andl	$-64, %eax
	movq	%rax, 792(%rsp)                 # 8-byte Spill
	movq	272(%rsp), %rcx                 # 8-byte Reload
	cmpl	1568(%rsp), %ecx                # 4-byte Folded Reload
	setg	%al
	movq	88(%rsp), %rdx                  # 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 760(%rsp)                 # 4-byte Spill
	cmpl	%ecx, 1036(%rsp)                # 4-byte Folded Reload
	setg	%sil
	orb	%al, %sil
	shrq	$23, %rdx
	andl	$256, %edx                      # imm = 0x100
	movq	%rdx, 1048(%rsp)                # 8-byte Spill
	movq	48(%rsp), %rcx                  # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	952(%rsp), %rdx                 # 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 756(%rsp)                 # 4-byte Spill
	cmpl	$39, %ecx
	setl	%bl
	orb	%al, %bl
	shrq	$21, %rdx
	andl	$1024, %edx                     # imm = 0x400
	movq	%rdx, 1800(%rsp)                # 8-byte Spill
	movq	208(%rsp), %rcx                 # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	960(%rsp), %rdx                 # 8-byte Reload
	addl	%edx, %ecx
	movq	%rdi, 160(%rsp)                 # 8-byte Spill
	movl	%ecx, 752(%rsp)                 # 4-byte Spill
	cmpl	%ecx, %edi
	setg	%r14b
	orb	%al, %r14b
	shrq	$19, %rdx
	andl	$4096, %edx                     # imm = 0x1000
	movq	%rdx, 1792(%rsp)                # 8-byte Spill
	movq	1312(%rsp), %rax                # 8-byte Reload
	shrq	$18, %rax
	andl	$8192, %eax                     # imm = 0x2000
	movq	%rax, 1784(%rsp)                # 8-byte Spill
	movq	640(%rsp), %rax                 # 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	72(%rsp), %rdx                  # 8-byte Reload
	addl	%edx, %eax
	movl	%eax, 748(%rsp)                 # 4-byte Spill
	cmpl	%eax, 1032(%rsp)                # 4-byte Folded Reload
	setg	%al
	orb	%cl, %al
	shrq	$16, %rdx
	andl	$32768, %edx                    # imm = 0x8000
	movq	%rdx, 1776(%rsp)                # 8-byte Spill
	movq	824(%rsp), %rdx                 # 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	1296(%rsp), %rdi                # 8-byte Reload
	addl	%edi, %edx
	movl	%edx, 744(%rsp)                 # 4-byte Spill
	cmpl	$32, %edx
	setl	%r12b
	orb	%cl, %r12b
	movzbl	%r12b, %ecx
	vmovd	%ecx, %xmm0
	shrq	$14, %rdi
	andl	$131072, %edi                   # imm = 0x20000
	movq	%rdi, 1768(%rsp)                # 8-byte Spill
	movq	528(%rsp), %rdx                 # 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	1288(%rsp), %rdi                # 8-byte Reload
	addl	%edi, %edx
	movl	%edx, 740(%rsp)                 # 4-byte Spill
	cmpl	$4, %edx
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	vpinsrb	$4, %ecx, %xmm0, %xmm0
	movzbl	%sil, %ecx
	vpinsrb	$8, %ecx, %xmm0, %xmm0
	shrq	$12, %rdi
	andl	$524288, %edi                   # imm = 0x80000
	movq	%rdi, 1760(%rsp)                # 8-byte Spill
	movq	1280(%rsp), %rdx                # 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	1248(%rsp), %rsi                # 8-byte Reload
	addl	%esi, %edx
	movl	%edx, 736(%rsp)                 # 4-byte Spill
	cmpl	$32, %edx
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	vpinsrb	$12, %ecx, %xmm0, %xmm8
	movzbl	%r9b, %ecx
	vmovd	%ecx, %xmm0
	movzbl	%bl, %ecx
	vpinsrb	$4, %ecx, %xmm0, %xmm0
	movzbl	%r14b, %ecx
	vpinsrb	$8, %ecx, %xmm0, %xmm0
	movzbl	%al, %eax
	vpinsrb	$12, %eax, %xmm0, %xmm9
	shrq	$10, %rsi
	andl	$2097152, %esi                  # imm = 0x200000
	movq	%rsi, 1752(%rsp)                # 8-byte Spill
	movq	1256(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	1224(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 732(%rsp)                 # 4-byte Spill
	cmpl	$32, %ecx
	setl	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
	vmovd	%eax, %xmm0
	shrq	$8, %rdx
	andl	$8388608, %edx                  # imm = 0x800000
	movq	%rdx, 1744(%rsp)                # 8-byte Spill
	movq	1240(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	1208(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 728(%rsp)                 # 4-byte Spill
	cmpl	$4, %ecx
	setl	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
	vpinsrb	$4, %eax, %xmm0, %xmm0
	shrq	$6, %rdx
	andl	$33554432, %edx                 # imm = 0x2000000
	movq	%rdx, 1736(%rsp)                # 8-byte Spill
	movq	1192(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	1176(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 572(%rsp)                 # 4-byte Spill
	cmpl	$8, %ecx
	setl	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
	vpinsrb	$8, %eax, %xmm0, %xmm0
	shrq	$4, %rdx
	andl	$134217728, %edx                # imm = 0x8000000
	movq	%rdx, 1728(%rsp)                # 8-byte Spill
	vmovdqa	2080(%rsp), %xmm1               # 16-byte Reload
	vmovd	%xmm1, %ecx
	testl	%ecx, %ecx
	setg	%al
	vpextrd	$1, %xmm1, %edx
	movq	%rcx, 2080(%rsp)                # 8-byte Spill
	addl	%edx, %ecx
	movl	%ecx, 1384(%rsp)                # 4-byte Spill
	cmpl	$4, %ecx
	setl	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
	vpinsrb	$12, %eax, %xmm0, %xmm10
	movq	%rdx, 1040(%rsp)                # 8-byte Spill
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	shrl	$2, %edx
	andl	$536870912, %edx                # imm = 0x20000000
	movq	%rdx, 1720(%rsp)                # 8-byte Spill
	movq	1160(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	1144(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 724(%rsp)                 # 4-byte Spill
	cmpl	$8, %ecx
	setl	%cl
	orb	%al, %cl
	movb	%cl, 780(%rsp)                  # 1-byte Spill
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              # imm = 0x80000000
	movq	%rdx, 1712(%rsp)                # 8-byte Spill
	movq	1152(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	1128(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 720(%rsp)                 # 4-byte Spill
	cmpl	$40, %ecx
	setl	%r13b
	orb	%al, %r13b
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              # imm = 0x80000000
	shlq	$2, %rdx
	movq	%rdx, 1704(%rsp)                # 8-byte Spill
	movq	1136(%rsp), %rdx                # 8-byte Reload
	testl	%edx, %edx
	setg	%al
	movq	1112(%rsp), %rcx                # 8-byte Reload
	addl	%ecx, %edx
	movl	%edx, 1012(%rsp)                # 4-byte Spill
	cmpl	$7, %edx
	setl	%r15b
	orb	%al, %r15b
                                        # kill: def $ecx killed $ecx killed $rcx def $rcx
	andl	$-2147483648, %ecx              # imm = 0x80000000
	shlq	$4, %rcx
	movq	%rcx, 1696(%rsp)                # 8-byte Spill
	movq	1120(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	1104(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 1008(%rsp)                # 4-byte Spill
	cmpl	$4, %ecx
	setl	%r14b
	orb	%al, %r14b
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              # imm = 0x80000000
	shlq	$6, %rdx
	movq	%rdx, 1688(%rsp)                # 8-byte Spill
	movq	1096(%rsp), %rax                # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-2147483648, %eax              # imm = 0x80000000
	shlq	$8, %rax
	movq	%rax, 1680(%rsp)                # 8-byte Spill
	movq	1088(%rsp), %rax                # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-2147483648, %eax              # imm = 0x80000000
	shlq	$10, %rax
	movq	%rax, 1672(%rsp)                # 8-byte Spill
	movq	1072(%rsp), %rax                # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-2147483648, %eax              # imm = 0x80000000
	shlq	$12, %rax
	movq	%rax, 1664(%rsp)                # 8-byte Spill
	movq	1064(%rsp), %rax                # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-2147483648, %eax              # imm = 0x80000000
	shlq	$14, %rax
	movq	%rax, 1656(%rsp)                # 8-byte Spill
	movq	1080(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	1056(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	movl	%ecx, 1004(%rsp)                # 4-byte Spill
	cmpl	$4, %ecx
	setl	%cl
	orb	%al, %cl
	movzbl	%cl, %eax
	shlq	$46, %rax
	movq	%rax, 1640(%rsp)                # 8-byte Spill
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              # imm = 0x80000000
	shlq	$16, %rdx
	movq	%rdx, 1648(%rsp)                # 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 1520(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 784(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$0, 408(%rsp)                   # 4-byte Folded Reload
	sete	%al
	movl	%eax, 772(%rsp)                 # 4-byte Spill
	xorl	%r12d, %r12d
	cmpl	$32, 832(%rsp)                  # 4-byte Folded Reload
	sete	%r12b
	xorl	%eax, %eax
	cmpl	$1, 1512(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 776(%rsp)                 # 4-byte Spill
	xorl	%ebx, %ebx
	cmpl	$1, 1504(%rsp)                  # 4-byte Folded Reload
	sete	%bl
	xorl	%r10d, %r10d
	cmpl	$0, 1272(%rsp)                  # 4-byte Folded Reload
	sete	%r10b
	xorl	%r9d, %r9d
	cmpl	$32, 1232(%rsp)                 # 4-byte Folded Reload
	sete	%r9b
	xorl	%r11d, %r11d
	cmpl	$0, 1264(%rsp)                  # 4-byte Folded Reload
	sete	%r11b
	xorl	%edi, %edi
	cmpl	$32, 1216(%rsp)                 # 4-byte Folded Reload
	sete	%dil
	xorl	%r8d, %r8d
	cmpl	$1, 1500(%rsp)                  # 4-byte Folded Reload
	sete	%r8b
	xorl	%edx, %edx
	cmpl	$0, 1200(%rsp)                  # 4-byte Folded Reload
	sete	%dl
	xorl	%esi, %esi
	cmpl	$8, 1184(%rsp)                  # 4-byte Folded Reload
	sete	%sil
	xorl	%ecx, %ecx
	cmpl	$1, 1492(%rsp)                  # 4-byte Folded Reload
	sete	%cl
	xorl	%eax, %eax
	cmpl	$0, 1168(%rsp)                  # 4-byte Folded Reload
	sete	%al
	vmovd	%eax, %xmm0
	xorl	%eax, %eax
	cmpl	$8, 288(%rsp)                   # 4-byte Folded Reload
	sete	%al
	vpinsrb	$4, %eax, %xmm0, %xmm0
	vpinsrb	$8, %ecx, %xmm0, %xmm0
	movzbl	788(%rsp), %eax                 # 1-byte Folded Reload
	vpinsrb	$12, %eax, %xmm0, %xmm4
	movzbl	780(%rsp), %eax                 # 1-byte Folded Reload
	vmovd	%eax, %xmm0
	movzbl	%r13b, %eax
	movq	1088(%rsp), %r13                # 8-byte Reload
	vpinsrb	$4, %eax, %xmm0, %xmm0
	movzbl	%r15b, %eax
	movq	1096(%rsp), %r15                # 8-byte Reload
	vpinsrb	$8, %eax, %xmm0, %xmm0
	movzbl	%r14b, %eax
	vpinsrb	$12, %eax, %xmm0, %xmm3
	vmovd	%edi, %xmm0
	movq	1072(%rsp), %rdi                # 8-byte Reload
	vpinsrb	$4, %edx, %xmm0, %xmm0
	vpinsrb	$8, %esi, %xmm0, %xmm0
	vpinsrb	$12, %r8d, %xmm0, %xmm5
	vmovd	%r10d, %xmm0
	vpinsrb	$4, %r9d, %xmm0, %xmm0
	vpinsrb	$8, %ebx, %xmm0, %xmm0
	movq	1064(%rsp), %rbx                # 8-byte Reload
	vpinsrb	$12, %r11d, %xmm0, %xmm7
	vmovd	772(%rsp), %xmm0                # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrb	$4, %r12d, %xmm0, %xmm0
	vpinsrb	$8, 784(%rsp), %xmm0, %xmm0     # 4-byte Folded Reload
	vpinsrb	$12, 776(%rsp), %xmm0, %xmm0    # 4-byte Folded Reload
	movl	1028(%rsp), %edx                # 4-byte Reload
	vmovd	%edx, %xmm6
	movl	1024(%rsp), %esi                # 4-byte Reload
	vpinsrd	$1, %esi, %xmm6, %xmm6
	movl	1020(%rsp), %r11d               # 4-byte Reload
	vpinsrd	$2, %r11d, %xmm6, %xmm6
	movl	1016(%rsp), %r14d               # 4-byte Reload
	vpinsrd	$3, %r14d, %xmm6, %xmm6
	vpxor	%xmm1, %xmm1, %xmm1
	vmovd	%r15d, %xmm2
	vpinsrd	$1, %r13d, %xmm2, %xmm2
	vpinsrd	$2, %edi, %xmm2, %xmm2
	vpinsrd	$3, %ebx, %xmm2, %xmm2
	vpcmpgtd	%xmm1, %xmm6, %xmm1
	vpaddd	%xmm6, %xmm2, %xmm6
	vmovdqa	.LCPI0_44(%rip), %xmm2          # xmm2 = [24,4,24,39]
	vpcmpgtd	%xmm6, %xmm2, %xmm2
	vpor	%xmm2, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vpand	.LCPI0_45(%rip), %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpor	%xmm2, %xmm1, %xmm1
	vpshufd	$78, %xmm1, %xmm2               # xmm2 = xmm1[2,3,0,1]
	vpor	%xmm2, %xmm1, %xmm1
	vmovq	%xmm1, %rax
	orq	1640(%rsp), %rax                # 8-byte Folded Reload
	orq	816(%rsp), %rax                 # 8-byte Folded Reload
	orq	808(%rsp), %rax                 # 8-byte Folded Reload
	orq	800(%rsp), %rax                 # 8-byte Folded Reload
	orq	792(%rsp), %rax                 # 8-byte Folded Reload
	orq	1048(%rsp), %rax                # 8-byte Folded Reload
	orq	1800(%rsp), %rax                # 8-byte Folded Reload
	orq	1792(%rsp), %rax                # 8-byte Folded Reload
	orq	1784(%rsp), %rax                # 8-byte Folded Reload
	orq	1776(%rsp), %rax                # 8-byte Folded Reload
	orq	1768(%rsp), %rax                # 8-byte Folded Reload
	orq	1760(%rsp), %rax                # 8-byte Folded Reload
	orq	1752(%rsp), %rax                # 8-byte Folded Reload
	orq	1744(%rsp), %rax                # 8-byte Folded Reload
	orq	1736(%rsp), %rax                # 8-byte Folded Reload
	orq	1728(%rsp), %rax                # 8-byte Folded Reload
	orq	1720(%rsp), %rax                # 8-byte Folded Reload
	orq	1712(%rsp), %rax                # 8-byte Folded Reload
	orq	1704(%rsp), %rax                # 8-byte Folded Reload
	orq	1696(%rsp), %rax                # 8-byte Folded Reload
	orq	1688(%rsp), %rax                # 8-byte Folded Reload
	orq	1680(%rsp), %rax                # 8-byte Folded Reload
	orq	1672(%rsp), %rax                # 8-byte Folded Reload
	orq	1664(%rsp), %rax                # 8-byte Folded Reload
	orq	1656(%rsp), %rax                # 8-byte Folded Reload
	vpslld	$31, %xmm4, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vmovapd	.LCPI0_50(%rip), %ymm2          # ymm2 = [2305843009213693952,4611686018427387904,1152921504606846976,0]
	vblendvpd	%ymm1, .LCPI0_49(%rip), %ymm2, %ymm1
	orq	1648(%rsp), %rax                # 8-byte Folded Reload
	vpslld	$31, %xmm8, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vpmovzxdq	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
	vpand	.LCPI0_46(%rip), %ymm2, %ymm2
	vpslld	$31, %xmm9, %xmm4
	vpsrad	$31, %xmm4, %xmm4
	vpmovzxdq	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero
	vpand	.LCPI0_47(%rip), %ymm4, %ymm4
	vpslld	$31, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vpand	.LCPI0_51(%rip), %ymm3, %ymm3
	vorpd	%ymm3, %ymm1, %ymm1
	vpslld	$31, %xmm10, %xmm3
	vpsrad	$31, %xmm3, %xmm3
	vpmovzxdq	%xmm3, %ymm3            # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero
	vpand	.LCPI0_48(%rip), %ymm3, %ymm3
	vpslld	$31, %xmm5, %xmm5
	vpsrad	$31, %xmm5, %xmm5
	vpmovsxdq	%xmm5, %ymm5
	vpandn	.LCPI0_52(%rip), %ymm5, %ymm5
	vpslld	$31, %xmm7, %xmm7
	vpsrad	$31, %xmm7, %xmm7
	vpmovsxdq	%xmm7, %ymm7
	vpandn	.LCPI0_53(%rip), %ymm7, %ymm7
	vpor	%ymm3, %ymm5, %ymm3
	vpor	%ymm1, %ymm7, %ymm1
	vpslld	$31, %xmm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vmovapd	.LCPI0_0(%rip), %xmm5           # xmm5 = [9223372036854775808,0]
	vmovapd	.LCPI0_54(%rip), %ymm7          # ymm7 = [9223934986808197120,1125899906842624,281474976710656,2251799813685248]
	vblendvpd	%ymm0, %ymm5, %ymm7, %ymm0
	vpor	%ymm1, %ymm2, %ymm1
	vorpd	%ymm3, %ymm0, %ymm0
	vpor	%ymm0, %ymm4, %ymm0
	vpor	%ymm1, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpor	%xmm1, %xmm0, %xmm0
	vpshufd	$78, %xmm0, %xmm1               # xmm1 = xmm0[2,3,0,1]
	vpor	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rcx
	orq	%rcx, %rax
	tzcntq	%rax, %rax
	cmpl	$62, %eax
	jbe	.LBB0_336
# %bb.111:                              # %no_errors_bb144
	xorl	%r9d, %r9d
	cmpl	$4, 1056(%rsp)                  # 4-byte Folded Reload
	setne	%r9b
	xorl	%r10d, %r10d
	cmpl	$39, %ebx
	setne	%r10b
	xorl	%ecx, %ecx
	cmpl	$24, %edi
	vmovd	528(%rsp), %xmm0                # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 1288(%rsp), %xmm0, %xmm0    # 4-byte Folded Reload
	vpinsrd	$2, 1280(%rsp), %xmm0, %xmm0    # 4-byte Folded Reload
	vpinsrd	$3, 1248(%rsp), %xmm0, %xmm0    # 4-byte Folded Reload
	setne	%cl
	vmovd	1524(%rsp), %xmm1               # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 824(%rsp), %xmm1, %xmm1     # 4-byte Folded Reload
	vpinsrd	$2, 1296(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vpinsrd	$3, 1516(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	xorl	%r8d, %r8d
	vmovd	872(%rsp), %xmm2                # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 1552(%rsp), %xmm2, %xmm2    # 4-byte Folded Reload
	vpinsrd	$2, 864(%rsp), %xmm2, %xmm2     # 4-byte Folded Reload
	vpinsrd	$3, 848(%rsp), %xmm2, %xmm2     # 4-byte Folded Reload
	cmpl	$0, 216(%rsp)                   # 4-byte Folded Reload
	vmovd	488(%rsp), %xmm3                # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vpinsrd	$1, 144(%rsp), %xmm3, %xmm3     # 4-byte Folded Reload
	vpinsrd	$2, 480(%rsp), %xmm3, %xmm3     # 4-byte Folded Reload
	vpinsrd	$3, 664(%rsp), %xmm3, %xmm3     # 4-byte Folded Reload
	setne	%r8b
	vmovd	1208(%rsp), %xmm4               # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 1192(%rsp), %xmm4, %xmm4    # 4-byte Folded Reload
	vpinsrd	$2, 1176(%rsp), %xmm4, %xmm4    # 4-byte Folded Reload
	vpinsrd	$3, 1496(%rsp), %xmm4, %xmm4    # 4-byte Folded Reload
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vmovd	1508(%rsp), %xmm1               # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 1256(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vpinsrd	$2, 1224(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vpinsrd	$3, 1240(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vmovd	1540(%rsp), %xmm3               # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vpinsrd	$1, 1536(%rsp), %xmm3, %xmm3    # 4-byte Folded Reload
	vpinsrd	$2, 1532(%rsp), %xmm3, %xmm3    # 4-byte Folded Reload
	vpinsrd	$3, 1528(%rsp), %xmm3, %xmm3    # 4-byte Folded Reload
	vinserti128	$1, %xmm4, %ymm1, %ymm1
	vmovd	1548(%rsp), %xmm4               # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrd	$1, 856(%rsp), %xmm4, %xmm4     # 4-byte Folded Reload
	vpinsrd	$2, 840(%rsp), %xmm4, %xmm4     # 4-byte Folded Reload
	vpinsrd	$3, 1544(%rsp), %xmm4, %xmm4    # 4-byte Folded Reload
	vinserti128	$1, %xmm3, %ymm4, %ymm3
	vpcmpeqd	.LCPI0_55(%rip), %ymm3, %ymm3
	vpmovsxdq	%xmm3, %ymm4
	vpcmpeqd	.LCPI0_56(%rip), %ymm1, %ymm1
	vpcmpeqd	.LCPI0_57(%rip), %ymm2, %ymm2
	vpmovsxdq	%xmm1, %ymm5
	vpmovsxdq	%xmm2, %ymm6
	vpcmpeqd	.LCPI0_58(%rip), %ymm0, %ymm0
	vpmovsxdq	%xmm0, %ymm7
	vextracti128	$1, %ymm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vextracti128	$1, %ymm1, %xmm1
	vextracti128	$1, %ymm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vextracti128	$1, %ymm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vpandn	.LCPI0_59(%rip), %ymm0, %ymm0
	vpandn	.LCPI0_60(%rip), %ymm2, %ymm2
	vpmovsxdq	%xmm1, %ymm1
	vpor	%ymm0, %ymm2, %ymm0
	vpandn	.LCPI0_61(%rip), %ymm1, %ymm1
	vpandn	.LCPI0_62(%rip), %ymm3, %ymm2
	vpor	%ymm1, %ymm2, %ymm1
	vpor	%ymm1, %ymm0, %ymm0
	vpandn	.LCPI0_63(%rip), %ymm7, %ymm1
	vpandn	.LCPI0_64(%rip), %ymm6, %ymm2
	vpandn	.LCPI0_65(%rip), %ymm5, %ymm3
	vpandn	.LCPI0_66(%rip), %ymm4, %ymm4
	vpor	%ymm1, %ymm2, %ymm1
	vmovd	1488(%rsp), %xmm2               # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 1152(%rsp), %xmm2, %xmm2    # 4-byte Folded Reload
	vpinsrd	$2, 1128(%rsp), %xmm2, %xmm2    # 4-byte Folded Reload
	vpinsrd	$3, 1136(%rsp), %xmm2, %xmm2    # 4-byte Folded Reload
	vpor	%ymm3, %ymm4, %ymm3
	movq	2080(%rsp), %r12                # 8-byte Reload
	vmovd	%r12d, %xmm4
	vpinsrd	$1, 1040(%rsp), %xmm4, %xmm4    # 4-byte Folded Reload
	vpinsrd	$2, 1160(%rsp), %xmm4, %xmm4    # 4-byte Folded Reload
	vpinsrd	$3, 1144(%rsp), %xmm4, %xmm4    # 4-byte Folded Reload
	vpor	%ymm3, %ymm1, %ymm1
	vmovd	%r15d, %xmm3
	vpinsrd	$1, 1484(%rsp), %xmm3, %xmm3    # 4-byte Folded Reload
	vpinsrd	$2, %esi, %xmm3, %xmm3
	vpinsrd	$3, %r13d, %xmm3, %xmm3
	vpor	%ymm0, %ymm1, %ymm0
	vmovd	1112(%rsp), %xmm1               # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 1120(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vpinsrd	$2, 1104(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vpinsrd	$3, %edx, %xmm1, %xmm1
	vinserti128	$1, %xmm2, %ymm4, %ymm2
	vinserti128	$1, %xmm3, %ymm1, %ymm1
	vpcmpeqd	.LCPI0_67(%rip), %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vpcmpeqd	.LCPI0_68(%rip), %ymm2, %ymm2
	vextracti128	$1, %ymm2, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vpmovsxdq	%xmm1, %ymm1
	vpmovsxdq	%xmm2, %ymm2
	vpandn	.LCPI0_69(%rip), %ymm2, %ymm2
	vpandn	.LCPI0_70(%rip), %ymm1, %ymm1
	vpandn	.LCPI0_71(%rip), %ymm4, %ymm4
	vpandn	.LCPI0_72(%rip), %ymm3, %ymm3
	vpor	%ymm1, %ymm2, %ymm1
	vpor	%ymm3, %ymm4, %ymm2
	vpor	%ymm2, %ymm1, %ymm1
	xorl	%edi, %edi
	cmpl	$1, 1480(%rsp)                  # 4-byte Folded Reload
	setne	%dil
	shlq	$49, %rdi
	xorl	%ebx, %ebx
	testl	%r11d, %r11d
	setne	%bl
	shlq	$50, %rbx
	shlq	$51, %rcx
	xorl	%esi, %esi
	testl	%r14d, %r14d
	setne	%sil
	shlq	$52, %rsi
	shlq	$53, %r10
	xorl	%eax, %eax
	cmpl	$0, 1080(%rsp)                  # 4-byte Folded Reload
	setne	%al
	vextracti128	$1, %ymm0, %xmm2
	vpor	%xmm2, %xmm0, %xmm0
	vpshufd	$78, %xmm0, %xmm2               # xmm2 = xmm0[2,3,0,1]
	vpor	%xmm2, %xmm0, %xmm0
	vmovq	%xmm0, %r11
	vextracti128	$1, %ymm1, %xmm0
	vpor	%xmm0, %xmm1, %xmm0
	vpshufd	$78, %xmm0, %xmm1               # xmm1 = xmm0[2,3,0,1]
	vpor	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rdx
	orq	%r11, %rdx
	orq	%rbx, %rcx
	orq	%rdi, %rcx
	orq	%rsi, %rcx
	orq	%r10, %rcx
	shlq	$54, %rax
	orq	%rax, %rcx
	shlq	$55, %r9
	orq	%r9, %rcx
	orq	%r8, %rcx
	orq	%rdx, %rcx
	movabsq	$-9223372036854775808, %rax     # imm = 0x8000000000000000
	orq	%rax, %rcx
	xorl	%eax, %eax
	tzcntq	%rcx, %rax
	cmpl	$55, %eax
	movl	432(%rsp), %r11d                # 4-byte Reload
	jbe	.LBB0_338
# %bb.112:                              # %no_errors_bb208
	movslq	968(%rsp), %r12                 # 4-byte Folded Reload
	movq	152(%rsp), %rdi                 # 8-byte Reload
	movslq	%edi, %rbx
	movq	%r12, %r13
	imulq	%rbx, %r13
	movslq	976(%rsp), %r9                  # 4-byte Folded Reload
	movq	%r13, %r14
	imulq	%r9, %r14
	movslq	952(%rsp), %r8                  # 4-byte Folded Reload
	movq	%r8, 408(%rsp)                  # 8-byte Spill
	movq	88(%rsp), %r15                  # 8-byte Reload
	movslq	%r15d, %rdx
	imulq	%rdx, %r8
	movslq	960(%rsp), %rax                 # 4-byte Folded Reload
	movq	%r8, %r10
	imulq	%rax, %r10
	movslq	1320(%rsp), %rsi                # 4-byte Folded Reload
	movq	%rsi, %rcx
	negq	%rcx
	cmovlq	%rsi, %rcx
	xorl	%esi, %esi
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%sil
	movq	%rsi, 288(%rsp)                 # 8-byte Spill
	movslq	%r11d, %r11
	movq	%r11, %rsi
	shlq	$5, %rsi
	movq	%rsi, %rcx
	negq	%rcx
	testl	%r11d, %r11d
	cmovnsq	%rsi, %rcx
	xorl	%esi, %esi
	movq	%rcx, 1208(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%sil
	movl	%esi, 488(%rsp)                 # 4-byte Spill
	movslq	512(%rsp), %rcx                 # 4-byte Folded Reload
	leaq	(,%rcx,8), %rsi
	leaq	(%rsi,%rsi,4), %r11
	movq	%r11, %rsi
	negq	%rsi
	testl	%ecx, %ecx
	cmovnsq	%r11, %rsi
	xorl	%ecx, %ecx
	movq	%rsi, 1200(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rsi               # imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 480(%rsp)                 # 4-byte Spill
	movslq	416(%rsp), %r11                 # 4-byte Folded Reload
	leaq	(,%r11,8), %rcx
	subq	%r11, %rcx
	movq	%rcx, %rsi
	negq	%rsi
	testl	%r11d, %r11d
	cmovnsq	%rcx, %rsi
	xorl	%ecx, %ecx
	movq	%rsi, 1192(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rsi               # imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 144(%rsp)                 # 4-byte Spill
	movslq	1440(%rsp), %rcx                # 4-byte Folded Reload
	imulq	$39, %rcx, %r11
	movq	%r11, %rsi
	negq	%rsi
	testl	%ecx, %ecx
	cmovnsq	%r11, %rsi
	xorl	%ecx, %ecx
	movq	%rsi, 1184(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rsi               # imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 216(%rsp)                 # 4-byte Spill
	movslq	424(%rsp), %rcx                 # 4-byte Folded Reload
	movq	%rcx, %rsi
	negq	%rsi
	cmovlq	%rcx, %rsi
	xorl	%ecx, %ecx
	movq	%rsi, 1168(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rsi               # imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 664(%rsp)                 # 4-byte Spill
	movq	%rbx, %rcx
	negq	%rcx
	testl	%edi, %edi
	cmovnsq	%rbx, %rcx
	xorl	%esi, %esi
	movq	%rcx, 1176(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%sil
	movl	%esi, 152(%rsp)                 # 4-byte Spill
	movslq	1400(%rsp), %rcx                # 4-byte Folded Reload
	imulq	%r12, %rcx
	movq	%rcx, %rsi
	negq	%rsi
	cmovlq	%rcx, %rsi
	xorl	%r11d, %r11d
	movq	%rsi, 1160(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rsi               # imm = 0x7FFFFFFF
	seta	%r11b
	xorl	%r12d, %r12d
	movq	%r13, 1248(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %r13               # imm = 0x7FFFFFFF
	setg	%r12b
	movslq	1392(%rsp), %rcx                # 4-byte Folded Reload
	imulq	%r9, %rcx
	movq	%rcx, %rsi
	negq	%rsi
	cmovlq	%rcx, %rsi
	xorl	%ecx, %ecx
	movq	%rsi, 1152(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rsi               # imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 864(%rsp)                 # 4-byte Spill
	xorl	%esi, %esi
	movq	%r14, 1240(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %r14               # imm = 0x7FFFFFFF
	setg	%sil
	movslq	632(%rsp), %rcx                 # 4-byte Folded Reload
	movq	%rcx, %rdi
	negq	%rdi
	cmovlq	%rcx, %rdi
	xorl	%ecx, %ecx
	movq	%rdi, 1144(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rdi               # imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 848(%rsp)                 # 4-byte Spill
	movq	%rdx, %rcx
	negq	%rcx
	testl	%r15d, %r15d
	cmovnsq	%rdx, %rcx
	xorl	%edx, %edx
	movq	%rcx, 1136(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%dl
	movl	%edx, 840(%rsp)                 # 4-byte Spill
	movslq	1344(%rsp), %rcx                # 4-byte Folded Reload
	imulq	408(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rcx, %rdx
	negq	%rdx
	cmovlq	%rcx, %rdx
	xorl	%ecx, %ecx
	movq	%rdx, 1128(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 88(%rsp)                  # 4-byte Spill
	xorl	%ecx, %ecx
	movq	%r8, 1232(%rsp)                 # 8-byte Spill
	cmpq	$2147483647, %r8                # imm = 0x7FFFFFFF
	setg	%cl
	movl	%ecx, 872(%rsp)                 # 4-byte Spill
	movslq	36(%rsp), %rdx                  # 4-byte Folded Reload
	movq	%rdx, %rcx
	movq	%rdx, 408(%rsp)                 # 8-byte Spill
	imulq	%rdx, %rax
	movq	%rax, %rcx
	negq	%rcx
	cmovlq	%rax, %rcx
	xorl	%eax, %eax
	movq	%rcx, 1120(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 968(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	movq	%r10, 1224(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %r10               # imm = 0x7FFFFFFF
	setg	%al
	movl	%eax, 856(%rsp)                 # 4-byte Spill
	movslq	1312(%rsp), %rax                # 4-byte Folded Reload
	movq	%rax, %rcx
	negq	%rcx
	cmovlq	%rax, %rcx
	xorl	%eax, %eax
	movq	%rcx, 1112(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 952(%rsp)                 # 4-byte Spill
	movq	72(%rsp), %rcx                  # 8-byte Reload
	movslq	%ecx, %rax
	movq	%rax, %rdx
	negq	%rdx
	testl	%ecx, %ecx
	cmovnsq	%rax, %rdx
	xorl	%eax, %eax
	movq	%rdx, 1104(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 832(%rsp)                 # 4-byte Spill
	movslq	2056(%rsp), %rax                # 4-byte Folded Reload
	leaq	(,%rax,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	movq	%rdx, 1096(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$19, %rax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	movslq	1560(%rsp), %rax                # 4-byte Folded Reload
	movq	%rax, %rcx
	shlq	$5, %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	xorl	%ebx, %ebx
	movq	%rdx, 1088(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	seta	%bl
	shlq	$20, %rbx
	movslq	2040(%rsp), %rax                # 4-byte Folded Reload
	leaq	(,%rax,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	movq	%rdx, 1080(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$21, %rax
	movq	%rax, 976(%rsp)                 # 8-byte Spill
	movq	2000(%rsp), %rcx                # 8-byte Reload
	leaq	(,%rcx,4), %rax
	movq	%rax, %rdx
	negq	%rdx
	testl	%ecx, %ecx
	cmovnsq	%rax, %rdx
	movq	%rdx, 1072(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$22, %rax
	movq	%rax, 960(%rsp)                 # 8-byte Spill
	movslq	520(%rsp), %rax                 # 4-byte Folded Reload
	leaq	(,%rax,8), %rcx
	leaq	(%rcx,%rcx,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	movq	%rdx, 1064(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$23, %rax
	movq	%rax, 824(%rsp)                 # 8-byte Spill
	movslq	1992(%rsp), %rax                # 4-byte Folded Reload
	leaq	(,%rax,8), %rcx
	subq	%rax, %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	movq	%rdx, 1056(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$24, %rax
	movq	%rax, 528(%rsp)                 # 8-byte Spill
	movslq	1336(%rsp), %rax                # 4-byte Folded Reload
	leaq	(,%rax,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	movq	%rdx, 816(%rsp)                 # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$25, %rax
	movq	%rax, 1296(%rsp)                # 8-byte Spill
	movslq	2024(%rsp), %rax                # 4-byte Folded Reload
	leaq	(,%rax,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	movq	%rdx, 808(%rsp)                 # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$26, %rax
	movq	%rax, 1288(%rsp)                # 8-byte Spill
	movslq	1556(%rsp), %rax                # 4-byte Folded Reload
	imulq	$39, %rax, %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	movq	%rdx, 800(%rsp)                 # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$27, %rax
	movq	%rax, 1280(%rsp)                # 8-byte Spill
	movslq	1424(%rsp), %rax                # 4-byte Folded Reload
	leaq	(,%rax,4), %rcx
	movq	%rcx, %rdx
	negq	%rdx
	testl	%eax, %eax
	cmovnsq	%rcx, %rdx
	xorl	%eax, %eax
	movq	%rdx, 792(%rsp)                 # 8-byte Spill
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	seta	%al
	movq	%rax, %rcx
	movq	56(%rsp), %rdx                  # 8-byte Reload
	shlq	$37, %rdx
	movabsq	$274877906944, %rax             # imm = 0x4000000000
	andq	%rax, %rdx
	movq	%rdx, 56(%rsp)                  # 8-byte Spill
	movq	200(%rsp), %rdx                 # 8-byte Reload
	shlq	$38, %rdx
	movabsq	$549755813888, %rax             # imm = 0x8000000000
	andq	%rax, %rdx
	movq	%rdx, 200(%rsp)                 # 8-byte Spill
	movq	2184(%rsp), %rdi                # 8-byte Reload
	shlq	$45, %rdi
	movabsq	$70368744177664, %rax           # imm = 0x400000000000
	andq	%rax, %rdi
	vmovd	%r12d, %xmm0
	vpinsrb	$4, %r11d, %xmm0, %xmm0
	vpinsrb	$8, %esi, %xmm0, %xmm0
	shlq	$28, %rcx
	movq	%rcx, 1272(%rsp)                # 8-byte Spill
	movq	2208(%rsp), %r10                # 8-byte Reload
	andl	$2, %r10d
	shlq	$28, %r10
	movq	2200(%rsp), %r14                # 8-byte Reload
	andl	$2, %r14d
	shlq	$29, %r14
	movq	2192(%rsp), %r12                # 8-byte Reload
	andl	$2, %r12d
	shlq	$30, %r12
	movq	96(%rsp), %rax                  # 8-byte Reload
	andl	$2, %eax
	shlq	$31, %rax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movq	576(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$32, %rax
	movq	%rax, 576(%rsp)                 # 8-byte Spill
	movq	128(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$33, %rax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movq	8(%rsp), %rax                   # 8-byte Reload
	andl	$2, %eax
	shlq	$34, %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	896(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$35, %rax
	movq	%rax, 896(%rsp)                 # 8-byte Spill
	movq	64(%rsp), %rax                  # 8-byte Reload
	andl	$2, %eax
	shlq	$36, %rax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movq	496(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$39, %rax
	movq	%rax, 496(%rsp)                 # 8-byte Spill
	movq	536(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$40, %rax
	movq	%rax, 536(%rsp)                 # 8-byte Spill
	movq	888(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$41, %rax
	movq	%rax, 888(%rsp)                 # 8-byte Spill
	movq	624(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$42, %rax
	movq	%rax, 624(%rsp)                 # 8-byte Spill
	movq	880(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$43, %rax
	movq	%rax, 880(%rsp)                 # 8-byte Spill
	movq	696(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$44, %rax
	movq	%rax, 696(%rsp)                 # 8-byte Spill
	movq	688(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$46, %rax
	movq	%rax, 688(%rsp)                 # 8-byte Spill
	movq	680(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$47, %rax
	movq	%rax, 680(%rsp)                 # 8-byte Spill
	xorl	%r13d, %r13d
	cmpq	$0, 704(%rsp)                   # 8-byte Folded Reload
	sete	%r13b
	xorl	%r15d, %r15d
	cmpq	$0, 2072(%rsp)                  # 8-byte Folded Reload
	sete	%r15b
	xorl	%eax, %eax
	cmpq	$0, 560(%rsp)                   # 8-byte Folded Reload
	sete	%al
	movl	%eax, 1256(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpq	$0, 1432(%rsp)                  # 8-byte Folded Reload
	sete	%al
	movl	%eax, 1264(%rsp)                # 4-byte Spill
	xorl	%r11d, %r11d
	cmpq	$0, 552(%rsp)                   # 8-byte Folded Reload
	sete	%r11b
	xorl	%ecx, %ecx
	cmpq	$0, 1456(%rsp)                  # 8-byte Folded Reload
	sete	%cl
	xorl	%r8d, %r8d
	cmpq	$0, 1448(%rsp)                  # 8-byte Folded Reload
	sete	%r8b
	xorl	%r9d, %r9d
	cmpq	$0, 992(%rsp)                   # 8-byte Folded Reload
	sete	%r9b
	xorl	%esi, %esi
	cmpq	$0, 1304(%rsp)                  # 8-byte Folded Reload
	sete	%sil
	xorl	%edx, %edx
	cmpq	$0, 1408(%rsp)                  # 8-byte Folded Reload
	sete	%dl
	xorl	%eax, %eax
	cmpq	$0, 1808(%rsp)                  # 8-byte Folded Reload
	sete	%al
	vpinsrb	$12, %eax, %xmm0, %xmm0
	vmovd	488(%rsp), %xmm1                # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrb	$4, %r11d, %xmm1, %xmm1
	vpinsrb	$8, 480(%rsp), %xmm1, %xmm1     # 4-byte Folded Reload
	vpinsrb	$12, %ecx, %xmm1, %xmm1
	vmovd	864(%rsp), %xmm2                # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrb	$4, 848(%rsp), %xmm2, %xmm2     # 4-byte Folded Reload
	xorl	%eax, %eax
	cmpq	$0, 80(%rsp)                    # 8-byte Folded Reload
	sete	%al
	vpinsrb	$8, %eax, %xmm2, %xmm2
	vpinsrb	$12, 840(%rsp), %xmm2, %xmm2    # 4-byte Folded Reload
	vmovd	664(%rsp), %xmm3                # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vpinsrb	$4, %esi, %xmm3, %xmm3
	vpinsrb	$8, %edx, %xmm3, %xmm3
	vpinsrb	$12, 152(%rsp), %xmm3, %xmm3    # 4-byte Folded Reload
	vmovd	968(%rsp), %xmm4                # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vpinsrb	$4, 952(%rsp), %xmm4, %xmm4     # 4-byte Folded Reload
	xorl	%eax, %eax
	cmpq	$0, 1472(%rsp)                  # 8-byte Folded Reload
	sete	%al
	xorl	%ecx, %ecx
	cmpq	$0, 712(%rsp)                   # 8-byte Folded Reload
	sete	%cl
	vpinsrb	$8, %ecx, %xmm4, %xmm4
	vpinsrb	$12, 832(%rsp), %xmm4, %xmm4    # 4-byte Folded Reload
	vmovd	872(%rsp), %xmm5                # 4-byte Folded Reload
                                        # xmm5 = mem[0],zero,zero,zero
	vpinsrb	$4, 88(%rsp), %xmm5, %xmm5      # 4-byte Folded Reload
	vpinsrb	$8, 856(%rsp), %xmm5, %xmm5     # 4-byte Folded Reload
	vpinsrb	$12, %eax, %xmm5, %xmm5
	vmovd	144(%rsp), %xmm6                # 4-byte Folded Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vpinsrb	$4, %r8d, %xmm6, %xmm6
	vpinsrb	$8, %r9d, %xmm6, %xmm6
	vpinsrb	$12, 216(%rsp), %xmm6, %xmm6    # 4-byte Folded Reload
	vmovd	%r13d, %xmm7
	vpinsrb	$4, %r15d, %xmm7, %xmm7
	vpinsrb	$8, 1256(%rsp), %xmm7, %xmm7    # 4-byte Folded Reload
	vpinsrb	$12, 1264(%rsp), %xmm7, %xmm7   # 4-byte Folded Reload
	orq	72(%rsp), %rbx                  # 8-byte Folded Reload
	orq	976(%rsp), %rbx                 # 8-byte Folded Reload
	orq	960(%rsp), %rbx                 # 8-byte Folded Reload
	orq	824(%rsp), %rbx                 # 8-byte Folded Reload
	orq	528(%rsp), %rbx                 # 8-byte Folded Reload
	orq	1296(%rsp), %rbx                # 8-byte Folded Reload
	orq	1288(%rsp), %rbx                # 8-byte Folded Reload
	orq	1280(%rsp), %rbx                # 8-byte Folded Reload
	orq	1272(%rsp), %rbx                # 8-byte Folded Reload
	orq	%r10, %rbx
	orq	%r14, %rbx
	orq	288(%rsp), %rbx                 # 8-byte Folded Reload
	orq	%r12, %rbx
	orq	96(%rsp), %rbx                  # 8-byte Folded Reload
	orq	576(%rsp), %rbx                 # 8-byte Folded Reload
	orq	128(%rsp), %rbx                 # 8-byte Folded Reload
	orq	8(%rsp), %rbx                   # 8-byte Folded Reload
	orq	896(%rsp), %rbx                 # 8-byte Folded Reload
	orq	64(%rsp), %rbx                  # 8-byte Folded Reload
	orq	56(%rsp), %rbx                  # 8-byte Folded Reload
	orq	200(%rsp), %rbx                 # 8-byte Folded Reload
	orq	496(%rsp), %rbx                 # 8-byte Folded Reload
	orq	536(%rsp), %rbx                 # 8-byte Folded Reload
	orq	888(%rsp), %rbx                 # 8-byte Folded Reload
	orq	624(%rsp), %rbx                 # 8-byte Folded Reload
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vpand	.LCPI0_74(%rip), %ymm1, %ymm1
	orq	880(%rsp), %rbx                 # 8-byte Folded Reload
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vpand	.LCPI0_75(%rip), %ymm2, %ymm2
	vpslld	$31, %xmm3, %xmm3
	vpsrad	$31, %xmm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vpand	.LCPI0_76(%rip), %ymm3, %ymm3
	vpslld	$31, %xmm4, %xmm4
	vpsrad	$31, %xmm4, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vpand	.LCPI0_77(%rip), %ymm4, %ymm4
	vpor	%ymm2, %ymm1, %ymm1
	vpor	%ymm4, %ymm3, %ymm2
	vpslld	$31, %xmm7, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vmovapd	.LCPI0_0(%rip), %xmm4           # xmm4 = [9223372036854775808,0]
	vblendvpd	%ymm3, .LCPI0_80(%rip), %ymm4, %ymm3
	vpor	%ymm2, %ymm1, %ymm1
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vpand	.LCPI0_73(%rip), %ymm0, %ymm0
	vpslld	$31, %xmm5, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vpand	.LCPI0_78(%rip), %ymm2, %ymm2
	vpslld	$31, %xmm6, %xmm4
	vpsrad	$31, %xmm4, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vpand	.LCPI0_79(%rip), %ymm4, %ymm4
	vpor	%ymm2, %ymm4, %ymm2
	vorpd	%ymm0, %ymm3, %ymm0
	vorpd	%ymm2, %ymm0, %ymm0
	vorpd	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm1
	vorpd	%xmm1, %xmm0, %xmm0
	vpermilps	$78, %xmm0, %xmm1       # xmm1 = xmm0[2,3,0,1]
	vorpd	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rax
	orq	%rax, %rbx
	orq	696(%rsp), %rdi                 # 8-byte Folded Reload
	orq	688(%rsp), %rdi                 # 8-byte Folded Reload
	orq	680(%rsp), %rdi                 # 8-byte Folded Reload
	orq	%rbx, %rdi
	xorl	%eax, %eax
	tzcntq	%rdi, %rax
	cmpl	$62, %eax
	jbe	.LBB0_340
# %bb.113:                              # %no_errors_bb265
	vmovdqa	%xmm15, 128(%rsp)               # 16-byte Spill
	vmovdqa	%xmm14, 576(%rsp)               # 16-byte Spill
	vmovdqa	%xmm13, 288(%rsp)               # 16-byte Spill
	xorl	%eax, %eax
	cmpq	$0, 2064(%rsp)                  # 8-byte Folded Reload
	sete	%al
	xorl	%ecx, %ecx
	cmpq	$0, 2048(%rsp)                  # 8-byte Folded Reload
	sete	%cl
	xorl	%edx, %edx
	cmpq	$0, 2008(%rsp)                  # 8-byte Folded Reload
	sete	%dl
	xorl	%esi, %esi
	cmpq	$0, 1416(%rsp)                  # 8-byte Folded Reload
	sete	%sil
	xorl	%edi, %edi
	cmpq	$0, 2032(%rsp)                  # 8-byte Folded Reload
	sete	%dil
	shlq	$4, %rdi
	xorl	%ebx, %ebx
	cmpq	$0, 2016(%rsp)                  # 8-byte Folded Reload
	sete	%bl
	shlq	$5, %rbx
	leaq	(%rax,%rcx,2), %rax
	leaq	(%rax,%rdx,4), %rax
	leaq	(%rax,%rsi,8), %rax
	orq	%rdi, %rax
	orq	%rbx, %rax
	movabsq	$-9223372036854775808, %rcx     # imm = 0x8000000000000000
	orq	%rcx, %rax
	tzcntq	%rax, %rax
	cmpl	$5, %eax
	movq	648(%rsp), %rcx                 # 8-byte Reload
	movq	16(%rbp), %r15
	movq	160(%rsp), %r13                 # 8-byte Reload
	jbe	.LBB0_342
# %bb.114:                              # %"produce squashed_head1_filter"
	movl	512(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 6448(%rsp)
	movl	416(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 6452(%rsp)
	movq	552(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 6456(%rsp)
	movq	%rcx, 6464(%rsp)
	leaq	7328(%rsp), %rax
	movq	%rax, 6472(%rsp)
	movq	$0, 6480(%rsp)
	leaq	train_cost_model.par_for.squashed_head1_filter.s0.s.s.s(%rip), %rsi
	leaq	6448(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$16, %ecx
	vzeroupper
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movq	40(%rsp), %rbx                  # 8-byte Reload
	jne	.LBB0_344
# %bb.115:                              # %"assert succeeded337"
	movl	%r13d, %r14d
	movq	%r14, 536(%rsp)                 # 8-byte Spill
	movq	%r14, %rdx
	shlq	$5, %rdx
	cmpl	$67108864, %r13d                # imm = 0x4000000
	jae	.LBB0_345
# %bb.116:                              # %"assert succeeded339"
	movq	%rdx, 8(%rsp)                   # 8-byte Spill
	movq	%rdx, %rsi
	orq	$4, %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB0_346
# %bb.117:                              # %"assert succeeded341"
	movq	%rax, %r14
	movq	1432(%rsp), %rax                # 8-byte Reload
	movq	%rax, 7112(%rsp)
	movq	%r15, 7120(%rsp)
	movq	%r14, 7128(%rsp)
	movq	$0, 7136(%rsp)
	leaq	train_cost_model.par_for.head1_conv.s0.w(%rip), %rsi
	xorl	%r15d, %r15d
	leaq	7112(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r13d, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movq	%r14, 896(%rsp)                 # 8-byte Spill
	jne	.LBB0_348
# %bb.118:                              # %"consume squashed_head1_filter"
	movq	544(%rsp), %rax                 # 8-byte Reload
	movl	1392(%rsp), %edi                # 4-byte Reload
	imull	%edi, %eax
	movq	504(%rsp), %rcx                 # 8-byte Reload
	movq	1400(%rsp), %rsi                # 8-byte Reload
	imull	%esi, %ecx
	movq	%rcx, 504(%rsp)                 # 8-byte Spill
	movq	1328(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %ecx
	movq	%rax, 544(%rsp)                 # 8-byte Spill
	addl	%eax, %ecx
	movl	%esi, 7144(%rsp)
	movl	%edi, 7148(%rsp)
	movl	%ecx, 960(%rsp)                 # 4-byte Spill
	movl	%ecx, 7152(%rsp)
	movq	%r14, 7160(%rsp)
	movq	$0, 7168(%rsp)
	movq	1408(%rsp), %rax                # 8-byte Reload
	movq	%rax, 7176(%rsp)
	movq	440(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 7184(%rsp)
	leaq	7328(%rsp), %rax
	movq	%rax, 7192(%rsp)
	movq	$0, 7200(%rsp)
	leaq	train_cost_model.par_for.head1_conv.s1.w(%rip), %rsi
	xorl	%r15d, %r15d
	leaq	7144(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r13d, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_348
# %bb.119:                              # %"assert succeeded345"
	vmovdqa	2096(%rsp), %xmm0               # 16-byte Reload
	vmovd	%xmm0, %r10d
	movl	%r10d, %r9d
	sarl	$3, %r9d
	cmpl	$3, %ebx
	movl	$2, %r11d
	cmovll	%ebx, %r11d
	testl	%r11d, %r11d
	movl	$1, %r8d
	cmovlel	%r8d, %r11d
	vmovdqa	576(%rsp), %xmm0                # 16-byte Reload
	vmovd	%xmm0, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	addl	$39, %eax
	movl	%eax, %esi
	sarl	$31, %esi
	xorl	%esi, %eax
	imulq	$1717986919, %rax, %rcx         # imm = 0x66666667
	shrq	$36, %rcx
	xorl	%esi, %ecx
	leal	1(%r13), %edi
	movl	%edi, 680(%rsp)                 # 4-byte Spill
	andl	$2147483646, %edi               # imm = 0x7FFFFFFE
	movl	%edi, %eax
	imull	%ecx, %eax
	movq	%rax, 496(%rsp)                 # 8-byte Spill
	decl	%eax
	movl	%eax, %r15d
	sarl	$31, %r15d
	movq	%rbx, %r12
	xorl	%ebx, %ebx
	testl	%ecx, %ecx
	sete	%bl
	subl	%r15d, %eax
	cltd
	movq	%rcx, 200(%rsp)                 # 8-byte Spill
	addl	%ebx, %ecx
	idivl	%ecx
	movl	%esi, %ecx
	notl	%ecx
	subl	%esi, %ecx
	andl	%r15d, %ecx
	addl	%eax, %ecx
	decl	%ebx
	andl	%ecx, %ebx
	decl	%edi
	cmpl	%ebx, %edi
	cmovgl	%ebx, %edi
	andl	$-2, %edi
	addl	%r11d, %edi
	cmpl	$2, %edi
	cmovgel	%r8d, %edi
	cmpl	%r12d, %edi
	cmovll	%r12d, %edi
	cmpl	$8, %r12d
	movl	$7, %ecx
	cmovll	%r12d, %ecx
	movl	$2147483648, %ebx               # imm = 0x80000000
	testl	%ecx, %ecx
	cmovlel	%r8d, %ecx
	leal	-1(%r13), %r14d
	movl	%r14d, %eax
	sarl	$31, %eax
	movl	%eax, %edx
	xorl	%r14d, %edx
	leaq	306783379(%rbx), %rsi
	imulq	%rdx, %rsi
	shrq	$34, %rsi
	xorl	%eax, %esi
	leal	(,%rsi,8), %eax
	subl	%esi, %eax
	addl	%ecx, %eax
	cmpl	%r13d, %eax
	cmovgl	%r13d, %eax
	cmpl	%eax, %edi
	cmovgel	%edi, %eax
	cmpl	$5, %r9d
	movl	$4, %ecx
	cmovll	%r9d, %ecx
	movl	%r10d, %edx
	sarl	$31, %edx
	xorl	%edx, %r10d
	imulq	$1717986919, %r10, %rsi         # imm = 0x66666667
	shrq	$36, %rsi
	xorl	%edx, %esi
	leal	(%rsi,%rsi,4), %edx
	vmovdqa	128(%rsp), %xmm0                # 16-byte Reload
	vmovd	%xmm0, %r15d
	leal	(%r15,%rdx,8), %edx
	vmovdqa	2112(%rsp), %xmm0               # 16-byte Reload
	vmovd	%xmm0, %esi
	cmpl	%edx, %esi
	cmovgl	%edx, %esi
	vmovdqa	2128(%rsp), %xmm0               # 16-byte Reload
	vmovd	%xmm0, %edx
	leal	(%rdx,%rcx,8), %r13d
	addl	%esi, %r13d
	vmovdqa	288(%rsp), %xmm0                # 16-byte Reload
	vmovd	%xmm0, %ecx
	cmpl	%ecx, %r13d
	movl	%ecx, 96(%rsp)                  # 4-byte Spill
	cmovgl	%ecx, %r13d
	movq	352(%rsp), %rcx                 # 8-byte Reload
	cmpl	%ecx, %r13d
	movl	%ecx, %r12d
	cmovgel	%r13d, %r12d
	movl	24(%rsp), %ecx                  # 4-byte Reload
	cmpl	%ecx, %r12d
	cmovll	%ecx, %r12d
	subl	%r15d, %r12d
	movl	%r12d, %ecx
	sarl	$31, %ecx
	andnl	%r12d, %ecx, %ecx
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %eax
	imulq	$156, %rcx, %rsi
	movq	%rsi, %rdx
	imulq	%rax, %rdx
	cmpq	$2147483647, %rdx               # imm = 0x7FFFFFFF
	ja	.LBB0_321
# %bb.120:                              # %"assert succeeded345"
	movl	%ecx, %edi
	shll	$2, %edi
	imulq	$39, %rdi, %rdi
	shrq	$32, %rdi
	shrq	$30, %rcx
	imulq	$39, %rcx, %rcx
	addq	%rdi, %rcx
	movl	%esi, %esi
	imulq	%rax, %rsi
	shrq	$32, %rsi
	imulq	%rax, %rcx
	addq	%rsi, %rcx
	movabsq	$4393751543808, %rax            # imm = 0x3FF00000000
	andq	%rcx, %rax
	jne	.LBB0_321
# %bb.121:                              # %"assert succeeded347"
	addq	$4, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	movq	320(%rsp), %r8                  # 8-byte Reload
	movq	1344(%rsp), %r9                 # 8-byte Reload
	je	.LBB0_347
# %bb.122:                              # %"assert succeeded349"
	movl	%r14d, 152(%rsp)                # 4-byte Spill
	leaq	306783379(%rbx), %rcx
	movq	536(%rsp), %rdx                 # 8-byte Reload
	imulq	%rcx, %rdx
	shrq	$31, %rdx
	andl	$-8, %edx
	movq	160(%rsp), %r10                 # 8-byte Reload
	leal	6(%r10), %ebx
	imulq	%rcx, %rbx
	shrq	$31, %rbx
	andl	$-8, %ebx
	movl	36(%rsp), %r11d                 # 4-byte Reload
	movq	208(%rsp), %rcx                 # 8-byte Reload
	imull	%r11d, %ecx
	movq	48(%rsp), %rdi                  # 8-byte Reload
	imull	%r9d, %edi
	movq	272(%rsp), %rsi                 # 8-byte Reload
	movq	%rdi, 48(%rsp)                  # 8-byte Spill
	addl	%edi, %esi
	movq	%rcx, 208(%rsp)                 # 8-byte Spill
	movq	%rsi, 688(%rsp)                 # 8-byte Spill
	leal	(%rsi,%rcx), %ecx
	movl	%r12d, 888(%rsp)                # 4-byte Spill
	movl	%r12d, 2256(%rsp)
	movl	%r9d, 2260(%rsp)
	movl	%r11d, 2264(%rsp)
	movq	%rdx, 72(%rsp)                  # 8-byte Spill
	movl	%edx, 2268(%rsp)
	movq	56(%rsp), %rdx                  # 8-byte Reload
	movl	%edx, 2272(%rsp)
	movl	%ecx, 88(%rsp)                  # 4-byte Spill
	movl	%ecx, 2276(%rsp)
	movl	%edx, 2280(%rsp)
	movl	%r10d, 2284(%rsp)
	movl	%r15d, 2288(%rsp)
	movl	96(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 2292(%rsp)
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movq	%rax, 2296(%rsp)
	movq	$0, 2304(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2312(%rsp)
	movq	%r8, 2320(%rsp)
	leaq	train_cost_model.par_for.normalized_schedule_features.s0.c.c.c(%rip), %rsi
	leaq	2256(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	%rbx, 696(%rsp)                 # 8-byte Spill
	movl	%ebx, %ecx
	callq	halide_do_par_for@PLT
	movl	$2147483648, %r9d               # imm = 0x80000000
	testl	%eax, %eax
	jne	.LBB0_349
# %bb.123:                              # %"assert succeeded351"
	movq	%r15, %r14
	movl	24(%rsp), %eax                  # 4-byte Reload
	cmpl	%eax, %r13d
	cmovll	%eax, %r13d
	movq	352(%rsp), %rsi                 # 8-byte Reload
	cmpl	$6, %esi
	movl	$5, %eax
	cmovll	%esi, %eax
	movl	1824(%rsp), %edi                # 4-byte Reload
	movl	%edi, %ecx
	sarl	$31, %ecx
	movl	%ecx, %edx
	xorl	%edi, %edx
	imulq	$1717986919, %rdx, %rdx         # imm = 0x66666667
	shrq	$33, %rdx
	xorl	%ecx, %edx
	leal	(%rdx,%rdx,4), %ecx
	addl	%eax, %ecx
	cmpl	%esi, %ecx
	cmovgl	%esi, %ecx
	cmpl	%ecx, %r13d
	cmovgel	%r13d, %ecx
	testl	%ecx, %ecx
	movl	$1, %ebx
	cmovgl	%ecx, %ebx
	subl	%r14d, %ebx
	movl	%ebx, %eax
	sarl	$31, %eax
	andnl	%ebx, %eax, %eax
	movq	8(%rsp), %rcx                   # 8-byte Reload
	leaq	(%rcx,%rcx,2), %rdi
	movq	%rdi, %rdx
	imulq	%rax, %rdx
	cmpq	%r9, %rdx
	jae	.LBB0_322
# %bb.124:                              # %"assert succeeded351"
	movabsq	$545460846592, %r8              # imm = 0x7F00000000
	movq	%rdi, %r15
	shrq	$32, %r15
	movl	%edi, %esi
	movq	%rsi, %rcx
	imulq	%rax, %rcx
	shrq	$32, %rcx
	imulq	%r15, %rax
	addq	%rcx, %rax
	andq	%r8, %rax
	jne	.LBB0_322
# %bb.125:                              # %"assert succeeded353"
	movq	%rsi, 144(%rsp)                 # 8-byte Spill
	movq	%rdi, 624(%rsp)                 # 8-byte Spill
	orq	$4, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	movq	160(%rsp), %rdx                 # 8-byte Reload
	movq	496(%rsp), %r12                 # 8-byte Reload
	movq	200(%rsp), %rsi                 # 8-byte Reload
	je	.LBB0_350
# %bb.126:                              # %"assert succeeded355"
	movq	%rax, %rcx
	movl	%ebx, 6128(%rsp)
	movl	%r14d, 6132(%rsp)
	movl	%esi, 6136(%rsp)
	movl	%edx, 6140(%rsp)
	movl	96(%rsp), %r13d                 # 4-byte Reload
	movl	%r13d, 6144(%rsp)
	movq	1456(%rsp), %rax                # 8-byte Reload
	movq	%rax, 6152(%rsp)
	movq	32(%rbp), %rax
	movq	%rax, 6160(%rsp)
	movq	%rcx, 6168(%rsp)
	movq	$0, 6176(%rsp)
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	leaq	train_cost_model.par_for.head2_conv.s0.n.n.n(%rip), %rsi
	leaq	6128(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r12d, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_352
# %bb.127:                              # %"consume normalized_schedule_features"
	movq	%r12, %rcx
	movl	%ebx, 880(%rsp)                 # 4-byte Spill
	movl	%ebx, 6488(%rsp)
	movl	1440(%rsp), %eax                # 4-byte Reload
	movl	%eax, 6492(%rsp)
	movl	888(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 6496(%rsp)
	movl	%r14d, 6500(%rsp)
	movq	200(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 6504(%rsp)
	movq	160(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 6508(%rsp)
	movl	%r13d, 6512(%rsp)
	movq	8(%rsp), %rax                   # 8-byte Reload
	movq	%rax, 6520(%rsp)
	movq	$0, 6528(%rsp)
	movq	1448(%rsp), %rax                # 8-byte Reload
	movq	%rax, 6536(%rsp)
	movq	24(%rbp), %rax
	movq	%rax, 6544(%rsp)
	movq	64(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 6552(%rsp)
	movq	$0, 6560(%rsp)
	leaq	train_cost_model.par_for.head2_conv.s1.n.n.n(%rip), %rsi
	leaq	6488(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_353
# %bb.128:                              # %"assert succeeded359"
	movl	1944(%rsp), %eax                # 4-byte Reload
	testl	%eax, %eax
	movl	$1, %edi
	cmovgl	%eax, %edi
	movl	%edi, %esi
	subl	%r14d, %esi
	leal	-1(%rsi), %eax
	andl	$-8, %eax
	addl	%r14d, %eax
	leal	-1(%rdi), %ecx
	cmpl	%eax, %ecx
	cmovlel	%ecx, %eax
	cmpl	$9, %esi
	movl	$8, %r13d
	cmovll	%esi, %r13d
	addl	%eax, %r13d
	cmpl	%edi, %r13d
	cmovgl	%edi, %r13d
	movq	352(%rsp), %rax                 # 8-byte Reload
	cmpl	%eax, %r13d
	movl	%eax, %ebx
	cmovgel	%r13d, %ebx
	movl	24(%rsp), %eax                  # 4-byte Reload
	cmpl	%eax, %ebx
	cmovll	%eax, %ebx
	subl	%r14d, %ebx
	movl	%ebx, %eax
	sarl	$31, %eax
	andnl	%ebx, %eax, %eax
	movq	624(%rsp), %rdx                 # 8-byte Reload
	imulq	%rax, %rdx
	movl	$2147483648, %ecx               # imm = 0x80000000
	cmpq	%rcx, %rdx
	jae	.LBB0_323
# %bb.129:                              # %"assert succeeded359"
	movq	144(%rsp), %rcx                 # 8-byte Reload
	imulq	%rax, %rcx
	shrq	$32, %rcx
	imulq	%rax, %r15
	addq	%rcx, %r15
	movabsq	$545460846592, %rax             # imm = 0x7F00000000
	andq	%rax, %r15
	jne	.LBB0_323
# %bb.130:                              # %"assert succeeded361"
	movq	%rsi, 144(%rsp)                 # 8-byte Spill
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	movl	$2147483648, %r15d              # imm = 0x80000000
	orq	$4, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	movl	96(%rsp), %edx                  # 4-byte Reload
	je	.LBB0_354
# %bb.131:                              # %"assert succeeded363"
	movl	880(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 2400(%rsp)
	movl	%ebx, 624(%rsp)                 # 4-byte Spill
	movl	%ebx, 2404(%rsp)
	movq	72(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 2408(%rsp)
	movq	56(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 2412(%rsp)
	movl	%ecx, 2416(%rsp)
	movq	160(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 2420(%rsp)
	movq	%r14, 496(%rsp)                 # 8-byte Spill
	movl	%r14d, 2424(%rsp)
	movl	%edx, 2428(%rsp)
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	%rcx, 2432(%rsp)
	movq	$0, 2440(%rsp)
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	movq	%rax, 2448(%rsp)
	movq	$0, 2456(%rsp)
	leaq	train_cost_model.par_for.head2_relu.s0.c.c.c(%rip), %rsi
	leaq	2400(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	696(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movq	352(%rsp), %rsi                 # 8-byte Reload
	jne	.LBB0_356
# %bb.132:                              # %"assert succeeded365"
	movl	1616(%rsp), %ecx                # 4-byte Reload
	cmpl	%r13d, %ecx
	movl	%r13d, %eax
	cmovgel	%ecx, %eax
	movl	1824(%rsp), %ecx                # 4-byte Reload
	andl	$-8, %ecx
	movq	448(%rsp), %rdx                 # 8-byte Reload
	addl	%ecx, %edx
	cmpl	%esi, %edx
	cmovgl	%esi, %edx
	cmpl	%edx, %eax
	movq	%rdx, 448(%rsp)                 # 8-byte Spill
	cmovll	%edx, %eax
	movl	984(%rsp), %ecx                 # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movl	1464(%rsp), %ecx                # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	movl	1564(%rsp), %ecx                # 4-byte Reload
	cmpl	%ecx, %eax
	cmovll	%ecx, %eax
	cmpl	%r13d, %eax
	cmovll	%r13d, %eax
	testl	%eax, %eax
	movl	$1, %esi
	cmovgl	%eax, %esi
	movl	%esi, %ecx
	movl	1568(%rsp), %r12d               # 4-byte Reload
	subl	%r12d, %ecx
	movl	%ecx, %eax
	sarl	$31, %eax
	movl	%ecx, 200(%rsp)                 # 4-byte Spill
	andnl	%ecx, %eax, %eax
	movq	536(%rsp), %rcx                 # 8-byte Reload
	movq	%rcx, %rdx
	shlq	$7, %rdx
	movq	%rdx, %rbx
	imulq	%rax, %rbx
	cmpq	%r15, %rbx
	jae	.LBB0_324
# %bb.133:                              # %"assert succeeded365"
	movabsq	$270582939648, %rdi             # imm = 0x3F00000000
	shrq	$25, %rcx
	movl	%edx, %edx
	imulq	%rax, %rdx
	shrq	$32, %rdx
	imulq	%rax, %rcx
	addq	%rdx, %rcx
	andq	%rdi, %rcx
	jne	.LBB0_324
# %bb.134:                              # %"assert succeeded367"
	movl	%esi, 696(%rsp)                 # 4-byte Spill
	orq	$4, %rbx
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	movq	40(%rbp), %rdx
	movq	160(%rsp), %r14                 # 8-byte Reload
	je	.LBB0_357
# %bb.135:                              # %"assert succeeded369"
	movq	%rax, %r15
	movl	%r14d, %eax
	andl	$2147483644, %eax               # imm = 0x7FFFFFFC
	movl	200(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 6024(%rsp)
	movl	432(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 6028(%rsp)
	movl	%eax, 6032(%rsp)
	movl	152(%rsp), %eax                 # 4-byte Reload
	sarl	%eax
	movq	144(%rsp), %r13                 # 8-byte Reload
	movl	%r13d, 6036(%rsp)
	movl	%r12d, 6040(%rsp)
	movl	%r13d, 6044(%rsp)
	movl	%eax, 6048(%rsp)
	leal	3(%r14), %ecx
	andl	$2147483644, %ecx               # imm = 0x7FFFFFFC
	movl	%r14d, 6052(%rsp)
	movq	496(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 6056(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 6060(%rsp)
	movq	704(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 6064(%rsp)
	movq	48(%rbp), %rax
	movq	%rax, 6072(%rsp)
	movq	%r15, 6080(%rsp)
	movq	$0, 6088(%rsp)
	movq	560(%rsp), %rbx                 # 8-byte Reload
	movq	%rbx, 6096(%rsp)
	movq	%rdx, 6104(%rsp)
	movq	896(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 6112(%rsp)
	movq	$0, 6120(%rsp)
	leaq	train_cost_model.par_for.conv1_stage2.s0.c.c.c(%rip), %rsi
	leaq	6024(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	callq	halide_do_par_for@PLT
	movl	%r12d, %ecx
	movl	%eax, %r12d
	testl	%eax, %eax
	movq	%r15, %rax
	movq	%r15, 72(%rsp)                  # 8-byte Spill
	jne	.LBB0_359
# %bb.136:                              # %"consume head2_relu"
	leal	(%r14,%r14), %eax
	andl	$-4, %eax
	movl	200(%rsp), %edx                 # 4-byte Reload
	movl	%edx, 3560(%rsp)
	movl	432(%rsp), %edx                 # 4-byte Reload
	movl	%edx, 3564(%rsp)
	movl	624(%rsp), %edx                 # 4-byte Reload
	movl	%edx, 3568(%rsp)
	movl	%eax, 3572(%rsp)
	movl	%r13d, 3576(%rsp)
	movl	%ecx, 3580(%rsp)
	movl	%r13d, 3584(%rsp)
	movl	%r14d, 3588(%rsp)
	movq	496(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 3592(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 3596(%rsp)
	movl	680(%rsp), %ecx                 # 4-byte Reload
	shrl	%ecx
	shll	$2, %ecx
	movq	%r15, 3600(%rsp)
	movq	$0, 3608(%rsp)
	movq	%rbx, 3616(%rsp)
	movq	40(%rbp), %rax
	movq	%rax, 3624(%rsp)
	movq	56(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3632(%rsp)
	movq	$0, 3640(%rsp)
	leaq	train_cost_model.par_for.conv1_stage2.s1.c.c.c(%rip), %rsi
	xorl	%r15d, %r15d
	leaq	3560(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_362
# %bb.137:                              # %"assert succeeded373"
	movq	1856(%rsp), %rcx                # 8-byte Reload
	addl	672(%rsp), %ecx                 # 4-byte Folded Reload
	movq	352(%rsp), %rax                 # 8-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movq	2216(%rsp), %rax                # 8-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	movl	1616(%rsp), %eax                # 4-byte Reload
	cmpl	%eax, %ecx
	cmovll	%eax, %ecx
	testl	%ecx, %ecx
	movl	$1, %eax
	cmovgl	%ecx, %eax
	vmovdqa	128(%rsp), %xmm0                # 16-byte Reload
	vpextrd	$1, %xmm0, %r12d
	subl	%r12d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %eax
	leaq	(,%rax,4), %rdx
	cmpl	$536870912, %eax                # imm = 0x20000000
	jae	.LBB0_360
# %bb.138:                              # %"assert succeeded375"
	addq	$4, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB0_361
# %bb.139:                              # %"assert succeeded377"
	movq	%rax, %r15
	vmovaps	576(%rsp), %xmm0                # 16-byte Reload
	vextractps	$1, %xmm0, %ebx
	leal	7(%rbx), %r14d
	sarl	$3, %ebx
	movl	%ebx, 4384(%rsp)
	movl	%r12d, 4388(%rsp)
	vmovaps	288(%rsp), %xmm0                # 16-byte Reload
	vextractps	$1, %xmm0, 4392(%rsp)
	sarl	$3, %r14d
	movq	%rax, 4400(%rsp)
	movq	$0, 4408(%rsp)
	leaq	train_cost_model.par_for.f7.s0.n.n(%rip), %rsi
	leaq	4384(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r14d, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movq	%r15, 128(%rsp)                 # 8-byte Spill
	jne	.LBB0_325
# %bb.140:                              # %"consume conv1_stage2"
	vcvtsi2ssl	1948(%rsp), %xmm1, %xmm0 # 4-byte Folded Reload
	movl	200(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3648(%rsp)
	movq	40(%rsp), %r13                  # 8-byte Reload
	movl	%r13d, 3652(%rsp)
	movq	1344(%rsp), %rax                # 8-byte Reload
	movl	%eax, 3656(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 3660(%rsp)
	movl	%ebx, 3664(%rsp)
	vmovapd	288(%rsp), %xmm1                # 16-byte Reload
	vextractps	$1, %xmm1, %eax
	movl	%r12d, 536(%rsp)                # 4-byte Spill
	movl	%r12d, 3668(%rsp)
	movl	88(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 3672(%rsp)
	movl	1568(%rsp), %ecx                # 4-byte Reload
	movl	%ecx, 3676(%rsp)
	vmovaps	%xmm0, 1616(%rsp)               # 16-byte Spill
	vmovss	%xmm0, 3680(%rsp)
	movl	%eax, 3684(%rsp)
	movq	72(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3688(%rsp)
	movq	$0, 3696(%rsp)
	movq	%r15, 3704(%rsp)
	movq	$0, 3712(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3720(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 3728(%rsp)
	leaq	train_cost_model.par_for.f7.s1.n.n(%rip), %rsi
	leaq	3648(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r14d, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_325
# %bb.141:                              # %"produce sum$1_1_d_def__"
	movl	$1065353216, 7328(%rsp)         # imm = 0x3F800000
	movq	352(%rsp), %r8                  # 8-byte Reload
	leal	7(%r8), %esi
	movl	%esi, %edx
	sarl	$3, %edx
	leal	1(%r13), %r12d
	sarl	%r12d
	imull	%edx, %r12d
	leal	-1(%r12), %eax
	movl	%eax, %edi
	sarl	$31, %edi
	xorl	%ecx, %ecx
	testl	%edx, %edx
	sete	%cl
	subl	%edi, %eax
	movq	%rdx, 576(%rsp)                 # 8-byte Spill
	leal	(%rdx,%rcx), %ebx
	cltd
	idivl	%ebx
	movq	%r8, %rbx
	sarl	$31, %esi
	movl	%esi, %edx
	notl	%edx
	subl	%esi, %edx
	andl	%edi, %edx
	addl	%eax, %edx
	decl	%ecx
	andl	%edx, %ecx
	testl	%ebx, %ebx
	movl	%ecx, %r15d
	movl	$0, %eax
	cmovgl	%eax, %r15d
	cmovlel	%eax, %ecx
	leal	(%r15,%r15), %r14d
	movl	%r13d, %eax
	subl	%r14d, %eax
	cmpl	$3, %eax
	movl	$2, %edx
	cmovll	%eax, %edx
	leal	-1(%r13), %eax
	sarl	%eax
	cmpl	%ecx, %eax
	cmovgl	%ecx, %eax
	leal	(%rdx,%rax,2), %eax
	cmpl	%r13d, %eax
	cmovgl	%r13d, %eax
	subl	%r14d, %eax
	movl	224(%rsp), %edx                 # 4-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ecx
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %eax
	movq	%rax, %rsi
	imulq	%rcx, %rsi
	leaq	(,%rcx,4), %rdx
	cmpq	$536870911, %rsi                # imm = 0x1FFFFFFF
	ja	.LBB0_326
# %bb.142:                              # %"produce sum$1_1_d_def__"
	movabsq	$4294967296, %r8                # imm = 0x100000000
	shrq	$30, %rcx
	movl	%edx, %edi
	imulq	%rax, %rdi
	shrq	$32, %rdi
	imulq	%rax, %rcx
	addq	%rdi, %rcx
	andq	%r8, %rcx
	jne	.LBB0_326
# %bb.143:                              # %"assert succeeded383"
	leaq	4(,%rsi,4), %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	movq	128(%rsp), %rdi                 # 8-byte Reload
	movl	536(%rsp), %edx                 # 4-byte Reload
	je	.LBB0_363
# %bb.144:                              # %"assert succeeded385"
	movq	%rax, %r13
	movl	64(%rbp), %ecx
	cmpl	%ecx, %ebx
	cmovlel	1824(%rsp), %ecx                # 4-byte Folded Reload
	movl	%ecx, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %esi
	movq	640(%rsp), %rax                 # 8-byte Reload
	subl	%eax, %esi
	movl	%ebx, 6352(%rsp)
	movl	224(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 6356(%rsp)
	movl	%r14d, 6360(%rsp)
	movq	40(%rsp), %rbx                  # 8-byte Reload
	movl	%ebx, 6364(%rsp)
	movq	576(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 6368(%rsp)
	movl	%esi, 848(%rsp)                 # 4-byte Spill
	movl	%esi, 6372(%rsp)
	movl	%edx, 6376(%rsp)
	movl	%eax, 6380(%rsp)
	movq	%r13, 6384(%rsp)
	movq	$0, 6392(%rsp)
	movq	%rdi, 6400(%rsp)
	movq	$0, 6408(%rsp)
	leaq	7328(%rsp), %rax
	movq	%rax, 6416(%rsp)
	movq	$0, 6424(%rsp)
	movq	712(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 6432(%rsp)
	movq	72(%rbp), %rax
	movq	%rax, 6440(%rsp)
	leaq	train_cost_model.par_for.f6_0_d_def__.s0.n.n.n(%rip), %rsi
	leaq	6352(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	%r12, 288(%rsp)                 # 8-byte Spill
	movl	%r12d, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_365
# %bb.145:                              # %"assert succeeded387"
	movl	%r15d, %eax
	sarl	$31, %eax
	andl	%r15d, %eax
	addl	%eax, %eax
	movl	%ebx, %ecx
	subl	%eax, %ecx
	movq	448(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andnl	%edx, %eax, %edi
	movl	%ecx, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %eax
	movq	%rdi, %rsi
	imulq	%rax, %rsi
	shlq	$7, %rsi
	movq	%rax, %rdx
	shlq	$7, %rdx
	movl	$2147483648, %ecx               # imm = 0x80000000
	cmpq	%rcx, %rsi
	jae	.LBB0_327
# %bb.146:                              # %"assert succeeded387"
	shrq	$25, %rax
	movl	%edx, %ecx
	imulq	%rdi, %rcx
	shrq	$32, %rcx
	imulq	%rdi, %rax
	addq	%rcx, %rax
	movabsq	$270582939648, %rcx             # imm = 0x3F00000000
	andq	%rcx, %rax
	jne	.LBB0_327
# %bb.147:                              # %"assert succeeded389"
	movq	%rbx, %r15
	movq	%rdi, 528(%rsp)                 # 8-byte Spill
	orq	$4, %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	testq	%rax, %rax
	movq	576(%rsp), %rcx                 # 8-byte Reload
	movq	288(%rsp), %rbx                 # 8-byte Reload
	je	.LBB0_367
# %bb.148:                              # %"assert succeeded391"
	movl	%r14d, %eax
	sarl	$31, %r14d
	movl	%eax, 160(%rsp)                 # 4-byte Spill
	andl	%eax, %r14d
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 2800(%rsp)
	movl	%r15d, 2804(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 2808(%rsp)
	movl	%ecx, 2812(%rsp)
	movl	%r14d, 2816(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2824(%rsp)
	movq	$0, 2832(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n(%rip), %rsi
	leaq	2800(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%ebx, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_314
# %bb.149:                              # %"consume f6_0_d_def__"
	movq	%rbx, %rcx
	movq	1344(%rsp), %r15                # 8-byte Reload
	leal	(%r15,%r15,4), %eax
	leal	(%r15,%rax,4), %eax
	movl	%eax, %edi
	movq	352(%rsp), %rsi                 # 8-byte Reload
	addl	%esi, %eax
	subl	688(%rsp), %eax                 # 4-byte Folded Reload
	subl	272(%rsp), %edi                 # 4-byte Folded Reload
	subl	48(%rsp), %edi                  # 4-byte Folded Reload
	movq	208(%rsp), %rdx                 # 8-byte Reload
	subl	%edx, %edi
	movl	%esi, 3736(%rsp)
	movl	224(%rsp), %esi                 # 4-byte Reload
	movl	%esi, 3740(%rsp)
	movl	160(%rsp), %esi                 # 4-byte Reload
	movl	%esi, 3744(%rsp)
	movq	40(%rsp), %rsi                  # 8-byte Reload
	movl	%esi, 3748(%rsp)
	movq	448(%rsp), %rsi                 # 8-byte Reload
	movl	%esi, 3752(%rsp)
	movl	36(%rsp), %esi                  # 4-byte Reload
	movl	%esi, 3756(%rsp)
	movq	576(%rsp), %rbx                 # 8-byte Reload
	movl	%ebx, 3760(%rsp)
	movl	%r14d, 3764(%rsp)
	movl	%edi, 3768(%rsp)
	subl	%edx, %eax
	movl	%eax, 3772(%rsp)
	movq	%r13, 3776(%rsp)
	movq	$0, 3784(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3792(%rsp)
	movq	$0, 3800(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3808(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 3816(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n(%rip), %rsi
	leaq	3736(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movq	%r13, 24(%rsp)                  # 8-byte Spill
	jne	.LBB0_315
# %bb.150:                              # %"consume f6_0_d_def__396"
	movq	%rbx, %rcx
	leal	(%r15,%r15,8), %ebx
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4416(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4420(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4424(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 4428(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4432(%rsp)
	movl	%r15d, 4436(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4440(%rsp)
	movl	%ecx, 4444(%rsp)
	movl	%r14d, 4448(%rsp)
	movl	88(%rsp), %r13d                 # 4-byte Reload
	movl	%r13d, 4452(%rsp)
	movl	%ebx, 4456(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4464(%rsp)
	movq	$0, 4472(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4480(%rsp)
	movq	$0, 4488(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4496(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 4504(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n(%rip), %rsi
	leaq	4416(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.151:                              # %"consume f6_0_d_def__399"
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4512(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4516(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4520(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 4524(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4528(%rsp)
	movl	%r15d, 4532(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4536(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4540(%rsp)
	movl	%r14d, 4544(%rsp)
	movl	%r13d, 4548(%rsp)
	movl	%ebx, 152(%rsp)                 # 4-byte Spill
	movl	%ebx, 4552(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4560(%rsp)
	movq	$0, 4568(%rsp)
	movq	96(%rsp), %r15                  # 8-byte Reload
	movq	%r15, 4576(%rsp)
	movq	$0, 4584(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4592(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 4600(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n(%rip), %rsi
	leaq	4512(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rbx                 # 8-byte Reload
	movl	%ebx, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.152:                              # %"consume f6_0_d_def__402"
	movq	%r15, %rsi
	movq	%rbx, %r9
	movq	352(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, %eax
	subl	272(%rsp), %eax                 # 4-byte Folded Reload
	subl	48(%rsp), %eax                  # 4-byte Folded Reload
	subl	208(%rsp), %eax                 # 4-byte Folded Reload
	movl	%ecx, 3824(%rsp)
	movl	224(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 3828(%rsp)
	movl	160(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 3832(%rsp)
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 3836(%rsp)
	movq	448(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 3840(%rsp)
	movl	36(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 3844(%rsp)
	movq	576(%rsp), %r15                 # 8-byte Reload
	movl	%r15d, 3848(%rsp)
	movl	%r14d, 3852(%rsp)
	movl	%r13d, 3856(%rsp)
	movl	%eax, 3860(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3864(%rsp)
	movq	$0, 3872(%rsp)
	movq	%rsi, 3880(%rsp)
	movq	$0, 3888(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3896(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 3904(%rsp)
	movq	%rsi, %rbx
	leaq	train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n(%rip), %rsi
	leaq	3824(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r9d, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movl	%r14d, 48(%rsp)                 # 4-byte Spill
	jne	.LBB0_313
# %bb.153:                              # %"consume f6_0_d_def__405"
	movq	%rbx, %rsi
	movq	%r15, %rcx
	movq	1344(%rsp), %r13                # 8-byte Reload
	leal	(%r13,%r13), %eax
	movq	%rax, 480(%rsp)                 # 8-byte Spill
	leal	(%rax,%rax,4), %r14d
	movl	%r13d, %r15d
	shll	$5, %r15d
	leal	(%r15,%r13,2), %eax
	movq	352(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, 2840(%rsp)
	movl	224(%rsp), %edx                 # 4-byte Reload
	movl	%edx, 2844(%rsp)
	movl	160(%rsp), %edx                 # 4-byte Reload
	movl	%edx, 2848(%rsp)
	movq	40(%rsp), %rdx                  # 8-byte Reload
	movl	%edx, 2852(%rsp)
	movq	448(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, 2856(%rsp)
	movl	36(%rsp), %edx                  # 4-byte Reload
	movl	%edx, 2860(%rsp)
	movl	%ecx, 2864(%rsp)
	movl	48(%rsp), %edx                  # 4-byte Reload
	movl	%edx, 2868(%rsp)
	movl	88(%rsp), %edx                  # 4-byte Reload
	movl	%edx, 2872(%rsp)
	movl	152(%rsp), %ebx                 # 4-byte Reload
	movl	%ebx, 2876(%rsp)
	movl	%r14d, 2880(%rsp)
	movl	%eax, 680(%rsp)                 # 4-byte Spill
	movl	%eax, 2884(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2888(%rsp)
	movq	$0, 2896(%rsp)
	movq	%rsi, 2904(%rsp)
	movq	$0, 2912(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2920(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 2928(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n(%rip), %rsi
	leaq	2840(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.154:                              # %"consume f6_0_d_def__408"
	leal	(%r15,%r13), %eax
	movq	352(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 2936(%rsp)
	movl	224(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 2940(%rsp)
	movl	160(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 2944(%rsp)
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 2948(%rsp)
	movq	448(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 2952(%rsp)
	movl	36(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 2956(%rsp)
	movq	576(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 2960(%rsp)
	movl	48(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 2964(%rsp)
	movl	88(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 2968(%rsp)
	movl	%ebx, 2972(%rsp)
	movl	%r14d, 488(%rsp)                # 4-byte Spill
	movl	%r14d, 2976(%rsp)
	movl	%eax, 856(%rsp)                 # 4-byte Spill
	movl	%eax, 2980(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2984(%rsp)
	movq	$0, 2992(%rsp)
	movq	96(%rsp), %r14                  # 8-byte Reload
	movq	%r14, 3000(%rsp)
	movq	$0, 3008(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3016(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 3024(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n(%rip), %rsi
	leaq	2936(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.155:                              # %"consume f6_0_d_def__411"
	leal	(,%r13,8), %eax
	leal	(%rax,%rax,2), %eax
	movq	352(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 4608(%rsp)
	movl	224(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 4612(%rsp)
	movl	160(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 4616(%rsp)
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 4620(%rsp)
	movq	448(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 4624(%rsp)
	movl	36(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 4628(%rsp)
	movq	576(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 4632(%rsp)
	movl	48(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 4636(%rsp)
	movl	88(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 4640(%rsp)
	movl	%eax, 144(%rsp)                 # 4-byte Spill
	movl	%eax, 4644(%rsp)
	movl	%r15d, 4648(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4656(%rsp)
	movq	$0, 4664(%rsp)
	movq	%r14, 4672(%rsp)
	movq	$0, 4680(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4688(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 4696(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n(%rip), %rsi
	leaq	4608(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.156:                              # %"consume f6_0_d_def__414"
	movq	%r15, %rbx
	leal	(%r13,%r13,4), %eax
	leal	(%rax,%rax,4), %eax
	movq	352(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 4704(%rsp)
	movl	224(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 4708(%rsp)
	movl	160(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 4712(%rsp)
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 4716(%rsp)
	movq	448(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 4720(%rsp)
	movl	36(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 4724(%rsp)
	movq	576(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 4728(%rsp)
	movl	48(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 4732(%rsp)
	movl	88(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 4736(%rsp)
	movl	%eax, 216(%rsp)                 # 4-byte Spill
	movl	%eax, 4740(%rsp)
	movl	%ebx, 4744(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4752(%rsp)
	movq	$0, 4760(%rsp)
	movq	%r14, 4768(%rsp)
	movq	$0, 4776(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4784(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 4792(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n(%rip), %rsi
	leaq	4704(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.157:                              # %"consume f6_0_d_def__417"
	movl	%ebx, %r15d
	subl	%r13d, %r15d
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4800(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4804(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4808(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 4812(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4816(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4820(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4824(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4828(%rsp)
	movl	88(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4832(%rsp)
	movl	144(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4836(%rsp)
	movl	%r15d, 4840(%rsp)
	movq	24(%rsp), %r13                  # 8-byte Reload
	movq	%r13, 4848(%rsp)
	movq	$0, 4856(%rsp)
	movq	%r14, 4864(%rsp)
	movq	$0, 4872(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4880(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 4888(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n(%rip), %rsi
	leaq	4800(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_314
# %bb.158:                              # %"consume f6_0_d_def__420"
	movq	%rbx, 688(%rsp)                 # 8-byte Spill
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4896(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4900(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4904(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 4908(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4912(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4916(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4920(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4924(%rsp)
	movl	88(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4928(%rsp)
	movl	216(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4932(%rsp)
	movl	%r15d, 4936(%rsp)
	movq	%r13, 4944(%rsp)
	movq	$0, 4952(%rsp)
	movq	%r14, 4960(%rsp)
	movq	$0, 4968(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4976(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 4984(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n(%rip), %rsi
	leaq	4896(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_314
# %bb.159:                              # %"consume f6_0_d_def__423"
	movq	1344(%rsp), %rbx                # 8-byte Reload
	leal	(%rbx,%rbx,8), %eax
	leal	(%rax,%rax,2), %ecx
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4992(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4996(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5000(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 5004(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5008(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5012(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5016(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5020(%rsp)
	movl	88(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5024(%rsp)
	movl	144(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5028(%rsp)
	movl	%ecx, 968(%rsp)                 # 4-byte Spill
	movl	%ecx, 5032(%rsp)
	movq	%r13, 5040(%rsp)
	movq	$0, 5048(%rsp)
	movq	%r14, 5056(%rsp)
	movq	$0, 5064(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5072(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 5080(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n(%rip), %rsi
	leaq	4992(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_314
# %bb.160:                              # %"consume f6_0_d_def__426"
	movl	%r15d, 208(%rsp)                # 4-byte Spill
	leal	(%rbx,%rbx,8), %eax
	leal	(%rax,%rax,2), %ecx
	addl	%ebx, %ecx
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5088(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5092(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5096(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 5100(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5104(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5108(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5112(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5116(%rsp)
	movl	88(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5120(%rsp)
	movl	216(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5124(%rsp)
	movl	%ecx, 272(%rsp)                 # 4-byte Spill
	movl	%ecx, 5128(%rsp)
	movq	%r13, 5136(%rsp)
	movq	$0, 5144(%rsp)
	movq	%r14, 5152(%rsp)
	movq	$0, 5160(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5168(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 5176(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n(%rip), %rsi
	leaq	5088(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_314
# %bb.161:                              # %"consume f6_0_d_def__429"
	leal	(%rbx,%rbx,4), %eax
	leal	(%rax,%rax,4), %ecx
	addl	%ebx, %ecx
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5184(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5188(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5192(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 5196(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5200(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5204(%rsp)
	movq	576(%rsp), %r15                 # 8-byte Reload
	movl	%r15d, 5208(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5212(%rsp)
	movl	88(%rsp), %ebx                  # 4-byte Reload
	movl	%ebx, 5216(%rsp)
	movl	144(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5220(%rsp)
	movl	%ecx, 952(%rsp)                 # 4-byte Spill
	movl	%ecx, 5224(%rsp)
	movq	%r13, 5232(%rsp)
	movq	$0, 5240(%rsp)
	movq	%r14, 5248(%rsp)
	movq	$0, 5256(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5264(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 5272(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n(%rip), %rsi
	leaq	5184(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.162:                              # %"consume f6_0_d_def__432"
	movl	%ebx, %r13d
	movq	480(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%rax,8), %eax
	movq	352(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 3912(%rsp)
	movl	224(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 3916(%rsp)
	movl	160(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 3920(%rsp)
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 3924(%rsp)
	movq	448(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 3928(%rsp)
	movl	36(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 3932(%rsp)
	movl	%r15d, 3936(%rsp)
	movl	48(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 3940(%rsp)
	movl	%ebx, 3944(%rsp)
	movl	%eax, 832(%rsp)                 # 4-byte Spill
	movl	%eax, 3948(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3952(%rsp)
	movq	$0, 3960(%rsp)
	movq	%r14, 3968(%rsp)
	movq	$0, 3976(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3984(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 3992(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n(%rip), %rsi
	leaq	3912(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movq	%r15, %r14
	movl	208(%rsp), %r15d                # 4-byte Reload
	jne	.LBB0_313
# %bb.163:                              # %"consume f6_0_d_def__435"
	movl	%r13d, %edx
	movq	1344(%rsp), %rbx                # 8-byte Reload
	leal	(%rbx,%rbx,8), %eax
	leal	(%rbx,%rax,2), %eax
	movq	352(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 4000(%rsp)
	movl	224(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 4004(%rsp)
	movl	160(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 4008(%rsp)
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 4012(%rsp)
	movq	448(%rsp), %r13                 # 8-byte Reload
	movl	%r13d, 4016(%rsp)
	movl	36(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 4020(%rsp)
	movl	%r14d, 4024(%rsp)
	movl	48(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 4028(%rsp)
	movl	%edx, 4032(%rsp)
	movl	%eax, 824(%rsp)                 # 4-byte Spill
	movl	%eax, 4036(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4040(%rsp)
	movq	$0, 4048(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4056(%rsp)
	movq	$0, 4064(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4072(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 4080(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n(%rip), %rsi
	leaq	4000(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_315
# %bb.164:                              # %"consume f6_0_d_def__438"
	movq	%r13, %rdx
	leal	(,%rbx,4), %eax
	leal	(%rax,%rax,2), %ecx
	subl	%ebx, %r15d
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4176(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4180(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4184(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 4188(%rsp)
	movl	%edx, 4192(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4196(%rsp)
	movl	%r14d, 4200(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4204(%rsp)
	movl	88(%rsp), %ebx                  # 4-byte Reload
	movl	%ebx, 4208(%rsp)
	movl	152(%rsp), %r13d                # 4-byte Reload
	movl	%r13d, 4212(%rsp)
	movl	488(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4216(%rsp)
	movl	%ecx, 864(%rsp)                 # 4-byte Spill
	movl	%ecx, 4220(%rsp)
	movl	%r15d, 4224(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4232(%rsp)
	movq	$0, 4240(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4248(%rsp)
	movq	$0, 4256(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4264(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 4272(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n(%rip), %rsi
	leaq	4176(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.165:                              # %"consume f6_0_d_def__441"
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4280(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4284(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4288(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 4292(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4296(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4300(%rsp)
	movl	%r14d, 4304(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4308(%rsp)
	movl	%ebx, 4312(%rsp)
	movl	%r13d, 4316(%rsp)
	movl	144(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4320(%rsp)
	movl	216(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4324(%rsp)
	movl	%r15d, 664(%rsp)                # 4-byte Spill
	movl	%r15d, 4328(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4336(%rsp)
	movq	$0, 4344(%rsp)
	movq	96(%rsp), %rbx                  # 8-byte Reload
	movq	%rbx, 4352(%rsp)
	movq	$0, 4360(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4368(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 4376(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n(%rip), %rsi
	leaq	4280(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.166:                              # %"consume f6_0_d_def__444"
	movq	%r14, %r13
	movq	%rbx, %rdx
	movq	1344(%rsp), %rcx                # 8-byte Reload
	leal	(%rcx,%rcx,4), %eax
	leal	(%rcx,%rax,2), %r15d
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5280(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5284(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5288(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 5292(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5296(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5300(%rsp)
	movl	%r13d, 5304(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5308(%rsp)
	movl	88(%rsp), %ebx                  # 4-byte Reload
	movl	%ebx, 5312(%rsp)
	movl	152(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5316(%rsp)
	movl	%r15d, 5320(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5328(%rsp)
	movq	$0, 5336(%rsp)
	movq	%rdx, 5344(%rsp)
	movq	$0, 5352(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5360(%rsp)
	movq	320(%rsp), %r14                 # 8-byte Reload
	movq	%r14, 5368(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n(%rip), %rsi
	leaq	5280(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.167:                              # %"consume f6_0_d_def__447"
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4088(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4092(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4096(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 4100(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 4104(%rsp)
	movl	%r13d, 4108(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 4112(%rsp)
	movl	%ebx, 4116(%rsp)
	movl	152(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 4120(%rsp)
	movl	%r15d, 4124(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4128(%rsp)
	movq	$0, 4136(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4144(%rsp)
	movq	$0, 4152(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 4160(%rsp)
	movq	%r14, 4168(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n(%rip), %rsi
	leaq	4088(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.168:                              # %"consume f6_0_d_def__450"
	movl	%ebx, %r14d
	movl	%r15d, %eax
	movq	%r13, %rbx
	movq	352(%rsp), %r15                 # 8-byte Reload
	movl	%r15d, 5376(%rsp)
	movl	224(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 5380(%rsp)
	movl	160(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 5384(%rsp)
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 5388(%rsp)
	movq	448(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 5392(%rsp)
	movl	36(%rsp), %r13d                 # 4-byte Reload
	movl	%r13d, 5396(%rsp)
	movl	%ebx, 5400(%rsp)
	movl	48(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 5404(%rsp)
	movl	%r14d, 5408(%rsp)
	movl	152(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 5412(%rsp)
	movl	%eax, 840(%rsp)                 # 4-byte Spill
	movl	%eax, 5416(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5424(%rsp)
	movq	$0, 5432(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5440(%rsp)
	movq	$0, 5448(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5456(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 5464(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n(%rip), %rsi
	leaq	5376(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.169:                              # %"consume f6_0_d_def__453"
	movq	1344(%rsp), %rcx                # 8-byte Reload
	leal	(%rcx,%rcx,8), %eax
	leal	(%rax,%rax,2), %ebx
	addl	%ecx, %ebx
	addl	%ecx, %ebx
	movl	%r15d, 3032(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3036(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3040(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 3044(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 3048(%rsp)
	movl	%r13d, 3052(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 3056(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 3060(%rsp)
	movl	%r14d, 3064(%rsp)
	movl	152(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3068(%rsp)
	movl	%ebx, 3072(%rsp)
	movl	%r14d, %r13d
	movl	664(%rsp), %r14d                # 4-byte Reload
	movl	%r14d, 3076(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3080(%rsp)
	movq	$0, 3088(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3096(%rsp)
	movq	$0, 3104(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3112(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 3120(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n(%rip), %rsi
	leaq	3032(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.170:                              # %"consume f6_0_d_def__456"
	movl	%r15d, 5472(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5476(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5480(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 5484(%rsp)
	movq	448(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5488(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 5492(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 5496(%rsp)
	movl	%r13d, 5500(%rsp)
	movl	152(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5504(%rsp)
	movl	%ebx, 5508(%rsp)
	movl	%r14d, 5512(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5520(%rsp)
	movq	$0, 5528(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5536(%rsp)
	movq	$0, 5544(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 5552(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 5560(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n(%rip), %rsi
	leaq	5472(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.171:                              # %"consume f6_0_d_def__459"
	movl	%r15d, 3128(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3132(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3136(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 3140(%rsp)
	movq	448(%rsp), %r13                 # 8-byte Reload
	movl	%r13d, 3144(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 3148(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 3152(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 3156(%rsp)
	movl	88(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 3160(%rsp)
	movl	152(%rsp), %r15d                # 4-byte Reload
	movl	%r15d, 3164(%rsp)
	movl	%ebx, 976(%rsp)                 # 4-byte Spill
	movl	%ebx, 3168(%rsp)
	movl	%r14d, 3172(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3176(%rsp)
	movq	$0, 3184(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3192(%rsp)
	movq	$0, 3200(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3208(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 3216(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n(%rip), %rsi
	leaq	3128(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.172:                              # %"consume f6_0_d_def__462"
	movq	%r13, %rcx
	movq	1344(%rsp), %r13                # 8-byte Reload
	movl	%r13d, %ebx
	shll	$4, %ebx
	addl	%r13d, %ebx
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 2592(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 2596(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 2600(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 2604(%rsp)
	movl	%ecx, 2608(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 2612(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 2616(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 2620(%rsp)
	movl	88(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 2624(%rsp)
	vmovaps	1616(%rsp), %xmm0               # 16-byte Reload
	vmovss	%xmm0, 2628(%rsp)
	movl	%r15d, 2632(%rsp)
	movl	488(%rsp), %r14d                # 4-byte Reload
	movl	%r14d, 2636(%rsp)
	movl	%ebx, 2640(%rsp)
	movl	216(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 2644(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2648(%rsp)
	movq	$0, 2656(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2664(%rsp)
	movq	$0, 2672(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2680(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 2688(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n(%rip), %rsi
	leaq	2592(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_313
# %bb.173:                              # %"consume f6_0_d_def__465"
	leal	(%r13,%r13,4), %eax
	leal	(%r13,%rax,4), %eax
	movq	352(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 3224(%rsp)
	movl	224(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 3228(%rsp)
	movl	160(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 3232(%rsp)
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 3236(%rsp)
	movq	448(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 3240(%rsp)
	movl	36(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 3244(%rsp)
	movq	576(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 3248(%rsp)
	movl	48(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 3252(%rsp)
	movl	88(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 3256(%rsp)
	vmovaps	1616(%rsp), %xmm0               # 16-byte Reload
	vmovss	%xmm0, 3260(%rsp)
	addl	%r13d, %eax
	movl	%r15d, 3264(%rsp)
	movl	%r14d, 3268(%rsp)
	movl	%ebx, 3272(%rsp)
	movl	%eax, 480(%rsp)                 # 4-byte Spill
	movl	%eax, 3276(%rsp)
	movl	144(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3280(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3288(%rsp)
	movq	$0, 3296(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3304(%rsp)
	movq	$0, 3312(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3320(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 3328(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n(%rip), %rsi
	leaq	3224(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_315
# %bb.174:                              # %"consume f6_0_d_def__468"
	movl	%ebx, %r13d
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 2696(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 2700(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 2704(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 2708(%rsp)
	movq	448(%rsp), %rbx                 # 8-byte Reload
	movl	%ebx, 2712(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 2716(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 2720(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 2724(%rsp)
	movl	88(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 2728(%rsp)
	vmovaps	1616(%rsp), %xmm0               # 16-byte Reload
	vmovss	%xmm0, 2732(%rsp)
	movl	%r15d, 2736(%rsp)
	movl	%r14d, 2740(%rsp)
	movl	%r13d, 2744(%rsp)
	movl	216(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 2748(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2752(%rsp)
	movq	$0, 2760(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2768(%rsp)
	movq	$0, 2776(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2784(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 2792(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n(%rip), %rsi
	leaq	2696(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_315
# %bb.175:                              # %"consume f6_0_d_def__471"
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 3336(%rsp)
	movl	224(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3340(%rsp)
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3344(%rsp)
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 3348(%rsp)
	movl	%ebx, 3352(%rsp)
	movl	36(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 3356(%rsp)
	movq	576(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 3360(%rsp)
	movl	48(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 3364(%rsp)
	movl	88(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 3368(%rsp)
	vmovaps	1616(%rsp), %xmm0               # 16-byte Reload
	vmovss	%xmm0, 3372(%rsp)
	movl	%r15d, 3376(%rsp)
	movl	%r14d, 3380(%rsp)
	movl	%r13d, 3384(%rsp)
	movl	480(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3388(%rsp)
	movl	144(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3392(%rsp)
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3400(%rsp)
	movq	$0, 3408(%rsp)
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3416(%rsp)
	movq	$0, 3424(%rsp)
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 3432(%rsp)
	movq	320(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 3440(%rsp)
	leaq	train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n(%rip), %rsi
	xorl	%r15d, %r15d
	leaq	3336(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_369
# %bb.176:                              # %call_destructor.exit880
	movl	%r13d, 872(%rsp)                # 4-byte Spill
	movq	528(%rsp), %rbx                 # 8-byte Reload
	movl	%ebx, %r14d
	xorl	%edi, %edi
	movq	24(%rsp), %rsi                  # 8-byte Reload
	callq	halide_free@PLT
	shlq	$2, %rbx
	cmpl	$536870912, %r14d               # imm = 0x20000000
	leaq	7328(%rsp), %r14
	jae	.LBB0_370
# %bb.177:                              # %"assert succeeded475"
	addq	$4, %rbx
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	movq	352(%rsp), %rbx                 # 8-byte Reload
	je	.LBB0_371
# %bb.178:                              # %"assert succeeded477"
	movq	576(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ecx
	movl	%ebx, %edx
	sarl	$3, %edx
	movl	%ebx, %esi
	sarl	$31, %esi
	movl	%esi, 288(%rsp)                 # 4-byte Spill
	andnl	%edx, %esi, %edx
	movl	%ebx, 6568(%rsp)
	movl	%edx, 24(%rsp)                  # 4-byte Spill
	movl	%edx, 6572(%rsp)
	movq	%r14, 6576(%rsp)
	movq	$0, 6584(%rsp)
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	movq	%rax, 6592(%rsp)
	movq	$0, 6600(%rsp)
	leaq	train_cost_model.par_for.sum_1_d_def__.s0.n.n(%rip), %rsi
	leaq	6568(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%ecx, 1344(%rsp)                # 4-byte Spill
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movq	40(%rsp), %rcx                  # 8-byte Reload
	jne	.LBB0_374
# %bb.179:                              # %"assert succeeded480"
	cmpl	$3, %ebx
	movl	$2, %eax
	cmovll	%ebx, %eax
	movl	1824(%rsp), %edx                # 4-byte Reload
	andl	$-2, %edx
	addl	%eax, %edx
	cmpl	%ebx, %edx
	cmovll	%ebx, %edx
	movq	448(%rsp), %rax                 # 8-byte Reload
	cmpl	%eax, %edx
	cmovll	%eax, %edx
	movl	%ecx, %r15d
	sarl	$31, %r15d
	andnl	%ecx, %r15d, %r12d
	movl	%edx, %eax
	sarl	$31, %eax
	movl	%edx, 1824(%rsp)                # 4-byte Spill
	andnl	%edx, %eax, %eax
	movq	%r12, %rbx
	shlq	$7, %rbx
	movq	%rbx, %rdx
	imulq	%rax, %rdx
	movl	$2147483648, %ecx               # imm = 0x80000000
	cmpq	%rcx, %rdx
	movq	96(%rsp), %r13                  # 8-byte Reload
	jae	.LBB0_328
# %bb.180:                              # %"assert succeeded480"
	movq	%r12, %rcx
	shrq	$25, %rcx
	movl	%ebx, %esi
	imulq	%rax, %rsi
	shrq	$32, %rsi
	imulq	%rax, %rcx
	addq	%rsi, %rcx
	movabsq	$270582939648, %rax             # imm = 0x3F00000000
	andq	%rax, %rcx
	jne	.LBB0_328
# %bb.181:                              # %"assert succeeded482"
	orq	$4, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	movl	1568(%rsp), %r8d                # 4-byte Reload
	movl	200(%rsp), %esi                 # 4-byte Reload
	movl	24(%rsp), %edx                  # 4-byte Reload
	movl	48(%rsp), %r9d                  # 4-byte Reload
	je	.LBB0_375
# %bb.182:                              # %"assert succeeded484"
	movq	576(%rsp), %rdi                 # 8-byte Reload
	subl	%edx, %edi
	movl	%edi, %ecx
	sarl	$31, %ecx
	andnl	%edi, %ecx, %edi
	movq	352(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 6184(%rsp)
	movl	%esi, 6188(%rsp)
	movl	1824(%rsp), %ecx                # 4-byte Reload
	movl	%ecx, 6192(%rsp)
	movq	40(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 6196(%rsp)
	movq	448(%rsp), %rsi                 # 8-byte Reload
	movl	%esi, 6200(%rsp)
	movl	%r15d, %esi
	andl	%ecx, %esi
	movl	%esi, 576(%rsp)                 # 4-byte Spill
	movl	%esi, 6204(%rsp)
	movl	%r12d, 6208(%rsp)
	movl	%r8d, 6212(%rsp)
	movl	%r9d, 6216(%rsp)
	movl	%edi, 6220(%rsp)
	movl	%edx, 6224(%rsp)
	movq	72(%rsp), %rcx                  # 8-byte Reload
	movq	%rcx, 6232(%rsp)
	movq	$0, 6240(%rsp)
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	movq	%rax, 6248(%rsp)
	movq	$0, 6256(%rsp)
	movq	%r13, 6264(%rsp)
	movq	$0, 6272(%rsp)
	movq	320(%rsp), %r14                 # 8-byte Reload
	movq	%r14, 6280(%rsp)
	movq	$0, 6288(%rsp)
	leaq	train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c(%rip), %rsi
	leaq	6184(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$16, %ecx
	callq	halide_do_par_for@PLT
                                        # kill: def $eax killed $eax def $rax
	testl	%eax, %eax
	jne	.LBB0_377
# %bb.183:                              # %call_destructor.exit882
	xorl	%edi, %edi
	movq	%r13, %rsi
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	%r14, %rsi
	callq	halide_free@PLT
	cmpl	$16777216, %r12d                # imm = 0x1000000
	jae	.LBB0_378
# %bb.184:                              # %"assert succeeded489"
	movq	%r12, 48(%rsp)                  # 8-byte Spill
	orq	$4, %rbx
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB0_379
# %bb.185:                              # %"assert succeeded491"
	movq	%rax, %r13
	movq	40(%rsp), %rbx                  # 8-byte Reload
	movl	%ebx, %eax
	sarl	$3, %eax
	andnl	%eax, %r15d, %r15d
	leal	7(%rbx), %eax
	sarl	$3, %eax
	subl	%r15d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %r14d
	movl	%ebx, 5568(%rsp)
	movl	%r14d, 5572(%rsp)
	movl	%r15d, 5576(%rsp)
	movq	%r13, 5584(%rsp)
	movq	$0, 5592(%rsp)
	leaq	train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c(%rip), %rsi
	leaq	5568(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_381
# %bb.186:                              # %"consume conv1_stage2_1_d_def__"
	movq	352(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %eax
	movl	%eax, %ecx
	sarl	%ecx
	movl	%edi, %edx
	sarl	%edx
	movl	288(%rsp), %esi                 # 4-byte Reload
	andnl	%edx, %esi, %edx
	sarl	$31, %eax
	andnl	%ecx, %eax, %eax
	subl	%edx, %ecx
	movl	%ecx, %esi
	sarl	$31, %esi
	andnl	%ecx, %esi, %ecx
	movl	%edi, 2464(%rsp)
	movl	1824(%rsp), %esi                # 4-byte Reload
	movl	%esi, 2468(%rsp)
	movl	%ebx, 2472(%rsp)
	movl	%ecx, 2476(%rsp)
	movl	%r14d, 2480(%rsp)
	movl	%eax, 2484(%rsp)
	movl	%edx, 2488(%rsp)
	movl	%r15d, 2492(%rsp)
	movq	%r13, 224(%rsp)                 # 8-byte Spill
	movq	%r13, 2496(%rsp)
	movq	$0, 2504(%rsp)
	movq	160(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 2512(%rsp)
	movq	$0, 2520(%rsp)
	leaq	train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c(%rip), %rsi
	xorl	%r14d, %r14d
	leaq	2464(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_331
# %bb.187:                              # %"assert succeeded497"
	movq	48(%rsp), %rbx                  # 8-byte Reload
	shlq	$5, %rbx
	leaq	4(%rbx), %rsi
	xorl	%edi, %edi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	movq	560(%rsp), %r14                 # 8-byte Reload
	movl	432(%rsp), %r13d                # 4-byte Reload
	je	.LBB0_330
# %bb.188:                              # %"assert succeeded499"
	movq	%rax, 5600(%rsp)
	movq	$0, 5608(%rsp)
	leaq	train_cost_model.par_for.head1_conv_1_d_def__.s0.w(%rip), %rsi
	movq	%rax, %r12
	leaq	5600(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	40(%rsp), %r15                  # 8-byte Reload
	movl	%r15d, %ecx
	callq	halide_do_par_for@PLT
                                        # kill: def $eax killed $eax def $rax
	testl	%eax, %eax
	movq	%r12, 528(%rsp)                 # 8-byte Spill
	jne	.LBB0_383
# %bb.189:                              # %"consume conv1_stage1_1_d_def__"
	movq	%rbx, 96(%rsp)                  # 8-byte Spill
	movl	%r13d, 7056(%rsp)
	movl	%r15d, 7060(%rsp)
	movq	224(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 7064(%rsp)
	movq	$0, 7072(%rsp)
	movq	%r14, 7080(%rsp)
	movq	40(%rbp), %rax
	movq	%rax, 7088(%rsp)
	movq	%r12, 7096(%rsp)
	movq	$0, 7104(%rsp)
	leaq	train_cost_model.par_for.head1_conv_1_d_def__.s1.w(%rip), %rsi
	leaq	7056(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r15d, %ecx
	callq	halide_do_par_for@PLT
                                        # kill: def $eax killed $eax def $rax
	testl	%eax, %eax
	jne	.LBB0_383
# %bb.190:                              # %"consume head1_conv_1_d_def__"
	movq	1400(%rsp), %rdx                # 8-byte Reload
	leal	(%rdx,%rdx), %eax
	leal	(%rax,%rax,2), %eax
	subl	1328(%rsp), %eax                # 4-byte Folded Reload
	subl	504(%rsp), %eax                 # 4-byte Folded Reload
	subl	544(%rsp), %eax                 # 4-byte Folded Reload
	movl	512(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 5904(%rsp)
	movl	416(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 5908(%rsp)
	movl	%r15d, 5912(%rsp)
	movl	%edx, 5916(%rsp)
	movl	1392(%rsp), %ecx                # 4-byte Reload
	movl	%ecx, 5920(%rsp)
	movl	576(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 5924(%rsp)
	movq	48(%rsp), %rcx                  # 8-byte Reload
	movl	%ecx, 5928(%rsp)
	movl	960(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 5932(%rsp)
	movq	1336(%rsp), %rbx                # 8-byte Reload
	leal	(%rbx,%rbx,2), %ecx
	movl	%ecx, 320(%rsp)                 # 4-byte Spill
	movl	%ecx, 5936(%rsp)
	movl	%eax, 5940(%rsp)
	movq	1992(%rsp), %r13                # 8-byte Reload
	leal	(%rbx,%r13,2), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, 5944(%rsp)
	movl	520(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5948(%rsp)
	movl	%r13d, 5952(%rsp)
	movq	%r12, 5960(%rsp)
	movq	$0, 5968(%rsp)
	movq	552(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 5976(%rsp)
	movq	648(%rsp), %r12                 # 8-byte Reload
	movq	%r12, 5984(%rsp)
	movq	1408(%rsp), %rax                # 8-byte Reload
	movq	%rax, 5992(%rsp)
	movq	440(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 6000(%rsp)
	movq	1416(%rsp), %r14                # 8-byte Reload
	movq	%r14, 6008(%rsp)
	movq	80(%rbp), %r15
	movq	%r15, 6016(%rsp)
	leaq	train_cost_model.par_for.updated_head1_filter.s1.v241.v241.v241(%rip), %rsi
	leaq	5904(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$80, %ecx
	callq	halide_do_par_for@PLT
                                        # kill: def $eax killed $eax def $rax
	testl	%eax, %eax
	jne	.LBB0_383
# %bb.191:                              # %"assert succeeded505"
	movl	520(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 5616(%rsp)
	movl	%r13d, 5620(%rsp)
	movl	%ebx, 5624(%rsp)
	movq	%r14, 5632(%rsp)
	movq	%r15, 5640(%rsp)
	leaq	train_cost_model.par_for.updated_head1_filter.s2.v241.v241.v241(%rip), %rsi
	leaq	5616(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$80, %ecx
	callq	halide_do_par_for@PLT
                                        # kill: def $eax killed $eax def $rax
	testl	%eax, %eax
	jne	.LBB0_383
# %bb.192:                              # %"assert succeeded507"
	leal	(%rbx,%rbx), %eax
	movl	320(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, 2224(%rsp)
	movl	%eax, 2228(%rsp)
	movl	520(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 2232(%rsp)
	movl	%r13d, 2236(%rsp)
	movq	%r14, 2240(%rsp)
	movq	%r15, 2248(%rsp)
	leaq	train_cost_model.par_for.updated_head1_filter.s3.v241.v241.v241(%rip), %rsi
	leaq	2224(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$80, %ecx
	callq	halide_do_par_for@PLT
                                        # kill: def $eax killed $eax def $rax
	testl	%eax, %eax
	jne	.LBB0_383
# %bb.193:                              # %"assert succeeded509"
	movq	%r13, %rsi
	movl	56(%rbp), %eax
	incl	%eax
	vcvtsi2ss	%eax, %xmm0, %xmm3
	vmulss	.LCPI0_81(%rip), %xmm3, %xmm0
	vroundss	$9, %xmm0, %xmm0, %xmm4
	vcvttss2si	%xmm4, %eax
	vmovss	.LCPI0_84(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI0_87(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovss	.LCPI0_88(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	cmpl	$127, %eax
	movq	40(%rsp), %r13                  # 8-byte Reload
	jg	.LBB0_195
# %bb.194:                              # %select.true.sink
	vmulss	.LCPI0_82(%rip), %xmm4, %xmm4
	vfmadd231ss	.LCPI0_83(%rip), %xmm3, %xmm4 # xmm4 = (xmm3 * mem) + xmm4
	vmulss	%xmm4, %xmm4, %xmm5
	movl	%eax, %ecx
	shll	$23, %ecx
	addl	$1065353216, %ecx               # imm = 0x3F800000
	vmovss	.LCPI0_85(%rip), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm1, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm1) + xmm6
	vfmadd213ss	.LCPI0_86(%rip), %xmm5, %xmm6 # xmm6 = (xmm5 * xmm6) + mem
	vmovss	.LCPI0_89(%rip), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm0, %xmm5, %xmm7     # xmm7 = (xmm5 * xmm0) + xmm7
	vfmadd213ss	.LCPI0_90(%rip), %xmm5, %xmm7 # xmm7 = (xmm5 * xmm7) + mem
	vfmadd213ss	%xmm2, %xmm5, %xmm6     # xmm6 = (xmm5 * xmm6) + xmm2
	vfmadd213ss	%xmm2, %xmm5, %xmm7     # xmm7 = (xmm5 * xmm7) + xmm2
	vfmadd231ss	%xmm6, %xmm4, %xmm7     # xmm7 = (xmm4 * xmm6) + xmm7
	vmovd	%ecx, %xmm4
	vfmadd213ss	.LCPI0_91(%rip), %xmm7, %xmm4 # xmm4 = (xmm7 * xmm4) + mem
	vmovss	.LCPI0_91(%rip), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vdivss	%xmm4, %xmm5, %xmm4
	jmp	.LBB0_196
.LBB0_195:
	vmovss	.LCPI0_92(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
.LBB0_196:                              # %select.end
	vmovaps	%xmm4, 320(%rsp)                # 16-byte Spill
	movq	80(%rbp), %rdx
	movq	1336(%rsp), %rdi                # 8-byte Reload
	vmovss	.LCPI0_91(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	cmpl	$-127, %eax
	jg	.LBB0_198
# %bb.197:                              # %select.end
	vmovaps	%xmm4, %xmm5
	vmovaps	%xmm4, 320(%rsp)                # 16-byte Spill
.LBB0_198:                              # %select.end
	vmulss	.LCPI0_93(%rip), %xmm3, %xmm5
	vroundss	$9, %xmm5, %xmm5, %xmm5
	vcvttss2si	%xmm5, %eax
	cmpl	$127, %eax
	jg	.LBB0_200
# %bb.199:                              # %select.true.sink746
	vmulss	.LCPI0_82(%rip), %xmm5, %xmm5
	vfmadd231ss	.LCPI0_94(%rip), %xmm3, %xmm5 # xmm5 = (xmm3 * mem) + xmm5
	vmulss	%xmm5, %xmm5, %xmm3
	movl	%eax, %ecx
	shll	$23, %ecx
	vfmadd213ss	.LCPI0_85(%rip), %xmm3, %xmm1 # xmm1 = (xmm3 * xmm1) + mem
	vfmadd213ss	.LCPI0_86(%rip), %xmm3, %xmm1 # xmm1 = (xmm3 * xmm1) + mem
	addl	$1065353216, %ecx               # imm = 0x3F800000
	vfmadd213ss	%xmm2, %xmm3, %xmm1     # xmm1 = (xmm3 * xmm1) + xmm2
	vfmadd213ss	.LCPI0_89(%rip), %xmm3, %xmm0 # xmm0 = (xmm3 * xmm0) + mem
	vfmadd213ss	.LCPI0_90(%rip), %xmm3, %xmm0 # xmm0 = (xmm3 * xmm0) + mem
	vfmadd213ss	%xmm2, %xmm3, %xmm0     # xmm0 = (xmm3 * xmm0) + xmm2
	vfmadd231ss	%xmm1, %xmm5, %xmm0     # xmm0 = (xmm5 * xmm1) + xmm0
	vmovd	%ecx, %xmm1
	vfmadd213ss	%xmm4, %xmm0, %xmm1     # xmm1 = (xmm0 * xmm1) + xmm4
	vmovss	.LCPI0_91(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm1, %xmm0, %xmm0
	vmovss	%xmm0, 448(%rsp)                # 4-byte Spill
	movl	520(%rsp), %ecx                 # 4-byte Reload
	cmpl	$-127, %eax
	jle	.LBB0_201
	jmp	.LBB0_202
.LBB0_200:
	vmovss	.LCPI0_92(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 448(%rsp)                # 4-byte Spill
	movl	520(%rsp), %ecx                 # 4-byte Reload
	cmpl	$-127, %eax
	jg	.LBB0_202
.LBB0_201:                              # %select.end745
	vmovss	.LCPI0_91(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 448(%rsp)                # 4-byte Spill
.LBB0_202:                              # %select.end745
	movl	432(%rsp), %r15d                # 4-byte Reload
	movl	512(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 2528(%rsp)
	movl	416(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 2532(%rsp)
	vmovss	1952(%rsp), %xmm0               # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 2536(%rsp)
	vmovss	448(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 2540(%rsp)
	vmovaps	320(%rsp), %xmm0                # 16-byte Reload
	vmovss	%xmm0, 2544(%rsp)
	movl	%ecx, 2548(%rsp)
	movl	%esi, 2552(%rsp)
	movl	%edi, 2556(%rsp)
	movq	552(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 2560(%rsp)
	movq	%r12, 2568(%rsp)
	movq	1416(%rsp), %rax                # 8-byte Reload
	movq	%rax, 2576(%rsp)
	movq	%rdx, 2584(%rsp)
	leaq	train_cost_model.par_for.updated_head1_filter.s4.v241.v241.v241(%rip), %rsi
	leaq	2528(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$80, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_388
# %bb.203:                              # %"produce head1_bias_im_0_d_def__"
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%ymm0, 7328(%rsp)
	movl	%r13d, %ebx
	testl	%r13d, %r13d
	movq	496(%rsp), %r14                 # 8-byte Reload
	movq	528(%rsp), %rsi                 # 8-byte Reload
	jle	.LBB0_212
# %bb.204:                              # %"for head1_bias_im_0_d_def__.s1.r1255$x.preheader"
	leaq	-1(%rbx), %rcx
	movl	%ebx, %eax
	andl	$7, %eax
	cmpq	$7, %rcx
	jae	.LBB0_206
# %bb.205:
	xorl	%ecx, %ecx
	jmp	.LBB0_208
.LBB0_206:                              # %"for head1_bias_im_0_d_def__.s1.r1255$x.preheader.new"
	movl	%ebx, %edx
	andl	$-8, %edx
	leaq	224(%rsi), %rdi
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB0_207:                              # %"for head1_bias_im_0_d_def__.s1.r1255$x"
                                        # =>This Inner Loop Header: Depth=1
	vaddps	-224(%rdi), %ymm0, %ymm0
	vaddps	-192(%rdi), %ymm0, %ymm0
	vaddps	-160(%rdi), %ymm0, %ymm0
	vaddps	-128(%rdi), %ymm0, %ymm0
	vaddps	-96(%rdi), %ymm0, %ymm0
	vaddps	-64(%rdi), %ymm0, %ymm0
	vaddps	-32(%rdi), %ymm0, %ymm0
	vaddps	(%rdi), %ymm0, %ymm0
	addq	$8, %rcx
	addq	$256, %rdi                      # imm = 0x100
	cmpq	%rcx, %rdx
	jne	.LBB0_207
.LBB0_208:                              # %call_destructor.exit884.loopexit.unr-lcssa
	testq	%rax, %rax
	je	.LBB0_211
# %bb.209:                              # %"for head1_bias_im_0_d_def__.s1.r1255$x.epil.preheader"
	shlq	$5, %rcx
	movq	%rsi, %rdx
	addq	%rcx, %rdx
	.p2align	4, 0x90
.LBB0_210:                              # %"for head1_bias_im_0_d_def__.s1.r1255$x.epil"
                                        # =>This Inner Loop Header: Depth=1
	vaddps	(%rdx), %ymm0, %ymm0
	addq	$32, %rdx
	decq	%rax
	jne	.LBB0_210
.LBB0_211:                              # %call_destructor.exit884.loopexit
	vmovaps	%ymm0, 7328(%rsp)
.LBB0_212:                              # %call_destructor.exit884
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
	vmovaps	7328(%rsp), %ymm0
	movq	2000(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%rdx,2), %rax
	movq	2008(%rsp), %rcx                # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
	vbroadcastss	.LCPI0_95(%rip), %ymm1  # ymm1 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	vmulps	(%rcx,%rdx,4), %ymm1, %ymm1
	vbroadcastss	.LCPI0_96(%rip), %ymm2  # ymm2 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vfmadd213ps	%ymm1, %ymm0, %ymm2     # ymm2 = (ymm0 * ymm2) + ymm1
	vmovups	%ymm2, (%rcx,%rdx,4)
	vmovups	(%rcx,%rax,4), %ymm0
	vbroadcastss	.LCPI0_97(%rip), %ymm1  # ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	.LCPI0_98(%rip), %ymm2  # ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rcx,%rdx,8), %ymm1, %ymm0 # ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rcx,%rdx,8)
	vmovss	448(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmulss	1952(%rsp), %xmm1, %xmm1        # 4-byte Folded Reload
	vbroadcastss	%xmm1, %ymm1
	vmulps	(%rcx,%rdx,4), %ymm1, %ymm1
	vbroadcastss	320(%rsp), %ymm2        # 16-byte Folded Reload
	vmulps	%ymm0, %ymm2, %ymm0
	vsqrtps	%ymm0, %ymm0
	vbroadcastss	.LCPI0_99(%rip), %ymm2  # ymm2 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm0, %ymm2, %ymm0
	vdivps	%ymm0, %ymm1, %ymm0
	movq	1432(%rsp), %rax                # 8-byte Reload
	vaddps	(%rax), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx)
	movl	288(%rsp), %eax                 # 4-byte Reload
	andnl	352(%rsp), %eax, %esi           # 4-byte Folded Reload
	movq	96(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rax,2), %rax
	movq	%rax, %rdx
	imulq	%rsi, %rdx
	movl	$2147483648, %ecx               # imm = 0x80000000
	cmpq	%rcx, %rdx
	jae	.LBB0_329
# %bb.213:                              # %call_destructor.exit884
	movq	%rax, %rcx
	shrq	$32, %rcx
	movl	%eax, %eax
	imulq	%rsi, %rax
	shrq	$32, %rax
	imulq	%rsi, %rcx
	addq	%rax, %rcx
	movabsq	$545460846592, %rax             # imm = 0x7F00000000
	andq	%rax, %rcx
	jne	.LBB0_329
# %bb.214:                              # %"assert succeeded514"
	movq	%rsi, 96(%rsp)                  # 8-byte Spill
	movq	%rbx, 648(%rsp)                 # 8-byte Spill
	orq	$4, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	vzeroupper
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB0_330
# %bb.215:                              # %"assert succeeded516"
	movq	%rax, %r12
	movq	352(%rsp), %rbx                 # 8-byte Reload
	leal	4(%rbx), %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	xorl	%ecx, %eax
	imulq	$1717986919, %rax, %rax         # imm = 0x66666667
	shrq	$33, %rax
	xorl	%ecx, %eax
	andnl	%eax, %ecx, %ecx
	movl	288(%rsp), %edx                 # 4-byte Reload
	movl	%edx, %eax
	xorl	%ebx, %eax
	imulq	$1717986919, %rax, %rax         # imm = 0x66666667
	shrq	$33, %rax
	xorl	%edx, %eax
	andnl	%eax, %edx, %eax
	movl	%ebx, 6688(%rsp)
	movl	1824(%rsp), %edx                # 4-byte Reload
	movl	%edx, 6692(%rsp)
	movl	%r15d, 6696(%rsp)
	movl	880(%rsp), %edx                 # 4-byte Reload
	movl	%edx, 6700(%rsp)
	movl	%r13d, 6704(%rsp)
	movl	%eax, 6708(%rsp)
	movl	576(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 6712(%rsp)
	movq	48(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 6716(%rsp)
	movl	%r14d, 6720(%rsp)
	movq	160(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 6728(%rsp)
	movq	$0, 6736(%rsp)
	movq	560(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 6744(%rsp)
	movq	40(%rbp), %rax
	movq	%rax, 6752(%rsp)
	movq	8(%rsp), %r15                   # 8-byte Reload
	movq	%r15, 6760(%rsp)
	movq	$0, 6768(%rsp)
	movq	%r12, 6776(%rsp)
	movq	$0, 6784(%rsp)
	leaq	train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n(%rip), %rsi
	leaq	6688(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	callq	halide_do_par_for@PLT
                                        # kill: def $eax killed $eax def $rax
	testl	%eax, %eax
	jne	.LBB0_390
# %bb.216:                              # %call_destructor.exit885
	xorl	%edi, %edi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movq	1424(%rsp), %rax                # 8-byte Reload
	leal	(%rax,%rax,2), %esi
	movl	888(%rsp), %edx                 # 4-byte Reload
	imull	$38, %edx, %eax
	subl	%r14d, %eax
	movl	1556(%rsp), %r13d               # 4-byte Reload
	imull	$38, %r13d, %ecx
	addl	%esi, %ecx
	movl	%ebx, 5648(%rsp)
	movl	%edx, 5652(%rsp)
	movq	40(%rsp), %rdx                  # 8-byte Reload
	movl	%edx, 5656(%rsp)
	movl	%eax, 5660(%rsp)
	movl	%ecx, 5664(%rsp)
	movl	%esi, 48(%rsp)                  # 4-byte Spill
	movl	%esi, 5668(%rsp)
	movl	%r14d, 5672(%rsp)
	movl	%r13d, 5676(%rsp)
	movq	%r12, 5680(%rsp)
	movq	$0, 5688(%rsp)
	movq	64(%rsp), %r15                  # 8-byte Reload
	movq	%r15, 5696(%rsp)
	movq	$0, 5704(%rsp)
	movq	2016(%rsp), %r14                # 8-byte Reload
	movq	%r14, 5712(%rsp)
	movq	96(%rbp), %rbx
	movq	%rbx, 5720(%rsp)
	leaq	train_cost_model.par_for.updated_head2_filter.s1.v246.v246.v246(%rip), %rsi
	leaq	5648(%rsp), %r8
	movl	$0, %edi
	xorl	%edx, %edx
	movl	$60, %ecx
	callq	halide_do_par_for@PLT
                                        # kill: def $eax killed $eax def $rax
	testl	%eax, %eax
	movq	%r12, 288(%rsp)                 # 8-byte Spill
	jne	.LBB0_391
# %bb.217:                              # %call_destructor.exit886
	xorl	%edi, %edi
	movq	%r15, %rsi
	callq	halide_free@PLT
	movl	%r13d, 2328(%rsp)
	movq	1424(%rsp), %rbx                # 8-byte Reload
	movl	%ebx, 2332(%rsp)
	movq	%r14, 2336(%rsp)
	movq	96(%rbp), %r15
	movq	%r15, 2344(%rsp)
	leaq	train_cost_model.par_for.updated_head2_filter.s2.v246.v246.v246(%rip), %rsi
	leaq	2328(%rsp), %r8
	movl	$0, %edi
	xorl	%edx, %edx
	movl	$60, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_317
# %bb.218:                              # %"assert succeeded525"
	leal	(%rbx,%rbx), %eax
	movl	48(%rsp), %ecx                  # 4-byte Reload
	movl	%ecx, 5728(%rsp)
	movl	%eax, 5732(%rsp)
	movl	%r13d, 5736(%rsp)
	movq	%r14, 5744(%rsp)
	movq	%r15, 5752(%rsp)
	leaq	train_cost_model.par_for.updated_head2_filter.s3.v246.v246.v246(%rip), %rsi
	leaq	5728(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$60, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_317
# %bb.219:                              # %"assert succeeded527"
	movl	1440(%rsp), %eax                # 4-byte Reload
	movl	%eax, 3448(%rsp)
	vmovss	1952(%rsp), %xmm0               # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 3452(%rsp)
	vmovss	448(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 3456(%rsp)
	vmovaps	320(%rsp), %xmm0                # 16-byte Reload
	vmovss	%xmm0, 3460(%rsp)
	movl	%r13d, 3464(%rsp)
	movl	%ebx, 3468(%rsp)
	movq	1448(%rsp), %rax                # 8-byte Reload
	movq	%rax, 3472(%rsp)
	movq	24(%rbp), %rax
	movq	%rax, 3480(%rsp)
	movq	%r14, 3488(%rsp)
	movq	%r15, 3496(%rsp)
	leaq	train_cost_model.par_for.updated_head2_filter.s4.v246.v246.v246(%rip), %rsi
	leaq	3448(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$60, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movq	40(%rsp), %r13                  # 8-byte Reload
	movq	2032(%rsp), %r14                # 8-byte Reload
	movq	2024(%rsp), %r15                # 8-byte Reload
	jne	.LBB0_317
# %bb.220:                              # %"consume head2_conv_1_d_def__530"
	leal	(%r15,%r15,2), %eax
	movq	352(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, 6792(%rsp)
	movl	%r13d, 6796(%rsp)
	movl	%eax, 6800(%rsp)
	movq	288(%rsp), %r12                 # 8-byte Reload
	movq	%r12, 6808(%rsp)
	movq	$0, 6816(%rsp)
	movq	%r14, 6824(%rsp)
	movq	104(%rbp), %rbx
	movq	%rbx, 6832(%rsp)
	leaq	train_cost_model.par_for.updated_head2_bias.s1.v249.v249(%rip), %rsi
	leaq	6792(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	halide_do_par_for@PLT
                                        # kill: def $eax killed $eax def $rax
	testl	%eax, %eax
	jne	.LBB0_392
# %bb.221:                              # %call_destructor.exit887
	xorl	%edi, %edi
	movq	%r12, %rsi
	callq	halide_free@PLT
	movl	%r15d, 6840(%rsp)
	movq	%r14, 6848(%rsp)
	movq	%rbx, 6856(%rsp)
	leaq	train_cost_model.par_for.updated_head2_bias.s2.v249.v249(%rip), %rsi
	leaq	6840(%rsp), %r8
	movl	$0, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_332
# %bb.222:                              # %"assert succeeded534"
	movl	%r15d, 6864(%rsp)
	movq	%r14, 6872(%rsp)
	movq	%rbx, 6880(%rsp)
	leaq	train_cost_model.par_for.updated_head2_bias.s3.v249.v249(%rip), %rsi
	leaq	6864(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_332
# %bb.223:                              # %"assert succeeded536"
	vmovss	1952(%rsp), %xmm0               # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 5760(%rsp)
	vmovss	448(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 5764(%rsp)
	vmovaps	320(%rsp), %xmm0                # 16-byte Reload
	vmovss	%xmm0, 5768(%rsp)
	movl	%r15d, 5772(%rsp)
	movq	1456(%rsp), %rax                # 8-byte Reload
	movq	%rax, 5776(%rsp)
	movq	32(%rbp), %rax
	movq	%rax, 5784(%rsp)
	movq	%r14, 5792(%rsp)
	movq	%rbx, 5800(%rsp)
	leaq	train_cost_model.par_for.updated_head2_bias.s4.v249.v249(%rip), %rsi
	xorl	%r14d, %r14d
	leaq	5760(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$3, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_396
# %bb.224:                              # %"produce filter1_im_0_d_def__"
	leaq	7328(%rsp), %r13
	movq	%r13, 5808(%rsp)
	movq	$0, 5816(%rsp)
	leaq	train_cost_model.par_for.filter1_im_0_d_def__.s0.v18(%rip), %rsi
	leaq	5808(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movq	2048(%rsp), %r12                # 8-byte Reload
	jne	.LBB0_397
# %bb.225:                              # %"consume head2_relu542"
	movq	352(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 6888(%rsp)
	movl	1824(%rsp), %eax                # 4-byte Reload
	movl	%eax, 6892(%rsp)
	movl	624(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 6896(%rsp)
	movq	40(%rsp), %r15                  # 8-byte Reload
	movl	%r15d, 6900(%rsp)
	movq	496(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 6904(%rsp)
	movq	160(%rsp), %r14                 # 8-byte Reload
	movq	%r14, 6912(%rsp)
	movq	$0, 6920(%rsp)
	movq	%r13, 6928(%rsp)
	movq	$0, 6936(%rsp)
	movq	56(%rsp), %rbx                  # 8-byte Reload
	movq	%rbx, 6944(%rsp)
	movq	$0, 6952(%rsp)
	leaq	train_cost_model.par_for.filter1_im_0_d_def__.s1.v18(%rip), %rsi
	leaq	6888(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_398
# %bb.226:                              # %call_destructor.exit889
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	%r14, %rsi
	callq	halide_free@PLT
	movl	%r15d, 7208(%rsp)
	movq	224(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 7216(%rsp)
	movq	$0, 7224(%rsp)
	movq	%r13, 7232(%rsp)
	movq	$0, 7240(%rsp)
	movq	896(%rsp), %rbx                 # 8-byte Reload
	movq	%rbx, 7248(%rsp)
	movq	$0, 7256(%rsp)
	leaq	train_cost_model.par_for.filter1_im_0_d_def__.s2.v18(%rip), %rsi
	leaq	7208(%rsp), %r8
	movl	$0, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_400
# %bb.227:                              # %call_destructor.exit890
	movq	%r13, %r15
	xorl	%edi, %edi
	movq	%rbx, %rsi
	callq	halide_free@PLT
	movq	2040(%rsp), %r14                # 8-byte Reload
	leal	(%r14,%r14,2), %ebx
	movl	%ebx, 6608(%rsp)
	movl	1560(%rsp), %r13d               # 4-byte Reload
	movl	%r13d, 6612(%rsp)
	movq	%r15, 6616(%rsp)
	movq	$0, 6624(%rsp)
	movq	%r12, 6632(%rsp)
	movq	112(%rbp), %r15
	movq	%r15, 6640(%rsp)
	leaq	train_cost_model.par_for.updated_filter1.s1.v251.v251.v251(%rip), %rsi
	leaq	6608(%rsp), %r8
	movl	$0, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_316
# %bb.228:                              # %"assert succeeded550"
	movl	%r13d, 2352(%rsp)
	movl	%r14d, 2356(%rsp)
	movq	%r12, 2360(%rsp)
	movq	%r15, 2368(%rsp)
	leaq	train_cost_model.par_for.updated_filter1.s2.v251.v251.v251(%rip), %rsi
	leaq	2352(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_316
# %bb.229:                              # %"assert succeeded552"
	leal	(%r14,%r14), %eax
	movl	%ebx, 5824(%rsp)
	movl	%eax, 5828(%rsp)
	movl	%r13d, 5832(%rsp)
	movq	%r12, 5840(%rsp)
	movq	%r15, 5848(%rsp)
	leaq	train_cost_model.par_for.updated_filter1.s3.v251.v251.v251(%rip), %rsi
	leaq	5824(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_316
# %bb.230:                              # %"assert succeeded554"
	movl	432(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3504(%rsp)
	vmovss	1952(%rsp), %xmm0               # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 3508(%rsp)
	vmovss	448(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 3512(%rsp)
	vmovaps	320(%rsp), %xmm0                # 16-byte Reload
	vmovss	%xmm0, 3516(%rsp)
	movl	%r13d, 3520(%rsp)
	movl	%r14d, 3524(%rsp)
	movq	560(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 3528(%rsp)
	movq	40(%rbp), %rax
	movq	%rax, 3536(%rsp)
	movq	%r12, 3544(%rsp)
	movq	%r15, 3552(%rsp)
	leaq	train_cost_model.par_for.updated_filter1.s4.v251.v251.v251(%rip), %rsi
	leaq	3504(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$32, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movq	40(%rsp), %r15                  # 8-byte Reload
	jne	.LBB0_316
# %bb.231:                              # %"consume conv1_stage1_1_d_def__557"
	movq	2056(%rsp), %r12                # 8-byte Reload
	leal	(%r12,%r12,2), %eax
	movl	%r15d, 6648(%rsp)
	movl	%eax, 6652(%rsp)
	movq	224(%rsp), %r14                 # 8-byte Reload
	movq	%r14, 6656(%rsp)
	movq	$0, 6664(%rsp)
	movq	2064(%rsp), %rbx                # 8-byte Reload
	movq	%rbx, 6672(%rsp)
	movq	120(%rbp), %r13
	movq	%r13, 6680(%rsp)
	leaq	train_cost_model.par_for.updated_bias1.s1.v254.v254(%rip), %rsi
	leaq	6648(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$4, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_401
# %bb.232:                              # %call_destructor.exit891
	xorl	%edi, %edi
	movq	%r14, %rsi
	callq	halide_free@PLT
	movl	%r12d, 6960(%rsp)
	movq	%rbx, 6968(%rsp)
	movq	%r13, 6976(%rsp)
	leaq	train_cost_model.par_for.updated_bias1.s2.v254.v254(%rip), %rsi
	leaq	6960(%rsp), %r8
	movl	$0, %edi
	xorl	%edx, %edx
	movl	$4, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_402
# %bb.233:                              # %"assert succeeded561"
	movl	%r12d, 6984(%rsp)
	movq	%rbx, 6992(%rsp)
	movq	%r13, 7000(%rsp)
	leaq	train_cost_model.par_for.updated_bias1.s3.v254.v254(%rip), %rsi
	leaq	6984(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$4, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_318
# %bb.234:                              # %"assert succeeded563"
	vmovss	1952(%rsp), %xmm0               # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 5856(%rsp)
	vmovss	448(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 5860(%rsp)
	vmovaps	320(%rsp), %xmm0                # 16-byte Reload
	vmovss	%xmm0, 5864(%rsp)
	movl	%r12d, 5868(%rsp)
	movq	704(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 5872(%rsp)
	movq	48(%rbp), %rax
	movq	%rax, 5880(%rsp)
	movq	%rbx, 5888(%rsp)
	movq	%r13, 5896(%rsp)
	leaq	train_cost_model.par_for.updated_bias1.s4.v254.v254(%rip), %rsi
	leaq	5856(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	$4, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_318
# %bb.235:                              # %"consume f7566"
	movq	672(%rsp), %rcx                 # 8-byte Reload
	movl	%ecx, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %eax
	movq	632(%rsp), %rdx                 # 8-byte Reload
	leal	7(%rdx), %ecx
	sarl	$3, %ecx
	movl	%edx, 7008(%rsp)
	sarl	$3, %edx
	movl	%edx, 7012(%rsp)
	movl	%eax, 7016(%rsp)
	movq	128(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 7024(%rsp)
	movq	$0, 7032(%rsp)
	movq	1808(%rsp), %rax                # 8-byte Reload
	movq	%rax, 7040(%rsp)
	movq	128(%rbp), %rax
	movq	%rax, 7048(%rsp)
	leaq	train_cost_model.par_for.prediction_output.s0.n.n(%rip), %rsi
	leaq	7008(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	movl	1568(%rsp), %ecx                # 4-byte Reload
	jne	.LBB0_318
# %bb.236:                              # %"assert succeeded568"
	movq	96(%rsp), %rdx                  # 8-byte Reload
	movl	%edx, %eax
	shlq	$2, %rdx
	cmpl	$536870912, %eax                # imm = 0x20000000
	jae	.LBB0_403
# %bb.237:                              # %"assert succeeded570"
	movl	%ecx, 1568(%rsp)                # 4-byte Spill
	addq	$4, %rdx
	xorl	%edi, %edi
	movq	%rdx, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB0_404
# %bb.238:                              # %"assert succeeded572"
	movq	%rax, %r13
	movq	352(%rsp), %rbx                 # 8-byte Reload
	movl	%ebx, 2376(%rsp)
	movl	24(%rsp), %r12d                 # 4-byte Reload
	movl	%r12d, 2380(%rsp)
	movq	%rax, 2384(%rsp)
	movq	$0, 2392(%rsp)
	leaq	train_cost_model.par_for.sum.s0.n.n(%rip), %rsi
	leaq	2376(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	1344(%rsp), %r14d               # 4-byte Reload
	movl	%r14d, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_333
# %bb.239:                              # %"consume conv1_stage2575"
	movl	%ebx, 6296(%rsp)
	movl	200(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 6300(%rsp)
	movl	%r15d, 6304(%rsp)
	movl	%r12d, 6308(%rsp)
	movl	1568(%rsp), %r12d               # 4-byte Reload
	movl	%r12d, 6312(%rsp)
	movq	72(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 6320(%rsp)
	movq	$0, 6328(%rsp)
	movq	%r13, 6336(%rsp)
	movq	$0, 6344(%rsp)
	leaq	train_cost_model.par_for.sum.s1.n.n(%rip), %rsi
	leaq	6296(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%r14d, %ecx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_333
# %bb.240:                              # %"produce sum$1"
	movl	$0, 7328(%rsp)
	testl	%ebx, %ebx
	movq	712(%rsp), %rax                 # 8-byte Reload
	movl	536(%rsp), %edx                 # 4-byte Reload
	jle	.LBB0_252
# %bb.241:                              # %"for sum$1.s1.r102$x.preheader"
	movslq	640(%rsp), %r9                  # 4-byte Folded Reload
	movslq	848(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	%edx, %r8
	vmovss	.LCPI0_91(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vdivss	(%rax,%rcx,4), %xmm0, %xmm1
	movl	%ebx, %r11d
	cmpl	$8, %ebx
	jae	.LBB0_243
# %bb.242:
	vxorps	%xmm2, %xmm2, %xmm2
	xorl	%ecx, %ecx
	movq	128(%rsp), %r14                 # 8-byte Reload
	jmp	.LBB0_249
.LBB0_243:                              # %vector.ph
	movl	%r11d, %ecx
	andl	$-8, %ecx
	vbroadcastss	%xmm1, %ymm2
	leaq	-8(%rcx), %rdx
	movq	%rdx, %r10
	shrq	$3, %r10
	incq	%r10
	testq	%rdx, %rdx
	movq	128(%rsp), %r14                 # 8-byte Reload
	je	.LBB0_312
# %bb.244:                              # %vector.ph.new
	leaq	(,%r9,4), %rsi
	movq	%rax, %rdx
	subq	%rsi, %rdx
	addq	$32, %rdx
	leaq	(,%r8,4), %rdi
	movq	%r14, %rsi
	subq	%rdi, %rsi
	addq	$32, %rsi
	movq	%r10, %rdi
	andq	$-2, %rdi
	negq	%rdi
	vxorps	%xmm3, %xmm3, %xmm3
	xorl	%ebx, %ebx
	vbroadcastss	.LCPI0_100(%rip), %ymm4 # ymm4 = [1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10]
	vbroadcastss	.LCPI0_91(%rip), %ymm5  # ymm5 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI0_101(%rip), %ymm6 # ymm6 = [9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6]
	.p2align	4, 0x90
.LBB0_245:                              # %vector.body
                                        # =>This Inner Loop Header: Depth=1
	vmulps	-32(%rsi,%rbx,4), %ymm2, %ymm7
	vmaxps	%ymm4, %ymm7, %ymm7
	vdivps	%ymm7, %ymm5, %ymm7
	vmulps	-32(%rdx,%rbx,4), %ymm2, %ymm8
	vdivps	%ymm8, %ymm5, %ymm8
	vmulps	(%rsi,%rbx,4), %ymm2, %ymm9
	vmaxps	%ymm4, %ymm9, %ymm9
	vdivps	%ymm9, %ymm5, %ymm9
	vsubps	%ymm8, %ymm7, %ymm7
	vmulps	(%rdx,%rbx,4), %ymm2, %ymm8
	vfmadd213ps	%ymm3, %ymm7, %ymm7     # ymm7 = (ymm7 * ymm7) + ymm3
	vdivps	%ymm8, %ymm5, %ymm3
	vfmadd231ps	(%r13,%rbx,4), %ymm6, %ymm7 # ymm7 = (ymm6 * mem) + ymm7
	vsubps	%ymm3, %ymm9, %ymm3
	vfmadd213ps	%ymm7, %ymm3, %ymm3     # ymm3 = (ymm3 * ymm3) + ymm7
	vfmadd231ps	32(%r13,%rbx,4), %ymm6, %ymm3 # ymm3 = (ymm6 * mem) + ymm3
	addq	$16, %rbx
	addq	$2, %rdi
	jne	.LBB0_245
# %bb.246:                              # %middle.block.unr-lcssa
	testb	$1, %r10b
	je	.LBB0_248
.LBB0_247:                              # %vector.body.epil
	movq	%rbx, %rdx
	subq	%r8, %rdx
	vmulps	(%r14,%rdx,4), %ymm2, %ymm4
	vbroadcastss	.LCPI0_100(%rip), %ymm5 # ymm5 = [1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10]
	vmaxps	%ymm5, %ymm4, %ymm4
	vbroadcastss	.LCPI0_91(%rip), %ymm5  # ymm5 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vdivps	%ymm4, %ymm5, %ymm4
	movq	%rbx, %rdx
	subq	%r9, %rdx
	vmulps	(%rax,%rdx,4), %ymm2, %ymm2
	vdivps	%ymm2, %ymm5, %ymm2
	vsubps	%ymm2, %ymm4, %ymm2
	vbroadcastss	.LCPI0_101(%rip), %ymm4 # ymm4 = [9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6]
	vfmadd213ps	%ymm3, %ymm2, %ymm2     # ymm2 = (ymm2 * ymm2) + ymm3
	vfmadd231ps	(%r13,%rbx,4), %ymm4, %ymm2 # ymm2 = (ymm4 * mem) + ymm2
	vmovaps	%ymm2, %ymm3
.LBB0_248:                              # %middle.block
	vextractf128	$1, %ymm3, %xmm2
	vaddps	%xmm2, %xmm3, %xmm2
	vpermilpd	$1, %xmm2, %xmm3        # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	cmpq	%r11, %rcx
	movq	40(%rsp), %r15                  # 8-byte Reload
	je	.LBB0_251
.LBB0_249:                              # %"for sum$1.s1.r102$x.preheader1581"
	shlq	$2, %r9
	subq	%r9, %rax
	shlq	$2, %r8
	movq	%r14, %rdx
	subq	%r8, %rdx
	vmovss	.LCPI0_100(%rip), %xmm3         # xmm3 = mem[0],zero,zero,zero
	vmovss	.LCPI0_101(%rip), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vmovaps	%xmm2, %xmm5
	.p2align	4, 0x90
.LBB0_250:                              # %"for sum$1.s1.r102$x"
                                        # =>This Inner Loop Header: Depth=1
	vmulss	(%rdx,%rcx,4), %xmm1, %xmm2
	vmaxss	%xmm3, %xmm2, %xmm2
	vmulss	(%rax,%rcx,4), %xmm1, %xmm6
	vdivss	%xmm2, %xmm0, %xmm2
	vdivss	%xmm6, %xmm0, %xmm6
	vsubss	%xmm6, %xmm2, %xmm2
	vfmadd213ss	%xmm5, %xmm2, %xmm2     # xmm2 = (xmm2 * xmm2) + xmm5
	vfmadd231ss	(%r13,%rcx,4), %xmm4, %xmm2 # xmm2 = (xmm4 * mem) + xmm2
	incq	%rcx
	vmovaps	%xmm2, %xmm5
	cmpq	%rcx, %r11
	jne	.LBB0_250
.LBB0_251:                              # %call_destructor.exit893.loopexit
	vmovss	%xmm2, 7328(%rsp)
.LBB0_252:                              # %call_destructor.exit893
	xorl	%edi, %edi
	movq	128(%rsp), %rsi                 # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	%r13, %rsi
	callq	halide_free@PLT
	movl	7328(%rsp), %eax
	movq	1304(%rsp), %rcx                # 8-byte Reload
	movl	%eax, (%rcx)
	movslq	88(%rsp), %rcx                  # 4-byte Folded Reload
	movslq	%r12d, %rax
	movq	%rax, 672(%rsp)                 # 8-byte Spill
	movl	696(%rsp), %eax                 # 4-byte Reload
	movq	%rax, 1304(%rsp)                # 8-byte Spill
	cmpl	$0, 424(%rsp)                   # 4-byte Folded Reload
	jle	.LBB0_260
# %bb.253:                              # %"for load_cost_output.s0.n.preheader"
	movslq	656(%rsp), %r9                  # 4-byte Folded Reload
	leaq	(,%r9,4), %rdx
	movq	%rcx, 1808(%rsp)                # 8-byte Spill
	leaq	(,%rcx,4), %rax
	subq	%rax, %rdx
	movq	%rdx, 416(%rsp)                 # 8-byte Spill
	movq	1304(%rsp), %r8                 # 8-byte Reload
	leaq	(%r9,%r8,8), %rcx
	movq	672(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%rdi,8), %rax
	subq	%rax, %rcx
	movq	%rcx, 352(%rsp)                 # 8-byte Spill
	leaq	(,%rdi,8), %rax
	movq	%rdi, %rsi
	subq	%rax, %rsi
	leaq	(%r8,%r8,2), %rcx
	leaq	(%r9,%rcx,2), %rdx
	addq	%rdx, %rsi
	movq	%rsi, 320(%rsp)                 # 8-byte Spill
	leaq	(%r8,%r8,4), %r13
	leaq	(%r9,%r13), %rdx
	leaq	(%rdi,%rdi), %rbx
	leaq	(%rbx,%rbx,2), %rsi
	subq	%rsi, %rdx
	movq	%rdx, 448(%rsp)                 # 8-byte Spill
	leaq	(,%r8,8), %rdx
	subq	%r8, %rdx
	addq	%r9, %rdx
	subq	%rax, %rdx
	movq	%rdx, 224(%rsp)                 # 8-byte Spill
	leaq	(%r8,%r8,8), %rdx
	addq	%r9, %rdx
	leaq	(%rbx,%rbx,4), %rax
	subq	%rax, %rdx
	movq	%rdx, 160(%rsp)                 # 8-byte Spill
	leaq	(%r9,%r13,2), %rsi
	leaq	(%rdi,%rdi,4), %rdx
	leaq	(%rdi,%rdx,2), %rax
	subq	%rax, %rsi
	movq	%rsi, 288(%rsp)                 # 8-byte Spill
	leaq	(%r8,%r13,2), %rax
	addq	%r9, %rax
	leaq	(,%rdi,4), %rbx
	leaq	(%rbx,%rbx,2), %rbx
	subq	%rbx, %rax
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	leaq	(%rdi,%rdi,2), %rbx
	leaq	(%rdi,%rbx,4), %rbx
	leaq	(%r9,%rcx,4), %rax
	subq	%rbx, %rax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	leaq	(%r8,%rcx,4), %rcx
	addq	%r9, %rcx
	movq	%rdi, %rax
	shlq	$4, %rax
	movq	%rdi, %rsi
	subq	%rax, %rsi
	addq	%rdi, %rsi
	addq	%rcx, %rsi
	movq	%rsi, 96(%rsp)                  # 8-byte Spill
	movq	%r8, %rsi
	shlq	$4, %rsi
	subq	%r8, %rsi
	subq	%r8, %rsi
	movq	%r9, 512(%rsp)                  # 8-byte Spill
	addq	%r9, %rsi
	leaq	(%rdx,%rdx,2), %rdx
	subq	%rdx, %rsi
	leaq	(,%r13,2), %rcx
	addq	%r13, %rcx
	addq	%r9, %rcx
	subq	%rax, %rcx
	movq	%rcx, 576(%rsp)                 # 8-byte Spill
	movl	88(%rsp), %edx                  # 4-byte Reload
	movl	832(%rsp), %r8d                 # 4-byte Reload
	subl	%edx, %r8d
	movl	824(%rsp), %ecx                 # 4-byte Reload
	subl	%edx, %ecx
	movl	952(%rsp), %r10d                # 4-byte Reload
	subl	%edx, %r10d
	subl	%edx, 272(%rsp)                 # 4-byte Folded Spill
	movl	968(%rsp), %r11d                # 4-byte Reload
	subl	%edx, %r11d
	movl	856(%rsp), %ebx                 # 4-byte Reload
	subl	%edx, %ebx
	movl	680(%rsp), %r14d                # 4-byte Reload
	subl	%edx, %r14d
	movl	208(%rsp), %r15d                # 4-byte Reload
	subl	%edx, %r15d
	movl	216(%rsp), %r13d                # 4-byte Reload
	subl	%edx, %r13d
	movq	688(%rsp), %r12                 # 8-byte Reload
	subl	%edx, %r12d
	movl	144(%rsp), %r9d                 # 4-byte Reload
	subl	%edx, %r9d
	movl	488(%rsp), %edi                 # 4-byte Reload
	subl	%edx, %edi
	movl	152(%rsp), %eax                 # 4-byte Reload
	subl	%edx, %eax
	movslq	%r8d, %rdx
	movslq	%ecx, %r8
	movslq	%r10d, %r10
	movslq	272(%rsp), %rcx                 # 4-byte Folded Reload
	movq	%rcx, 1824(%rsp)                # 8-byte Spill
	movslq	%r11d, %r11
	movslq	%ebx, %rbx
	movslq	%r14d, %rcx
	movq	%rcx, 1568(%rsp)                # 8-byte Spill
	movslq	%r15d, %rcx
	movq	%rcx, 1344(%rsp)                # 8-byte Spill
	movslq	%r13d, %r13
	movslq	%r12d, %r12
	movslq	%r9d, %r9
	movslq	%edi, %rdi
	movslq	%eax, %r14
	movq	512(%rsp), %rcx                 # 8-byte Reload
	addq	%rcx, %rdx
	movq	80(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 544(%rsp)                 # 8-byte Spill
	addq	%rcx, %r8
	leaq	(%rax,%r8,4), %rdx
	movq	%rdx, 504(%rsp)                 # 8-byte Spill
	addq	%rcx, %r10
	leaq	(%rax,%r10,4), %rdx
	movq	%rdx, 272(%rsp)                 # 8-byte Spill
	movq	1824(%rsp), %rdx                # 8-byte Reload
	addq	%rcx, %rdx
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 712(%rsp)                 # 8-byte Spill
	addq	%rcx, %r11
	leaq	(%rax,%r11,4), %rdx
	movq	%rdx, 704(%rsp)                 # 8-byte Spill
	addq	%rcx, %rbx
	leaq	(%rax,%rbx,4), %rdx
	movq	%rdx, 640(%rsp)                 # 8-byte Spill
	movq	40(%rsp), %r15                  # 8-byte Reload
	movq	1568(%rsp), %rdx                # 8-byte Reload
	addq	%rcx, %rdx
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 632(%rsp)                 # 8-byte Spill
	addq	%rax, 416(%rsp)                 # 8-byte Folded Spill
	movq	1344(%rsp), %rdx                # 8-byte Reload
	addq	%rcx, %rdx
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 1456(%rsp)                # 8-byte Spill
	addq	%rcx, %r13
	leaq	(%rax,%r13,4), %rdx
	movq	%rdx, 1448(%rsp)                # 8-byte Spill
	addq	%rcx, %r12
	leaq	(%rax,%r12,4), %rdx
	movq	%rdx, 1440(%rsp)                # 8-byte Spill
	addq	%rcx, %r9
	leaq	(%rax,%r9,4), %rdx
	movq	%rdx, 1432(%rsp)                # 8-byte Spill
	addq	%rcx, %rdi
	leaq	(%rax,%rdi,4), %rdx
	movq	%rdx, 1424(%rsp)                # 8-byte Spill
	addq	%rcx, %r14
	movq	%rcx, %rbx
	leaq	(%rax,%r14,4), %rax
	movq	%rax, 1416(%rsp)                # 8-byte Spill
	movq	72(%rsp), %rax                  # 8-byte Reload
	movq	352(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	movq	%rdx, 1336(%rsp)                # 8-byte Spill
	movslq	200(%rsp), %rdi                 # 4-byte Folded Reload
	shlq	$7, %rdi
	movq	320(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	movq	%rdx, 1408(%rsp)                # 8-byte Spill
	movq	448(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	movq	%rdx, 1328(%rsp)                # 8-byte Spill
	movq	224(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	movq	%rdx, 1400(%rsp)                # 8-byte Spill
	movq	160(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	movq	%rdx, 1392(%rsp)                # 8-byte Spill
	movq	288(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	movq	%rdx, 496(%rsp)                 # 8-byte Spill
	movq	48(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	movq	%rdx, 536(%rsp)                 # 8-byte Spill
	movq	24(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	movq	%rdx, 888(%rsp)                 # 8-byte Spill
	movq	96(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rax,%rcx,4), %rdx
	movq	%rdx, 624(%rsp)                 # 8-byte Spill
	leaq	(%rax,%rsi,4), %rdx
	movq	%rdx, 880(%rsp)                 # 8-byte Spill
	movq	576(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	movq	%rax, 696(%rsp)                 # 8-byte Spill
	vmovss	.LCPI0_91(%rip), %xmm8          # xmm8 = mem[0],zero,zero,zero
	vpxor	%xmm9, %xmm9, %xmm9
	vmovaps	.LCPI0_102(%rip), %ymm10        # ymm10 = [0,0,1,2,1,3,1,3]
	vpxor	%xmm11, %xmm11, %xmm11
	movq	408(%rsp), %rax                 # 8-byte Reload
	leaq	(,%rax,4), %rcx
	xorl	%r12d, %r12d
	movq	%rbx, %rax
	movq	%rdi, 688(%rsp)                 # 8-byte Spill
	movq	%rcx, 680(%rsp)                 # 8-byte Spill
	movq	688(%rsp), %r13                 # 8-byte Reload
	.p2align	4, 0x90
.LBB0_254:                              # %"for load_cost_output.s0.n"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_256 Depth 2
	movq	%rax, 552(%rsp)                 # 8-byte Spill
	vpxor	%xmm12, %xmm12, %xmm12
	movl	$0, %edi
	testl	%r15d, %r15d
	jle	.LBB0_258
# %bb.255:                              # %"for f1.s1.r77$x.preheader"
                                        #   in Loop: Header=BB0_254 Depth=1
	vpxor	%xmm12, %xmm12, %xmm12
	movq	696(%rsp), %r10                 # 8-byte Reload
	movq	880(%rsp), %rbx                 # 8-byte Reload
	movq	624(%rsp), %rdi                 # 8-byte Reload
	movq	888(%rsp), %r8                  # 8-byte Reload
	movq	536(%rsp), %rdx                 # 8-byte Reload
	movq	496(%rsp), %rcx                 # 8-byte Reload
	movq	1392(%rsp), %r14                # 8-byte Reload
	movq	1400(%rsp), %rax                # 8-byte Reload
	movq	%rax, 352(%rsp)                 # 8-byte Spill
	movq	1328(%rsp), %rax                # 8-byte Reload
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	movq	1408(%rsp), %rax                # 8-byte Reload
	movq	%rax, 448(%rsp)                 # 8-byte Spill
	movq	1336(%rsp), %rax                # 8-byte Reload
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	movq	1416(%rsp), %rax                # 8-byte Reload
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	movq	1424(%rsp), %rax                # 8-byte Reload
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	movq	1432(%rsp), %rax                # 8-byte Reload
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	movq	1440(%rsp), %rax                # 8-byte Reload
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	1448(%rsp), %rax                # 8-byte Reload
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movq	1456(%rsp), %rax                # 8-byte Reload
	movq	%rax, 576(%rsp)                 # 8-byte Spill
	movq	416(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 1344(%rsp)                # 8-byte Spill
	movq	632(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 1568(%rsp)                # 8-byte Spill
	movq	640(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 1824(%rsp)                # 8-byte Spill
	movq	704(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 440(%rsp)                 # 8-byte Spill
	movq	712(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movq	272(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 1952(%rsp)                # 8-byte Spill
	movq	504(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 432(%rsp)                 # 8-byte Spill
	movq	544(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	648(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	movq	680(%rsp), %rsi                 # 8-byte Reload
	.p2align	4, 0x90
.LBB0_256:                              # %"for f1.s1.r77$x"
                                        #   Parent Loop BB0_254 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	160(%rsp), %rax                 # 8-byte Reload
	vmovss	(%rax,%r12), %xmm5              # xmm5 = mem[0],zero,zero,zero
	movq	288(%rsp), %rax                 # 8-byte Reload
	vmulss	(%rax,%r12), %xmm5, %xmm5
	movq	%rdx, 656(%rsp)                 # 8-byte Spill
	vmovss	(%rdx,%r12), %xmm6              # xmm6 = mem[0],zero,zero,zero
	movq	%rcx, 520(%rsp)                 # 8-byte Spill
	vinsertps	$16, (%rcx,%r12), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	%r10, 424(%rsp)                 # 8-byte Spill
	vmovss	(%r10,%r12), %xmm7              # xmm7 = mem[0],zero,zero,zero
	movq	%r14, 64(%rsp)                  # 8-byte Spill
	vinsertps	$32, (%r14,%r12), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rbx, 560(%rsp)                 # 8-byte Spill
	vinsertps	$16, (%rbx,%r12), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	224(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, (%rax,%r12), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	movq	48(%rsp), %rax                  # 8-byte Reload
	vmovss	(%rax,%r12), %xmm0              # xmm0 = mem[0],zero,zero,zero
	movq	%rdi, 896(%rsp)                 # 8-byte Spill
	vinsertps	$32, (%rdi,%r12), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vmaxss	%xmm8, %xmm5, %xmm5
	movq	%r8, 208(%rsp)                  # 8-byte Spill
	vinsertps	$48, (%r8,%r12), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm7, %ymm6
	vmaxps	%ymm9, %ymm6, %ymm6
	movq	24(%rsp), %rax                  # 8-byte Reload
	vmovss	(%rax,%r12), %xmm7              # xmm7 = mem[0],zero,zero,zero
	movq	96(%rsp), %rax                  # 8-byte Reload
	vmovss	(%rax,%r12), %xmm1              # xmm1 = mem[0],zero,zero,zero
	movq	1568(%rsp), %rbx                # 8-byte Reload
	vmovss	(%rbx,%r12), %xmm2              # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, %xmm0, %xmm5, %xmm5 # xmm5 = xmm5[0],xmm0[0],xmm5[2,3]
	movq	1824(%rsp), %r9                 # 8-byte Reload
	vinsertps	$16, (%r9,%r12), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, %xmm7, %xmm2, %xmm2 # xmm2 = xmm2[0,1],xmm7[0],xmm2[3]
	vinsertps	$32, %xmm7, %xmm5, %xmm5 # xmm5 = xmm5[0,1],xmm7[0],xmm5[3]
	movq	576(%rsp), %r14                 # 8-byte Reload
	vbroadcastss	(%r14,%r12), %xmm7
	movq	440(%rsp), %r8                  # 8-byte Reload
	vinsertps	$32, (%r8,%r12), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, %xmm1, %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],xmm1[0]
	vinsertps	$48, %xmm1, %xmm2, %xmm1 # xmm1 = xmm2[0,1,2],xmm1[0]
	movq	128(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, (%rdi,%r12), %xmm7, %xmm2 # xmm2 = xmm7[0,1,2],mem[0]
	vpermps	%ymm5, %ymm10, %ymm5
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vmulps	%ymm6, %ymm1, %ymm1
	vmulps	%ymm1, %ymm5, %ymm1
	movq	1344(%rsp), %r11                # 8-byte Reload
	vmovss	(%r11,%r12), %xmm2              # xmm2 = mem[0],zero,zero,zero
	movq	352(%rsp), %rax                 # 8-byte Reload
	vmovss	(%rax,%r12), %xmm5              # xmm5 = mem[0],zero,zero,zero
	vmaxss	%xmm11, %xmm5, %xmm6
	movq	320(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rdx,%r12), %xmm5              # xmm5 = mem[0],zero,zero,zero
	vmaxss	%xmm11, %xmm5, %xmm7
	movq	448(%rsp), %r10                 # 8-byte Reload
	vmovss	(%r10,%r12), %xmm5              # xmm5 = mem[0],zero,zero,zero
	vmaxss	%xmm11, %xmm5, %xmm3
	movq	1952(%rsp), %r15                # 8-byte Reload
	vmulss	(%r15,%r12), %xmm0, %xmm0
	vextractf128	$1, %ymm1, %xmm5
	vaddps	%xmm5, %xmm1, %xmm1
	movq	432(%rsp), %rcx                 # 8-byte Reload
	vmulss	(%rcx,%r12), %xmm2, %xmm4
	vpermilpd	$1, %xmm1, %xmm5        # xmm5 = xmm1[1,0]
	vaddps	%xmm5, %xmm1, %xmm1
	movq	8(%rsp), %rax                   # 8-byte Reload
	vmulss	(%rax,%r12), %xmm2, %xmm2
	vmovshdup	%xmm1, %xmm5            # xmm5 = xmm1[1,1,3,3]
	vaddss	%xmm5, %xmm1, %xmm5
	addq	%rsi, %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	vfmadd231ss	%xmm0, %xmm6, %xmm5     # xmm5 = (xmm6 * xmm0) + xmm5
	addq	%rsi, %rcx
	movq	%rcx, 432(%rsp)                 # 8-byte Spill
	addq	%rsi, %r15
	movq	%r15, 1952(%rsp)                # 8-byte Spill
	addq	%rsi, %rdi
	movq	%rdi, 128(%rsp)                 # 8-byte Spill
	vfmadd231ss	%xmm4, %xmm7, %xmm5     # xmm5 = (xmm7 * xmm4) + xmm5
	addq	%rsi, %r8
	movq	%r8, 440(%rsp)                  # 8-byte Spill
	addq	%rsi, %r9
	movq	%r9, 1824(%rsp)                 # 8-byte Spill
	addq	%rsi, %rbx
	movq	%rbx, 1568(%rsp)                # 8-byte Spill
	vfmadd231ss	%xmm2, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm2) + xmm5
	addq	%rsi, %r11
	movq	%r11, 1344(%rsp)                # 8-byte Spill
	addq	%rsi, %r14
	movq	%r14, 576(%rsp)                 # 8-byte Spill
	addq	%rsi, 96(%rsp)                  # 8-byte Folded Spill
	addq	%rsi, 24(%rsp)                  # 8-byte Folded Spill
	addq	%rsi, 48(%rsp)                  # 8-byte Folded Spill
	addq	%rsi, 288(%rsp)                 # 8-byte Folded Spill
	addq	%rsi, 160(%rsp)                 # 8-byte Folded Spill
	addq	%r13, 224(%rsp)                 # 8-byte Folded Spill
	addq	%r13, %r10
	movq	%r10, 448(%rsp)                 # 8-byte Spill
	addq	%r13, %rdx
	movq	%rdx, 320(%rsp)                 # 8-byte Spill
	addq	%r13, 352(%rsp)                 # 8-byte Folded Spill
	vaddss	%xmm5, %xmm12, %xmm12
	movq	64(%rsp), %r14                  # 8-byte Reload
	addq	%r13, %r14
	movq	520(%rsp), %rcx                 # 8-byte Reload
	addq	%r13, %rcx
	movq	656(%rsp), %rdx                 # 8-byte Reload
	addq	%r13, %rdx
	movq	208(%rsp), %r8                  # 8-byte Reload
	addq	%r13, %r8
	movq	896(%rsp), %rdi                 # 8-byte Reload
	addq	%r13, %rdi
	movq	560(%rsp), %rbx                 # 8-byte Reload
	addq	%r13, %rbx
	movq	424(%rsp), %r10                 # 8-byte Reload
	addq	%r13, %r10
	decq	56(%rsp)                        # 8-byte Folded Spill
	jne	.LBB0_256
# %bb.257:                              # %"consume f1.loopexit"
                                        #   in Loop: Header=BB0_254 Depth=1
	vmovd	%xmm12, %edi
.LBB0_258:                              # %"consume f1"
                                        #   in Loop: Header=BB0_254 Depth=1
	movq	552(%rsp), %rax                 # 8-byte Reload
	movq	%rax, %rdx
	subq	512(%rsp), %rdx                 # 8-byte Folded Reload
	movq	992(%rsp), %rsi                 # 8-byte Reload
	movl	%edi, (%rsi,%rdx,4)
	incq	%rax
	addq	$4, %r12
	cmpl	%eax, 984(%rsp)                 # 4-byte Folded Reload
	movq	40(%rsp), %r15                  # 8-byte Reload
	jne	.LBB0_254
# %bb.259:                              # %"consume conv1_stage2580.loopexit"
	vmovd	%xmm12, 7328(%rsp)
	movq	1808(%rsp), %rcx                # 8-byte Reload
.LBB0_260:                              # %"consume conv1_stage2580"
	cmpl	$0, 1312(%rsp)                  # 4-byte Folded Reload
	jle	.LBB0_272
# %bb.261:                              # %"for store_cost_output.s0.n.preheader"
	movslq	1984(%rsp), %r11                # 4-byte Folded Reload
	leaq	(,%r11,4), %r15
	shlq	$2, %rcx
	subq	%rcx, %r15
	movq	1304(%rsp), %rdi                # 8-byte Reload
	movq	%rdi, %r14
	shlq	$4, %r14
	leaq	(%r14,%r11), %rdx
	movq	672(%rsp), %rax                 # 8-byte Reload
	movq	%rax, %rcx
	shlq	$4, %rcx
	addq	%rax, %rcx
	subq	%rcx, %rdx
	movq	%rdx, 352(%rsp)                 # 8-byte Spill
	leaq	(%rdi,%rdi,4), %r8
	leaq	(%rdi,%r8,4), %rcx
	leaq	(%rcx,%r11), %rdx
	leaq	(%rax,%rax,4), %rsi
	leaq	(%rax,%rsi,4), %rbx
	leaq	(%rax,%rbx), %rsi
	subq	%rsi, %rdx
	movq	%rdx, 320(%rsp)                 # 8-byte Spill
	leaq	(%r11,%r8,4), %rdx
	subq	%rbx, %rdx
	movq	%rdx, 448(%rsp)                 # 8-byte Spill
	leaq	(%rdi,%rdi,8), %rdx
	leaq	(%rdi,%rdx,2), %rsi
	addq	%r11, %rsi
	leaq	(,%rax,4), %rbx
	leaq	(%rbx,%rbx,4), %rbx
	subq	%rbx, %rsi
	movq	%rsi, 224(%rsp)                 # 8-byte Spill
	leaq	(%r11,%rdx,2), %rsi
	leaq	(%rax,%rax,8), %rdx
	leaq	(%rax,%rdx,2), %rdx
	subq	%rdx, %rsi
	movq	%rsi, 160(%rsp)                 # 8-byte Spill
	addq	%rdi, %r14
	addq	%r11, %r14
	leaq	(%rax,%rax), %rdx
	leaq	(%rdx,%rdx,8), %rdx
	subq	%rdx, %r14
	leaq	(%rax,%rax,2), %rsi
	shlq	$3, %rsi
	movq	%rax, %r12
	subq	%rsi, %r12
	addq	%rdi, %rcx
	addq	%r11, %rcx
	addq	%rcx, %r12
	leaq	(%rdi,%rdi,2), %rcx
	shlq	$3, %rcx
	subq	%rdi, %rcx
	addq	%r11, %rcx
	leaq	(,%rax,8), %rsi
	leaq	(%rsi,%rsi,2), %rsi
	subq	%rsi, %rcx
	movl	88(%rsp), %edx                  # 4-byte Reload
	movl	840(%rsp), %esi                 # 4-byte Reload
	subl	%edx, %esi
	movl	976(%rsp), %r9d                 # 4-byte Reload
	subl	%edx, %r9d
	movl	216(%rsp), %r13d                # 4-byte Reload
	subl	%edx, %r13d
	movl	144(%rsp), %ebx                 # 4-byte Reload
	subl	%edx, %ebx
	movl	864(%rsp), %r8d                 # 4-byte Reload
	subl	%edx, %r8d
	movl	488(%rsp), %edi                 # 4-byte Reload
	subl	%edx, %edi
	movl	152(%rsp), %eax                 # 4-byte Reload
	subl	%edx, %eax
	movl	664(%rsp), %r10d                # 4-byte Reload
	subl	%edx, %r10d
	movslq	%esi, %rdx
	movslq	%r9d, %rsi
	movslq	%r13d, %r13
	movslq	%ebx, %rbx
	movslq	%r8d, %r8
	movslq	%edi, %rdi
	movslq	%eax, %r9
	movslq	%r10d, %r10
	addq	%r11, %rdx
	movq	80(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 896(%rsp)                 # 8-byte Spill
	addq	%r11, %rsi
	leaq	(%rax,%rsi,4), %rdx
	movq	%rdx, 208(%rsp)                 # 8-byte Spill
	addq	%rax, %r15
	movq	%r15, 520(%rsp)                 # 8-byte Spill
	addq	%r11, %r13
	leaq	(%rax,%r13,4), %rdx
	movq	%rdx, 64(%rsp)                  # 8-byte Spill
	addq	%r11, %rbx
	leaq	(%rax,%rbx,4), %rdx
	movq	%rdx, 56(%rsp)                  # 8-byte Spill
	movq	40(%rsp), %r15                  # 8-byte Reload
	addq	%r11, %r8
	leaq	(%rax,%r8,4), %rdx
	movq	%rdx, 552(%rsp)                 # 8-byte Spill
	addq	%r11, %rdi
	leaq	(%rax,%rdi,4), %rdx
	movq	%rdx, 992(%rsp)                 # 8-byte Spill
	addq	%r11, %r9
	leaq	(%rax,%r9,4), %rdx
	movq	%rdx, 984(%rsp)                 # 8-byte Spill
	addq	%r11, %r10
	leaq	(%rax,%r10,4), %rax
	movq	%rax, 512(%rsp)                 # 8-byte Spill
	movq	72(%rsp), %rax                  # 8-byte Reload
	movq	352(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 416(%rsp)                 # 8-byte Spill
	movslq	200(%rsp), %rdx                 # 4-byte Folded Reload
	shlq	$7, %rdx
	movq	%rdx, 8(%rsp)                   # 8-byte Spill
	movq	320(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 544(%rsp)                 # 8-byte Spill
	movq	448(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 504(%rsp)                 # 8-byte Spill
	movq	224(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 272(%rsp)                 # 8-byte Spill
	movq	160(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 712(%rsp)                 # 8-byte Spill
	leaq	(%rax,%r14,4), %rdx
	movq	%rdx, 704(%rsp)                 # 8-byte Spill
	leaq	(%rax,%r12,4), %rdx
	movq	%rdx, 640(%rsp)                 # 8-byte Spill
	leaq	(%rax,%rcx,4), %rax
	movq	%rax, 632(%rsp)                 # 8-byte Spill
	vmovss	.LCPI0_91(%rip), %xmm11         # xmm11 = mem[0],zero,zero,zero
	vmovss	.LCPI0_103(%rip), %xmm8         # xmm8 = mem[0],zero,zero,zero
	vpxor	%xmm12, %xmm12, %xmm12
	vmovss	.LCPI0_104(%rip), %xmm9         # xmm9 = mem[0],zero,zero,zero
	movq	408(%rsp), %rax                 # 8-byte Reload
	leaq	(,%rax,4), %rax
	movq	%rax, 424(%rsp)                 # 8-byte Spill
	xorl	%ecx, %ecx
	movq	%r11, 560(%rsp)                 # 8-byte Spill
	vpxor	%xmm10, %xmm10, %xmm10
	.p2align	4, 0x90
.LBB0_262:                              # %"for store_cost_output.s0.n"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_266 Depth 2
	movq	%r11, 656(%rsp)                 # 8-byte Spill
	testl	%r15d, %r15d
	jle	.LBB0_270
# %bb.263:                              # %"for f3.s1.r77$x.preheader"
                                        #   in Loop: Header=BB0_262 Depth=1
	movq	632(%rsp), %rdx                 # 8-byte Reload
	movq	640(%rsp), %r10                 # 8-byte Reload
	movq	704(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 352(%rsp)                 # 8-byte Spill
	movq	712(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	movq	272(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	movq	504(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	movq	544(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	movq	416(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	512(%rsp), %rsi                 # 8-byte Reload
	movq	984(%rsp), %rbx                 # 8-byte Reload
	movq	992(%rsp), %rdi                 # 8-byte Reload
	movq	552(%rsp), %r11                 # 8-byte Reload
	movq	56(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 448(%rsp)                 # 8-byte Spill
	vpxor	%xmm14, %xmm14, %xmm14
	movq	64(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	movq	520(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movq	208(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 576(%rsp)                 # 8-byte Spill
	movq	896(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 1344(%rsp)                # 8-byte Spill
	xorl	%eax, %eax
	jmp	.LBB0_266
	.p2align	4, 0x90
.LBB0_264:                              # %"for f3.s1.r77$x"
                                        #   in Loop: Header=BB0_266 Depth=2
	vxorps	%xmm3, %xmm3, %xmm3
	movq	448(%rsp), %rdi                 # 8-byte Reload
	movq	224(%rsp), %rsi                 # 8-byte Reload
.LBB0_265:                              # %"for f3.s1.r77$x"
                                        #   in Loop: Header=BB0_266 Depth=2
	movq	576(%rsp), %r8                  # 8-byte Reload
	vmulss	(%r8,%rcx), %xmm5, %xmm0
	vmovd	%ebx, %xmm5
	movq	24(%rsp), %r15                  # 8-byte Reload
	vmovss	(%r15,%rcx), %xmm2              # xmm2 = mem[0],zero,zero,zero
	movq	160(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, (%rdx,%rcx), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	352(%rsp), %rax                 # 8-byte Reload
	vmovss	(%rax,%rcx), %xmm7              # xmm7 = mem[0],zero,zero,zero
	movq	288(%rsp), %r12                 # 8-byte Reload
	vinsertps	$16, (%r12,%rcx), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmaxps	%xmm12, %xmm7, %xmm7
	movq	320(%rsp), %r13                 # 8-byte Reload
	vmovss	(%r13,%rcx), %xmm4              # xmm4 = mem[0],zero,zero,zero
	movq	48(%rsp), %r14                  # 8-byte Reload
	vinsertps	$16, (%r14,%rcx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmaxps	%xmm12, %xmm4, %xmm4
	vpbroadcastb	%xmm5, %xmm5
	vpslld	$31, %xmm5, %xmm5
	vblendvps	%xmm5, %xmm7, %xmm4, %xmm4
	vmovd	%r9d, %xmm5
	vmaxps	%xmm12, %xmm2, %xmm2
	vpbroadcastb	%xmm5, %xmm5
	vpslld	$31, %xmm5, %xmm5
	vblendvps	%xmm5, %xmm2, %xmm4, %xmm2
	movq	1344(%rsp), %rbx                # 8-byte Reload
	vinsertps	$16, (%rbx,%rcx), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmulps	%xmm2, %xmm0, %xmm0
	vdivss	%xmm1, %xmm0, %xmm1
	vmovshdup	%xmm0, %xmm0            # xmm0 = xmm0[1,1,3,3]
	vaddss	%xmm0, %xmm1, %xmm0
	movq	96(%rsp), %r10                  # 8-byte Reload
	vfmadd132ss	(%r10,%rcx), %xmm3, %xmm0 # xmm0 = (xmm0 * mem) + xmm3
	vfmadd213ss	%xmm0, %xmm13, %xmm6    # xmm6 = (xmm13 * xmm6) + xmm0
	vfmadd231ss	%xmm9, %xmm6, %xmm14    # xmm14 = (xmm6 * xmm9) + xmm14
	movq	432(%rsp), %rax                 # 8-byte Reload
	incq	%rax
	movq	424(%rsp), %r9                  # 8-byte Reload
	addq	%r9, %rbx
	movq	%rbx, 1344(%rsp)                # 8-byte Spill
	addq	%r9, %r8
	movq	%r8, 576(%rsp)                  # 8-byte Spill
	addq	%r9, %r10
	movq	%r10, 96(%rsp)                  # 8-byte Spill
	addq	%r9, %rsi
	movq	%rsi, 224(%rsp)                 # 8-byte Spill
	addq	%r9, %rdi
	movq	%rdi, 448(%rsp)                 # 8-byte Spill
	addq	%r9, %r11
	movq	128(%rsp), %rdi                 # 8-byte Reload
	addq	%r9, %rdi
	movq	440(%rsp), %rbx                 # 8-byte Reload
	addq	%r9, %rbx
	movq	1824(%rsp), %rsi                # 8-byte Reload
	addq	%r9, %rsi
	movq	8(%rsp), %r9                    # 8-byte Reload
	addq	%r9, %r15
	movq	%r15, 24(%rsp)                  # 8-byte Spill
	addq	%r9, %r14
	movq	%r14, 48(%rsp)                  # 8-byte Spill
	addq	%r9, %r12
	movq	%r12, 288(%rsp)                 # 8-byte Spill
	addq	%r9, %rdx
	movq	%rdx, 160(%rsp)                 # 8-byte Spill
	addq	%r9, %r13
	movq	%r13, 320(%rsp)                 # 8-byte Spill
	addq	%r9, 352(%rsp)                  # 8-byte Folded Spill
	movq	1952(%rsp), %r10                # 8-byte Reload
	addq	%r9, %r10
	movq	1568(%rsp), %rdx                # 8-byte Reload
	addq	%r9, %rdx
	cmpq	%rax, 648(%rsp)                 # 8-byte Folded Reload
	je	.LBB0_268
.LBB0_266:                              # %"for f3.s1.r77$x"
                                        #   Parent Loop BB0_262 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	%rsi, 1824(%rsp)                # 8-byte Spill
	vmovss	(%rsi,%rcx), %xmm1              # xmm1 = mem[0],zero,zero,zero
	movq	%rbx, 440(%rsp)                 # 8-byte Spill
	vmovss	(%rbx,%rcx), %xmm5              # xmm5 = mem[0],zero,zero,zero
	testq	%rax, %rax
	sete	%bl
	vucomiss	%xmm5, %xmm11
	setb	%r9b
	vmaxss	%xmm11, %xmm1, %xmm1
	vmovss	(%rdx,%rcx), %xmm3              # xmm3 = mem[0],zero,zero,zero
	vdivss	%xmm1, %xmm8, %xmm6
	vminss	%xmm5, %xmm6, %xmm6
	vmulss	(%r11,%rcx), %xmm6, %xmm6
	vmaxss	%xmm10, %xmm3, %xmm13
	vmulss	%xmm6, %xmm5, %xmm3
	vmulss	(%rdi,%rcx), %xmm3, %xmm6
	movq	%rdx, 1568(%rsp)                # 8-byte Spill
	movq	%rdi, 128(%rsp)                 # 8-byte Spill
	movq	%r10, 1952(%rsp)                # 8-byte Spill
	movq	%rax, 432(%rsp)                 # 8-byte Spill
	jae	.LBB0_264
# %bb.267:                              #   in Loop: Header=BB0_266 Depth=2
	vmovss	(%r10,%rcx), %xmm3              # xmm3 = mem[0],zero,zero,zero
	vmaxss	%xmm10, %xmm3, %xmm3
	movq	448(%rsp), %rdi                 # 8-byte Reload
	vmovss	(%rdi,%rcx), %xmm0              # xmm0 = mem[0],zero,zero,zero
	movq	224(%rsp), %rsi                 # 8-byte Reload
	vaddss	(%rsi,%rcx), %xmm0, %xmm0
	vmulss	%xmm0, %xmm3, %xmm0
	vdivss	%xmm1, %xmm0, %xmm3
	jmp	.LBB0_265
	.p2align	4, 0x90
.LBB0_268:                              # %"consume f3.loopexit"
                                        #   in Loop: Header=BB0_262 Depth=1
	vmovd	%xmm14, %eax
.LBB0_269:                              # %"consume f3"
                                        #   in Loop: Header=BB0_262 Depth=1
	movq	656(%rsp), %r11                 # 8-byte Reload
	movq	%r11, %rdx
	subq	560(%rsp), %rdx                 # 8-byte Folded Reload
	movq	1472(%rsp), %rsi                # 8-byte Reload
	movl	%eax, (%rsi,%rdx,4)
	incq	%r11
	addq	$4, %rcx
	cmpl	%r11d, 1464(%rsp)               # 4-byte Folded Reload
	movq	40(%rsp), %r15                  # 8-byte Reload
	jne	.LBB0_262
	jmp	.LBB0_271
.LBB0_270:                              #   in Loop: Header=BB0_262 Depth=1
	vpxor	%xmm14, %xmm14, %xmm14
	xorl	%eax, %eax
	jmp	.LBB0_269
.LBB0_271:                              # %"consume conv1_stage2581.loopexit"
	vmovd	%xmm14, 7328(%rsp)
.LBB0_272:                              # %"consume conv1_stage2581"
	cmpl	$0, 1320(%rsp)                  # 4-byte Folded Reload
	movl	36(%rsp), %edx                  # 4-byte Reload
	vmovaps	1616(%rsp), %xmm11              # 16-byte Reload
	movl	872(%rsp), %r9d                 # 4-byte Reload
	jle	.LBB0_285
# %bb.273:                              # %"for compute_cost_output.s0.n.preheader"
	movslq	1816(%rsp), %r10                # 4-byte Folded Reload
	movl	88(%rsp), %eax                  # 4-byte Reload
	subl	%eax, %r9d
	movl	488(%rsp), %r8d                 # 4-byte Reload
	subl	%eax, %r8d
	movl	152(%rsp), %r11d                # 4-byte Reload
	subl	%eax, %r11d
	subl	%eax, 216(%rsp)                 # 4-byte Folded Spill
	subl	%eax, 144(%rsp)                 # 4-byte Folded Spill
	subl	%eax, 480(%rsp)                 # 4-byte Folded Spill
	movslq	200(%rsp), %r12                 # 4-byte Folded Reload
	cmpl	$31, %r15d
	seta	352(%rsp)                       # 1-byte Folded Spill
	leaq	(%r12,%r12,2), %rcx
	cmpl	$1, %edx
	sete	416(%rsp)                       # 1-byte Folded Spill
	vmovq	%rcx, %xmm0
	leaq	(,%r10,4), %rdi
	movq	1304(%rsp), %rcx                # 8-byte Reload
	leaq	(%rdi,%rcx,8), %r13
	movq	672(%rsp), %rsi                 # 8-byte Reload
	leaq	(,%rsi,4), %rbx
	leaq	(%rbx,%rbx,2), %rdx
	subq	%rdx, %r13
	leaq	(%rcx,%rcx,2), %rdx
	addq	%r10, %rcx
	addq	%rsi, %rsi
	subq	%rsi, %rcx
	addq	%r10, %rdx
	subq	%rbx, %rdx
	subq	%rbx, %rdi
	movslq	%r9d, %rbx
	movslq	%r8d, %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movslq	%r11d, %r9
	movslq	216(%rsp), %r14                 # 4-byte Folded Reload
	movslq	144(%rsp), %r11                 # 4-byte Folded Reload
	movslq	480(%rsp), %rsi                 # 4-byte Folded Reload
	movb	416(%rsp), %al                  # 1-byte Reload
	andb	352(%rsp), %al                  # 1-byte Folded Reload
	movb	%al, 416(%rsp)                  # 1-byte Spill
	movq	648(%rsp), %rax                 # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-32, %eax
	movq	%rax, 272(%rsp)                 # 8-byte Spill
	vbroadcastss	%xmm11, %ymm8
	vmovd	200(%rsp), %xmm1                # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm1, %ymm9
	vmovq	%r12, %xmm1
	vpbroadcastq	%xmm1, %ymm1
	vpbroadcastq	%xmm0, %ymm10
	addq	%r10, %rbx
	movq	%rbx, 424(%rsp)                 # 8-byte Spill
	movq	408(%rsp), %rax                 # 8-byte Reload
	shlq	$7, %rax
	movq	%rax, 520(%rsp)                 # 8-byte Spill
	movq	8(%rsp), %r8                    # 8-byte Reload
	addq	%r10, %r8
	addq	%r10, %r9
	movq	%r9, %rbx
	addq	%r10, %r14
	addq	%r10, %r11
	addq	%r10, %rsi
	movq	72(%rsp), %rax                  # 8-byte Reload
	addq	%rax, %r13
	movq	%r13, 704(%rsp)                 # 8-byte Spill
	shlq	$7, %r12
	movq	%r12, 504(%rsp)                 # 8-byte Spill
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, 640(%rsp)                 # 8-byte Spill
	leaq	(%rax,%rcx,4), %rcx
	movq	%rcx, 632(%rsp)                 # 8-byte Spill
	addq	%rax, %rdi
	movq	%rdi, 712(%rsp)                 # 8-byte Spill
	vmovss	.LCPI0_91(%rip), %xmm12         # xmm12 = mem[0],zero,zero,zero
	vxorps	%xmm7, %xmm7, %xmm7
	movq	408(%rsp), %rax                 # 8-byte Reload
	leaq	(,%rax,4), %rdi
	xorl	%r13d, %r13d
	movq	%rsi, 552(%rsp)                 # 8-byte Spill
	movq	%r11, %r9
	movq	%r11, 1472(%rsp)                # 8-byte Spill
	movq	%r14, %r11
	movq	%r14, 1464(%rsp)                # 8-byte Spill
	movq	%rbx, %r14
	movq	%rbx, 992(%rsp)                 # 8-byte Spill
	movq	%r8, %rax
	movq	%r8, 984(%rsp)                  # 8-byte Spill
	movq	424(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 512(%rsp)                 # 8-byte Spill
	movq	80(%rsp), %rbx                  # 8-byte Reload
	movq	%r10, 544(%rsp)                 # 8-byte Spill
	movq	%r10, 208(%rsp)                 # 8-byte Spill
	movq	%r8, 8(%rsp)                    # 8-byte Spill
	movq	%r14, 656(%rsp)                 # 8-byte Spill
	movq	%r11, 560(%rsp)                 # 8-byte Spill
	movq	%r9, 48(%rsp)                   # 8-byte Spill
	movq	%rsi, 24(%rsp)                  # 8-byte Spill
	vmovdqa	%ymm1, 96(%rsp)                 # 32-byte Spill
	movq	%rdi, 128(%rsp)                 # 8-byte Spill
	vmovdqa	%ymm9, 352(%rsp)                # 32-byte Spill
	vmovdqa	%ymm10, 576(%rsp)               # 32-byte Spill
	vmovaps	%ymm8, 896(%rsp)                # 32-byte Spill
	.p2align	4, 0x90
.LBB0_274:                              # %"for compute_cost_output.s0.n"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_277 Depth 2
                                        #     Child Loop BB0_281 Depth 2
	vpxor	%xmm0, %xmm0, %xmm0
	movl	$0, %eax
	testl	%r15d, %r15d
	jle	.LBB0_283
# %bb.275:                              # %"for f5.s1.r77$x.preheader"
                                        #   in Loop: Header=BB0_274 Depth=1
	movq	%r13, 64(%rsp)                  # 8-byte Spill
	cmpb	$0, 416(%rsp)                   # 1-byte Folded Reload
	movq	%rbx, 56(%rsp)                  # 8-byte Spill
	je	.LBB0_279
# %bb.276:                              # %vector.ph1425
                                        #   in Loop: Header=BB0_274 Depth=1
	movq	208(%rsp), %rax                 # 8-byte Reload
	subq	672(%rsp), %rax                 # 8-byte Folded Reload
	vmovq	%rax, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 1952(%rsp)               # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 448(%rsp)                # 32-byte Spill
	movq	%rbx, %r9
	movq	272(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 432(%rsp)                 # 8-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 224(%rsp)                # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 160(%rsp)                # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 288(%rsp)                # 32-byte Spill
	vmovdqa	.LCPI0_105(%rip), %ymm1         # ymm1 = [0,1,2,3,4,5,6,7]
	movq	72(%rsp), %r12                  # 8-byte Reload
	.p2align	4, 0x90
.LBB0_277:                              # %vector.body1422
                                        #   Parent Loop BB0_274 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vpbroadcastd	.LCPI0_7(%rip), %ymm0   # ymm0 = [8,8,8,8,8,8,8,8]
	vpbroadcastd	.LCPI0_106(%rip), %ymm14 # ymm14 = [16,16,16,16,16,16,16,16]
	vmovdqa	352(%rsp), %ymm3                # 32-byte Reload
	vmovdqa	%ymm1, %ymm2
	vpmulld	%ymm3, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm2, %ymm0
	vmovdqa	%ymm2, %ymm10
	vmovdqa	%ymm2, 320(%rsp)                # 32-byte Spill
	vpmulld	%ymm3, %ymm0, %ymm5
	vmovdqa	%ymm5, 1344(%rsp)               # 32-byte Spill
	vextracti128	$1, %ymm1, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vpmovsxdq	%xmm1, %ymm8
	vpsllq	$5, %ymm8, %ymm1
	vpsllq	$5, %ymm0, %ymm2
	vmovdqa	1952(%rsp), %ymm3               # 32-byte Reload
	vpaddq	%ymm3, %ymm2, %ymm7
	vpaddq	%ymm3, %ymm1, %ymm4
	vmovdqa	%ymm3, %ymm12
	vpextrq	$1, %xmm4, %rbx
	vpmovsxdq	%xmm5, %ymm11
	vmovdqa	%ymm11, 1568(%rsp)              # 32-byte Spill
	vmovq	%xmm4, %rdx
	vextracti128	$1, %ymm4, %xmm1
	vpextrq	$1, %xmm1, %rax
	vmovq	%xmm1, %rcx
	vmovq	%xmm7, %rdi
	vpextrq	$1, %xmm7, %rsi
	vmovss	(%r12,%rdx,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r12,%rbx,4), %xmm1, %xmm2 # xmm2 = xmm1[0],mem[0],xmm1[2,3]
	vextracti128	$1, %ymm7, %xmm1
	vmovdqa	96(%rsp), %ymm6                 # 32-byte Reload
	vpaddq	%ymm6, %ymm7, %ymm3
	vpaddq	%ymm6, %ymm4, %ymm5
	vmovdqa	%ymm6, %ymm9
	vmovq	%xmm5, %rbx
	vmovss	(%r12,%rdi,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm5, %rdi
	vinsertps	$32, (%r12,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vextracti128	$1, %ymm5, %xmm5
	vmovq	%xmm5, %rdx
	vinsertps	$16, (%r12,%rsi,4), %xmm6, %xmm15 # xmm15 = xmm6[0],mem[0],xmm6[2,3]
	vpextrq	$1, %xmm5, %rcx
	vinsertps	$48, (%r12,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%ymm2, 1824(%rsp)               # 32-byte Spill
	vmovq	%xmm3, %rsi
	vpextrq	$1, %xmm3, %rax
	vmovq	%xmm1, %r8
	vextracti128	$1, %ymm3, %xmm6
	vmovss	(%r12,%rsi,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm1, %rsi
	vmovss	(%r12,%rbx,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r12,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpsllq	$5, %ymm11, %ymm3
	vinsertps	$32, (%r12,%rdx,4), %xmm1, %xmm2 # xmm2 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$16, (%r12,%rax,4), %xmm5, %xmm1 # xmm1 = xmm5[0],mem[0],xmm5[2,3]
	vpsllq	$4, %ymm8, %ymm5
	vpsllq	$4, %ymm0, %ymm0
	vinsertps	$48, (%r12,%rcx,4), %xmm2, %xmm8 # xmm8 = xmm2[0,1,2],mem[0]
	vpaddq	%ymm0, %ymm9, %ymm2
	vpaddq	%ymm5, %ymm9, %ymm0
	vpaddq	%ymm0, %ymm0, %ymm5
	vpaddq	%ymm3, %ymm12, %ymm0
	vpaddq	%ymm2, %ymm2, %ymm2
	vpaddq	%ymm2, %ymm12, %ymm2
	vinsertps	$32, (%r12,%r8,4), %xmm15, %xmm11 # xmm11 = xmm15[0,1],mem[0],xmm15[3]
	vpaddq	%ymm5, %ymm12, %ymm3
	vpextrq	$1, %xmm3, %rbx
	vmovq	%xmm6, %rcx
	vmovq	%xmm3, %rdx
	vextracti128	$1, %ymm3, %xmm3
	vpextrq	$1, %xmm6, %rdi
	vmovq	%xmm3, %r11
	vpextrq	$1, %xmm3, %r14
	vextracti128	$1, %ymm0, %xmm5
	vmovq	%xmm2, %rax
	vpextrq	$1, %xmm2, %r10
	vmovq	%xmm5, 440(%rsp)                # 8-byte Folded Spill
	vextracti128	$1, %ymm2, %xmm2
	vmovss	(%r12,%rax,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm5, %rax
	vmovss	(%r12,%rdx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%r12,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovq	%xmm2, %r8
	vpextrq	$1, %xmm2, %rbx
	movq	24(%rsp), %rdx                  # 8-byte Reload
	vmovups	(%r9,%rdx,4), %ymm2
	vpaddd	%ymm14, %ymm10, %ymm15
	movq	48(%rsp), %rdx                  # 8-byte Reload
	vmulps	(%r9,%rdx,4), %ymm2, %ymm6
	vinsertps	$48, (%r12,%rsi,4), %xmm11, %xmm11 # xmm11 = xmm11[0,1,2],mem[0]
	vmovdqa	576(%rsp), %ymm13               # 32-byte Reload
	vpaddq	%ymm7, %ymm13, %ymm2
	vmovq	%xmm2, %rdx
	vinsertps	$32, (%r12,%rcx,4), %xmm1, %xmm7 # xmm7 = xmm1[0,1],mem[0],xmm1[3]
	vpextrq	$1, %xmm2, %rcx
	vextracti128	$1, %ymm2, %xmm2
	vmovaps	1824(%rsp), %ymm1               # 32-byte Reload
	vinsertf128	$1, %xmm11, %ymm1, %ymm1
	vmovss	(%r12,%rdx,4), %xmm14           # xmm14 = mem[0],zero,zero,zero
	vmovq	%xmm2, %rdx
	vinsertps	$48, (%r12,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vpextrq	$1, %xmm2, %rsi
	movq	560(%rsp), %r15                 # 8-byte Reload
	vmovups	(%r9,%r15,4), %ymm11
	vinsertf128	$1, %xmm7, %ymm8, %ymm10
	movq	424(%rsp), %r13                 # 8-byte Reload
	vpxor	%xmm9, %xmm9, %xmm9
	vcmpeqps	(%r9,%r13,4), %ymm9, %ymm8
	vpmulld	352(%rsp), %ymm15, %ymm15       # 32-byte Folded Reload
	vmovdqa	1344(%rsp), %ymm2               # 32-byte Reload
	vextracti128	$1, %ymm2, %xmm2
	vpaddq	%ymm4, %ymm13, %ymm4
	vinsertps	$32, (%r12,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrq	$1, %xmm4, %rdi
	vinsertps	$16, (%r12,%rcx,4), %xmm14, %xmm7 # xmm7 = xmm14[0],mem[0],xmm14[2,3]
	vmovq	%xmm4, %rcx
	vextracti128	$1, %ymm4, %xmm4
	vinsertps	$16, (%r12,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vpextrq	$1, %xmm4, %r11
	vinsertps	$32, (%r12,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vmovq	%xmm4, %rdx
	vmovss	(%r12,%rcx,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vinsertps	$32, (%r12,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, (%r12,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, (%r12,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$32, (%r12,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r12,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vpmovsxdq	%xmm2, %ymm2
	vinsertps	$48, (%r12,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm4, %ymm4
	vpmovsxdq	%xmm15, %ymm7
	vinsertps	$48, (%r12,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm5, %ymm5
	vpsllq	$5, %ymm2, %ymm3
	vmaxps	%ymm9, %ymm10, %ymm10
	vmaxps	%ymm9, %ymm4, %ymm4
	vmulps	%ymm10, %ymm11, %ymm10
	vmulps	%ymm4, %ymm11, %ymm14
	vpsllq	$5, %ymm7, %ymm4
	vpaddq	%ymm4, %ymm12, %ymm4
	vpaddq	%ymm3, %ymm12, %ymm11
	vmovq	%xmm0, %rcx
	vpextrq	$1, %xmm0, %rdx
	vmovq	%xmm11, %rdi
	vpextrq	$1, %xmm11, %rbx
	vmaxps	%ymm9, %ymm1, %ymm1
	vfmadd231ps	%ymm1, %ymm6, %ymm10    # ymm10 = (ymm6 * ymm1) + ymm10
	vextracti128	$1, %ymm4, %xmm1
	vmaxps	%ymm9, %ymm5, %ymm3
	vfmadd231ps	%ymm3, %ymm6, %ymm14    # ymm14 = (ymm6 * ymm3) + ymm14
	vextracti128	$1, %ymm11, %xmm3
	vmovq	%xmm1, %r10
	vmovss	(%r12,%rcx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm1, %r8
	vinsertps	$16, (%r12,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vblendvps	%ymm8, %ymm10, %ymm14, %ymm1
	vmovaps	%ymm1, 1344(%rsp)               # 32-byte Spill
	vmovdqa	96(%rsp), %ymm9                 # 32-byte Reload
	vpaddq	%ymm9, %ymm11, %ymm6
	vpaddq	%ymm0, %ymm9, %ymm8
	vmovq	%xmm8, %rdx
	vmovss	(%r12,%rdi,4), %xmm14           # xmm14 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm8, %rdi
	movq	440(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$32, (%r12,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vextracti128	$1, %ymm8, %xmm1
	vmovq	%xmm1, %rcx
	vinsertps	$16, (%r12,%rbx,4), %xmm14, %xmm8 # xmm8 = xmm14[0],mem[0],xmm14[2,3]
	vpextrq	$1, %xmm1, %rbx
	vinsertps	$48, (%r12,%rax,4), %xmm5, %xmm14 # xmm14 = xmm5[0,1,2],mem[0]
	vmovq	%xmm6, %rax
	vpextrq	$1, %xmm6, %rsi
	vmovq	%xmm3, %r11
	vextracti128	$1, %ymm6, %xmm5
	vmovss	(%r12,%rdx,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm3, %r14
	vinsertps	$16, (%r12,%rdi,4), %xmm6, %xmm3 # xmm3 = xmm6[0],mem[0],xmm6[2,3]
	vmovdqa	1568(%rsp), %ymm1               # 32-byte Reload
	vpsllq	$4, %ymm1, %ymm6
	vmovss	(%r12,%rax,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vpsllq	$4, %ymm2, %ymm2
	vpaddq	%ymm2, %ymm9, %ymm2
	vinsertps	$32, (%r12,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vpaddq	%ymm6, %ymm9, %ymm6
	vpaddq	%ymm6, %ymm6, %ymm6
	vinsertps	$16, (%r12,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpaddq	%ymm2, %ymm2, %ymm2
	vmovdqa	%ymm12, %ymm10
	vpaddq	%ymm2, %ymm12, %ymm2
	vinsertps	$48, (%r12,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vpaddq	%ymm6, %ymm12, %ymm6
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm5, %rcx
	vmovq	%xmm6, %rsi
	vextracti128	$1, %ymm6, %xmm6
	vpextrq	$1, %xmm5, %rdi
	vpextrq	$1, %xmm6, %rbx
	vmovq	%xmm2, %rdx
	vmovss	(%r12,%rdx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vmovq	%xmm6, %rdx
	vmovss	(%r12,%rsi,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vinsertps	$32, (%r12,%r11,4), %xmm8, %xmm8 # xmm8 = xmm8[0,1],mem[0],xmm8[3]
	vpextrq	$1, %xmm2, %rsi
	vinsertps	$16, (%r12,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$16, (%r12,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, (%r12,%r14,4), %xmm8, %xmm8 # xmm8 = xmm8[0,1,2],mem[0]
	vinsertps	$32, (%r12,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$32, (%r12,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vextracti128	$1, %ymm2, %xmm2
	vpaddq	%ymm0, %ymm13, %ymm0
	vmovq	%xmm0, %rax
	vmovq	%xmm2, %rcx
	vpextrq	$1, %xmm0, %rdx
	vinsertps	$48, (%r12,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %rsi
	vinsertps	$32, (%r12,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrq	$1, %xmm0, %rcx
	vpbroadcastd	.LCPI0_107(%rip), %ymm0 # ymm0 = [24,24,24,24,24,24,24,24]
	vpaddd	320(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vmovdqa	%ymm0, 1568(%rsp)               # 32-byte Spill
	vinsertps	$48, (%r12,%rbx,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1,2],mem[0]
	vpextrq	$1, %xmm2, %rdi
	movq	24(%rsp), %r11                  # 8-byte Reload
	vmovups	32(%r9,%r11,4), %ymm2
	movq	48(%rsp), %r14                  # 8-byte Reload
	vmulps	32(%r9,%r14,4), %ymm2, %ymm12
	vinsertf128	$1, %xmm8, %ymm14, %ymm8
	vmovups	32(%r9,%r15,4), %ymm6
	vextracti128	$1, %ymm15, %xmm2
	vinsertps	$48, (%r12,%rdi,4), %xmm5, %xmm15 # xmm15 = xmm5[0,1,2],mem[0]
	vpmovsxdq	%xmm2, %ymm14
	vpaddq	%ymm13, %ymm11, %ymm2
	vmovdqa	%ymm13, %ymm5
	vmovq	%xmm2, %rdi
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	vmovss	(%r12,%rdi,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rdi
	vextracti128	$1, %ymm2, %xmm2
	vinsertps	$16, (%r12,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovq	%xmm2, %rdi
	vinsertf128	$1, %xmm15, %ymm0, %ymm11
	vmovss	(%r12,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rax
	vpsllq	$5, %ymm14, %ymm2
	vinsertps	$16, (%r12,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r12,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovq	%xmm4, %rdx
	vpextrq	$1, %xmm4, %rdi
	vpaddq	%ymm2, %ymm10, %ymm2
	vinsertps	$32, (%r12,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrq	$1, %xmm2, %rsi
	vinsertps	$48, (%r12,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, (%r12,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovq	%xmm2, %rax
	vinsertf128	$1, %xmm3, %ymm0, %ymm3
	vmovss	(%r12,%rdx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vxorps	%xmm15, %xmm15, %xmm15
	vmaxps	%ymm15, %ymm1, %ymm1
	vmulps	%ymm1, %ymm6, %ymm13
	vmaxps	%ymm15, %ymm3, %ymm3
	vxorps	%xmm1, %xmm1, %xmm1
	vmulps	%ymm3, %ymm6, %ymm15
	vextracti128	$1, %ymm2, %xmm3
	vinsertps	$16, (%r12,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmaxps	%ymm1, %ymm8, %ymm8
	vfmadd231ps	%ymm8, %ymm12, %ymm13   # ymm13 = (ymm12 * ymm8) + ymm13
	vpaddq	%ymm2, %ymm9, %ymm8
	vmaxps	%ymm1, %ymm11, %ymm11
	vfmadd231ps	%ymm11, %ymm12, %ymm15  # ymm15 = (ymm12 * ymm11) + ymm15
	vmovss	(%r12,%rax,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vpaddq	%ymm4, %ymm9, %ymm11
	vmovq	%xmm11, %rax
	vinsertps	$32, (%r12,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrq	$1, %xmm11, %rcx
	vextracti128	$1, %ymm11, %xmm6
	vmovq	%xmm6, %rdx
	vpextrq	$1, %xmm6, %rdi
	vinsertps	$16, (%r12,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpextrq	$1, %xmm8, %rsi
	vinsertps	$48, (%r12,%r8,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1,2],mem[0]
	vmovq	%xmm8, %rbx
	vextracti128	$1, %ymm8, %xmm6
	vmovss	(%r12,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovq	%xmm3, %rax
	vinsertps	$16, (%r12,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrq	$1, %xmm3, %rcx
	vmovss	(%r12,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vpsllq	$4, %ymm7, %ymm7
	vinsertps	$32, (%r12,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpsllq	$4, %ymm14, %ymm8
	vpaddq	%ymm9, %ymm8, %ymm8
	vinsertps	$16, (%r12,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vpaddq	%ymm7, %ymm9, %ymm7
	vpaddq	%ymm7, %ymm7, %ymm7
	vinsertps	$48, (%r12,%rdi,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	vpaddq	%ymm8, %ymm8, %ymm8
	vpaddq	%ymm10, %ymm8, %ymm8
	vpaddq	%ymm7, %ymm10, %ymm7
	vmovq	%xmm6, %rdx
	vpextrq	$1, %xmm7, %rsi
	vpextrq	$1, %xmm6, %r8
	vmovq	%xmm7, %rbx
	vextracti128	$1, %ymm7, %xmm6
	vmovq	%xmm8, %rdi
	vmovss	(%r12,%rdi,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm6, %rdi
	vinsertps	$32, (%r12,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovq	%xmm6, %rax
	vmovss	(%r12,%rbx,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm8, %rbx
	vinsertps	$16, (%r12,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$16, (%r12,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, (%r12,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r12,%rdi,4), %xmm6, %xmm14 # xmm14 = xmm6[0,1,2],mem[0]
	vextracti128	$1, %ymm8, %xmm0
	vpaddq	%ymm5, %ymm2, %ymm2
	vinsertps	$48, (%r12,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vpaddq	%ymm5, %ymm4, %ymm4
	vpextrq	$1, %xmm4, %rax
	vmovq	%xmm0, %rcx
	vmovq	%xmm4, %rsi
	vextracti128	$1, %ymm4, %xmm4
	vpextrq	$1, %xmm0, %r10
	vmovq	%xmm2, %rbx
	vmovss	(%r12,%rbx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm2, %rbx
	vinsertps	$16, (%r12,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r12,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovq	%xmm4, %rdx
	vpextrq	$1, %xmm4, %rbx
	vmovups	64(%r9,%r11,4), %ymm4
	vmulps	64(%r9,%r14,4), %ymm4, %ymm4
	vextracti128	$1, %ymm2, %xmm2
	vinsertf128	$1, %xmm1, %ymm11, %ymm1
	vmovss	(%r12,%rsi,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmovq	%xmm2, %rsi
	vinsertps	$48, (%r12,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vpextrq	$1, %xmm2, %rdi
	vmovups	64(%r9,%r15,4), %ymm2
	vinsertf128	$1, %xmm3, %ymm12, %ymm3
	vxorps	%xmm11, %xmm11, %xmm11
	vcmpeqps	32(%r9,%r13,4), %ymm11, %ymm8
	vmovdqa	1568(%rsp), %ymm5               # 32-byte Reload
	vpmulld	352(%rsp), %ymm5, %ymm5         # 32-byte Folded Reload
	vmaxps	%ymm11, %ymm1, %ymm1
	vmaxps	%ymm11, %ymm3, %ymm3
	vxorps	%xmm12, %xmm12, %xmm12
	vmulps	%ymm3, %ymm2, %ymm11
	vfmadd231ps	%ymm1, %ymm4, %ymm11    # ymm11 = (ymm4 * ymm1) + ymm11
	vcmpeqps	64(%r9,%r13,4), %ymm12, %ymm1
	vinsertps	$32, (%r12,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$16, (%r12,%rax,4), %xmm6, %xmm3 # xmm3 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, (%r12,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$32, (%r12,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%r12,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, (%r12,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, (%r12,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm14, %ymm6
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vextracti128	$1, %ymm5, %xmm3
	vpmovsxdq	%xmm3, %ymm14
	vpmovsxdq	%xmm5, %ymm5
	vmaxps	%ymm12, %ymm0, %ymm0
	vmulps	%ymm0, %ymm2, %ymm2
	vpsllq	$5, %ymm5, %ymm0
	vpaddq	%ymm0, %ymm10, %ymm3
	vmaxps	%ymm12, %ymm6, %ymm0
	vfmadd231ps	%ymm0, %ymm4, %ymm2     # ymm2 = (ymm4 * ymm0) + ymm2
	vpsllq	$5, %ymm14, %ymm0
	vmovq	%xmm3, %rax
	vpextrq	$1, %xmm3, %rcx
	vpaddq	%ymm0, %ymm10, %ymm4
	vextracti128	$1, %ymm3, %xmm0
	vmovq	%xmm0, %rdx
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm4, %rdi
	vmovss	(%r12,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm4, %rax
	vinsertps	$16, (%r12,%rcx,4), %xmm0, %xmm6 # xmm6 = xmm0[0],mem[0],xmm0[2,3]
	vblendvps	%ymm8, %ymm13, %ymm15, %ymm15
	vpaddq	%ymm3, %ymm9, %ymm8
	vpextrq	$1, %xmm8, %rcx
	vmovss	(%r12,%rdi,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovq	%xmm8, %rdi
	vextracti128	$1, %ymm8, %xmm7
	vpextrq	$1, %xmm7, %r8
	vmovq	%xmm7, %rbx
	vpaddq	%ymm4, %ymm9, %ymm7
	vinsertps	$32, (%r12,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vblendvps	%ymm1, %ymm11, %ymm2, %ymm8
	vmovss	(%r12,%rdi,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmovq	%xmm7, %rdx
	vinsertps	$16, (%r12,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpextrq	$1, %xmm7, %rcx
	vpsllq	$4, %ymm5, %ymm5
	vpaddq	%ymm5, %ymm9, %ymm5
	vinsertps	$16, (%r12,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpaddq	%ymm5, %ymm5, %ymm5
	vpaddq	%ymm5, %ymm10, %ymm5
	vinsertps	$48, (%r12,%rsi,4), %xmm6, %xmm13 # xmm13 = xmm6[0,1,2],mem[0]
	vmovq	%xmm5, %rax
	vpextrq	$1, %xmm5, %rsi
	vmovss	(%r12,%rdx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vextracti128	$1, %ymm5, %xmm5
	vmovq	%xmm5, %rdx
	vpextrq	$1, %xmm5, %r10
	vextracti128	$1, %ymm4, %xmm5
	vinsertps	$32, (%r12,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovq	%xmm5, %rbx
	vextracti128	$1, %ymm7, %xmm7
	vpsllq	$4, %ymm14, %ymm11
	vinsertps	$16, (%r12,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpaddq	%ymm9, %ymm11, %ymm11
	vpaddq	%ymm11, %ymm11, %ymm11
	vinsertps	$48, (%r12,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vpaddq	%ymm10, %ymm11, %ymm11
	vmovq	%xmm11, %rcx
	vinsertps	$32, (%r12,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrq	$1, %xmm11, %rbx
	vpextrq	$1, %xmm5, %r8
	vmovss	(%r12,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%r12,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovq	%xmm7, %rax
	vinsertps	$32, (%r12,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrq	$1, %xmm7, %rdx
	vmovss	(%r12,%rcx,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vmovdqa	576(%rsp), %ymm9                # 32-byte Reload
	vpaddq	%ymm4, %ymm9, %ymm4
	vinsertps	$16, (%r12,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrq	$1, %xmm4, %rcx
	vinsertps	$48, (%r12,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovq	%xmm4, %rsi
	vextracti128	$1, %ymm4, %xmm4
	vpextrq	$1, %xmm4, %rbx
	vmovq	%xmm4, %rdi
	vmovss	(%r12,%rsi,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vinsertps	$48, (%r12,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, (%r12,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r12,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, (%r12,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r12,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vextracti128	$1, %ymm11, %xmm6
	vmovq	%xmm6, %rax
	vinsertps	$32, (%r12,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%r12,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vpextrq	$1, %xmm6, %rax
	vpaddq	%ymm3, %ymm9, %ymm3
	vinsertps	$48, (%r12,%rax,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vmovq	%xmm3, %rax
	vmovss	(%r12,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm3, %rax
	vextracti128	$1, %ymm3, %xmm3
	vinsertps	$16, (%r12,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovq	%xmm3, %rax
	vinsertps	$32, (%r12,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrq	$1, %xmm3, %rax
	vinsertps	$48, (%r12,%rax,4), %xmm7, %xmm3 # xmm3 = xmm7[0,1,2],mem[0]
	vmovups	96(%r9,%r11,4), %ymm7
	vmulps	96(%r9,%r14,4), %ymm7, %ymm7
	vinsertf128	$1, %xmm0, %ymm13, %ymm0
	vmovups	96(%r9,%r15,4), %ymm11
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	movq	656(%rsp), %rcx                 # 8-byte Reload
	vmovups	(%r9,%rcx,4), %ymm2
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vmovups	32(%r9,%rcx,4), %ymm6
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vmovups	64(%r9,%rcx,4), %ymm4
	movq	8(%rsp), %rax                   # 8-byte Reload
	vmulps	(%r9,%rax,4), %ymm2, %ymm2
	vmulps	32(%r9,%rax,4), %ymm6, %ymm6
	vmulps	64(%r9,%rax,4), %ymm4, %ymm4
	vmaxps	%ymm12, %ymm0, %ymm0
	vmaxps	%ymm12, %ymm1, %ymm1
	vmulps	%ymm1, %ymm11, %ymm1
	vfmadd231ps	%ymm0, %ymm7, %ymm1     # ymm1 = (ymm7 * ymm0) + ymm1
	vmovups	96(%r9,%rcx,4), %ymm0
	vmulps	96(%r9,%rax,4), %ymm0, %ymm0
	vmaxps	%ymm12, %ymm3, %ymm3
	vmulps	%ymm3, %ymm11, %ymm3
	vcmpeqps	96(%r9,%r13,4), %ymm12, %ymm11
	vmaxps	%ymm12, %ymm5, %ymm5
	vfmadd231ps	%ymm5, %ymm7, %ymm3     # ymm3 = (ymm7 * ymm5) + ymm3
	vblendvps	%ymm11, %ymm1, %ymm3, %ymm1
	vbroadcastss	.LCPI0_91(%rip), %ymm3  # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmaxps	%ymm3, %ymm2, %ymm2
	vmovaps	896(%rsp), %ymm7                # 32-byte Reload
	vdivps	%ymm7, %ymm2, %ymm2
	vmaxps	%ymm3, %ymm6, %ymm5
	vdivps	%ymm7, %ymm5, %ymm5
	vmaxps	%ymm3, %ymm4, %ymm4
	vdivps	%ymm7, %ymm4, %ymm4
	vmaxps	%ymm3, %ymm0, %ymm0
	vdivps	%ymm7, %ymm0, %ymm0
	vroundps	$10, %ymm2, %ymm6
	vmulps	1344(%rsp), %ymm6, %ymm6        # 32-byte Folded Reload
	vroundps	$10, %ymm5, %ymm7
	vmulps	%ymm7, %ymm15, %ymm7
	vroundps	$10, %ymm4, %ymm10
	vmulps	%ymm8, %ymm10, %ymm8
	vroundps	$10, %ymm0, %ymm10
	vmulps	%ymm1, %ymm10, %ymm1
	vmaxps	%ymm3, %ymm2, %ymm2
	vdivps	%ymm2, %ymm6, %ymm2
	vmaxps	%ymm3, %ymm5, %ymm5
	vdivps	%ymm5, %ymm7, %ymm5
	vmaxps	%ymm3, %ymm4, %ymm4
	vdivps	%ymm4, %ymm8, %ymm4
	vmovaps	448(%rsp), %ymm6                # 32-byte Reload
	vaddps	%ymm2, %ymm6, %ymm6
	vmovaps	%ymm6, 448(%rsp)                # 32-byte Spill
	vmovaps	224(%rsp), %ymm2                # 32-byte Reload
	vaddps	%ymm5, %ymm2, %ymm2
	vmovaps	%ymm2, 224(%rsp)                # 32-byte Spill
	vmaxps	%ymm3, %ymm0, %ymm0
	vmovaps	160(%rsp), %ymm2                # 32-byte Reload
	vaddps	%ymm4, %ymm2, %ymm2
	vmovaps	%ymm2, 160(%rsp)                # 32-byte Spill
	vdivps	%ymm0, %ymm1, %ymm0
	vmovaps	288(%rsp), %ymm1                # 32-byte Reload
	vaddps	%ymm0, %ymm1, %ymm1
	vmovaps	%ymm1, 288(%rsp)                # 32-byte Spill
	vpbroadcastd	.LCPI0_108(%rip), %ymm0 # ymm0 = [32,32,32,32,32,32,32,32]
	vmovdqa	320(%rsp), %ymm1                # 32-byte Reload
	vpaddd	%ymm0, %ymm1, %ymm1
	addq	520(%rsp), %r9                  # 8-byte Folded Reload
	addq	$-32, 432(%rsp)                 # 8-byte Folded Spill
	jne	.LBB0_277
# %bb.278:                              # %middle.block1420
                                        #   in Loop: Header=BB0_274 Depth=1
	vmovaps	224(%rsp), %ymm0                # 32-byte Reload
	vaddps	448(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vaddps	160(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vaddps	288(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	movq	272(%rsp), %rcx                 # 8-byte Reload
	movq	%rcx, %r15
	movl	%ecx, %eax
	cmpq	648(%rsp), %rcx                 # 8-byte Folded Reload
	vmovaps	1616(%rsp), %xmm11              # 16-byte Reload
	movq	504(%rsp), %r12                 # 8-byte Reload
	vmovss	.LCPI0_91(%rip), %xmm12         # xmm12 = mem[0],zero,zero,zero
	vxorps	%xmm7, %xmm7, %xmm7
	movq	64(%rsp), %r13                  # 8-byte Reload
	jne	.LBB0_280
	jmp	.LBB0_282
	.p2align	4, 0x90
.LBB0_279:                              #   in Loop: Header=BB0_274 Depth=1
	xorl	%r15d, %r15d
	vpxor	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
	movq	504(%rsp), %r12                 # 8-byte Reload
	movq	64(%rsp), %r13                  # 8-byte Reload
.LBB0_280:                              # %"for f5.s1.r77$x.preheader1572"
                                        #   in Loop: Header=BB0_274 Depth=1
	movq	648(%rsp), %rcx                 # 8-byte Reload
	subq	%r15, %rcx
	imulq	408(%rsp), %r15                 # 8-byte Folded Reload
	movq	512(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%r15), %r8
	movq	984(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%r15), %r9
	movq	992(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%r15), %r10
	movq	1464(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%r15), %r11
	movq	1472(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%r15), %r14
	addq	552(%rsp), %r15                 # 8-byte Folded Reload
	imull	200(%rsp), %eax                 # 4-byte Folded Reload
	movslq	%eax, %rsi
	shlq	$7, %rsi
	movq	704(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rsi), %rdi
	movq	640(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rsi), %rbx
	movq	632(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rsi), %rdx
	addq	712(%rsp), %rsi                 # 8-byte Folded Reload
	movq	80(%rsp), %rax                  # 8-byte Reload
	.p2align	4, 0x90
.LBB0_281:                              # %"for f5.s1.r77$x"
                                        #   Parent Loop BB0_274 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%r15,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	(%rax,%r14,4), %xmm1, %xmm1
	vmovss	(%rax,%r10,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	(%rax,%r9,4), %xmm2, %xmm2
	vmaxss	%xmm12, %xmm2, %xmm2
	vmovss	(%rax,%r11,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmovss	(%rsi,%r13), %xmm4              # xmm4 = mem[0],zero,zero,zero
	vmaxss	%xmm7, %xmm4, %xmm4
	vmovss	(%rdx,%r13), %xmm5              # xmm5 = mem[0],zero,zero,zero
	vmaxss	%xmm7, %xmm5, %xmm5
	vmulss	%xmm5, %xmm3, %xmm5
	vmovss	(%rdi,%r13), %xmm6              # xmm6 = mem[0],zero,zero,zero
	vmaxss	%xmm7, %xmm6, %xmm6
	vfmadd231ss	%xmm4, %xmm1, %xmm5     # xmm5 = (xmm1 * xmm4) + xmm5
	vmovss	(%rbx,%r13), %xmm4              # xmm4 = mem[0],zero,zero,zero
	vmaxss	%xmm7, %xmm4, %xmm4
	vmulss	%xmm4, %xmm3, %xmm3
	vfmadd231ss	%xmm6, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm6) + xmm3
	vcmpeqss	(%rax,%r8,4), %xmm7, %xmm1
	vblendvps	%xmm1, %xmm5, %xmm3, %xmm1
	vdivss	%xmm11, %xmm2, %xmm2
	vroundss	$10, %xmm2, %xmm2, %xmm3
	vmulss	%xmm1, %xmm3, %xmm1
	vmaxss	%xmm12, %xmm2, %xmm2
	vdivss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	addq	128(%rsp), %rax                 # 8-byte Folded Reload
	addq	%r12, %rdi
	addq	%r12, %rbx
	addq	%r12, %rdx
	addq	%r12, %rsi
	decq	%rcx
	jne	.LBB0_281
.LBB0_282:                              # %"consume f5.loopexit"
                                        #   in Loop: Header=BB0_274 Depth=1
	vmovd	%xmm0, %eax
	movq	40(%rsp), %r15                  # 8-byte Reload
	movq	56(%rsp), %rbx                  # 8-byte Reload
.LBB0_283:                              # %"consume f5"
                                        #   in Loop: Header=BB0_274 Depth=1
	movq	%r15, %r10
	movq	208(%rsp), %r15                 # 8-byte Reload
	movq	%r15, %rcx
	subq	544(%rsp), %rcx                 # 8-byte Folded Reload
	movq	2072(%rsp), %rdx                # 8-byte Reload
	movl	%eax, (%rdx,%rcx,4)
	incq	%r15
	addq	$4, %rbx
	incq	512(%rsp)                       # 8-byte Folded Spill
	incq	984(%rsp)                       # 8-byte Folded Spill
	incq	992(%rsp)                       # 8-byte Folded Spill
	incq	1464(%rsp)                      # 8-byte Folded Spill
	incq	1472(%rsp)                      # 8-byte Folded Spill
	incq	552(%rsp)                       # 8-byte Folded Spill
	addq	$4, %r13
	movq	%r15, 208(%rsp)                 # 8-byte Spill
	cmpl	%r15d, 1564(%rsp)               # 4-byte Folded Reload
	movq	%r10, %r15
	jne	.LBB0_274
# %bb.284:                              # %call_destructor.exit894.loopexit
	vmovd	%xmm0, 7328(%rsp)
.LBB0_285:                              # %call_destructor.exit894
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	movq	72(%rsp), %rsi                  # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 896(%rsp)                 # 8-byte Spill
	xorl	%r12d, %r12d
.LBB0_286:                              # %call_destructor.exit800.thread1030
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	testl	%r12d, %r12d
	sete	%bl
	testb	%bl, %bl
	jne	.LBB0_289
.LBB0_287:                              # %call_destructor.exit801
	testq	%r15, %r15
	je	.LBB0_289
# %bb.288:
	xorl	%edi, %edi
	movq	%r15, %rsi
	movq	%r14, %r15
	movq	%rdx, %r14
	vzeroupper
	callq	halide_free@PLT
	movq	%r14, %rdx
	movq	%r15, %r14
.LBB0_289:                              # %call_destructor.exit802
	testq	%rdx, %rdx
	sete	%al
	orb	%bl, %al
	jne	.LBB0_291
# %bb.290:
	xorl	%edi, %edi
	movq	%rdx, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB0_291:                              # %call_destructor.exit803
	testq	%r14, %r14
	sete	%al
	orb	%bl, %al
	jne	.LBB0_293
# %bb.292:
	xorl	%edi, %edi
	movq	%r14, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB0_293:                              # %call_destructor.exit804
	movq	96(%rsp), %rsi                  # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%bl, %al
	jne	.LBB0_295
# %bb.294:
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB0_295:                              # %call_destructor.exit805
	testq	%r13, %r13
	sete	%al
	orb	%bl, %al
	movq	64(%rsp), %r14                  # 8-byte Reload
	jne	.LBB0_297
.LBB0_296:                              # %call_destructor.exit806.sink.split
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	movq	%r13, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB0_297:                              # %call_destructor.exit806
	movq	128(%rsp), %rsi                 # 8-byte Reload
	testq	%rsi, %rsi
	je	.LBB0_300
# %bb.298:                              # %call_destructor.exit806
	testb	%bl, %bl
	jne	.LBB0_300
# %bb.299:
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB0_300:                              # %call_destructor.exit807
	movq	72(%rsp), %rsi                  # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%bl, %al
	jne	.LBB0_302
# %bb.301:
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB0_302:                              # %call_destructor.exit808
	movq	56(%rsp), %rsi                  # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%bl, %al
	jne	.LBB0_304
# %bb.303:
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB0_304:                              # %call_destructor.exit809
	movq	8(%rsp), %rsi                   # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%bl, %al
	jne	.LBB0_306
# %bb.305:
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB0_306:                              # %call_destructor.exit810
	testq	%r14, %r14
	sete	%al
	orb	%bl, %al
	jne	.LBB0_308
# %bb.307:
	xorl	%edi, %edi
	movq	%r14, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB0_308:                              # %call_destructor.exit811
	movq	896(%rsp), %rsi                 # 8-byte Reload
	testq	%rsi, %rsi
	sete	%al
	orb	%bl, %al
	jne	.LBB0_310
# %bb.309:
	xorl	%edi, %edi
	vzeroupper
	callq	halide_free@PLT
.LBB0_310:                              # %call_destructor.exit812
	movl	%r12d, %eax
.LBB0_311:                              # %call_destructor.exit812
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	vzeroupper
	retq
.LBB0_76:                               # %after_bb76
	.cfi_def_cfa %rbp, 16
	movq	40(%rdx), %rax
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%xmm0, (%rdx)
	movq	$0, 16(%rdx)
	movabsq	$12884975618, %rcx              # imm = 0x300012002
	movq	%rcx, 32(%rdx)
	vmovaps	.LCPI0_13(%rip), %xmm0          # xmm0 = [0,24,1,0]
	vmovups	%xmm0, (%rax)
	movq	40(%rdx), %rax
	vmovaps	.LCPI0_14(%rip), %xmm0          # xmm0 = [0,39,24,0]
	vmovups	%xmm0, 16(%rax)
	movq	40(%rdx), %rax
	vmovdqa	.LCPI0_23(%rip), %xmm0          # xmm0 = [0,4,936,0]
	vmovdqu	%xmm0, 32(%rax)
	movq	$0, 24(%rdx)
	cmpq	$0, 16(%rdx)
	jne	.LBB0_61
	jmp	.LBB0_64
.LBB0_312:
	vxorps	%xmm3, %xmm3, %xmm3
	xorl	%ebx, %ebx
	testb	$1, %r10b
	jne	.LBB0_247
	jmp	.LBB0_248
.LBB0_313:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	movq	8(%rsp), %rcx                   # 8-byte Reload
	xorl	%r15d, %r15d
	movq	24(%rsp), %r13                  # 8-byte Reload
	jmp	.LBB0_286
.LBB0_314:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	jmp	.LBB0_366
.LBB0_383:
	movq	%rax, %r12
.LBB0_389:                              # %call_destructor.exit800
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	movq	528(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB0_395
.LBB0_315:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	24(%rsp), %r13                  # 8-byte Reload
	xorl	%r15d, %r15d
	jmp	.LBB0_286
.LBB0_316:
	xorl	%r14d, %r14d
	xorl	%ecx, %ecx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%ecx, %ecx
	movq	%rcx, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	movq	%rdx, 64(%rsp)                  # 8-byte Spill
	xorl	%edx, %edx
	movq	%rdx, 896(%rsp)                 # 8-byte Spill
	movl	%eax, %r12d
	movq	224(%rsp), %r15                 # 8-byte Reload
	xorl	%edx, %edx
	jmp	.LBB0_286
.LBB0_317:
	movl	%eax, %r12d
	movq	288(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB0_393
.LBB0_318:
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%ecx, %ecx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%ecx, %ecx
	movq	%rcx, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%esi, %esi
	movq	%rsi, 64(%rsp)                  # 8-byte Spill
	xorl	%esi, %esi
	movq	%rsi, 896(%rsp)                 # 8-byte Spill
	movl	%eax, %r12d
	xorl	%r15d, %r15d
	jmp	.LBB0_286
.LBB0_319:                              # %entry
	leaq	.LJTI0_0(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB0_320:                              # %assert_failed
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB0_432
.LBB0_321:                              # %"assert failed346"
	leaq	.Lstr.123(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_348
.LBB0_322:                              # %"assert failed352"
	leaq	.Lstr.124(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_351
.LBB0_323:                              # %"assert failed360"
	leaq	.Lstr.125(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_355
.LBB0_324:                              # %"assert failed366"
	leaq	.Lstr.126(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%rbx, %rdx
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_358
.LBB0_325:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	jmp	.LBB0_366
.LBB0_326:                              # %"assert failed382"
	imulq	%rax, %rdx
	leaq	.Lstr.129(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_364
.LBB0_327:                              # %"assert failed388"
	imulq	%rdi, %rdx
	leaq	.Lstr.131(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_368
.LBB0_328:                              # %"assert failed481"
	leaq	.Lstr.133(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_376
.LBB0_329:                              # %"assert failed513"
	leaq	.Lstr.137(%rip), %rsi
	xorl	%r14d, %r14d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_331
.LBB0_330:                              # %"assert failed498"
	xorl	%r14d, %r14d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_331:
	movl	%eax, %r12d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	224(%rsp), %r15                 # 8-byte Reload
	movq	160(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_286
.LBB0_332:
	movl	%eax, %r12d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movq	224(%rsp), %r15                 # 8-byte Reload
	jmp	.LBB0_382
.LBB0_333:
	xorl	%ecx, %ecx
	movq	%rcx, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	xorl	%r14d, %r14d
	xorl	%ecx, %ecx
	movq	%rcx, 896(%rsp)                 # 8-byte Spill
	movl	%eax, %r12d
	jmp	.LBB0_296
.LBB0_334:                              # %true_bb77
	movl	%r8d, %r12d
	movl	1000(%rsp), %r15d               # 4-byte Reload
	movl	1868(%rsp), %r14d               # 4-byte Reload
	movl	1872(%rsp), %r11d               # 4-byte Reload
	movl	1876(%rsp), %r10d               # 4-byte Reload
	movl	1880(%rsp), %r8d                # 4-byte Reload
	movl	1884(%rsp), %ebx                # 4-byte Reload
	movl	1888(%rsp), %r9d                # 4-byte Reload
	movl	1892(%rsp), %edi                # 4-byte Reload
	movl	1896(%rsp), %esi                # 4-byte Reload
	leaq	.LJTI0_1(%rip), %rdx
	movslq	(%rdx,%rax,4), %rax
	addq	%rdx, %rax
	jmpq	*%rax
.LBB0_335:                              # %assert_failed81
	leaq	.Lstr.23(%rip), %rsi
	xorl	%edi, %edi
	movl	1800(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_336:                              # %no_errors_bb80
	movl	728(%rsp), %r11d                # 4-byte Reload
	movl	732(%rsp), %r13d                # 4-byte Reload
	movl	736(%rsp), %r12d                # 4-byte Reload
	movl	740(%rsp), %r15d                # 4-byte Reload
	movq	1040(%rsp), %rcx                # 8-byte Reload
	movl	744(%rsp), %r8d                 # 4-byte Reload
	movl	748(%rsp), %r10d                # 4-byte Reload
	movl	752(%rsp), %r9d                 # 4-byte Reload
	movl	756(%rsp), %ebx                 # 4-byte Reload
	movl	760(%rsp), %r14d                # 4-byte Reload
	movl	764(%rsp), %esi                 # 4-byte Reload
	movl	768(%rsp), %edx                 # 4-byte Reload
	leaq	.LJTI0_2(%rip), %rdi
	movslq	(%rdi,%rax,4), %rax
	addq	%rdi, %rax
	jmpq	*%rax
.LBB0_337:                              # %assert_failed145
	leaq	.Lstr.31(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	152(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_338:                              # %no_errors_bb144
	movq	1040(%rsp), %rbx                # 8-byte Reload
	leaq	.LJTI0_3(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB0_339:                              # %assert_failed209
	leaq	.Lstr.61(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	216(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_619
.LBB0_340:                              # %no_errors_bb208
	movq	1168(%rsp), %r13                # 8-byte Reload
	movq	1184(%rsp), %r12                # 8-byte Reload
	movq	1192(%rsp), %r15                # 8-byte Reload
	movq	1200(%rsp), %r14                # 8-byte Reload
	movq	1208(%rsp), %r11                # 8-byte Reload
	movq	1216(%rsp), %r10                # 8-byte Reload
	movq	1224(%rsp), %r9                 # 8-byte Reload
	movq	1232(%rsp), %r8                 # 8-byte Reload
	movq	1240(%rsp), %rbx                # 8-byte Reload
	movq	1248(%rsp), %rdx                # 8-byte Reload
	leaq	.LJTI0_4(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB0_341:                              # %assert_failed266
	leaq	.Lstr.20(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r10, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_342:                              # %no_errors_bb265
	leaq	.LJTI0_5(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB0_343:                              # %assert_failed330
	leaq	.Lstr.36(%rip), %rsi
	jmp	.LBB0_412
.LBB0_344:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 896(%rsp)                 # 8-byte Spill
	xorl	%r15d, %r15d
	jmp	.LBB0_286
.LBB0_345:                              # %"assert failed338"
	leaq	.Lstr.122(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	halide_error_buffer_allocation_too_large@PLT
	movl	%eax, %r12d
	jmp	.LBB0_108
.LBB0_346:                              # %"assert failed340"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	movl	%eax, %r12d
	jmp	.LBB0_108
.LBB0_347:                              # %"assert failed348"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_348:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%eax, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	jmp	.LBB0_286
.LBB0_349:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%r15d, %r15d
	jmp	.LBB0_286
.LBB0_350:                              # %"assert failed354"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_351:                              # %call_destructor.exit800.thread1030
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	jmp	.LBB0_286
.LBB0_352:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	movq	8(%rsp), %rcx                   # 8-byte Reload
	xorl	%r13d, %r13d
	xorl	%r15d, %r15d
	jmp	.LBB0_286
.LBB0_353:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	jmp	.LBB0_366
.LBB0_354:                              # %"assert failed362"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_355:                              # %call_destructor.exit800.thread1030
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	movq	8(%rsp), %rcx                   # 8-byte Reload
	jmp	.LBB0_286
.LBB0_356:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	jmp	.LBB0_366
.LBB0_357:                              # %"assert failed368"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_358:                              # %call_destructor.exit800.thread1030
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	movq	8(%rsp), %rcx                   # 8-byte Reload
	jmp	.LBB0_286
.LBB0_359:
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	jmp	.LBB0_366
.LBB0_360:                              # %"assert failed374"
	leaq	.Lstr.128(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_362
.LBB0_361:                              # %"assert failed376"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_362:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movq	8(%rsp), %rcx                   # 8-byte Reload
	jmp	.LBB0_286
.LBB0_363:                              # %"assert failed384"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_364:                              # %call_destructor.exit800.thread1030
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	jmp	.LBB0_373
.LBB0_365:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
.LBB0_366:                              # %call_destructor.exit800.thread1030
	movq	8(%rsp), %rcx                   # 8-byte Reload
	xorl	%r15d, %r15d
	jmp	.LBB0_286
.LBB0_367:                              # %"assert failed390"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_368:                              # %call_destructor.exit800.thread1030
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movq	8(%rsp), %rcx                   # 8-byte Reload
	jmp	.LBB0_286
.LBB0_369:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	24(%rsp), %r13                  # 8-byte Reload
	jmp	.LBB0_286
.LBB0_370:                              # %"assert failed474"
	leaq	.Lstr.132(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%rbx, %rdx
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_372
.LBB0_371:                              # %"assert failed476"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_372:                              # %call_destructor.exit800.thread1030
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r14d, %r14d
.LBB0_373:                              # %call_destructor.exit800.thread1030
	xorl	%r13d, %r13d
	movq	8(%rsp), %rcx                   # 8-byte Reload
	jmp	.LBB0_286
.LBB0_374:
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r13d, %r13d
	movq	8(%rsp), %rcx                   # 8-byte Reload
	xorl	%r15d, %r15d
	movq	320(%rsp), %r14                 # 8-byte Reload
	jmp	.LBB0_286
.LBB0_375:                              # %"assert failed483"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_376:                              # %call_destructor.exit800.thread1030
	movl	%eax, %r12d
	xorl	%edx, %edx
	xorl	%r13d, %r13d
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	320(%rsp), %r14                 # 8-byte Reload
	jmp	.LBB0_286
.LBB0_377:
	xorl	%r13d, %r13d
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	160(%rsp), %rdx                 # 8-byte Reload
	xorl	%r15d, %r15d
	movq	%rax, %r12
	jmp	.LBB0_286
.LBB0_378:                              # %"assert failed488"
	leaq	.Lstr.134(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%rbx, %rdx
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_380
.LBB0_379:                              # %"assert failed490"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_380:                              # %call_destructor.exit800.thread1030
	movl	%eax, %r12d
	xorl	%r14d, %r14d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	160(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_286
.LBB0_381:
	movl	%eax, %r12d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	%r13, %r15
	xorl	%r13d, %r13d
.LBB0_382:                              # %call_destructor.exit800.thread1030
	movq	160(%rsp), %rdx                 # 8-byte Reload
	xorl	%r14d, %r14d
	jmp	.LBB0_286
.LBB0_388:
	movl	%eax, %r12d
	jmp	.LBB0_389
.LBB0_390:
	movq	%r12, %rsi
	movq	%rax, %r12
	jmp	.LBB0_394
.LBB0_391:
	movq	288(%rsp), %rsi                 # 8-byte Reload
	movq	%rax, %r12
	xorl	%eax, %eax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	jmp	.LBB0_394
.LBB0_392:
	movq	%r12, %rsi
	movq	%rax, %r12
.LBB0_393:                              # %call_destructor.exit
	xorl	%eax, %eax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
.LBB0_394:                              # %call_destructor.exit
	xorl	%ebx, %ebx
	xorl	%edi, %edi
.LBB0_395:                              # %call_destructor.exit801
	callq	halide_free@PLT
	xorl	%r13d, %r13d
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	xorl	%r14d, %r14d
	movq	224(%rsp), %r15                 # 8-byte Reload
	movq	160(%rsp), %rdx                 # 8-byte Reload
	testb	%bl, %bl
	je	.LBB0_287
	jmp	.LBB0_289
.LBB0_396:
	xorl	%ecx, %ecx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	movq	%rdx, 64(%rsp)                  # 8-byte Spill
	movl	%eax, %r12d
	movq	160(%rsp), %rdx                 # 8-byte Reload
	movq	224(%rsp), %r15                 # 8-byte Reload
	jmp	.LBB0_286
.LBB0_397:
	xorl	%ecx, %ecx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	movq	%rdx, 64(%rsp)                  # 8-byte Spill
	movl	%eax, %r12d
	movq	160(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_399
.LBB0_398:
	xorl	%ecx, %ecx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	movq	%rdx, 64(%rsp)                  # 8-byte Spill
	movl	%eax, %r12d
	movq	%r14, %rdx
.LBB0_399:                              # %call_destructor.exit800.thread1030
	xorl	%r14d, %r14d
	movq	224(%rsp), %r15                 # 8-byte Reload
	jmp	.LBB0_286
.LBB0_400:
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%ecx, %ecx
	movq	%rcx, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%esi, %esi
	movq	%rsi, 64(%rsp)                  # 8-byte Spill
	movl	%eax, %r12d
	movq	224(%rsp), %r15                 # 8-byte Reload
	xorl	%r14d, %r14d
	jmp	.LBB0_286
.LBB0_401:
	xorl	%ecx, %ecx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%ecx, %ecx
	movq	%rcx, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	movq	%rdx, 64(%rsp)                  # 8-byte Spill
	xorl	%edx, %edx
	movq	%rdx, 896(%rsp)                 # 8-byte Spill
	movl	%eax, %r12d
	xorl	%edx, %edx
	movq	%r14, %r15
	xorl	%r14d, %r14d
	jmp	.LBB0_286
.LBB0_402:
	xorl	%r15d, %r15d
	xorl	%r14d, %r14d
	xorl	%ecx, %ecx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%ecx, %ecx
	movq	%rcx, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	movq	%rdx, 64(%rsp)                  # 8-byte Spill
	xorl	%edx, %edx
	movq	%rdx, 896(%rsp)                 # 8-byte Spill
	movl	%eax, %r12d
	xorl	%edx, %edx
	jmp	.LBB0_286
.LBB0_403:                              # %"assert failed569"
	leaq	.Lstr.139(%rip), %rsi
	xorl	%r15d, %r15d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_405
.LBB0_404:                              # %"assert failed571"
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_405:                              # %call_destructor.exit800.thread1030
	xorl	%edx, %edx
	xorl	%r14d, %r14d
	xorl	%ecx, %ecx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	xorl	%r13d, %r13d
	xorl	%ecx, %ecx
	movq	%rcx, 56(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	xorl	%esi, %esi
	movq	%rsi, 64(%rsp)                  # 8-byte Spill
	xorl	%esi, %esi
	movq	%rsi, 896(%rsp)                 # 8-byte Spill
	movl	%eax, %r12d
	jmp	.LBB0_286
.LBB0_406:                              # %"assert failed"
	movq	1384(%rsp), %rax                # 8-byte Reload
	addl	$23, %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.22(%rip), %rsi
	xorl	%r15d, %r15d
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movl	572(%rsp), %r9d                 # 4-byte Reload
	callq	halide_error_constraints_make_required_region_smaller@PLT
	movl	%eax, %r12d
	jmp	.LBB0_108
.LBB0_407:                              # %assert_failed331
	leaq	.Lstr.37(%rip), %rsi
	jmp	.LBB0_412
.LBB0_408:                              # %assert_failed332
	leaq	.Lstr.38(%rip), %rsi
	jmp	.LBB0_412
.LBB0_409:                              # %assert_failed333
	leaq	.Lstr.39(%rip), %rsi
	jmp	.LBB0_412
.LBB0_410:                              # %assert_failed334
	leaq	.Lstr.40(%rip), %rsi
	jmp	.LBB0_412
.LBB0_411:                              # %assert_failed335
	leaq	.Lstr.41(%rip), %rsi
.LBB0_412:                              # %assert_failed315
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_host_is_null@PLT
	jmp	.LBB0_311
.LBB0_413:                              # %assert_failed1
	leaq	.Lstr.3(%rip), %rsi
	jmp	.LBB0_432
.LBB0_414:                              # %assert_failed2
	leaq	.Lstr.4(%rip), %rsi
	jmp	.LBB0_432
.LBB0_415:                              # %assert_failed3
	leaq	.Lstr.5(%rip), %rsi
	jmp	.LBB0_432
.LBB0_416:                              # %assert_failed4
	leaq	.Lstr.6(%rip), %rsi
	jmp	.LBB0_432
.LBB0_417:                              # %assert_failed5
	leaq	.Lstr.7(%rip), %rsi
	jmp	.LBB0_432
.LBB0_418:                              # %assert_failed6
	leaq	.Lstr.8(%rip), %rsi
	jmp	.LBB0_432
.LBB0_419:                              # %assert_failed7
	leaq	.Lstr.9(%rip), %rsi
	jmp	.LBB0_432
.LBB0_420:                              # %assert_failed8
	leaq	.Lstr.10(%rip), %rsi
	jmp	.LBB0_432
.LBB0_421:                              # %assert_failed9
	leaq	.Lstr.11(%rip), %rsi
	jmp	.LBB0_432
.LBB0_422:                              # %assert_failed10
	leaq	.Lstr.12(%rip), %rsi
	jmp	.LBB0_432
.LBB0_423:                              # %assert_failed11
	leaq	.Lstr.13(%rip), %rsi
	jmp	.LBB0_432
.LBB0_424:                              # %assert_failed12
	leaq	.Lstr.14(%rip), %rsi
	jmp	.LBB0_432
.LBB0_425:                              # %assert_failed13
	leaq	.Lstr.15(%rip), %rsi
	jmp	.LBB0_432
.LBB0_426:                              # %assert_failed14
	leaq	.Lstr.16(%rip), %rsi
	jmp	.LBB0_432
.LBB0_427:                              # %assert_failed15
	leaq	.Lstr.17(%rip), %rsi
	jmp	.LBB0_432
.LBB0_428:                              # %assert_failed16
	leaq	.Lstr.18(%rip), %rsi
	jmp	.LBB0_432
.LBB0_429:                              # %assert_failed17
	leaq	.Lstr.19(%rip), %rsi
	jmp	.LBB0_432
.LBB0_430:                              # %assert_failed18
	leaq	.Lstr.20(%rip), %rsi
	jmp	.LBB0_432
.LBB0_431:                              # %assert_failed19
	leaq	.Lstr.21(%rip), %rsi
.LBB0_432:                              # %assert_failed
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_buffer_argument_is_null@PLT
	jmp	.LBB0_311
.LBB0_433:                              # %assert_failed210
	leaq	.Lstr.62(%rip), %rsi
	leaq	.Lstr.63(%rip), %rcx
	xorl	%edi, %edi
	movq	488(%rsp), %rdx                 # 8-byte Reload
                                        # kill: def $edx killed $edx killed $rdx
	movl	$40, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_434:                              # %assert_failed211
	leaq	.Lstr.64(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	144(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_619
.LBB0_435:                              # %assert_failed212
	leaq	.Lstr.65(%rip), %rsi
	leaq	.Lstr.66(%rip), %rcx
	xorl	%edi, %edi
	movq	480(%rsp), %rdx                 # 8-byte Reload
                                        # kill: def $edx killed $edx killed $rdx
	movl	$7, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_436:                              # %assert_failed213
	leaq	.Lstr.67(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1552(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_437:                              # %assert_failed214
	leaq	.Lstr.68(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	664(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_488
.LBB0_438:                              # %assert_failed215
	leaq	.Lstr.69(%rip), %rsi
	leaq	.Lstr.70(%rip), %rcx
	xorl	%edi, %edi
	movq	872(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_624
.LBB0_439:                              # %assert_failed216
	leaq	.Lstr.71(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1548(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_440:                              # %assert_failed217
	leaq	.Lstr.72(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	864(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_488
.LBB0_441:                              # %assert_failed218
	leaq	.Lstr.73(%rip), %rsi
	leaq	.Lstr.70(%rip), %rcx
	xorl	%edi, %edi
	movq	848(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_624
.LBB0_442:                              # %assert_failed219
	leaq	.Lstr.74(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	856(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_488
.LBB0_443:                              # %assert_failed220
	leaq	.Lstr.75(%rip), %rsi
	leaq	.Lstr.76(%rip), %rcx
	xorl	%edi, %edi
	movq	840(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_486
.LBB0_444:                              # %assert_failed221
	leaq	.Lstr.77(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1544(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_445:                              # %assert_failed222
	leaq	.Lstr.78(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1540(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_446:                              # %assert_failed223
	leaq	.Lstr.79(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1536(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_447:                              # %assert_failed224
	leaq	.Lstr.80(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1532(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_448:                              # %assert_failed225
	leaq	.Lstr.81(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1528(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_449:                              # %assert_failed226
	leaq	.Lstr.82(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1524(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_450:                              # %assert_failed227
	leaq	.Lstr.83(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1516(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_451:                              # %assert_failed228
	leaq	.Lstr.84(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	824(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_488
.LBB0_452:                              # %assert_failed229
	leaq	.Lstr.85(%rip), %rsi
	leaq	.Lstr.47(%rip), %rcx
	xorl	%edi, %edi
	movq	1296(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_612
.LBB0_453:                              # %assert_failed230
	leaq	.Lstr.86(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	528(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_488
.LBB0_454:                              # %assert_failed231
	leaq	.Lstr.87(%rip), %rsi
	leaq	.Lstr.88(%rip), %rcx
	xorl	%edi, %edi
	movq	1288(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_627
.LBB0_455:                              # %assert_failed232
	leaq	.Lstr.89(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1508(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_456:                              # %assert_failed233
	leaq	.Lstr.90(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1280(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_488
.LBB0_457:                              # %assert_failed234
	leaq	.Lstr.91(%rip), %rsi
	leaq	.Lstr.47(%rip), %rcx
	xorl	%edi, %edi
	movq	1248(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_612
.LBB0_458:                              # %assert_failed235
	leaq	.Lstr.92(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1256(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_488
.LBB0_459:                              # %assert_failed236
	leaq	.Lstr.93(%rip), %rsi
	leaq	.Lstr.47(%rip), %rcx
	xorl	%edi, %edi
	movq	1224(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_612
.LBB0_461:                              # %assert_failed237
	leaq	.Lstr.94(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1240(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_488
.LBB0_462:                              # %assert_failed238
	leaq	.Lstr.95(%rip), %rsi
	leaq	.Lstr.88(%rip), %rcx
	xorl	%edi, %edi
	movq	1208(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_627
.LBB0_463:                              # %assert_failed239
	leaq	.Lstr.96(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1496(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_464:                              # %assert_failed240
	leaq	.Lstr.97(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1192(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_488
.LBB0_465:                              # %assert_failed241
	leaq	.Lstr.98(%rip), %rsi
	leaq	.Lstr.57(%rip), %rcx
	xorl	%edi, %edi
	movq	1176(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_621
.LBB0_466:                              # %assert_failed242
	movq	%r12, %rdx
	leaq	.Lstr.99(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	jmp	.LBB0_488
.LBB0_467:                              # %assert_failed243
	leaq	.Lstr.100(%rip), %rsi
	leaq	.Lstr.88(%rip), %rcx
	xorl	%edi, %edi
	movl	%ebx, %edx
	jmp	.LBB0_628
.LBB0_468:                              # %assert_failed244
	leaq	.Lstr.101(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1488(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_469:                              # %assert_failed245
	leaq	.Lstr.102(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1160(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_488
.LBB0_470:                              # %assert_failed246
	leaq	.Lstr.103(%rip), %rsi
	leaq	.Lstr.57(%rip), %rcx
	xorl	%edi, %edi
	movq	1144(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_621
.LBB0_472:                              # %assert_failed247
	leaq	.Lstr.104(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1152(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_488
.LBB0_473:                              # %assert_failed248
	leaq	.Lstr.105(%rip), %rsi
	leaq	.Lstr.63(%rip), %rcx
	xorl	%edi, %edi
	movq	1128(%rsp), %rdx                # 8-byte Reload
                                        # kill: def $edx killed $edx killed $rdx
	movl	$40, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_474:                              # %assert_failed249
	leaq	.Lstr.106(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1136(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_488
.LBB0_475:                              # %assert_failed250
	leaq	.Lstr.107(%rip), %rsi
	leaq	.Lstr.66(%rip), %rcx
	xorl	%edi, %edi
	movq	1112(%rsp), %rdx                # 8-byte Reload
                                        # kill: def $edx killed $edx killed $rdx
	movl	$7, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_476:                              # %assert_failed251
	leaq	.Lstr.108(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1120(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_488
.LBB0_477:                              # %assert_failed252
	leaq	.Lstr.109(%rip), %rsi
	leaq	.Lstr.88(%rip), %rcx
	xorl	%edi, %edi
	movq	1104(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_627
.LBB0_478:                              # %assert_failed253
	leaq	.Lstr.110(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1484(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_479:                              # %assert_failed254
	leaq	.Lstr.111(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movl	1028(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_489
.LBB0_480:                              # %assert_failed256
	leaq	.Lstr.113(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movl	1024(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_489
.LBB0_481:                              # %assert_failed257
	leaq	.Lstr.114(%rip), %rsi
	leaq	.Lstr.88(%rip), %rcx
	xorl	%edi, %edi
	movq	1088(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_627
.LBB0_482:                              # %assert_failed258
	leaq	.Lstr.115(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1480(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_484:                              # %assert_failed259
	leaq	.Lstr.116(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movl	1020(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_489
.LBB0_485:                              # %assert_failed262
	leaq	.Lstr.119(%rip), %rsi
	leaq	.Lstr.76(%rip), %rcx
	xorl	%edi, %edi
	movq	1064(%rsp), %rdx                # 8-byte Reload
.LBB0_486:                              # %assert_failed220
                                        # kill: def $edx killed $edx killed $rdx
	movl	$39, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_487:                              # %assert_failed263
	leaq	.Lstr.120(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1080(%rsp), %rdx                # 8-byte Reload
.LBB0_488:                              # %assert_failed242
                                        # kill: def $edx killed $edx killed $rdx
.LBB0_489:                              # %assert_failed242
	xorl	%r8d, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_490:                              # %assert_failed82
	leaq	.Lstr.23(%rip), %rsi
	xorl	%edi, %edi
	movl	788(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_517
.LBB0_491:                              # %assert_failed83
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	movl	1792(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_492:                              # %assert_failed84
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	movl	1784(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_517
.LBB0_493:                              # %assert_failed85
	leaq	.Lstr.22(%rip), %rsi
	xorl	%edi, %edi
	movl	1776(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_494:                              # %assert_failed86
	leaq	.Lstr.22(%rip), %rsi
	xorl	%edi, %edi
	movl	1040(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_528
.LBB0_495:                              # %assert_failed87
	leaq	.Lstr.25(%rip), %rsi
	xorl	%edi, %edi
	movl	1768(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_496:                              # %assert_failed88
	leaq	.Lstr.25(%rip), %rsi
	xorl	%edi, %edi
	movl	1760(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_517
.LBB0_497:                              # %assert_failed89
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	1752(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_498:                              # %assert_failed90
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	1744(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_532
.LBB0_499:                              # %assert_failed91
	leaq	.Lstr.27(%rip), %rsi
	xorl	%edi, %edi
	movl	1736(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_500:                              # %assert_failed92
	leaq	.Lstr.27(%rip), %rsi
	xorl	%edi, %edi
	movl	1728(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_517
.LBB0_501:                              # %assert_failed93
	leaq	.Lstr.28(%rip), %rsi
	xorl	%edi, %edi
	movl	1720(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_502:                              # %assert_failed94
	leaq	.Lstr.28(%rip), %rsi
	xorl	%edi, %edi
	movl	1712(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_528
.LBB0_503:                              # %assert_failed95
	leaq	.Lstr.29(%rip), %rsi
	xorl	%edi, %edi
	movl	1704(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_504:                              # %assert_failed96
	leaq	.Lstr.29(%rip), %rsi
	xorl	%edi, %edi
	movl	1696(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_517
.LBB0_505:                              # %assert_failed97
	leaq	.Lstr.30(%rip), %rsi
	xorl	%edi, %edi
	movl	1688(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_506:                              # %assert_failed98
	leaq	.Lstr.30(%rip), %rsi
	xorl	%edi, %edi
	movl	1680(%rsp), %edx                # 4-byte Reload
	xorl	%ecx, %ecx
	vzeroupper
	callq	halide_error_bad_dimensions@PLT
	jmp	.LBB0_311
.LBB0_507:                              # %assert_failed99
	leaq	.Lstr.31(%rip), %rsi
	xorl	%edi, %edi
	movl	1672(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_508:                              # %assert_failed100
	leaq	.Lstr.31(%rip), %rsi
	xorl	%edi, %edi
	movl	1664(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_532
.LBB0_509:                              # %assert_failed101
	leaq	.Lstr.32(%rip), %rsi
	xorl	%edi, %edi
	movl	1656(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_510:                              # %assert_failed102
	leaq	.Lstr.32(%rip), %rsi
	xorl	%edi, %edi
	movl	1648(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_517
.LBB0_511:                              # %assert_failed103
	leaq	.Lstr.33(%rip), %rsi
	xorl	%edi, %edi
	movl	1640(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_530
.LBB0_512:                              # %assert_failed104
	leaq	.Lstr.33(%rip), %rsi
	xorl	%edi, %edi
	movl	784(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_532
.LBB0_513:                              # %assert_failed105
	leaq	.Lstr.34(%rip), %rsi
	xorl	%edi, %edi
	movl	780(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_530
.LBB0_514:                              # %assert_failed106
	leaq	.Lstr.34(%rip), %rsi
	xorl	%edi, %edi
	movl	776(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_517
.LBB0_515:                              # %assert_failed107
	leaq	.Lstr.35(%rip), %rsi
	xorl	%edi, %edi
	movl	772(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_530
.LBB0_516:                              # %assert_failed108
	leaq	.Lstr.35(%rip), %rsi
	xorl	%edi, %edi
	movl	768(%rsp), %edx                 # 4-byte Reload
.LBB0_517:                              # %assert_failed82
	movl	$1, %ecx
	vzeroupper
	callq	halide_error_bad_dimensions@PLT
	jmp	.LBB0_311
.LBB0_518:                              # %assert_failed109
	leaq	.Lstr.36(%rip), %rsi
	xorl	%edi, %edi
	movl	764(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_530
.LBB0_519:                              # %assert_failed110
	leaq	.Lstr.36(%rip), %rsi
	xorl	%edi, %edi
	movl	760(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_528
.LBB0_520:                              # %assert_failed111
	leaq	.Lstr.37(%rip), %rsi
	xorl	%edi, %edi
	movl	756(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_530
.LBB0_521:                              # %assert_failed112
	leaq	.Lstr.37(%rip), %rsi
	xorl	%edi, %edi
	movl	752(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_532
.LBB0_522:                              # %assert_failed113
	leaq	.Lstr.38(%rip), %rsi
	xorl	%edi, %edi
	movl	748(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_530
.LBB0_523:                              # %assert_failed114
	leaq	.Lstr.38(%rip), %rsi
	xorl	%edi, %edi
	movl	744(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_528
.LBB0_524:                              # %assert_failed115
	leaq	.Lstr.39(%rip), %rsi
	xorl	%edi, %edi
	movl	740(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_530
.LBB0_525:                              # %assert_failed116
	leaq	.Lstr.39(%rip), %rsi
	xorl	%edi, %edi
	movl	736(%rsp), %edx                 # 4-byte Reload
	movl	$4, %ecx
	vzeroupper
	callq	halide_error_bad_dimensions@PLT
	jmp	.LBB0_311
.LBB0_526:                              # %assert_failed117
	leaq	.Lstr.40(%rip), %rsi
	xorl	%edi, %edi
	movl	732(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_530
.LBB0_527:                              # %assert_failed118
	leaq	.Lstr.40(%rip), %rsi
	xorl	%edi, %edi
	movl	728(%rsp), %edx                 # 4-byte Reload
.LBB0_528:                              # %assert_failed86
	movl	$2, %ecx
	vzeroupper
	callq	halide_error_bad_dimensions@PLT
	jmp	.LBB0_311
.LBB0_529:                              # %assert_failed119
	leaq	.Lstr.41(%rip), %rsi
	xorl	%edi, %edi
	movl	724(%rsp), %edx                 # 4-byte Reload
.LBB0_530:                              # %assert_failed81
	movl	$73730, %ecx                    # imm = 0x12002
	vzeroupper
	callq	halide_error_bad_type@PLT
	jmp	.LBB0_311
.LBB0_531:                              # %assert_failed120
	leaq	.Lstr.41(%rip), %rsi
	xorl	%edi, %edi
	movl	720(%rsp), %edx                 # 4-byte Reload
.LBB0_532:                              # %assert_failed90
	movl	$3, %ecx
	vzeroupper
	callq	halide_error_bad_dimensions@PLT
	jmp	.LBB0_311
.LBB0_533:                              # %assert_failed121
	decl	%esi
	movl	%esi, (%rsp)
	leaq	.Lstr.23(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	408(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_534:                              # %assert_failed122
	leaq	.Lstr.23(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	832(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_535:                              # %assert_failed123
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1320(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_536:                              # %assert_failed124
	decl	%edi
	movl	%edi, (%rsp)
	leaq	.Lstr.22(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	1272(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_537:                              # %assert_failed125
	leaq	.Lstr.22(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1232(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_538:                              # %assert_failed126
	movq	1384(%rsp), %r8                 # 8-byte Reload
	addl	$23, %r8d
	movl	%r9d, %ebx
	decl	%ebx
	movl	%ebx, (%rsp)
	leaq	.Lstr.22(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	572(%rsp), %ecx                 # 4-byte Reload
                                        # kill: def $r8d killed $r8d killed $r8
	movq	1264(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_539:                              # %assert_failed127
	leaq	.Lstr.22(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1216(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_540:                              # %assert_failed128
	decl	%ebx
	movl	%ebx, (%rsp)
	leaq	.Lstr.25(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movq	1200(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_541:                              # %assert_failed129
	leaq	.Lstr.25(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1184(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_542:                              # %assert_failed130
	decl	%r8d
	movl	%r8d, (%rsp)
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movq	1168(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_543:                              # %assert_failed131
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	288(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_544:                              # %assert_failed132
	decl	%r10d
	movl	%r10d, (%rsp)
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$39, %r8d
	movq	216(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_545:                              # %assert_failed133
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	488(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_546:                              # %assert_failed134
	decl	%r11d
	movl	%r11d, (%rsp)
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$6, %r8d
	movq	144(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_547:                              # %assert_failed135
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	480(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_548:                              # %assert_failed136
	decl	%r14d
	movl	%r14d, (%rsp)
	leaq	.Lstr.27(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movq	664(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_549:                              # %assert_failed137
	leaq	.Lstr.27(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	872(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_550:                              # %assert_failed138
	decl	%r15d
	movl	%r15d, (%rsp)
	leaq	.Lstr.28(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movq	864(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_551:                              # %assert_failed139
	leaq	.Lstr.28(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	848(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_552:                              # %assert_failed140
	decl	%r12d
	movl	%r12d, (%rsp)
	leaq	.Lstr.28(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$38, %r8d
	movq	856(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_553:                              # %assert_failed141
	leaq	.Lstr.28(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	840(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_554:                              # %assert_failed142
	leaq	.Lstr.29(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	424(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_555:                              # %assert_failed143
	decl	%ecx
	movl	%ecx, (%rsp)
	leaq	.Lstr.31(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$39, %r8d
	movq	1328(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_556:                              # %assert_failed146
	decl	%edx
	movl	%edx, (%rsp)
	leaq	.Lstr.31(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$6, %r8d
	movq	504(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_557:                              # %assert_failed147
	leaq	.Lstr.31(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	968(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_558:                              # %assert_failed148
	movq	160(%rsp), %r8                  # 8-byte Reload
	decl	%r8d
	decl	%esi
	movl	%esi, (%rsp)
	leaq	.Lstr.31(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
                                        # kill: def $r8d killed $r8d killed $r8
	movq	544(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_559:                              # %assert_failed149
	leaq	.Lstr.31(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	976(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_560:                              # %assert_failed150
	movq	672(%rsp), %rcx                 # 8-byte Reload
	movq	1856(%rsp), %rax                # 8-byte Reload
	leal	(%rax,%rcx), %r8d
	decl	%r8d
	movl	1940(%rsp), %eax                # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.32(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	%ecx, %r9d
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_561:                              # %assert_failed151
	leaq	.Lstr.32(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	632(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_562:                              # %assert_failed152
	movl	1036(%rsp), %r8d                # 4-byte Reload
	decl	%r8d
	movl	%r14d, %edi
	decl	%edi
	movl	%edi, (%rsp)
	leaq	.Lstr.33(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	1568(%rsp), %ecx                # 4-byte Reload
	movq	272(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_563:                              # %assert_failed153
	leaq	.Lstr.33(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	88(%rsp), %rcx                  # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_564:                              # %assert_failed154
	decl	%ebx
	movl	%ebx, (%rsp)
	leaq	.Lstr.33(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$38, %r8d
	movq	48(%rsp), %r9                   # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_565:                              # %assert_failed155
	leaq	.Lstr.33(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	952(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_566:                              # %assert_failed156
	movq	160(%rsp), %r8                  # 8-byte Reload
	decl	%r8d
	decl	%r9d
	movl	%r9d, (%rsp)
	leaq	.Lstr.33(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
                                        # kill: def $r8d killed $r8d killed $r8
	movq	208(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_567:                              # %assert_failed157
	leaq	.Lstr.33(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	960(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_568:                              # %assert_failed158
	leaq	.Lstr.34(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1312(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_569:                              # %assert_failed159
	movl	1032(%rsp), %r8d                # 4-byte Reload
	decl	%r8d
	decl	%r10d
	movl	%r10d, (%rsp)
	leaq	.Lstr.35(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movq	640(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_570:                              # %assert_failed160
	leaq	.Lstr.35(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	72(%rsp), %rcx                  # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_571:                              # %assert_failed161
	decl	%r8d
	movl	%r8d, (%rsp)
	leaq	.Lstr.36(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	824(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_572:                              # %assert_failed162
	leaq	.Lstr.36(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1296(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_573:                              # %assert_failed163
	decl	%r15d
	movl	%r15d, (%rsp)
	leaq	.Lstr.36(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movq	528(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_574:                              # %assert_failed164
	leaq	.Lstr.36(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1288(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_575:                              # %assert_failed165
	decl	%r12d
	movl	%r12d, (%rsp)
	leaq	.Lstr.37(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	1280(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_576:                              # %assert_failed166
	leaq	.Lstr.37(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1248(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_577:                              # %assert_failed167
	decl	%r13d
	movl	%r13d, (%rsp)
	leaq	.Lstr.37(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	1256(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_578:                              # %assert_failed168
	leaq	.Lstr.37(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1224(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_579:                              # %assert_failed169
	decl	%r11d
	movl	%r11d, (%rsp)
	leaq	.Lstr.37(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movq	1240(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_580:                              # %assert_failed170
	leaq	.Lstr.37(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	1208(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_581:                              # %assert_failed171
	movl	572(%rsp), %eax                 # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.38(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movq	1192(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_582:                              # %assert_failed172
	leaq	.Lstr.38(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1176(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_583:                              # %assert_failed173
	movl	1384(%rsp), %eax                # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.38(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movq	2080(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_584:                              # %assert_failed174
	leaq	.Lstr.38(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_585:                              # %assert_failed175
	movl	724(%rsp), %eax                 # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.39(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movq	1160(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_586:                              # %assert_failed176
	leaq	.Lstr.39(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1144(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_587:                              # %assert_failed177
	movl	720(%rsp), %eax                 # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.39(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$39, %r8d
	movq	1152(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_588:                              # %assert_failed178
	leaq	.Lstr.39(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1128(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_589:                              # %assert_failed179
	movl	1012(%rsp), %eax                # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.39(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$6, %r8d
	movq	1136(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_590:                              # %assert_failed180
	leaq	.Lstr.39(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	1112(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_591:                              # %assert_failed181
	movl	1008(%rsp), %eax                # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.39(%rip), %rsi
	xorl	%edi, %edi
	movl	$3, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movq	1120(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_592:                              # %assert_failed182
	leaq	.Lstr.39(%rip), %rsi
	xorl	%edi, %edi
	movl	$3, %edx
	movq	1104(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_593:                              # %assert_failed183
	vmovd	%xmm6, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.40(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movl	1028(%rsp), %r9d                # 4-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_594:                              # %assert_failed184
	leaq	.Lstr.40(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1096(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_595:                              # %assert_failed185
	vpextrd	$1, %xmm6, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.40(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movl	1024(%rsp), %r9d                # 4-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_596:                              # %assert_failed186
	leaq	.Lstr.40(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1088(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_597:                              # %assert_failed187
	vpextrd	$2, %xmm6, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.41(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movl	1020(%rsp), %r9d                # 4-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_598:                              # %assert_failed188
	leaq	.Lstr.41(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1072(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_599:                              # %assert_failed189
	vpextrd	$3, %xmm6, %eax
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.41(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$38, %r8d
	movl	1016(%rsp), %r9d                # 4-byte Reload
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_600:                              # %assert_failed190
	leaq	.Lstr.41(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1064(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_601:                              # %assert_failed191
	movl	1004(%rsp), %eax                # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.41(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$3, %r8d
	movq	1080(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_311
.LBB0_602:                              # %assert_failed192
	leaq	.Lstr.41(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	1056(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_311
.LBB0_603:                              # %assert_failed193
	leaq	.Lstr.42(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1520(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_604:                              # %assert_failed194
	leaq	.Lstr.44(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	408(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_619
.LBB0_605:                              # %assert_failed195
	leaq	.Lstr.46(%rip), %rsi
	leaq	.Lstr.47(%rip), %rcx
	xorl	%edi, %edi
	movq	832(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_612
.LBB0_606:                              # %assert_failed196
	leaq	.Lstr.48(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1512(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_607:                              # %assert_failed197
	leaq	.Lstr.49(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1504(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_608:                              # %assert_failed198
	leaq	.Lstr.50(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1272(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_619
.LBB0_609:                              # %assert_failed199
	leaq	.Lstr.51(%rip), %rsi
	leaq	.Lstr.47(%rip), %rcx
	xorl	%edi, %edi
	movq	1232(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_612
.LBB0_610:                              # %assert_failed200
	leaq	.Lstr.52(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1264(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_619
.LBB0_611:                              # %assert_failed201
	leaq	.Lstr.53(%rip), %rsi
	leaq	.Lstr.47(%rip), %rcx
	xorl	%edi, %edi
	movq	1216(%rsp), %rdx                # 8-byte Reload
.LBB0_612:                              # %assert_failed195
                                        # kill: def $edx killed $edx killed $rdx
	movl	$32, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_613:                              # %assert_failed202
	leaq	.Lstr.54(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1500(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_617
.LBB0_614:                              # %assert_failed203
	leaq	.Lstr.55(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1200(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_619
.LBB0_615:                              # %assert_failed204
	leaq	.Lstr.56(%rip), %rsi
	leaq	.Lstr.57(%rip), %rcx
	xorl	%edi, %edi
	movq	1184(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_621
.LBB0_616:                              # %assert_failed205
	leaq	.Lstr.58(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movl	1492(%rsp), %edx                # 4-byte Reload
.LBB0_617:                              # %assert_failed193
	movl	$1, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_618:                              # %assert_failed206
	leaq	.Lstr.59(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movq	1168(%rsp), %rdx                # 8-byte Reload
.LBB0_619:                              # %assert_failed194
                                        # kill: def $edx killed $edx killed $rdx
	xorl	%r8d, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_620:                              # %assert_failed207
	leaq	.Lstr.60(%rip), %rsi
	leaq	.Lstr.57(%rip), %rcx
	xorl	%edi, %edi
	movq	288(%rsp), %rdx                 # 8-byte Reload
.LBB0_621:                              # %assert_failed204
                                        # kill: def $edx killed $edx killed $rdx
	movl	$8, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_622:                              # %assert_failed255
	leaq	.Lstr.112(%rip), %rsi
	leaq	.Lstr.70(%rip), %rcx
	xorl	%edi, %edi
	movq	1096(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_624
.LBB0_623:                              # %assert_failed260
	leaq	.Lstr.117(%rip), %rsi
	leaq	.Lstr.70(%rip), %rcx
	xorl	%edi, %edi
	movq	1072(%rsp), %rdx                # 8-byte Reload
.LBB0_624:                              # %assert_failed215
                                        # kill: def $edx killed $edx killed $rdx
	movl	$24, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_625:                              # %assert_failed261
	leaq	.Lstr.118(%rip), %rsi
	leaq	.Lstr.45(%rip), %rcx
	xorl	%edi, %edi
	movl	1016(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_489
.LBB0_626:                              # %assert_failed264
	leaq	.Lstr.121(%rip), %rsi
	leaq	.Lstr.88(%rip), %rcx
	xorl	%edi, %edi
	movq	1056(%rsp), %rdx                # 8-byte Reload
.LBB0_627:                              # %assert_failed231
                                        # kill: def $edx killed $edx killed $rdx
.LBB0_628:                              # %assert_failed231
	movl	$4, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_311
.LBB0_629:                              # %assert_failed267
	leaq	.Lstr.19(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r11, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_630:                              # %assert_failed268
	leaq	.Lstr.17(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r14, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_631:                              # %assert_failed269
	leaq	.Lstr.17(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r15, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_632:                              # %assert_failed270
	leaq	.Lstr.15(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r12, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_633:                              # %assert_failed271
	leaq	.Lstr.14(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r13, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_634:                              # %assert_failed272
	leaq	.Lstr.12(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1176(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_635:                              # %assert_failed273
	leaq	.Lstr.12(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1160(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_636:                              # %assert_failed274
	leaq	.Lstr.12(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB0_311
.LBB0_637:                              # %assert_failed275
	leaq	.Lstr.12(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1152(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_638:                              # %assert_failed276
	leaq	.Lstr.12(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%rbx, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB0_311
.LBB0_639:                              # %assert_failed277
	leaq	.Lstr.11(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1144(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_640:                              # %assert_failed278
	leaq	.Lstr.10(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1136(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_641:                              # %assert_failed279
	leaq	.Lstr.10(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1128(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_642:                              # %assert_failed280
	leaq	.Lstr.10(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r8, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB0_311
.LBB0_643:                              # %assert_failed281
	leaq	.Lstr.10(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1120(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_644:                              # %assert_failed282
	leaq	.Lstr.10(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r9, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB0_311
.LBB0_645:                              # %assert_failed283
	leaq	.Lstr.9(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1112(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_646:                              # %assert_failed284
	leaq	.Lstr.8(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1104(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_647:                              # %assert_failed285
	leaq	.Lstr.7(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1096(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_648:                              # %assert_failed286
	leaq	.Lstr.6(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1088(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_649:                              # %assert_failed287
	leaq	.Lstr.6(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1080(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_650:                              # %assert_failed288
	leaq	.Lstr.5(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1072(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_651:                              # %assert_failed289
	leaq	.Lstr.4(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1064(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_652:                              # %assert_failed290
	leaq	.Lstr.4(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1056(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_653:                              # %assert_failed291
	leaq	.Lstr.4(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	816(%rsp), %rdx                 # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_654:                              # %assert_failed292
	leaq	.Lstr.3(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	808(%rsp), %rdx                 # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_655:                              # %assert_failed293
	leaq	.Lstr(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	800(%rsp), %rdx                 # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_656:                              # %assert_failed294
	leaq	.Lstr(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	792(%rsp), %rdx                 # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_311
.LBB0_657:                              # %assert_failed295
	leaq	.Lstr.23(%rip), %rsi
	jmp	.LBB0_677
.LBB0_658:                              # %assert_failed296
	leaq	.Lstr.24(%rip), %rsi
	jmp	.LBB0_677
.LBB0_659:                              # %assert_failed297
	leaq	.Lstr.22(%rip), %rsi
	jmp	.LBB0_677
.LBB0_660:                              # %assert_failed298
	leaq	.Lstr.25(%rip), %rsi
	jmp	.LBB0_677
.LBB0_661:                              # %assert_failed299
	leaq	.Lstr.26(%rip), %rsi
	jmp	.LBB0_677
.LBB0_662:                              # %assert_failed300
	leaq	.Lstr.27(%rip), %rsi
	jmp	.LBB0_677
.LBB0_663:                              # %assert_failed301
	leaq	.Lstr.28(%rip), %rsi
	jmp	.LBB0_677
.LBB0_664:                              # %assert_failed302
	leaq	.Lstr.29(%rip), %rsi
	jmp	.LBB0_677
.LBB0_665:                              # %assert_failed303
	leaq	.Lstr.30(%rip), %rsi
	jmp	.LBB0_677
.LBB0_666:                              # %assert_failed304
	leaq	.Lstr.31(%rip), %rsi
	jmp	.LBB0_677
.LBB0_667:                              # %assert_failed305
	leaq	.Lstr.32(%rip), %rsi
	jmp	.LBB0_677
.LBB0_668:                              # %assert_failed306
	leaq	.Lstr.33(%rip), %rsi
	jmp	.LBB0_677
.LBB0_669:                              # %assert_failed307
	leaq	.Lstr.34(%rip), %rsi
	jmp	.LBB0_677
.LBB0_670:                              # %assert_failed308
	leaq	.Lstr.35(%rip), %rsi
	jmp	.LBB0_677
.LBB0_671:                              # %assert_failed309
	leaq	.Lstr.36(%rip), %rsi
	jmp	.LBB0_677
.LBB0_672:                              # %assert_failed310
	leaq	.Lstr.37(%rip), %rsi
	jmp	.LBB0_677
.LBB0_673:                              # %assert_failed311
	leaq	.Lstr.38(%rip), %rsi
	jmp	.LBB0_677
.LBB0_674:                              # %assert_failed312
	leaq	.Lstr.39(%rip), %rsi
	jmp	.LBB0_677
.LBB0_675:                              # %assert_failed313
	leaq	.Lstr.40(%rip), %rsi
	jmp	.LBB0_677
.LBB0_676:                              # %assert_failed314
	leaq	.Lstr.41(%rip), %rsi
.LBB0_677:                              # %assert_failed295
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_device_dirty_with_no_device_support@PLT
	jmp	.LBB0_311
.LBB0_678:                              # %assert_failed315
	leaq	.Lstr.23(%rip), %rsi
	jmp	.LBB0_412
.LBB0_679:                              # %assert_failed316
	leaq	.Lstr.24(%rip), %rsi
	jmp	.LBB0_412
.LBB0_680:                              # %assert_failed317
	leaq	.Lstr.22(%rip), %rsi
	jmp	.LBB0_412
.LBB0_681:                              # %assert_failed318
	leaq	.Lstr.25(%rip), %rsi
	jmp	.LBB0_412
.LBB0_682:                              # %assert_failed319
	leaq	.Lstr.26(%rip), %rsi
	jmp	.LBB0_412
.LBB0_683:                              # %assert_failed320
	leaq	.Lstr.27(%rip), %rsi
	jmp	.LBB0_412
.LBB0_684:                              # %assert_failed321
	leaq	.Lstr.28(%rip), %rsi
	jmp	.LBB0_412
.LBB0_685:                              # %assert_failed322
	leaq	.Lstr.29(%rip), %rsi
	jmp	.LBB0_412
.LBB0_686:                              # %assert_failed323
	leaq	.Lstr.30(%rip), %rsi
	jmp	.LBB0_412
.LBB0_687:                              # %assert_failed324
	leaq	.Lstr.31(%rip), %rsi
	jmp	.LBB0_412
.LBB0_688:                              # %assert_failed325
	leaq	.Lstr.32(%rip), %rsi
	jmp	.LBB0_412
.LBB0_689:                              # %assert_failed326
	leaq	.Lstr.33(%rip), %rsi
	jmp	.LBB0_412
.LBB0_690:                              # %assert_failed327
	leaq	.Lstr.34(%rip), %rsi
	jmp	.LBB0_412
.LBB0_691:                              # %assert_failed328
	leaq	.Lstr.35(%rip), %rsi
	jmp	.LBB0_412
.Lfunc_end0:
	.size	train_cost_model, .Lfunc_end0-train_cost_model
	.cfi_endproc
	.section	.rodata.train_cost_model,"a",@progbits
	.p2align	2
.LJTI0_0:
	.long	.LBB0_320-.LJTI0_0
	.long	.LBB0_413-.LJTI0_0
	.long	.LBB0_414-.LJTI0_0
	.long	.LBB0_415-.LJTI0_0
	.long	.LBB0_416-.LJTI0_0
	.long	.LBB0_417-.LJTI0_0
	.long	.LBB0_418-.LJTI0_0
	.long	.LBB0_419-.LJTI0_0
	.long	.LBB0_420-.LJTI0_0
	.long	.LBB0_421-.LJTI0_0
	.long	.LBB0_422-.LJTI0_0
	.long	.LBB0_423-.LJTI0_0
	.long	.LBB0_424-.LJTI0_0
	.long	.LBB0_425-.LJTI0_0
	.long	.LBB0_426-.LJTI0_0
	.long	.LBB0_427-.LJTI0_0
	.long	.LBB0_428-.LJTI0_0
	.long	.LBB0_429-.LJTI0_0
	.long	.LBB0_430-.LJTI0_0
	.long	.LBB0_431-.LJTI0_0
.LJTI0_1:
	.long	.LBB0_335-.LJTI0_1
	.long	.LBB0_490-.LJTI0_1
	.long	.LBB0_491-.LJTI0_1
	.long	.LBB0_492-.LJTI0_1
	.long	.LBB0_493-.LJTI0_1
	.long	.LBB0_494-.LJTI0_1
	.long	.LBB0_495-.LJTI0_1
	.long	.LBB0_496-.LJTI0_1
	.long	.LBB0_497-.LJTI0_1
	.long	.LBB0_498-.LJTI0_1
	.long	.LBB0_499-.LJTI0_1
	.long	.LBB0_500-.LJTI0_1
	.long	.LBB0_501-.LJTI0_1
	.long	.LBB0_502-.LJTI0_1
	.long	.LBB0_503-.LJTI0_1
	.long	.LBB0_504-.LJTI0_1
	.long	.LBB0_505-.LJTI0_1
	.long	.LBB0_506-.LJTI0_1
	.long	.LBB0_507-.LJTI0_1
	.long	.LBB0_508-.LJTI0_1
	.long	.LBB0_509-.LJTI0_1
	.long	.LBB0_510-.LJTI0_1
	.long	.LBB0_511-.LJTI0_1
	.long	.LBB0_512-.LJTI0_1
	.long	.LBB0_513-.LJTI0_1
	.long	.LBB0_514-.LJTI0_1
	.long	.LBB0_515-.LJTI0_1
	.long	.LBB0_516-.LJTI0_1
	.long	.LBB0_518-.LJTI0_1
	.long	.LBB0_519-.LJTI0_1
	.long	.LBB0_520-.LJTI0_1
	.long	.LBB0_521-.LJTI0_1
	.long	.LBB0_522-.LJTI0_1
	.long	.LBB0_523-.LJTI0_1
	.long	.LBB0_524-.LJTI0_1
	.long	.LBB0_525-.LJTI0_1
	.long	.LBB0_526-.LJTI0_1
	.long	.LBB0_527-.LJTI0_1
	.long	.LBB0_529-.LJTI0_1
	.long	.LBB0_531-.LJTI0_1
	.long	.LBB0_533-.LJTI0_1
	.long	.LBB0_534-.LJTI0_1
	.long	.LBB0_535-.LJTI0_1
	.long	.LBB0_536-.LJTI0_1
	.long	.LBB0_537-.LJTI0_1
	.long	.LBB0_538-.LJTI0_1
	.long	.LBB0_539-.LJTI0_1
	.long	.LBB0_540-.LJTI0_1
	.long	.LBB0_541-.LJTI0_1
	.long	.LBB0_542-.LJTI0_1
	.long	.LBB0_543-.LJTI0_1
	.long	.LBB0_544-.LJTI0_1
	.long	.LBB0_545-.LJTI0_1
	.long	.LBB0_546-.LJTI0_1
	.long	.LBB0_547-.LJTI0_1
	.long	.LBB0_548-.LJTI0_1
	.long	.LBB0_549-.LJTI0_1
	.long	.LBB0_550-.LJTI0_1
	.long	.LBB0_551-.LJTI0_1
	.long	.LBB0_552-.LJTI0_1
	.long	.LBB0_553-.LJTI0_1
	.long	.LBB0_554-.LJTI0_1
	.long	.LBB0_555-.LJTI0_1
.LJTI0_2:
	.long	.LBB0_337-.LJTI0_2
	.long	.LBB0_556-.LJTI0_2
	.long	.LBB0_557-.LJTI0_2
	.long	.LBB0_558-.LJTI0_2
	.long	.LBB0_559-.LJTI0_2
	.long	.LBB0_560-.LJTI0_2
	.long	.LBB0_561-.LJTI0_2
	.long	.LBB0_562-.LJTI0_2
	.long	.LBB0_563-.LJTI0_2
	.long	.LBB0_564-.LJTI0_2
	.long	.LBB0_565-.LJTI0_2
	.long	.LBB0_566-.LJTI0_2
	.long	.LBB0_567-.LJTI0_2
	.long	.LBB0_568-.LJTI0_2
	.long	.LBB0_569-.LJTI0_2
	.long	.LBB0_570-.LJTI0_2
	.long	.LBB0_571-.LJTI0_2
	.long	.LBB0_572-.LJTI0_2
	.long	.LBB0_573-.LJTI0_2
	.long	.LBB0_574-.LJTI0_2
	.long	.LBB0_575-.LJTI0_2
	.long	.LBB0_576-.LJTI0_2
	.long	.LBB0_577-.LJTI0_2
	.long	.LBB0_578-.LJTI0_2
	.long	.LBB0_579-.LJTI0_2
	.long	.LBB0_580-.LJTI0_2
	.long	.LBB0_581-.LJTI0_2
	.long	.LBB0_582-.LJTI0_2
	.long	.LBB0_583-.LJTI0_2
	.long	.LBB0_584-.LJTI0_2
	.long	.LBB0_585-.LJTI0_2
	.long	.LBB0_586-.LJTI0_2
	.long	.LBB0_587-.LJTI0_2
	.long	.LBB0_588-.LJTI0_2
	.long	.LBB0_589-.LJTI0_2
	.long	.LBB0_590-.LJTI0_2
	.long	.LBB0_591-.LJTI0_2
	.long	.LBB0_592-.LJTI0_2
	.long	.LBB0_593-.LJTI0_2
	.long	.LBB0_594-.LJTI0_2
	.long	.LBB0_595-.LJTI0_2
	.long	.LBB0_596-.LJTI0_2
	.long	.LBB0_597-.LJTI0_2
	.long	.LBB0_598-.LJTI0_2
	.long	.LBB0_599-.LJTI0_2
	.long	.LBB0_600-.LJTI0_2
	.long	.LBB0_601-.LJTI0_2
	.long	.LBB0_602-.LJTI0_2
	.long	.LBB0_603-.LJTI0_2
	.long	.LBB0_604-.LJTI0_2
	.long	.LBB0_605-.LJTI0_2
	.long	.LBB0_606-.LJTI0_2
	.long	.LBB0_607-.LJTI0_2
	.long	.LBB0_608-.LJTI0_2
	.long	.LBB0_609-.LJTI0_2
	.long	.LBB0_610-.LJTI0_2
	.long	.LBB0_611-.LJTI0_2
	.long	.LBB0_613-.LJTI0_2
	.long	.LBB0_614-.LJTI0_2
	.long	.LBB0_615-.LJTI0_2
	.long	.LBB0_616-.LJTI0_2
	.long	.LBB0_618-.LJTI0_2
	.long	.LBB0_620-.LJTI0_2
.LJTI0_3:
	.long	.LBB0_339-.LJTI0_3
	.long	.LBB0_433-.LJTI0_3
	.long	.LBB0_434-.LJTI0_3
	.long	.LBB0_435-.LJTI0_3
	.long	.LBB0_436-.LJTI0_3
	.long	.LBB0_437-.LJTI0_3
	.long	.LBB0_438-.LJTI0_3
	.long	.LBB0_439-.LJTI0_3
	.long	.LBB0_440-.LJTI0_3
	.long	.LBB0_441-.LJTI0_3
	.long	.LBB0_442-.LJTI0_3
	.long	.LBB0_443-.LJTI0_3
	.long	.LBB0_444-.LJTI0_3
	.long	.LBB0_445-.LJTI0_3
	.long	.LBB0_446-.LJTI0_3
	.long	.LBB0_447-.LJTI0_3
	.long	.LBB0_448-.LJTI0_3
	.long	.LBB0_449-.LJTI0_3
	.long	.LBB0_450-.LJTI0_3
	.long	.LBB0_451-.LJTI0_3
	.long	.LBB0_452-.LJTI0_3
	.long	.LBB0_453-.LJTI0_3
	.long	.LBB0_454-.LJTI0_3
	.long	.LBB0_455-.LJTI0_3
	.long	.LBB0_456-.LJTI0_3
	.long	.LBB0_457-.LJTI0_3
	.long	.LBB0_458-.LJTI0_3
	.long	.LBB0_459-.LJTI0_3
	.long	.LBB0_461-.LJTI0_3
	.long	.LBB0_462-.LJTI0_3
	.long	.LBB0_463-.LJTI0_3
	.long	.LBB0_464-.LJTI0_3
	.long	.LBB0_465-.LJTI0_3
	.long	.LBB0_466-.LJTI0_3
	.long	.LBB0_467-.LJTI0_3
	.long	.LBB0_468-.LJTI0_3
	.long	.LBB0_469-.LJTI0_3
	.long	.LBB0_470-.LJTI0_3
	.long	.LBB0_472-.LJTI0_3
	.long	.LBB0_473-.LJTI0_3
	.long	.LBB0_474-.LJTI0_3
	.long	.LBB0_475-.LJTI0_3
	.long	.LBB0_476-.LJTI0_3
	.long	.LBB0_477-.LJTI0_3
	.long	.LBB0_478-.LJTI0_3
	.long	.LBB0_479-.LJTI0_3
	.long	.LBB0_622-.LJTI0_3
	.long	.LBB0_480-.LJTI0_3
	.long	.LBB0_481-.LJTI0_3
	.long	.LBB0_482-.LJTI0_3
	.long	.LBB0_484-.LJTI0_3
	.long	.LBB0_623-.LJTI0_3
	.long	.LBB0_625-.LJTI0_3
	.long	.LBB0_485-.LJTI0_3
	.long	.LBB0_487-.LJTI0_3
	.long	.LBB0_626-.LJTI0_3
.LJTI0_4:
	.long	.LBB0_341-.LJTI0_4
	.long	.LBB0_629-.LJTI0_4
	.long	.LBB0_630-.LJTI0_4
	.long	.LBB0_631-.LJTI0_4
	.long	.LBB0_632-.LJTI0_4
	.long	.LBB0_633-.LJTI0_4
	.long	.LBB0_634-.LJTI0_4
	.long	.LBB0_635-.LJTI0_4
	.long	.LBB0_636-.LJTI0_4
	.long	.LBB0_637-.LJTI0_4
	.long	.LBB0_638-.LJTI0_4
	.long	.LBB0_639-.LJTI0_4
	.long	.LBB0_640-.LJTI0_4
	.long	.LBB0_641-.LJTI0_4
	.long	.LBB0_642-.LJTI0_4
	.long	.LBB0_643-.LJTI0_4
	.long	.LBB0_644-.LJTI0_4
	.long	.LBB0_645-.LJTI0_4
	.long	.LBB0_646-.LJTI0_4
	.long	.LBB0_647-.LJTI0_4
	.long	.LBB0_648-.LJTI0_4
	.long	.LBB0_649-.LJTI0_4
	.long	.LBB0_650-.LJTI0_4
	.long	.LBB0_651-.LJTI0_4
	.long	.LBB0_652-.LJTI0_4
	.long	.LBB0_653-.LJTI0_4
	.long	.LBB0_654-.LJTI0_4
	.long	.LBB0_655-.LJTI0_4
	.long	.LBB0_656-.LJTI0_4
	.long	.LBB0_657-.LJTI0_4
	.long	.LBB0_658-.LJTI0_4
	.long	.LBB0_659-.LJTI0_4
	.long	.LBB0_660-.LJTI0_4
	.long	.LBB0_661-.LJTI0_4
	.long	.LBB0_662-.LJTI0_4
	.long	.LBB0_663-.LJTI0_4
	.long	.LBB0_664-.LJTI0_4
	.long	.LBB0_665-.LJTI0_4
	.long	.LBB0_666-.LJTI0_4
	.long	.LBB0_667-.LJTI0_4
	.long	.LBB0_668-.LJTI0_4
	.long	.LBB0_669-.LJTI0_4
	.long	.LBB0_670-.LJTI0_4
	.long	.LBB0_671-.LJTI0_4
	.long	.LBB0_672-.LJTI0_4
	.long	.LBB0_673-.LJTI0_4
	.long	.LBB0_674-.LJTI0_4
	.long	.LBB0_675-.LJTI0_4
	.long	.LBB0_676-.LJTI0_4
	.long	.LBB0_678-.LJTI0_4
	.long	.LBB0_679-.LJTI0_4
	.long	.LBB0_680-.LJTI0_4
	.long	.LBB0_681-.LJTI0_4
	.long	.LBB0_682-.LJTI0_4
	.long	.LBB0_683-.LJTI0_4
	.long	.LBB0_684-.LJTI0_4
	.long	.LBB0_685-.LJTI0_4
	.long	.LBB0_686-.LJTI0_4
	.long	.LBB0_687-.LJTI0_4
	.long	.LBB0_688-.LJTI0_4
	.long	.LBB0_689-.LJTI0_4
	.long	.LBB0_690-.LJTI0_4
	.long	.LBB0_691-.LJTI0_4
.LJTI0_5:
	.long	.LBB0_343-.LJTI0_5
	.long	.LBB0_407-.LJTI0_5
	.long	.LBB0_408-.LJTI0_5
	.long	.LBB0_409-.LJTI0_5
	.long	.LBB0_410-.LJTI0_5
	.long	.LBB0_411-.LJTI0_5
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.squashed_head1_filter.s0.s.s.s
.LCPI1_0:
	.long	0xbfb8aa3b                      # float -1.44269502
.LCPI1_1:
	.long	0x3f317200                      # float 0.693145751
.LCPI1_2:
	.long	0x35bfbe8e                      # float 1.42860677E-6
.LCPI1_3:
	.long	4294967169                      # 0xffffff81
.LCPI1_4:
	.long	128                             # 0x80
.LCPI1_5:
	.long	0xb9a797f3                      # float -3.19659332E-4
.LCPI1_6:
	.long	0xbc0b192a                      # float -0.00848988629
.LCPI1_7:
	.long	0xbe2aae1f                      # float -0.166679844
.LCPI1_8:
	.long	0xbf800000                      # float -1
.LCPI1_9:
	.long	0x3f800000                      # float 1
.LCPI1_10:
	.long	0x3a9c2e66                      # float 0.00119156833
.LCPI1_11:
	.long	0x3d2a66bc                      # float 0.0416018814
.LCPI1_12:
	.long	0x3effffde                      # float 0.499998987
	.section	.text.train_cost_model.par_for.squashed_head1_filter.s0.s.s.s,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.squashed_head1_filter.s0.s.s.s,@function
train_cost_model.par_for.squashed_head1_filter.s0.s.s.s: # @train_cost_model.par_for.squashed_head1_filter.s0.s.s.s
# %bb.0:                                # %entry
	pushq	%rbx
                                        # kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %rax
	movslq	4(%rdx), %r10
	movq	8(%rdx), %r8
	movq	24(%rdx), %r9
	cmpl	$11, %esi
	jg	.LBB1_5
# %bb.1:                                # %true_bb
	movl	%esi, %r11d
	sarl	$2, %r11d
	addl	%r11d, %r11d
	movslq	%r11d, %rdx
	leaq	(%rdx,%rdx,4), %rcx
	shlq	$6, %rcx
	movl	%esi, %edi
	andl	$3, %edi
	leaq	(%rdi,%rdi,4), %rbx
	shlq	$4, %rbx
	addq	%rcx, %rbx
	leaq	(%r9,%rbx,4), %rcx
	imulq	%rax, %rdi
	leaq	(%rdi,%rdi,4), %rdi
	imulq	%r10, %rdx
	leaq	(%rdx,%rdi,2), %rdx
	leaq	(%r8,%rdx,4), %rdi
	leaq	(,%rax,4), %rdx
	xorl	%ebx, %ebx
	vbroadcastss	.LCPI1_0(%rip), %ymm5   # ymm5 = [-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0]
	vbroadcastss	.LCPI1_1(%rip), %ymm10  # ymm10 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vbroadcastss	.LCPI1_2(%rip), %ymm0   # ymm0 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vmovups	%ymm0, -48(%rsp)                # 32-byte Spill
	vpbroadcastd	.LCPI1_3(%rip), %ymm14  # ymm14 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vpbroadcastd	.LCPI1_4(%rip), %ymm15  # ymm15 = [128,128,128,128,128,128,128,128]
	vbroadcastss	.LCPI1_5(%rip), %ymm0   # ymm0 = [-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4]
	vmovups	%ymm0, -80(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI1_6(%rip), %ymm0   # ymm0 = [-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3]
	vmovups	%ymm0, -112(%rsp)               # 32-byte Spill
	vbroadcastss	.LCPI1_7(%rip), %ymm7   # ymm7 = [-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1]
	vbroadcastss	.LCPI1_8(%rip), %ymm8   # ymm8 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vbroadcastss	.LCPI1_9(%rip), %ymm9   # ymm9 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI1_10(%rip), %ymm2  # ymm2 = [1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3]
	vbroadcastss	.LCPI1_11(%rip), %ymm11 # ymm11 = [4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2]
	vbroadcastss	.LCPI1_12(%rip), %ymm12 # ymm12 = [4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1]
	vpbroadcastd	.LCPI1_9(%rip), %ymm13  # ymm13 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	.p2align	4, 0x90
.LBB1_2:                                # %"for squashed_head1_filter.s0.s.si"
                                        # =>This Inner Loop Header: Depth=1
	vmovdqa	%ymm14, %ymm6
	vmovups	(%rdi), %ymm14
	vmovdqa	%ymm15, %ymm1
	vmulps	%ymm5, %ymm14, %ymm15
	vroundps	$1, %ymm15, %ymm15
	vfmadd231ps	%ymm10, %ymm15, %ymm14  # ymm14 = (ymm15 * ymm10) + ymm14
	vfmadd231ps	-48(%rsp), %ymm15, %ymm14 # 32-byte Folded Reload
                                        # ymm14 = (ymm15 * mem) + ymm14
	vmulps	%ymm14, %ymm14, %ymm0
	vmovaps	%ymm5, %ymm3
	vmovups	-80(%rsp), %ymm5                # 32-byte Reload
	vfmadd213ps	-112(%rsp), %ymm0, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm0 * ymm5) + mem
	vfmadd213ps	%ymm7, %ymm0, %ymm5     # ymm5 = (ymm0 * ymm5) + ymm7
	vfmadd213ps	%ymm8, %ymm0, %ymm5     # ymm5 = (ymm0 * ymm5) + ymm8
	vmovaps	%ymm10, %ymm4
	vmovaps	%ymm2, %ymm10
	vfmadd213ps	%ymm11, %ymm0, %ymm10   # ymm10 = (ymm0 * ymm10) + ymm11
	vfmadd213ps	%ymm12, %ymm0, %ymm10   # ymm10 = (ymm0 * ymm10) + ymm12
	vfmadd213ps	%ymm9, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm10) + ymm9
	vfmadd231ps	%ymm5, %ymm14, %ymm10   # ymm10 = (ymm14 * ymm5) + ymm10
	vmovdqa	%ymm6, %ymm14
	vcvttps2dq	%ymm15, %ymm0
	vmovdqa	%ymm1, %ymm15
	vpslld	$23, %ymm0, %ymm5
	vpaddd	%ymm5, %ymm13, %ymm5
	vfmadd213ps	%ymm9, %ymm10, %ymm5    # ymm5 = (ymm10 * ymm5) + ymm9
	vdivps	%ymm5, %ymm9, %ymm5
	vpcmpgtd	%ymm0, %ymm1, %ymm10
	vpand	%ymm5, %ymm10, %ymm5
	vmovaps	%ymm4, %ymm10
	vpcmpgtd	%ymm6, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm5, %ymm9, %ymm0
	vmovaps	%ymm3, %ymm5
	vmovaps	%ymm0, (%rcx,%rbx)
	addq	$32, %rbx
	addq	%rdx, %rdi
	cmpq	$320, %rbx                      # imm = 0x140
	jne	.LBB1_2
# %bb.3:                                # %"end for squashed_head1_filter.s0.s.si"
	orl	$1, %r11d
	movslq	%r11d, %rdi
	leaq	(%rdi,%rdi,4), %rcx
	shlq	$6, %rcx
	andl	$3, %esi
	leaq	(%rsi,%rsi,4), %rbx
	shlq	$4, %rbx
	addq	%rcx, %rbx
	leaq	(%r9,%rbx,4), %rcx
	imulq	%rsi, %rax
	leaq	(%rax,%rax,4), %rax
	imulq	%rdi, %r10
	leaq	(%r10,%rax,2), %rax
	leaq	(%r8,%rax,4), %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB1_4:                                # %"for squashed_head1_filter.s0.s.si.1"
                                        # =>This Inner Loop Header: Depth=1
	vmovups	(%rax), %ymm0
	vmulps	%ymm3, %ymm0, %ymm5
	vroundps	$1, %ymm5, %ymm5
	vfmadd231ps	%ymm10, %ymm5, %ymm0    # ymm0 = (ymm5 * ymm10) + ymm0
	vfmadd231ps	-48(%rsp), %ymm5, %ymm0 # 32-byte Folded Reload
                                        # ymm0 = (ymm5 * mem) + ymm0
	vmulps	%ymm0, %ymm0, %ymm10
	vmovups	-80(%rsp), %ymm14               # 32-byte Reload
	vfmadd213ps	-112(%rsp), %ymm10, %ymm14 # 32-byte Folded Reload
                                        # ymm14 = (ymm10 * ymm14) + mem
	vfmadd213ps	%ymm7, %ymm10, %ymm14   # ymm14 = (ymm10 * ymm14) + ymm7
	vfmadd213ps	%ymm8, %ymm10, %ymm14   # ymm14 = (ymm10 * ymm14) + ymm8
	vmovaps	%ymm2, %ymm15
	vfmadd213ps	%ymm11, %ymm10, %ymm15  # ymm15 = (ymm10 * ymm15) + ymm11
	vfmadd213ps	%ymm12, %ymm10, %ymm15  # ymm15 = (ymm10 * ymm15) + ymm12
	vfmadd213ps	%ymm9, %ymm10, %ymm15   # ymm15 = (ymm10 * ymm15) + ymm9
	vfmadd231ps	%ymm14, %ymm0, %ymm15   # ymm15 = (ymm0 * ymm14) + ymm15
	vmovdqa	%ymm6, %ymm14
	vcvttps2dq	%ymm5, %ymm0
	vpslld	$23, %ymm0, %ymm5
	vpaddd	%ymm5, %ymm13, %ymm5
	vfmadd213ps	%ymm9, %ymm15, %ymm5    # ymm5 = (ymm15 * ymm5) + ymm9
	vmovdqa	%ymm1, %ymm10
	vdivps	%ymm5, %ymm9, %ymm5
	vpcmpgtd	%ymm0, %ymm1, %ymm10
	vpand	%ymm5, %ymm10, %ymm5
	vmovaps	%ymm4, %ymm10
	vpcmpgtd	%ymm6, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm5, %ymm9, %ymm0
	vmovaps	%ymm0, (%rcx,%rsi)
	addq	$32, %rsi
	addq	%rdx, %rax
	cmpq	$320, %rsi                      # imm = 0x140
	jne	.LBB1_4
	jmp	.LBB1_7
.LBB1_5:                                # %false_bb
	addl	%r10d, %r10d
	leal	(%r10,%r10,2), %ecx
	movslq	%ecx, %rdx
	andl	$3, %esi
	leaq	(%rsi,%rsi,4), %rcx
	shlq	$6, %rcx
	addq	%r9, %rcx
	addq	$7680, %rcx                     # imm = 0x1E00
	imulq	%rax, %rsi
	leaq	(%rsi,%rsi,4), %rsi
	leaq	(%rdx,%rsi,2), %rdx
	leaq	(%r8,%rdx,4), %rdx
	shlq	$2, %rax
	xorl	%esi, %esi
	vbroadcastss	.LCPI1_0(%rip), %ymm0   # ymm0 = [-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0]
	vmovups	%ymm0, -48(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI1_1(%rip), %ymm0   # ymm0 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vmovups	%ymm0, -80(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI1_2(%rip), %ymm0   # ymm0 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vmovups	%ymm0, -112(%rsp)               # 32-byte Spill
	vpbroadcastd	.LCPI1_3(%rip), %ymm3   # ymm3 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vpbroadcastd	.LCPI1_4(%rip), %ymm4   # ymm4 = [128,128,128,128,128,128,128,128]
	vbroadcastss	.LCPI1_5(%rip), %ymm1   # ymm1 = [-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4]
	vbroadcastss	.LCPI1_6(%rip), %ymm6   # ymm6 = [-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3]
	vbroadcastss	.LCPI1_7(%rip), %ymm7   # ymm7 = [-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1]
	vbroadcastss	.LCPI1_8(%rip), %ymm8   # ymm8 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vbroadcastss	.LCPI1_9(%rip), %ymm9   # ymm9 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI1_10(%rip), %ymm2  # ymm2 = [1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3]
	vbroadcastss	.LCPI1_11(%rip), %ymm11 # ymm11 = [4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2]
	vbroadcastss	.LCPI1_12(%rip), %ymm12 # ymm12 = [4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1]
	vpbroadcastd	.LCPI1_9(%rip), %ymm13  # ymm13 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	.p2align	4, 0x90
.LBB1_6:                                # %"for squashed_head1_filter.s0.s.si1"
                                        # =>This Inner Loop Header: Depth=1
	vmovups	(%rdx), %ymm14
	vmulps	-48(%rsp), %ymm14, %ymm15       # 32-byte Folded Reload
	vroundps	$1, %ymm15, %ymm15
	vfmadd231ps	-80(%rsp), %ymm15, %ymm14 # 32-byte Folded Reload
                                        # ymm14 = (ymm15 * mem) + ymm14
	vfmadd231ps	-112(%rsp), %ymm15, %ymm14 # 32-byte Folded Reload
                                        # ymm14 = (ymm15 * mem) + ymm14
	vmulps	%ymm14, %ymm14, %ymm0
	vmovaps	%ymm1, %ymm5
	vfmadd213ps	%ymm6, %ymm0, %ymm5     # ymm5 = (ymm0 * ymm5) + ymm6
	vfmadd213ps	%ymm7, %ymm0, %ymm5     # ymm5 = (ymm0 * ymm5) + ymm7
	vfmadd213ps	%ymm8, %ymm0, %ymm5     # ymm5 = (ymm0 * ymm5) + ymm8
	vmovaps	%ymm2, %ymm10
	vfmadd213ps	%ymm11, %ymm0, %ymm10   # ymm10 = (ymm0 * ymm10) + ymm11
	vfmadd213ps	%ymm12, %ymm0, %ymm10   # ymm10 = (ymm0 * ymm10) + ymm12
	vfmadd213ps	%ymm9, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm10) + ymm9
	vfmadd231ps	%ymm5, %ymm14, %ymm10   # ymm10 = (ymm14 * ymm5) + ymm10
	vcvttps2dq	%ymm15, %ymm0
	vpslld	$23, %ymm0, %ymm5
	vpaddd	%ymm5, %ymm13, %ymm5
	vfmadd213ps	%ymm9, %ymm10, %ymm5    # ymm5 = (ymm10 * ymm5) + ymm9
	vdivps	%ymm5, %ymm9, %ymm5
	vpcmpgtd	%ymm0, %ymm4, %ymm10
	vpand	%ymm5, %ymm10, %ymm5
	vpcmpgtd	%ymm3, %ymm0, %ymm0
	vblendvps	%ymm0, %ymm5, %ymm9, %ymm0
	vmovaps	%ymm0, (%rcx,%rsi)
	addq	$32, %rsi
	addq	%rax, %rdx
	cmpq	$320, %rsi                      # imm = 0x140
	jne	.LBB1_6
.LBB1_7:                                # %destructor_block
	xorl	%eax, %eax
	popq	%rbx
	vzeroupper
	retq
.Lfunc_end1:
	.size	train_cost_model.par_for.squashed_head1_filter.s0.s.s.s, .Lfunc_end1-train_cost_model.par_for.squashed_head1_filter.s0.s.s.s
                                        # -- End function
	.section	.text.train_cost_model.par_for.head1_conv.s0.w,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.head1_conv.s0.w
	.type	train_cost_model.par_for.head1_conv.s0.w,@function
train_cost_model.par_for.head1_conv.s0.w: # @train_cost_model.par_for.head1_conv.s0.w
# %bb.0:                                # %entry
	movq	(%rdx), %rax
	movq	16(%rdx), %rcx
	vmovups	(%rax), %ymm0
	movslq	%esi, %rax
	shlq	$5, %rax
	vmovaps	%ymm0, (%rcx,%rax)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end2:
	.size	train_cost_model.par_for.head1_conv.s0.w, .Lfunc_end2-train_cost_model.par_for.head1_conv.s0.w
                                        # -- End function
	.section	.text.train_cost_model.par_for.head1_conv.s1.w,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.head1_conv.s1.w
	.type	train_cost_model.par_for.head1_conv.s1.w,@function
train_cost_model.par_for.head1_conv.s1.w: # @train_cost_model.par_for.head1_conv.s1.w
# %bb.0:                                # %entry
                                        # kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r11
	movq	16(%rdx), %r8
	movq	32(%rdx), %rdi
	leal	(,%rsi,8), %eax
	imull	4(%rdx), %esi
	movq	48(%rdx), %rcx
	subl	8(%rdx), %esi
	movslq	%eax, %r9
	vmovaps	(%r8,%r9,4), %ymm0
	movslq	%esi, %rsi
	leaq	(%rdi,%rsi,4), %rdx
	movl	$16, %eax
	.p2align	4, 0x90
.LBB3_1:                                # %"for head1_conv.s1.r31$x"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rdx,%rax), %ymm1
	vfmadd132ps	-128(%rcx,%rax,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rdx,%rax), %ymm0
	vfmadd132ps	-96(%rcx,%rax,8), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rdx,%rax), %ymm1
	vfmadd132ps	-64(%rcx,%rax,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rdx,%rax), %ymm2
	vfmadd132ps	-32(%rcx,%rax,8), %ymm1, %ymm2 # ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm0
	vfmadd132ps	(%rcx,%rax,8), %ymm2, %ymm0 # ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rax
	cmpq	$176, %rax
	jne	.LBB3_1
# %bb.2:                                # %"end for head1_conv.s1.r31$x"
	leaq	(%r11,%rsi), %rax
	leaq	(%rdi,%rax,4), %rdx
	movl	$16, %eax
	.p2align	4, 0x90
.LBB3_3:                                # %"for head1_conv.s1.r31$x.1"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rdx,%rax), %ymm1
	vfmadd132ps	1152(%rcx,%rax,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rdx,%rax), %ymm0
	vfmadd132ps	1184(%rcx,%rax,8), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rdx,%rax), %ymm1
	vfmadd132ps	1216(%rcx,%rax,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rdx,%rax), %ymm2
	vfmadd132ps	1248(%rcx,%rax,8), %ymm1, %ymm2 # ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm0
	vfmadd132ps	1280(%rcx,%rax,8), %ymm2, %ymm0 # ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rax
	cmpq	$176, %rax
	jne	.LBB3_3
# %bb.4:                                # %"end for head1_conv.s1.r31$x.1"
	leaq	(%rsi,%r11,2), %rax
	leaq	(%rdi,%rax,4), %rdx
	addq	$16, %rdx
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB3_5:                                # %"for head1_conv.s1.r31$x.2"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rdx,%rax), %ymm1
	vfmadd132ps	2560(%rcx,%rax,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rdx,%rax), %ymm0
	vfmadd132ps	2592(%rcx,%rax,8), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rdx,%rax), %ymm1
	vfmadd132ps	2624(%rcx,%rax,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rdx,%rax), %ymm2
	vfmadd132ps	2656(%rcx,%rax,8), %ymm1, %ymm2 # ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm0
	vfmadd132ps	2688(%rcx,%rax,8), %ymm2, %ymm0 # ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rax
	cmpq	$160, %rax
	jne	.LBB3_5
# %bb.6:                                # %"end for head1_conv.s1.r31$x.2"
	leaq	(%r11,%r11,2), %r10
	leaq	(%r10,%rsi), %rax
	leaq	(%rdi,%rax,4), %rdx
	addq	$16, %rdx
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB3_7:                                # %"for head1_conv.s1.r31$x.3"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rdx,%rax), %ymm1
	vfmadd132ps	3840(%rcx,%rax,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rdx,%rax), %ymm0
	vfmadd132ps	3872(%rcx,%rax,8), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rdx,%rax), %ymm1
	vfmadd132ps	3904(%rcx,%rax,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rdx,%rax), %ymm2
	vfmadd132ps	3936(%rcx,%rax,8), %ymm1, %ymm2 # ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm0
	vfmadd132ps	3968(%rcx,%rax,8), %ymm2, %ymm0 # ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rax
	cmpq	$160, %rax
	jne	.LBB3_7
# %bb.8:                                # %"end for head1_conv.s1.r31$x.3"
	leaq	(%rsi,%r11,4), %rax
	leaq	(%rdi,%rax,4), %rdx
	addq	$16, %rdx
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB3_9:                                # %"for head1_conv.s1.r31$x.4"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rdx,%rax), %ymm1
	vfmadd132ps	5120(%rcx,%rax,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rdx,%rax), %ymm0
	vfmadd132ps	5152(%rcx,%rax,8), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rdx,%rax), %ymm1
	vfmadd132ps	5184(%rcx,%rax,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rdx,%rax), %ymm2
	vfmadd132ps	5216(%rcx,%rax,8), %ymm1, %ymm2 # ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rdx,%rax), %ymm0
	vfmadd132ps	5248(%rcx,%rax,8), %ymm2, %ymm0 # ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rax
	cmpq	$160, %rax
	jne	.LBB3_9
# %bb.10:                               # %"end for head1_conv.s1.r31$x.4"
	leaq	(%r11,%r11,4), %rax
	addq	%rsi, %rax
	leaq	(%rdi,%rax,4), %rax
	addq	$16, %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB3_11:                               # %"for head1_conv.s1.r31$x.5"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rax,%rdx), %ymm1
	vfmadd132ps	6400(%rcx,%rdx,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rax,%rdx), %ymm0
	vfmadd132ps	6432(%rcx,%rdx,8), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rax,%rdx), %ymm1
	vfmadd132ps	6464(%rcx,%rdx,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rax,%rdx), %ymm2
	vfmadd132ps	6496(%rcx,%rdx,8), %ymm1, %ymm2 # ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rax,%rdx), %ymm0
	vfmadd132ps	6528(%rcx,%rdx,8), %ymm2, %ymm0 # ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rdx
	cmpq	$160, %rdx
	jne	.LBB3_11
# %bb.12:                               # %"end for head1_conv.s1.r31$x.5"
	leaq	(%rsi,%r10,2), %rax
	leaq	(%rdi,%rax,4), %rax
	addq	$16, %rax
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB3_13:                               # %"for head1_conv.s1.r31$x.6"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	-16(%rax,%rdx), %ymm1
	vfmadd132ps	7680(%rcx,%rdx,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-12(%rax,%rdx), %ymm0
	vfmadd132ps	7712(%rcx,%rdx,8), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	-8(%rax,%rdx), %ymm1
	vfmadd132ps	7744(%rcx,%rdx,8), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	-4(%rax,%rdx), %ymm2
	vfmadd132ps	7776(%rcx,%rdx,8), %ymm1, %ymm2 # ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rax,%rdx), %ymm0
	vfmadd132ps	7808(%rcx,%rdx,8), %ymm2, %ymm0 # ymm0 = (ymm0 * mem) + ymm2
	addq	$20, %rdx
	cmpq	$160, %rdx
	jne	.LBB3_13
# %bb.14:                               # %"end for head1_conv.s1.r31$x.6"
	vmovaps	%ymm0, (%r8,%r9,4)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end3:
	.size	train_cost_model.par_for.head1_conv.s1.w, .Lfunc_end3-train_cost_model.par_for.head1_conv.s1.w
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.normalized_schedule_features.s0.c.c.c
.LCPI4_0:
	.long	0x3f800000                      # float 1
.LCPI4_1:
	.long	2155872255                      # 0x807fffff
.LCPI4_2:
	.long	0xbf800000                      # float -1
.LCPI4_3:
	.long	4294967169                      # 0xffffff81
.LCPI4_4:
	.long	0x3f317218                      # float 0.693147182
.LCPI4_5:
	.long	0x3d9c7946                      # float 0.0764031857
.LCPI4_6:
	.long	0x3e5333c6                      # float 0.206252187
.LCPI4_7:
	.long	0x3eaa99cd                      # float 0.333204657
.LCPI4_8:
	.long	0xbe266e2a                      # float -0.162529618
.LCPI4_9:
	.long	0xbe809085                      # float -0.251102597
.LCPI4_10:
	.long	0xbefffcbe                      # float -0.499975145
	.section	.text.train_cost_model.par_for.normalized_schedule_features.s0.c.c.c,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.normalized_schedule_features.s0.c.c.c,@function
train_cost_model.par_for.normalized_schedule_features.s0.c.c.c: # @train_cost_model.par_for.normalized_schedule_features.s0.c.c.c
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$392, %rsp                      # imm = 0x188
	movl	%esi, %eax
	movl	(%rdx), %ebx
	movl	4(%rdx), %ebp
	movl	8(%rdx), %r8d
	movl	20(%rdx), %r13d
	movl	24(%rdx), %r9d
	movslq	32(%rdx), %r10
	movl	36(%rdx), %edi
	movq	40(%rdx), %rcx
	movq	%rcx, -72(%rsp)                 # 8-byte Spill
	movq	56(%rdx), %rcx
	movq	%rcx, -80(%rsp)                 # 8-byte Spill
	cmpl	%esi, 12(%rdx)
	movl	%ebx, -100(%rsp)                # 4-byte Spill
	movl	%ebp, -104(%rsp)                # 4-byte Spill
	movl	%r8d, -96(%rsp)                 # 4-byte Spill
	movl	%edi, -92(%rsp)                 # 4-byte Spill
	movq	%r10, -16(%rsp)                 # 8-byte Spill
	jle	.LBB4_11
# %bb.1:                                # %true_bb
	movl	16(%rdx), %r15d
	movl	%eax, %r12d
	andl	$7, %r12d
	leal	(%r12,%r12,4), %r11d
	movl	%r15d, %r14d
	sarl	$3, %r14d
	sarl	$3, %eax
	addl	$7, %r9d
	sarl	$3, %r9d
	cmpl	$34, %r11d
	movl	$34, %edx
	cmoval	%r11d, %edx
	movl	$39, %esi
	subl	%edx, %esi
	movq	%rsi, -64(%rsp)                 # 8-byte Spill
	movslq	%r14d, %rdx
	movq	%rdx, 24(%rsp)                  # 8-byte Spill
	movslq	%r9d, %rdx
	movq	%rdx, 352(%rsp)                 # 8-byte Spill
	movl	%edi, %esi
	subl	%r10d, %esi
	movl	%r15d, -48(%rsp)                # 4-byte Spill
	andl	$-8, %r15d
	subl	%r15d, %esi
	movl	%esi, 128(%rsp)                 # 4-byte Spill
	imull	$273, %eax, %esi                # imm = 0x111
	addl	%r11d, %esi
	imull	%ebx, %esi
	movl	%esi, %ecx
	subl	%r10d, %ecx
	imull	%r8d, %eax
	movl	%ebp, %edx
	leal	(,%rax,8), %ebp
	subl	%eax, %ebp
	imull	%edx, %r12d
	leal	(%r12,%r12,4), %r8d
	leal	(%r8,%rbp), %edx
	addl	%r10d, %ebp
	addl	%r8d, %ebp
	imull	$39, %ebx, %eax
	movl	%eax, -8(%rsp)                  # 4-byte Spill
	subl	%r13d, %edx
	addl	%r15d, %esi
	addl	%r15d, %ebp
	subl	%r13d, %ebp
	xorl	%eax, %eax
	vbroadcastss	.LCPI4_0(%rip), %ymm0   # ymm0 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI4_1(%rip), %ymm7   # ymm7 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vpbroadcastd	.LCPI4_0(%rip), %ymm3   # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI4_2(%rip), %ymm4   # ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vpbroadcastd	.LCPI4_3(%rip), %ymm5   # ymm5 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vbroadcastss	.LCPI4_4(%rip), %ymm12  # ymm12 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	vbroadcastss	.LCPI4_5(%rip), %ymm6   # ymm6 = [7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2]
	vbroadcastss	.LCPI4_6(%rip), %ymm2   # ymm2 = [2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1]
	vbroadcastss	.LCPI4_7(%rip), %ymm8   # ymm8 = [3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1]
	vbroadcastss	.LCPI4_8(%rip), %ymm9   # ymm9 = [-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1]
	vbroadcastss	.LCPI4_9(%rip), %ymm10  # ymm10 = [-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1]
	vbroadcastss	.LCPI4_10(%rip), %ymm11 # ymm11 = [-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1]
	vmovss	.LCPI4_0(%rip), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movq	%r9, -24(%rsp)                  # 8-byte Spill
	movq	%r14, -56(%rsp)                 # 8-byte Spill
	vmovups	%ymm7, 320(%rsp)                # 32-byte Spill
	vmovdqu	%ymm3, 288(%rsp)                # 32-byte Spill
	vmovups	%ymm4, 96(%rsp)                 # 32-byte Spill
	vmovdqu	%ymm5, 64(%rsp)                 # 32-byte Spill
	vmovups	%ymm0, 32(%rsp)                 # 32-byte Spill
	vmovups	%ymm6, 256(%rsp)                # 32-byte Spill
	vmovups	%ymm8, 224(%rsp)                # 32-byte Spill
	vmovups	%ymm2, 192(%rsp)                # 32-byte Spill
	vmovups	%ymm11, 160(%rsp)               # 32-byte Spill
	jmp	.LBB4_2
	.p2align	4, 0x90
.LBB4_24:                               # %"end for normalized_schedule_features.s0.c.ci"
                                        #   in Loop: Header=BB4_2 Depth=1
	movq	8(%rsp), %rax                   # 8-byte Reload
	incl	%eax
	movl	(%rsp), %ecx                    # 4-byte Reload
	movl	-8(%rsp), %ebp                  # 4-byte Reload
	addl	%ebp, %ecx
	movl	-96(%rsp), %r8d                 # 4-byte Reload
	movl	-4(%rsp), %edx                  # 4-byte Reload
	addl	%r8d, %edx
	movl	4(%rsp), %esi                   # 4-byte Reload
	addl	%ebp, %esi
	movq	16(%rsp), %rbp                  # 8-byte Reload
	addl	%r8d, %ebp
	cmpl	$7, %eax
	je	.LBB4_27
.LBB4_2:                                # %"for normalized_schedule_features.s0.s.si"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB4_3 Depth 2
                                        #       Child Loop BB4_5 Depth 3
                                        #       Child Loop BB4_8 Depth 3
                                        #         Child Loop BB4_19 Depth 4
                                        #         Child Loop BB4_21 Depth 4
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	%rbp, 16(%rsp)                  # 8-byte Spill
	movl	%ebp, %eax
	movl	%ebp, -88(%rsp)                 # 4-byte Spill
	movl	%esi, 4(%rsp)                   # 4-byte Spill
	movl	%esi, -112(%rsp)                # 4-byte Spill
	movl	%edx, -4(%rsp)                  # 4-byte Spill
	movl	%edx, %ebx
	movl	%ecx, (%rsp)                    # 4-byte Spill
	movl	%ecx, %r8d
	xorl	%edx, %edx
	jmp	.LBB4_3
	.p2align	4, 0x90
.LBB4_23:                               # %"end for normalized_schedule_features.s0.n.n2"
                                        #   in Loop: Header=BB4_3 Depth=2
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movl	-100(%rsp), %eax                # 4-byte Reload
	movl	-32(%rsp), %r8d                 # 4-byte Reload
	addl	%eax, %r8d
	movl	-104(%rsp), %ecx                # 4-byte Reload
	movl	-108(%rsp), %ebx                # 4-byte Reload
	addl	%ecx, %ebx
	addl	%eax, -112(%rsp)                # 4-byte Folded Spill
	addl	%ecx, -88(%rsp)                 # 4-byte Folded Spill
	cmpq	-64(%rsp), %rdx                 # 8-byte Folded Reload
	movq	-24(%rsp), %r9                  # 8-byte Reload
	movq	-56(%rsp), %r14                 # 8-byte Reload
	vmovups	320(%rsp), %ymm7                # 32-byte Reload
	vmovdqu	288(%rsp), %ymm3                # 32-byte Reload
	vmovups	96(%rsp), %ymm4                 # 32-byte Reload
	vmovdqu	64(%rsp), %ymm5                 # 32-byte Reload
	vmovups	32(%rsp), %ymm0                 # 32-byte Reload
	vmovups	256(%rsp), %ymm6                # 32-byte Reload
	vmovups	224(%rsp), %ymm8                # 32-byte Reload
	vmovups	192(%rsp), %ymm2                # 32-byte Reload
	je	.LBB4_24
.LBB4_3:                                # %"for normalized_schedule_features.s0.c.ci"
                                        #   Parent Loop BB4_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB4_5 Depth 3
                                        #       Child Loop BB4_8 Depth 3
                                        #         Child Loop BB4_19 Depth 4
                                        #         Child Loop BB4_21 Depth 4
	movq	%rdx, -40(%rsp)                 # 8-byte Spill
	cmpl	$8, -48(%rsp)                   # 4-byte Folded Reload
	vmovaps	%ymm9, %ymm11
	jl	.LBB4_6
# %bb.4:                                # %"for normalized_schedule_features.s0.n.n.preheader"
                                        #   in Loop: Header=BB4_3 Depth=2
	vmovaps	%ymm7, %ymm9
	movslq	%r8d, %rcx
	addq	%r10, %rcx
	movslq	%ebx, %rdx
	addq	%r10, %rdx
	movq	-72(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %rcx
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %rdx
	xorl	%esi, %esi
	movq	%r14, %rbp
	.p2align	4, 0x90
.LBB4_5:                                # %"for normalized_schedule_features.s0.n.n"
                                        #   Parent Loop BB4_2 Depth=1
                                        #     Parent Loop BB4_3 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovaps	%ymm12, %ymm7
	vaddps	(%rdx,%rsi), %ymm0, %ymm14
	vandps	%ymm9, %ymm14, %ymm13
	vpsrad	$22, %ymm13, %ymm15
	vpslld	$23, %ymm15, %ymm12
	vpsubd	%ymm12, %ymm13, %ymm12
	vpaddd	%ymm3, %ymm12, %ymm12
	vaddps	%ymm4, %ymm12, %ymm12
	vmulps	%ymm12, %ymm12, %ymm13
	vpsrad	$23, %ymm14, %ymm14
	vpaddd	%ymm5, %ymm14, %ymm14
	vpaddd	%ymm14, %ymm15, %ymm14
	vcvtdq2ps	%ymm14, %ymm14
	vmovaps	%ymm6, %ymm15
	vfmadd213ps	%ymm2, %ymm13, %ymm15   # ymm15 = (ymm13 * ymm15) + ymm2
	vfmadd213ps	%ymm8, %ymm13, %ymm15   # ymm15 = (ymm13 * ymm15) + ymm8
	vfmadd213ps	%ymm0, %ymm13, %ymm15   # ymm15 = (ymm13 * ymm15) + ymm0
	vmulps	%ymm15, %ymm12, %ymm12
	vmovaps	%ymm11, %ymm15
	vfmadd213ps	%ymm10, %ymm13, %ymm15  # ymm15 = (ymm13 * ymm15) + ymm10
	vfmadd213ps	160(%rsp), %ymm13, %ymm15 # 32-byte Folded Reload
                                        # ymm15 = (ymm13 * ymm15) + mem
	vfmadd213ps	%ymm12, %ymm13, %ymm15  # ymm15 = (ymm13 * ymm15) + ymm12
	vfmadd231ps	%ymm14, %ymm7, %ymm15   # ymm15 = (ymm7 * ymm14) + ymm15
	vmovaps	%ymm7, %ymm12
	vmovups	%ymm15, (%rcx,%rsi)
	addq	$32, %rsi
	decq	%rbp
	jne	.LBB4_5
.LBB4_6:                                # %"end for normalized_schedule_features.s0.n.n"
                                        #   in Loop: Header=BB4_3 Depth=2
	movl	%r8d, -32(%rsp)                 # 4-byte Spill
	movl	%ebx, -108(%rsp)                # 4-byte Spill
	cmpl	%r9d, %r14d
	vmovss	.LCPI4_2(%rip), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vmovss	.LCPI4_5(%rip), %xmm14          # xmm14 = mem[0],zero,zero,zero
	vmovss	.LCPI4_6(%rip), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	.LCPI4_7(%rip), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vmovss	.LCPI4_8(%rip), %xmm15          # xmm15 = mem[0],zero,zero,zero
	vmovaps	%ymm11, %ymm9
	vmovups	160(%rsp), %ymm11               # 32-byte Reload
	jge	.LBB4_23
# %bb.7:                                # %"for normalized_schedule_features.s0.n.n1.preheader"
                                        #   in Loop: Header=BB4_3 Depth=2
	xorl	%ebp, %ebp
	movl	128(%rsp), %r14d                # 4-byte Reload
	movl	%r14d, %esi
	movl	-88(%rsp), %r8d                 # 4-byte Reload
	movl	-112(%rsp), %r15d               # 4-byte Reload
	movq	24(%rsp), %r13                  # 8-byte Reload
	jmp	.LBB4_8
	.p2align	4, 0x90
.LBB4_22:                               # %"end for normalized_schedule_features.s0.n.ni"
                                        #   in Loop: Header=BB4_8 Depth=3
	incq	%r13
	incl	%ebp
	addl	$-8, %r14d
	addl	$8, %r15d
	addl	$8, %r8d
	addl	$-8, %esi
	cmpq	352(%rsp), %r13                 # 8-byte Folded Reload
	movl	-92(%rsp), %edi                 # 4-byte Reload
	je	.LBB4_23
.LBB4_8:                                # %"for normalized_schedule_features.s0.n.n1"
                                        #   Parent Loop BB4_2 Depth=1
                                        #     Parent Loop BB4_3 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB4_19 Depth 4
                                        #         Child Loop BB4_21 Depth 4
	cmpl	$9, %esi
	movl	$8, %r12d
	cmovll	%esi, %r12d
	cmpl	$9, %r14d
	movl	$8, %ecx
	cmovll	%r14d, %ecx
	leal	(,%rbp,8), %edx
	movl	128(%rsp), %eax                 # 4-byte Reload
	movl	%eax, %ebx
	subl	%edx, %ebx
	cmpl	$9, %ebx
	movl	$8, %eax
	cmovgel	%eax, %ebx
	leal	(%r10,%r13,8), %edx
	cmpl	%edx, %edi
	jle	.LBB4_22
# %bb.9:                                # %"for normalized_schedule_features.s0.n.ni.preheader"
                                        #   in Loop: Header=BB4_8 Depth=3
	movslq	%r15d, %rdx
	movq	-72(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %r9
	movslq	%r8d, %rdx
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %r11
	cmpl	$7, %ebx
	ja	.LBB4_18
# %bb.10:                               #   in Loop: Header=BB4_8 Depth=3
	xorl	%edx, %edx
	vmovss	.LCPI4_9(%rip), %xmm13          # xmm13 = mem[0],zero,zero,zero
	vmovss	.LCPI4_10(%rip), %xmm8          # xmm8 = mem[0],zero,zero,zero
	jmp	.LBB4_21
	.p2align	4, 0x90
.LBB4_18:                               # %vector.ph63
                                        #   in Loop: Header=BB4_8 Depth=3
	andl	$-8, %ecx
	movl	%ebx, %edx
	andl	$-8, %edx
	xorl	%r10d, %r10d
	vmovups	320(%rsp), %ymm2                # 32-byte Reload
	vmovdqu	288(%rsp), %ymm3                # 32-byte Reload
	vmovups	96(%rsp), %ymm4                 # 32-byte Reload
	vmovdqu	64(%rsp), %ymm5                 # 32-byte Reload
	vmovaps	%ymm9, %ymm1
	vmovaps	%ymm11, %ymm9
	vmovaps	%ymm10, %ymm11
	vmovups	256(%rsp), %ymm10               # 32-byte Reload
	vmovups	224(%rsp), %ymm6                # 32-byte Reload
	vmovups	192(%rsp), %ymm8                # 32-byte Reload
	vmovaps	%ymm12, %ymm7
	vmovups	32(%rsp), %ymm0                 # 32-byte Reload
	.p2align	4, 0x90
.LBB4_19:                               # %vector.body60
                                        #   Parent Loop BB4_2 Depth=1
                                        #     Parent Loop BB4_3 Depth=2
                                        #       Parent Loop BB4_8 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vaddps	(%r11,%r10,4), %ymm0, %ymm12
	vandps	%ymm2, %ymm12, %ymm13
	vpsrad	$22, %ymm13, %ymm14
	vpslld	$23, %ymm14, %ymm15
	vpsubd	%ymm15, %ymm13, %ymm13
	vpaddd	%ymm3, %ymm13, %ymm13
	vaddps	%ymm4, %ymm13, %ymm13
	vmulps	%ymm13, %ymm13, %ymm15
	vpsrad	$23, %ymm12, %ymm12
	vpaddd	%ymm5, %ymm12, %ymm12
	vpaddd	%ymm12, %ymm14, %ymm12
	vcvtdq2ps	%ymm12, %ymm12
	vmovaps	%ymm10, %ymm14
	vfmadd213ps	%ymm8, %ymm15, %ymm14   # ymm14 = (ymm15 * ymm14) + ymm8
	vfmadd213ps	%ymm6, %ymm15, %ymm14   # ymm14 = (ymm15 * ymm14) + ymm6
	vfmadd213ps	%ymm0, %ymm15, %ymm14   # ymm14 = (ymm15 * ymm14) + ymm0
	vmulps	%ymm14, %ymm13, %ymm13
	vmovaps	%ymm1, %ymm14
	vfmadd213ps	%ymm11, %ymm15, %ymm14  # ymm14 = (ymm15 * ymm14) + ymm11
	vfmadd213ps	%ymm9, %ymm15, %ymm14   # ymm14 = (ymm15 * ymm14) + ymm9
	vfmadd213ps	%ymm13, %ymm15, %ymm14  # ymm14 = (ymm15 * ymm14) + ymm13
	vfmadd231ps	%ymm12, %ymm7, %ymm14   # ymm14 = (ymm7 * ymm12) + ymm14
	vmovups	%ymm14, (%r9,%r10,4)
	addq	$8, %r10
	cmpq	%r10, %rcx
	jne	.LBB4_19
# %bb.20:                               # %middle.block58
                                        #   in Loop: Header=BB4_8 Depth=3
	cmpq	%rbx, %rdx
	movq	-16(%rsp), %r10                 # 8-byte Reload
	vmovaps	%ymm7, %ymm12
	vmovaps	%ymm11, %ymm10
	vmovaps	%ymm9, %ymm11
	vmovaps	%ymm1, %ymm9
	vmovss	.LCPI4_0(%rip), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI4_2(%rip), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vmovss	.LCPI4_5(%rip), %xmm14          # xmm14 = mem[0],zero,zero,zero
	vmovss	.LCPI4_6(%rip), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	.LCPI4_7(%rip), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vmovss	.LCPI4_8(%rip), %xmm15          # xmm15 = mem[0],zero,zero,zero
	vmovss	.LCPI4_9(%rip), %xmm13          # xmm13 = mem[0],zero,zero,zero
	vmovss	.LCPI4_10(%rip), %xmm8          # xmm8 = mem[0],zero,zero,zero
	je	.LBB4_22
	.p2align	4, 0x90
.LBB4_21:                               # %"for normalized_schedule_features.s0.n.ni"
                                        #   Parent Loop BB4_2 Depth=1
                                        #     Parent Loop BB4_3 Depth=2
                                        #       Parent Loop BB4_8 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vaddss	(%r11,%rdx,4), %xmm1, %xmm2
	vmovd	%xmm2, %ecx
	movl	%ecx, %eax
	andl	$-2139095041, %eax              # imm = 0x807FFFFF
	movl	%eax, %ebx
	sarl	$22, %ebx
	sarl	$23, %ecx
	addl	%ebx, %ecx
	addl	$-127, %ecx
                                        # kill: def $ebx killed $ebx killed $rbx def $rbx
	shll	$23, %ebx
	negl	%ebx
	addl	%ebx, %eax
	addl	$1065353216, %eax               # imm = 0x3F800000
	vmovd	%eax, %xmm2
	vaddss	%xmm7, %xmm2, %xmm2
	vmulss	%xmm2, %xmm2, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	vcvtsi2ss	%ecx, %xmm4, %xmm4
	vmovaps	%xmm14, %xmm5
	vfmadd213ss	%xmm0, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm5) + xmm0
	vfmadd213ss	%xmm6, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm5) + xmm6
	vfmadd213ss	%xmm1, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm5) + xmm1
	vmulss	%xmm5, %xmm2, %xmm2
	vmovaps	%xmm15, %xmm5
	vfmadd213ss	%xmm13, %xmm3, %xmm5    # xmm5 = (xmm3 * xmm5) + xmm13
	vfmadd213ss	%xmm8, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm5) + xmm8
	vfmadd213ss	%xmm2, %xmm3, %xmm5     # xmm5 = (xmm3 * xmm5) + xmm2
	vmovss	.LCPI4_4(%rip), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vfmadd231ss	%xmm2, %xmm4, %xmm5     # xmm5 = (xmm4 * xmm2) + xmm5
	vmovss	%xmm5, (%r9,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %r12
	jne	.LBB4_21
	jmp	.LBB4_22
.LBB4_11:                               # %false_bb
	movl	28(%rdx), %edx
	movl	%eax, %r14d
	sarl	$3, %r14d
	movl	%eax, %ecx
	andl	$-8, %ecx
	subl	%r14d, %ecx
	movq	%rcx, %rsi
	movq	%rcx, -48(%rsp)                 # 8-byte Spill
	subl	%ecx, %edx
	cmpl	$8, %edx
	movl	$7, %ecx
	cmovll	%edx, %ecx
	movl	%ecx, -56(%rsp)                 # 4-byte Spill
	testl	%edx, %edx
	jle	.LBB4_27
# %bb.12:                               # %"for normalized_schedule_features.s0.s.si5.preheader"
	andl	$7, %eax
	leal	(%rax,%rax,4), %r15d
	cmpl	$34, %r15d
	movl	$34, %edx
	cmoval	%r15d, %edx
	movl	$39, %ecx
	subl	%edx, %ecx
	movq	%rcx, -88(%rsp)                 # 8-byte Spill
	leal	7(%r9), %edx
	sarl	$3, %edx
	imull	$39, %ebx, %r12d
	movl	%edi, %r11d
	subl	%r10d, %r11d
	movq	%rax, %rsi
	imull	$273, %r14d, %eax               # imm = 0x111
	movq	%r15, %rcx
	movq	%r15, 192(%rsp)                 # 8-byte Spill
	addl	%r15d, %eax
	imull	%ebx, %eax
	imull	%r8d, %r14d
	leal	(,%r14,8), %ecx
	subl	%r14d, %ecx
	addl	%r10d, %ecx
	imull	%ebp, %esi
	leal	(%rsi,%rsi,4), %ebp
	addl	%ecx, %ebp
	subl	%r13d, %ebp
	xorl	%esi, %esi
	vmovss	.LCPI4_0(%rip), %xmm10          # xmm10 = mem[0],zero,zero,zero
	vmovss	.LCPI4_2(%rip), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vmovss	.LCPI4_5(%rip), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI4_6(%rip), %xmm5           # xmm5 = mem[0],zero,zero,zero
	vmovss	.LCPI4_7(%rip), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	.LCPI4_8(%rip), %xmm8           # xmm8 = mem[0],zero,zero,zero
	vmovss	.LCPI4_9(%rip), %xmm15          # xmm15 = mem[0],zero,zero,zero
	vmovss	.LCPI4_10(%rip), %xmm11         # xmm11 = mem[0],zero,zero,zero
	vmovss	.LCPI4_4(%rip), %xmm12          # xmm12 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI4_0(%rip), %ymm9   # ymm9 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI4_1(%rip), %ymm3   # ymm3 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vmovups	%ymm3, 160(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI4_0(%rip), %ymm3   # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmovups	%ymm3, 128(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI4_2(%rip), %ymm3   # ymm3 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vmovups	%ymm3, 352(%rsp)                # 32-byte Spill
	vpbroadcastd	.LCPI4_3(%rip), %ymm13  # ymm13 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vbroadcastss	.LCPI4_4(%rip), %ymm14  # ymm14 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	movq	%r9, -24(%rsp)                  # 8-byte Spill
	movl	%r13d, -40(%rsp)                # 4-byte Spill
	movl	%r12d, -64(%rsp)                # 4-byte Spill
	jmp	.LBB4_13
	.p2align	4, 0x90
.LBB4_26:                               # %"end for normalized_schedule_features.s0.c.ci9"
                                        #   in Loop: Header=BB4_13 Depth=1
	movq	-32(%rsp), %rsi                 # 8-byte Reload
	incl	%esi
	movl	-64(%rsp), %r12d                # 4-byte Reload
	movl	-112(%rsp), %eax                # 4-byte Reload
	addl	%r12d, %eax
	movl	-96(%rsp), %r8d                 # 4-byte Reload
	movl	-108(%rsp), %ebp                # 4-byte Reload
	addl	%r8d, %ebp
	cmpl	-56(%rsp), %esi                 # 4-byte Folded Reload
	movq	-16(%rsp), %r10                 # 8-byte Reload
	movl	-40(%rsp), %r13d                # 4-byte Reload
	je	.LBB4_27
.LBB4_13:                               # %"for normalized_schedule_features.s0.s.si5"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB4_14 Depth 2
                                        #       Child Loop BB4_16 Depth 3
                                        #         Child Loop BB4_33 Depth 4
                                        #         Child Loop BB4_31 Depth 4
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	movq	%rsi, -32(%rsp)                 # 8-byte Spill
	addl	%esi, %ecx
	imull	%ecx, %r12d
	subl	%r10d, %r12d
	movl	%r12d, 224(%rsp)                # 4-byte Spill
	imull	%r8d, %ecx
	subl	%r13d, %ecx
	movl	%ecx, 256(%rsp)                 # 4-byte Spill
	movl	%ebp, -108(%rsp)                # 4-byte Spill
	movl	%ebp, 96(%rsp)                  # 4-byte Spill
	movl	%eax, -112(%rsp)                # 4-byte Spill
	movl	%eax, 64(%rsp)                  # 4-byte Spill
	xorl	%eax, %eax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	jmp	.LBB4_14
	.p2align	4, 0x90
.LBB4_25:                               # %"end for normalized_schedule_features.s0.n.n12"
                                        #   in Loop: Header=BB4_14 Depth=2
	movq	32(%rsp), %rcx                  # 8-byte Reload
	incq	%rcx
	movl	64(%rsp), %eax                  # 4-byte Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	movl	%eax, 64(%rsp)                  # 4-byte Spill
	movl	96(%rsp), %eax                  # 4-byte Reload
	addl	-104(%rsp), %eax                # 4-byte Folded Reload
	movl	%eax, 96(%rsp)                  # 4-byte Spill
	movq	%rcx, %rax
	movq	%rcx, 32(%rsp)                  # 8-byte Spill
	cmpq	-88(%rsp), %rcx                 # 8-byte Folded Reload
	movq	-24(%rsp), %r9                  # 8-byte Reload
	je	.LBB4_26
.LBB4_14:                               # %"for normalized_schedule_features.s0.c.ci8"
                                        #   Parent Loop BB4_13 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB4_16 Depth 3
                                        #         Child Loop BB4_33 Depth 4
                                        #         Child Loop BB4_31 Depth 4
	testl	%r9d, %r9d
	jle	.LBB4_25
# %bb.15:                               # %"for normalized_schedule_features.s0.n.n11.preheader"
                                        #   in Loop: Header=BB4_14 Depth=2
	movq	192(%rsp), %rax                 # 8-byte Reload
	movq	32(%rsp), %rcx                  # 8-byte Reload
	addl	%ecx, %eax
	movl	%eax, %ecx
	imull	-104(%rsp), %ecx                # 4-byte Folded Reload
	addl	256(%rsp), %ecx                 # 4-byte Folded Reload
	imull	-100(%rsp), %eax                # 4-byte Folded Reload
	addl	224(%rsp), %eax                 # 4-byte Folded Reload
	movslq	%ecx, %rcx
	movq	%rcx, 320(%rsp)                 # 8-byte Spill
	cltq
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	movl	96(%rsp), %r8d                  # 4-byte Reload
	movl	64(%rsp), %r14d                 # 4-byte Reload
	movl	%r11d, %esi
	xorl	%ebp, %ebp
	jmp	.LBB4_16
	.p2align	4, 0x90
.LBB4_17:                               # %true_bb15
                                        #   in Loop: Header=BB4_16 Depth=3
	movslq	%ecx, %rcx
	movq	320(%rsp), %rax                 # 8-byte Reload
	addq	%rcx, %rax
	movq	-80(%rsp), %rdi                 # 8-byte Reload
	vaddps	(%rdi,%rax,4), %ymm9, %ymm15
	vandps	160(%rsp), %ymm15, %ymm8        # 32-byte Folded Reload
	vpsrad	$22, %ymm8, %ymm3
	vpslld	$23, %ymm3, %ymm4
	vpsubd	%ymm4, %ymm8, %ymm4
	vpaddd	128(%rsp), %ymm4, %ymm4         # 32-byte Folded Reload
	vaddps	352(%rsp), %ymm4, %ymm4         # 32-byte Folded Reload
	vmulps	%ymm4, %ymm4, %ymm8
	vpsrad	$23, %ymm15, %ymm15
	vpaddd	%ymm13, %ymm15, %ymm15
	vpaddd	%ymm3, %ymm15, %ymm3
	vcvtdq2ps	%ymm3, %ymm3
	vbroadcastss	.LCPI4_5(%rip), %ymm15  # ymm15 = [7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2]
	vbroadcastss	.LCPI4_6(%rip), %ymm2   # ymm2 = [2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1]
	vfmadd231ps	%ymm15, %ymm8, %ymm2    # ymm2 = (ymm8 * ymm15) + ymm2
	vbroadcastss	.LCPI4_7(%rip), %ymm15  # ymm15 = [3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1]
	vfmadd231ps	%ymm2, %ymm8, %ymm15    # ymm15 = (ymm8 * ymm2) + ymm15
	vfmadd213ps	%ymm9, %ymm8, %ymm15    # ymm15 = (ymm8 * ymm15) + ymm9
	vmulps	%ymm4, %ymm15, %ymm2
	vbroadcastss	.LCPI4_8(%rip), %ymm4   # ymm4 = [-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1]
	vbroadcastss	.LCPI4_9(%rip), %ymm15  # ymm15 = [-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1]
	vfmadd231ps	%ymm4, %ymm8, %ymm15    # ymm15 = (ymm8 * ymm4) + ymm15
	vbroadcastss	.LCPI4_10(%rip), %ymm4  # ymm4 = [-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1]
	vfmadd231ps	%ymm15, %ymm8, %ymm4    # ymm4 = (ymm8 * ymm15) + ymm4
	vmovss	.LCPI4_9(%rip), %xmm15          # xmm15 = mem[0],zero,zero,zero
	vfmadd213ps	%ymm2, %ymm8, %ymm4     # ymm4 = (ymm8 * ymm4) + ymm2
	vmovss	.LCPI4_8(%rip), %xmm8           # xmm8 = mem[0],zero,zero,zero
	vmovss	.LCPI4_7(%rip), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	.LCPI4_6(%rip), %xmm5           # xmm5 = mem[0],zero,zero,zero
	vmovss	.LCPI4_5(%rip), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI4_2(%rip), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vfmadd231ps	%ymm3, %ymm14, %ymm4    # ymm4 = (ymm14 * ymm3) + ymm4
	addq	288(%rsp), %rcx                 # 8-byte Folded Reload
	movq	-72(%rsp), %rax                 # 8-byte Reload
	vmovups	%ymm4, (%rax,%rcx,4)
.LBB4_35:                               # %after_bb17
                                        #   in Loop: Header=BB4_16 Depth=3
	incq	%rbp
	addl	$-8, %esi
	addl	$8, %r14d
	addl	$8, %r8d
	cmpq	%rdx, %rbp
	movl	-92(%rsp), %edi                 # 4-byte Reload
	je	.LBB4_25
.LBB4_16:                               # %"for normalized_schedule_features.s0.n.n11"
                                        #   Parent Loop BB4_13 Depth=1
                                        #     Parent Loop BB4_14 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB4_33 Depth 4
                                        #         Child Loop BB4_31 Depth 4
	cmpl	$9, %esi
	movl	$8, %r10d
	cmovll	%esi, %r10d
	leal	(,%rbp,8), %eax
	movl	%r11d, %r13d
	subl	%eax, %r13d
	cmpl	$9, %r13d
	movl	$8, %eax
	cmovgel	%eax, %r13d
	movq	-16(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%rbp,8), %ecx
	leal	(%rax,%rbp,8), %eax
	addl	$8, %eax
	cmpl	%edi, %eax
	jle	.LBB4_17
# %bb.28:                               # %false_bb16
                                        #   in Loop: Header=BB4_16 Depth=3
	cmpl	%ecx, %edi
	jle	.LBB4_35
# %bb.29:                               # %"for normalized_schedule_features.s0.n.ni18.preheader"
                                        #   in Loop: Header=BB4_16 Depth=3
	movslq	%r14d, %rax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %r12
	movslq	%r8d, %rax
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %r15
	cmpl	$7, %r13d
	ja	.LBB4_32
# %bb.30:                               #   in Loop: Header=BB4_16 Depth=3
	xorl	%r9d, %r9d
	jmp	.LBB4_31
	.p2align	4, 0x90
.LBB4_32:                               # %vector.ph
                                        #   in Loop: Header=BB4_16 Depth=3
	movl	%r10d, %ecx
	andl	$-8, %ecx
	movl	%r13d, %r9d
	andl	$-8, %r9d
	xorl	%ebx, %ebx
	vmovups	160(%rsp), %ymm0                # 32-byte Reload
	vmovdqu	128(%rsp), %ymm1                # 32-byte Reload
	vmovups	352(%rsp), %ymm5                # 32-byte Reload
	.p2align	4, 0x90
.LBB4_33:                               # %vector.body
                                        #   Parent Loop BB4_13 Depth=1
                                        #     Parent Loop BB4_14 Depth=2
                                        #       Parent Loop BB4_16 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vaddps	(%r15,%rbx,4), %ymm9, %ymm2
	vandps	%ymm0, %ymm2, %ymm3
	vpsrad	$22, %ymm3, %ymm4
	vpslld	$23, %ymm4, %ymm8
	vpsubd	%ymm8, %ymm3, %ymm3
	vpaddd	%ymm1, %ymm3, %ymm3
	vaddps	%ymm5, %ymm3, %ymm3
	vmulps	%ymm3, %ymm3, %ymm8
	vpsrad	$23, %ymm2, %ymm2
	vpaddd	%ymm2, %ymm13, %ymm2
	vpaddd	%ymm2, %ymm4, %ymm2
	vcvtdq2ps	%ymm2, %ymm2
	vbroadcastss	.LCPI4_5(%rip), %ymm4   # ymm4 = [7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2]
	vbroadcastss	.LCPI4_6(%rip), %ymm15  # ymm15 = [2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1]
	vfmadd231ps	%ymm4, %ymm8, %ymm15    # ymm15 = (ymm8 * ymm4) + ymm15
	vbroadcastss	.LCPI4_7(%rip), %ymm4   # ymm4 = [3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1]
	vfmadd231ps	%ymm15, %ymm8, %ymm4    # ymm4 = (ymm8 * ymm15) + ymm4
	vfmadd213ps	%ymm9, %ymm8, %ymm4     # ymm4 = (ymm8 * ymm4) + ymm9
	vmulps	%ymm4, %ymm3, %ymm3
	vbroadcastss	.LCPI4_8(%rip), %ymm4   # ymm4 = [-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1,-1.62529618E-1]
	vbroadcastss	.LCPI4_9(%rip), %ymm15  # ymm15 = [-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1]
	vfmadd231ps	%ymm4, %ymm8, %ymm15    # ymm15 = (ymm8 * ymm4) + ymm15
	vbroadcastss	.LCPI4_10(%rip), %ymm4  # ymm4 = [-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1]
	vfmadd231ps	%ymm15, %ymm8, %ymm4    # ymm4 = (ymm8 * ymm15) + ymm4
	vfmadd213ps	%ymm3, %ymm8, %ymm4     # ymm4 = (ymm8 * ymm4) + ymm3
	vfmadd231ps	%ymm2, %ymm14, %ymm4    # ymm4 = (ymm14 * ymm2) + ymm4
	vmovups	%ymm4, (%r12,%rbx,4)
	addq	$8, %rbx
	cmpq	%rbx, %rcx
	jne	.LBB4_33
# %bb.34:                               # %middle.block
                                        #   in Loop: Header=BB4_16 Depth=3
	cmpq	%r13, %r9
	vmovss	.LCPI4_2(%rip), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vmovss	.LCPI4_5(%rip), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI4_6(%rip), %xmm5           # xmm5 = mem[0],zero,zero,zero
	vmovss	.LCPI4_7(%rip), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vmovss	.LCPI4_8(%rip), %xmm8           # xmm8 = mem[0],zero,zero,zero
	vmovss	.LCPI4_9(%rip), %xmm15          # xmm15 = mem[0],zero,zero,zero
	je	.LBB4_35
	.p2align	4, 0x90
.LBB4_31:                               # %"for normalized_schedule_features.s0.n.ni18"
                                        #   Parent Loop BB4_13 Depth=1
                                        #     Parent Loop BB4_14 Depth=2
                                        #       Parent Loop BB4_16 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vaddss	(%r15,%r9,4), %xmm10, %xmm2
	vmovd	%xmm2, %eax
	movl	%eax, %ecx
	andl	$-2139095041, %ecx              # imm = 0x807FFFFF
	movl	%ecx, %ebx
	sarl	$22, %ebx
	sarl	$23, %eax
	addl	%ebx, %eax
	addl	$-127, %eax
                                        # kill: def $ebx killed $ebx killed $rbx def $rbx
	shll	$23, %ebx
	negl	%ebx
	addl	%ebx, %ecx
	addl	$1065353216, %ecx               # imm = 0x3F800000
	vmovd	%ecx, %xmm2
	vaddss	%xmm7, %xmm2, %xmm2
	vmulss	%xmm2, %xmm2, %xmm3
	vcvtsi2ss	%eax, %xmm10, %xmm4
	vmovaps	%xmm1, %xmm6
	vfmadd213ss	%xmm5, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) + xmm5
	vfmadd213ss	%xmm0, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) + xmm0
	vfmadd213ss	%xmm10, %xmm3, %xmm6    # xmm6 = (xmm3 * xmm6) + xmm10
	vmulss	%xmm6, %xmm2, %xmm2
	vmovaps	%xmm8, %xmm6
	vfmadd213ss	%xmm15, %xmm3, %xmm6    # xmm6 = (xmm3 * xmm6) + xmm15
	vfmadd213ss	%xmm11, %xmm3, %xmm6    # xmm6 = (xmm3 * xmm6) + xmm11
	vfmadd213ss	%xmm2, %xmm3, %xmm6     # xmm6 = (xmm3 * xmm6) + xmm2
	vfmadd231ss	%xmm12, %xmm4, %xmm6    # xmm6 = (xmm4 * xmm12) + xmm6
	vmovss	%xmm6, (%r12,%r9,4)
	incq	%r9
	cmpq	%r9, %r10
	jne	.LBB4_31
	jmp	.LBB4_35
.LBB4_27:                               # %destructor_block
	xorl	%eax, %eax
	addq	$392, %rsp                      # imm = 0x188
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end4:
	.size	train_cost_model.par_for.normalized_schedule_features.s0.c.c.c, .Lfunc_end4-train_cost_model.par_for.normalized_schedule_features.s0.c.c.c
                                        # -- End function
	.section	.text.train_cost_model.par_for.head2_conv.s0.n.n.n,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.head2_conv.s0.n.n.n
	.type	train_cost_model.par_for.head2_conv.s0.n.n.n,@function
train_cost_model.par_for.head2_conv.s0.n.n.n: # @train_cost_model.par_for.head2_conv.s0.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$64, %rsp
	movq	%rdx, %r8
	movl	%esi, %r14d
	movl	4(%rdx), %r9d
	movl	8(%rdx), %ecx
	movl	12(%rdx), %r10d
	movl	16(%rdx), %r13d
	movl	%esi, %r11d
	sarl	$31, %r11d
	xorl	%ebx, %ebx
	testl	%ecx, %ecx
	sete	%bl
	movl	%ebx, %edi
	negl	%edi
	movl	%ecx, %esi
	subl	%r11d, %r14d
	orl	%ecx, %edi
	movl	%r14d, %eax
	cltd
	idivl	%edi
	sarl	$31, %esi
	movl	%esi, %ebp
	xorl	%ecx, %ebp
	movl	%esi, %edi
	notl	%edi
	addl	%edi, %ebp
	andl	%r11d, %ebp
	addl	%edx, %ebp
	leal	-1(%rbx), %r15d
	andl	%r15d, %ebp
	addl	%ebx, %ecx
	movl	%r14d, %eax
	cltd
	idivl	%ecx
	shll	$3, %ebp
	leal	(%rbp,%rbp,4), %ecx
	subl	%esi, %edi
	andl	%r11d, %edi
	addl	%eax, %edi
	andl	%r15d, %edi
	movl	%edi, %eax
	andl	$-2, %eax
	subl	%r9d, %r13d
	movl	%r13d, %r14d
	subl	%ecx, %r14d
	movl	%r14d, %ecx
	sarl	$3, %ecx
	cmpl	$6, %ecx
	movl	$5, %r9d
	cmovgel	%r9d, %ecx
	xorl	%ebx, %ebx
	cmpl	$7, %r14d
	cmovgl	%ecx, %ebx
	movl	%ebx, -120(%rsp)                # 4-byte Spill
	leal	7(%r14), %ecx
	sarl	$3, %ecx
	cmpl	$6, %ecx
	cmovgel	%r9d, %ecx
	movl	%ecx, -124(%rsp)                # 4-byte Spill
	subl	%eax, %r10d
	jle	.LBB5_29
# %bb.1:                                # %"for head2_conv.s0.w.wi.preheader"
	movslq	(%r8), %r9
	movq	24(%r8), %rsi
	movq	40(%r8), %r8
	movl	%edi, %eax
	andl	$1, %eax
	shll	$2, %eax
	xorl	%ecx, %ecx
	cmpl	$1, %r10d
	sete	%cl
	movl	$2, %ebx
	subq	%rcx, %rbx
	movq	%rbx, -64(%rsp)                 # 8-byte Spill
	movl	-120(%rsp), %ecx                # 4-byte Reload
	movl	-124(%rsp), %ebx                # 4-byte Reload
	cmpl	%ecx, %ebx
	leal	(%rax,%rax,2), %r10d
	movl	%ecx, %eax
	cmovgel	%ebx, %eax
	movl	%ecx, %ebx
	movslq	%ecx, %r11
	movslq	%eax, %rdx
	leal	(,%r11,8), %eax
	subl	%eax, %r13d
	leal	(%rbp,%rbp,4), %eax
	subl	%eax, %r13d
	leaq	-1(%rbx), %rcx
	movq	%rcx, -48(%rsp)                 # 8-byte Spill
	movl	%ebx, %ecx
	andl	$7, %ecx
	movq	%rcx, -96(%rsp)                 # 8-byte Spill
	andl	$-8, %ebx
	movl	%edi, %ecx
	andl	$1, %ecx
	imulq	%r9, %rcx
	leaq	(%rcx,%rcx,2), %r15
	shlq	$4, %r15
	leaq	(%r8,%r15), %rcx
	addq	$224, %rcx
	movq	%rcx, -80(%rsp)                 # 8-byte Spill
	shrl	%edi
	movl	%r9d, %ecx
	imull	%edi, %ecx
	leal	(%rcx,%rcx,2), %ebp
	shll	$4, %ebp
	addl	%eax, %ebp
	leal	(%rdi,%rdi,2), %ecx
	shll	$4, %ecx
	movq	%r10, %rdi
	movq	%r10, -24(%rsp)                 # 8-byte Spill
	addl	%r10d, %ecx
	imull	%r9d, %ecx
	addl	%eax, %ecx
	leal	(,%r9,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -116(%rsp)                # 4-byte Spill
	movl	%ebp, %eax
	addq	%r8, %r15
	movq	%r15, -72(%rsp)                 # 8-byte Spill
	movq	%r11, -40(%rsp)                 # 8-byte Spill
	leal	(%rcx,%r11,8), %ecx
	movq	%r9, -16(%rsp)                  # 8-byte Spill
	leaq	(,%r9,4), %rdi
	movq	%rdi, -56(%rsp)                 # 8-byte Spill
	leaq	992(%r8), %rdi
	movq	%rdi, 16(%rsp)                  # 8-byte Spill
	movq	%r8, 56(%rsp)                   # 8-byte Spill
	leaq	96(%r8), %rdi
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	xorl	%edi, %edi
	movq	%rdi, -104(%rsp)                # 8-byte Spill
	movq	%r14, 48(%rsp)                  # 8-byte Spill
	movq	%rsi, 40(%rsp)                  # 8-byte Spill
	movq	%rbx, -32(%rsp)                 # 8-byte Spill
	movq	%rdx, 32(%rsp)                  # 8-byte Spill
	jmp	.LBB5_2
	.p2align	4, 0x90
.LBB5_28:                               # %"end for head2_conv.s0.c.ci"
                                        #   in Loop: Header=BB5_2 Depth=1
	movq	-104(%rsp), %rbp                # 8-byte Reload
	incq	%rbp
	movl	-108(%rsp), %eax                # 4-byte Reload
	movl	-116(%rsp), %ecx                # 4-byte Reload
	addl	%ecx, %eax
	movl	-112(%rsp), %edi                # 4-byte Reload
	addl	%ecx, %edi
	movl	%edi, %ecx
	movq	%rbp, %rdi
	movq	%rbp, -104(%rsp)                # 8-byte Spill
	cmpq	-64(%rsp), %rbp                 # 8-byte Folded Reload
	je	.LBB5_29
.LBB5_2:                                # %"for head2_conv.s0.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB5_3 Depth 2
                                        #       Child Loop BB5_7 Depth 3
                                        #       Child Loop BB5_10 Depth 3
                                        #       Child Loop BB5_13 Depth 3
                                        #         Child Loop BB5_21 Depth 4
                                        #         Child Loop BB5_24 Depth 4
                                        #         Child Loop BB5_17 Depth 4
	movl	%eax, -108(%rsp)                # 4-byte Spill
	cltq
	movl	%ecx, %r8d
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rbp
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdi
	movl	%r8d, -112(%rsp)                # 4-byte Spill
	movl	%r8d, -128(%rsp)                # 4-byte Spill
	xorl	%ecx, %ecx
	jmp	.LBB5_3
	.p2align	4, 0x90
.LBB5_27:                               # %"end for head2_conv.s0.n.ni.ni2"
                                        #   in Loop: Header=BB5_3 Depth=2
	movq	-8(%rsp), %rcx                  # 8-byte Reload
	incq	%rcx
	movq	-56(%rsp), %rax                 # 8-byte Reload
	movq	(%rsp), %rdi                    # 8-byte Reload
	addq	%rax, %rdi
	movq	8(%rsp), %rbp                   # 8-byte Reload
	addq	%rax, %rbp
	movl	-128(%rsp), %eax                # 4-byte Reload
	addl	-16(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -128(%rsp)                # 4-byte Spill
	cmpq	$12, %rcx
	movq	-32(%rsp), %rbx                 # 8-byte Reload
	je	.LBB5_28
.LBB5_3:                                # %"for head2_conv.s0.c.ci"
                                        #   Parent Loop BB5_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB5_7 Depth 3
                                        #       Child Loop BB5_10 Depth 3
                                        #       Child Loop BB5_13 Depth 3
                                        #         Child Loop BB5_21 Depth 4
                                        #         Child Loop BB5_24 Depth 4
                                        #         Child Loop BB5_17 Depth 4
	movq	-24(%rsp), %rax                 # 8-byte Reload
	movq	%rcx, -8(%rsp)                  # 8-byte Spill
	addq	%rcx, %rax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	cmpl	$8, %r14d
	jl	.LBB5_11
# %bb.4:                                # %"for head2_conv.s0.n.ni.ni.preheader"
                                        #   in Loop: Header=BB5_3 Depth=2
	movq	-88(%rsp), %rax                 # 8-byte Reload
	vpbroadcastd	(%rsi,%rax,4), %ymm0
	cmpq	$7, -48(%rsp)                   # 8-byte Folded Reload
	jae	.LBB5_6
# %bb.5:                                #   in Loop: Header=BB5_3 Depth=2
	xorl	%eax, %eax
	jmp	.LBB5_8
	.p2align	4, 0x90
.LBB5_6:                                # %"for head2_conv.s0.n.ni.ni.preheader1"
                                        #   in Loop: Header=BB5_3 Depth=2
	movq	%rdi, %rcx
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB5_7:                                # %"for head2_conv.s0.n.ni.ni"
                                        #   Parent Loop BB5_2 Depth=1
                                        #     Parent Loop BB5_3 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	%ymm0, -224(%rcx)
	vmovdqu	%ymm0, -192(%rcx)
	vmovdqu	%ymm0, -160(%rcx)
	vmovdqu	%ymm0, -128(%rcx)
	vmovdqu	%ymm0, -96(%rcx)
	vmovdqu	%ymm0, -64(%rcx)
	vmovdqu	%ymm0, -32(%rcx)
	vmovdqu	%ymm0, (%rcx)
	addq	$8, %rax
	addq	$256, %rcx                      # imm = 0x100
	cmpq	%rax, %rbx
	jne	.LBB5_7
.LBB5_8:                                # %"end for head2_conv.s0.n.ni.ni.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB5_3 Depth=2
	cmpq	$0, -96(%rsp)                   # 8-byte Folded Reload
	je	.LBB5_11
# %bb.9:                                # %"for head2_conv.s0.n.ni.ni.epil.preheader"
                                        #   in Loop: Header=BB5_3 Depth=2
	shlq	$5, %rax
	addq	%rbp, %rax
	movq	-96(%rsp), %rcx                 # 8-byte Reload
	.p2align	4, 0x90
.LBB5_10:                               # %"for head2_conv.s0.n.ni.ni.epil"
                                        #   Parent Loop BB5_2 Depth=1
                                        #     Parent Loop BB5_3 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	%ymm0, (%rax)
	addq	$32, %rax
	decq	%rcx
	jne	.LBB5_10
.LBB5_11:                               # %"end for head2_conv.s0.n.ni.ni"
                                        #   in Loop: Header=BB5_3 Depth=2
	movq	%rdi, (%rsp)                    # 8-byte Spill
	movq	%rbp, 8(%rsp)                   # 8-byte Spill
	movl	-124(%rsp), %eax                # 4-byte Reload
	cmpl	-120(%rsp), %eax                # 4-byte Folded Reload
	jle	.LBB5_27
# %bb.12:                               # %"for head2_conv.s0.n.ni.ni1.preheader"
                                        #   in Loop: Header=BB5_3 Depth=2
	xorl	%r8d, %r8d
	movl	%r13d, %edi
	movl	-128(%rsp), %r15d               # 4-byte Reload
	movq	-40(%rsp), %r11                 # 8-byte Reload
	jmp	.LBB5_13
	.p2align	4, 0x90
.LBB5_26:                               # %"end for head2_conv.s0.n.ni.nii"
                                        #   in Loop: Header=BB5_13 Depth=3
	incq	%r11
	incl	%r8d
	addl	$8, %r15d
	addl	$-8, %edi
	cmpq	%rdx, %r11
	je	.LBB5_27
.LBB5_13:                               # %"for head2_conv.s0.n.ni.ni1"
                                        #   Parent Loop BB5_2 Depth=1
                                        #     Parent Loop BB5_3 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB5_21 Depth 4
                                        #         Child Loop BB5_24 Depth 4
                                        #         Child Loop BB5_17 Depth 4
	cmpl	$9, %edi
	movl	$8, %ebp
	cmovll	%edi, %ebp
	leal	(,%r8,8), %eax
	movl	%r13d, %r10d
	subl	%eax, %r10d
	cmpl	$9, %r10d
	movl	$8, %eax
	cmovll	%r10d, %eax
	cmpl	$9, %r10d
	movl	$8, %ecx
	cmovgel	%ecx, %r10d
	leal	(,%r11,8), %ecx
	cmpl	%ecx, %r14d
	jle	.LBB5_26
# %bb.14:                               # %"for head2_conv.s0.n.ni.nii.preheader"
                                        #   in Loop: Header=BB5_13 Depth=3
	movslq	%r15d, %rbx
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	movl	(%rsi,%rcx,4), %r12d
	cmpl	$31, %r10d
	ja	.LBB5_18
# %bb.15:                               #   in Loop: Header=BB5_13 Depth=3
	xorl	%r9d, %r9d
	jmp	.LBB5_16
	.p2align	4, 0x90
.LBB5_18:                               # %vector.ph
                                        #   in Loop: Header=BB5_13 Depth=3
	movl	%ebp, %esi
	andl	$-32, %esi
	addq	$-32, %rsi
	shrq	$5, %rsi
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r14
	shrq	$5, %r14
	incq	%r14
	movl	%r10d, %r9d
	andl	$-32, %r9d
	vmovd	%r12d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	cmpq	$224, %rax
	jae	.LBB5_20
# %bb.19:                               #   in Loop: Header=BB5_13 Depth=3
	xorl	%eax, %eax
	jmp	.LBB5_22
	.p2align	4, 0x90
.LBB5_20:                               # %vector.ph.new
                                        #   in Loop: Header=BB5_13 Depth=3
	movq	16(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rbx,4), %rdx
	leaq	1(%rsi), %rcx
	andq	$-8, %rcx
	negq	%rcx
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB5_21:                               # %vector.body
                                        #   Parent Loop BB5_2 Depth=1
                                        #     Parent Loop BB5_3 Depth=2
                                        #       Parent Loop BB5_13 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovdqu	%ymm0, -992(%rdx,%rax,4)
	vmovdqu	%ymm0, -960(%rdx,%rax,4)
	vmovdqu	%ymm0, -928(%rdx,%rax,4)
	vmovdqu	%ymm0, -896(%rdx,%rax,4)
	vmovdqu	%ymm0, -864(%rdx,%rax,4)
	vmovdqu	%ymm0, -832(%rdx,%rax,4)
	vmovdqu	%ymm0, -800(%rdx,%rax,4)
	vmovdqu	%ymm0, -768(%rdx,%rax,4)
	vmovdqu	%ymm0, -736(%rdx,%rax,4)
	vmovdqu	%ymm0, -704(%rdx,%rax,4)
	vmovdqu	%ymm0, -672(%rdx,%rax,4)
	vmovdqu	%ymm0, -640(%rdx,%rax,4)
	vmovdqu	%ymm0, -608(%rdx,%rax,4)
	vmovdqu	%ymm0, -576(%rdx,%rax,4)
	vmovdqu	%ymm0, -544(%rdx,%rax,4)
	vmovdqu	%ymm0, -512(%rdx,%rax,4)
	vmovdqu	%ymm0, -480(%rdx,%rax,4)
	vmovdqu	%ymm0, -448(%rdx,%rax,4)
	vmovdqu	%ymm0, -416(%rdx,%rax,4)
	vmovdqu	%ymm0, -384(%rdx,%rax,4)
	vmovdqu	%ymm0, -352(%rdx,%rax,4)
	vmovdqu	%ymm0, -320(%rdx,%rax,4)
	vmovdqu	%ymm0, -288(%rdx,%rax,4)
	vmovdqu	%ymm0, -256(%rdx,%rax,4)
	vmovdqu	%ymm0, -224(%rdx,%rax,4)
	vmovdqu	%ymm0, -192(%rdx,%rax,4)
	vmovdqu	%ymm0, -160(%rdx,%rax,4)
	vmovdqu	%ymm0, -128(%rdx,%rax,4)
	vmovdqu	%ymm0, -96(%rdx,%rax,4)
	vmovdqu	%ymm0, -64(%rdx,%rax,4)
	vmovdqu	%ymm0, -32(%rdx,%rax,4)
	vmovdqu	%ymm0, (%rdx,%rax,4)
	addq	$256, %rax                      # imm = 0x100
	addq	$8, %rcx
	jne	.LBB5_21
.LBB5_22:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB5_13 Depth=3
	testb	$7, %r14b
	je	.LBB5_25
# %bb.23:                               # %vector.body.epil.preheader
                                        #   in Loop: Header=BB5_13 Depth=3
	incb	%sil
	movzbl	%sil, %ecx
	andl	$7, %ecx
	negq	%rcx
	addq	%rbx, %rax
	movq	24(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	.p2align	4, 0x90
.LBB5_24:                               # %vector.body.epil
                                        #   Parent Loop BB5_2 Depth=1
                                        #     Parent Loop BB5_3 Depth=2
                                        #       Parent Loop BB5_13 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovdqu	%ymm0, -96(%rax)
	vmovdqu	%ymm0, -64(%rax)
	vmovdqu	%ymm0, -32(%rax)
	vmovdqu	%ymm0, (%rax)
	subq	$-128, %rax
	incq	%rcx
	jne	.LBB5_24
.LBB5_25:                               # %middle.block
                                        #   in Loop: Header=BB5_13 Depth=3
	cmpq	%r10, %r9
	movq	48(%rsp), %r14                  # 8-byte Reload
	movq	40(%rsp), %rsi                  # 8-byte Reload
	movq	32(%rsp), %rdx                  # 8-byte Reload
	je	.LBB5_26
.LBB5_16:                               # %"for head2_conv.s0.n.ni.nii.preheader25"
                                        #   in Loop: Header=BB5_13 Depth=3
	movq	56(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	.p2align	4, 0x90
.LBB5_17:                               # %"for head2_conv.s0.n.ni.nii"
                                        #   Parent Loop BB5_2 Depth=1
                                        #     Parent Loop BB5_3 Depth=2
                                        #       Parent Loop BB5_13 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%r12d, (%rax,%r9,4)
	incq	%r9
	cmpq	%r9, %rbp
	jne	.LBB5_17
	jmp	.LBB5_26
.LBB5_29:                               # %destructor_block
	xorl	%eax, %eax
	addq	$64, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end5:
	.size	train_cost_model.par_for.head2_conv.s0.n.n.n, .Lfunc_end5-train_cost_model.par_for.head2_conv.s0.n.n.n
                                        # -- End function
	.section	.text.train_cost_model.par_for.head2_conv.s1.n.n.n,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.head2_conv.s1.n.n.n
	.type	train_cost_model.par_for.head2_conv.s1.n.n.n,@function
train_cost_model.par_for.head2_conv.s1.n.n.n: # @train_cost_model.par_for.head2_conv.s1.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$240, %rsp
	movq	%rdx, %r9
	movl	%esi, %r14d
	movl	12(%rdx), %r8d
	movl	16(%rdx), %ebp
	movl	20(%rdx), %r10d
	movl	24(%rdx), %r12d
	movl	%esi, %r11d
	sarl	$31, %r11d
	xorl	%ebx, %ebx
	testl	%ebp, %ebp
	sete	%bl
	movl	%ebx, %ecx
	negl	%ecx
	movl	%ebp, %esi
	subl	%r11d, %r14d
	orl	%ebp, %ecx
	movl	%r14d, %eax
	cltd
	idivl	%ecx
	sarl	$31, %esi
	movl	%esi, %edi
	xorl	%ebp, %edi
	movl	%esi, %ecx
	notl	%ecx
	addl	%ecx, %edi
	andl	%r11d, %edi
	addl	%edx, %edi
	leal	-1(%rbx), %r15d
	andl	%r15d, %edi
	addl	%ebx, %ebp
	movl	%r14d, %eax
	cltd
	idivl	%ebp
	shll	$3, %edi
	leal	(%rdi,%rdi,4), %edx
	subl	%esi, %ecx
	andl	%r11d, %ecx
	addl	%eax, %ecx
	andl	%r15d, %ecx
	movl	%ecx, %ebp
	andl	$-2, %ebp
	subl	%r8d, %r12d
	movl	%r12d, -128(%rsp)               # 4-byte Spill
	movl	%r12d, %ebx
	movq	%rdx, -64(%rsp)                 # 8-byte Spill
	subl	%edx, %ebx
	movl	%ebx, %eax
	sarl	$3, %eax
	cmpl	$6, %eax
	movl	$5, %edx
	cmovgel	%edx, %eax
	xorl	%esi, %esi
	cmpl	$7, %ebx
	cmovgl	%eax, %esi
	movl	%esi, -116(%rsp)                # 4-byte Spill
	movq	%rbx, -56(%rsp)                 # 8-byte Spill
	leal	7(%rbx), %eax
	sarl	$3, %eax
	cmpl	$6, %eax
	cmovgel	%edx, %eax
	movl	%eax, -120(%rsp)                # 4-byte Spill
	movq	%rbp, 16(%rsp)                  # 8-byte Spill
	subl	%ebp, %r10d
	jle	.LBB6_16
# %bb.1:                                # %"for head2_conv.s1.w.wi.preheader"
	movl	(%r9), %r11d
	movslq	8(%r9), %rbx
	movq	32(%r9), %r14
	movq	48(%r9), %r15
	movq	64(%r9), %rax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	movl	%ecx, %eax
	andl	$1, %eax
	shll	$2, %eax
	xorl	%edx, %edx
	cmpl	$1, %r10d
	sete	%dl
	movl	$2, %esi
	subq	%rdx, %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	imull	$39, %ebx, %edx
	movl	%edx, -124(%rsp)                # 4-byte Spill
	movl	-116(%rsp), %r10d               # 4-byte Reload
	movl	-120(%rsp), %edx                # 4-byte Reload
	cmpl	%r10d, %edx
	leal	(%rax,%rax,2), %r8d
	movslq	4(%r9), %rbp
	movl	%r10d, %eax
	cmovgel	%edx, %eax
	movslq	%r10d, %r9
	cltq
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	leal	(,%r9,8), %eax
	movl	-128(%rsp), %esi                # 4-byte Reload
	subl	%eax, %esi
	leal	(%rdi,%rdi,4), %eax
	subl	%eax, %esi
	movl	%esi, -128(%rsp)                # 4-byte Spill
	leaq	(,%rbp,4), %rdx
	leaq	(%rdx,%rdx,2), %rsi
	shrl	%ecx
	movl	%ebx, %edx
	imull	%ecx, %edx
	imull	$78, %edx, %edi
	movslq	-64(%rsp), %r12                 # 4-byte Folded Reload
	leaq	(,%r12,4), %rdx
	leaq	(%rdx,%rbx,8), %r13
	movq	%r13, (%rsp)                    # 8-byte Spill
	leaq	(%rdx,%rbx,4), %rdx
	movq	%rdx, -8(%rsp)                  # 8-byte Spill
	movl	-124(%rsp), %edx                # 4-byte Reload
	movq	%r9, 48(%rsp)                   # 8-byte Spill
	leal	(%rax,%r9,8), %eax
	movl	%eax, -76(%rsp)                 # 4-byte Spill
	leal	(%rcx,%rcx,2), %ecx
	shll	$4, %ecx
	addl	%r8d, %ecx
	imull	%r11d, %ecx
	leal	(,%r11,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -88(%rsp)                 # 4-byte Spill
	movl	%r10d, %eax
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	movq	%r15, -40(%rsp)                 # 8-byte Spill
	movq	%r8, 56(%rsp)                   # 8-byte Spill
	leaq	(%r15,%r8,4), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	movq	%rsi, %r15
	movl	%ecx, %esi
	movq	%rbx, 176(%rsp)                 # 8-byte Spill
	leaq	(,%rbx,4), %r9
	leaq	(%r9,%r9,2), %r13
	movq	-112(%rsp), %rax                # 8-byte Reload
	leaq	96(%rax), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	leaq	96(%r14), %rax
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	xorl	%ebx, %ebx
	movq	%r13, 208(%rsp)                 # 8-byte Spill
	movq	%r11, 64(%rsp)                  # 8-byte Spill
	movq	%r14, -96(%rsp)                 # 8-byte Spill
	movq	%r15, 40(%rsp)                  # 8-byte Spill
	movq	%r12, 32(%rsp)                  # 8-byte Spill
	jmp	.LBB6_2
	.p2align	4, 0x90
.LBB6_15:                               # %"end for head2_conv.s1.c.ci"
                                        #   in Loop: Header=BB6_2 Depth=1
	movq	24(%rsp), %rbx                  # 8-byte Reload
	incq	%rbx
	movl	-124(%rsp), %edx                # 4-byte Reload
	movl	-80(%rsp), %edi                 # 4-byte Reload
	addl	%edx, %edi
	movl	-84(%rsp), %esi                 # 4-byte Reload
	addl	-88(%rsp), %esi                 # 4-byte Folded Reload
	cmpq	8(%rsp), %rbx                   # 8-byte Folded Reload
	je	.LBB6_16
.LBB6_2:                                # %"for head2_conv.s1.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB6_3 Depth 2
                                        #       Child Loop BB6_5 Depth 3
                                        #         Child Loop BB6_6 Depth 4
                                        #       Child Loop BB6_10 Depth 3
                                        #         Child Loop BB6_11 Depth 4
                                        #           Child Loop BB6_20 Depth 5
                                        #           Child Loop BB6_24 Depth 5
	movl	%edi, -80(%rsp)                 # 4-byte Spill
	movslq	%edi, %rax
	movq	(%rsp), %rcx                    # 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	%rcx, 96(%rsp)                  # 8-byte Spill
	movq	-8(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	%rcx, 88(%rsp)                  # 8-byte Spill
	leaq	(%r12,%rax), %rcx
	shlq	$2, %rcx
	movq	%rcx, 80(%rsp)                  # 8-byte Spill
	movq	16(%rsp), %rcx                  # 8-byte Reload
	movq	%rbx, 24(%rsp)                  # 8-byte Spill
	addl	%ebx, %ecx
	imull	%ecx, %edx
	shll	$3, %ecx
	leal	(%rcx,%rcx,2), %ecx
	movq	%rcx, 72(%rsp)                  # 8-byte Spill
	movslq	%edx, %rcx
	movq	%rcx, 184(%rsp)                 # 8-byte Spill
	movq	-24(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	%rcx, 144(%rsp)                 # 8-byte Spill
	movq	-112(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 136(%rsp)                 # 8-byte Spill
	movl	%esi, -84(%rsp)                 # 4-byte Spill
	movl	%esi, %eax
	movq	-16(%rsp), %rcx                 # 8-byte Reload
	movq	%rcx, -48(%rsp)                 # 8-byte Spill
	xorl	%edx, %edx
	jmp	.LBB6_3
	.p2align	4, 0x90
.LBB6_14:                               # %"end for head2_conv.s1.n.ni.ni2"
                                        #   in Loop: Header=BB6_3 Depth=2
	movq	104(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	addq	$4, -48(%rsp)                   # 8-byte Folded Spill
	movq	64(%rsp), %r11                  # 8-byte Reload
	movq	-104(%rsp), %rax                # 8-byte Reload
	addl	%r11d, %eax
	cmpq	$12, %rdx
	movq	-96(%rsp), %r14                 # 8-byte Reload
	movq	40(%rsp), %r15                  # 8-byte Reload
	movq	32(%rsp), %r12                  # 8-byte Reload
	je	.LBB6_15
.LBB6_3:                                # %"for head2_conv.s1.c.ci"
                                        #   Parent Loop BB6_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB6_5 Depth 3
                                        #         Child Loop BB6_6 Depth 4
                                        #       Child Loop BB6_10 Depth 3
                                        #         Child Loop BB6_11 Depth 4
                                        #           Child Loop BB6_20 Depth 5
                                        #           Child Loop BB6_24 Depth 5
	movq	56(%rsp), %rcx                  # 8-byte Reload
	movq	%rdx, 104(%rsp)                 # 8-byte Spill
	addq	%rcx, %rdx
	movq	72(%rsp), %rcx                  # 8-byte Reload
	movq	%rdx, 232(%rsp)                 # 8-byte Spill
	addl	%edx, %ecx
	imull	%r11d, %ecx
	cltq
	movq	%rax, -104(%rsp)                # 8-byte Spill
	movslq	%ecx, %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	cmpl	$8, -56(%rsp)                   # 4-byte Folded Reload
	jl	.LBB6_8
# %bb.4:                                # %"for head2_conv.s1.n.ni.ni.preheader"
                                        #   in Loop: Header=BB6_3 Depth=2
	movq	80(%rsp), %rax                  # 8-byte Reload
	movq	88(%rsp), %rcx                  # 8-byte Reload
	movq	96(%rsp), %rdx                  # 8-byte Reload
	xorl	%r10d, %r10d
	.p2align	4, 0x90
.LBB6_5:                                # %"for head2_conv.s1.n.ni.ni"
                                        #   Parent Loop BB6_2 Depth=1
                                        #     Parent Loop BB6_3 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB6_6 Depth 4
	leaq	(%r12,%r10,8), %r8
	addq	-32(%rsp), %r8                  # 8-byte Folded Reload
	vmovups	(%r14,%r8,4), %ymm0
	movl	$39, %ebx
	movq	-112(%rsp), %rdi                # 8-byte Reload
	movq	-48(%rsp), %rsi                 # 8-byte Reload
	.p2align	4, 0x90
.LBB6_6:                                # %"for head2_conv.s1.r40$x"
                                        #   Parent Loop BB6_2 Depth=1
                                        #     Parent Loop BB6_3 Depth=2
                                        #       Parent Loop BB6_5 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vbroadcastss	(%rsi), %ymm1
	vfmadd132ps	(%rdi,%rax), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rsi,%rbp,4), %ymm2
	vfmadd132ps	(%rdi,%rcx), %ymm1, %ymm2 # ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rsi,%rbp,8), %ymm0
	vfmadd132ps	(%rdi,%rdx), %ymm2, %ymm0 # ymm0 = (ymm0 * mem) + ymm2
	addq	%r15, %rsi
	addq	%r13, %rdi
	addq	$-3, %rbx
	jne	.LBB6_6
# %bb.7:                                # %"end for head2_conv.s1.r40$x"
                                        #   in Loop: Header=BB6_5 Depth=3
	vmovups	%ymm0, (%r14,%r8,4)
	incq	%r10
	addq	$32, %rdx
	addq	$32, %rcx
	addq	$32, %rax
	cmpq	120(%rsp), %r10                 # 8-byte Folded Reload
	jne	.LBB6_5
.LBB6_8:                                # %"end for head2_conv.s1.n.ni.ni"
                                        #   in Loop: Header=BB6_3 Depth=2
	movl	-120(%rsp), %eax                # 4-byte Reload
	cmpl	-116(%rsp), %eax                # 4-byte Folded Reload
	jle	.LBB6_14
# %bb.9:                                # %"for head2_conv.s1.n.ni.ni1.preheader"
                                        #   in Loop: Header=BB6_3 Depth=2
	movq	-96(%rsp), %rax                 # 8-byte Reload
	movq	-104(%rsp), %rcx                # 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	movq	%rax, 152(%rsp)                 # 8-byte Spill
	xorl	%edi, %edi
	movl	-128(%rsp), %esi                # 4-byte Reload
	movl	-76(%rsp), %eax                 # 4-byte Reload
	movq	48(%rsp), %r10                  # 8-byte Reload
	jmp	.LBB6_10
	.p2align	4, 0x90
.LBB6_26:                               # %"end for head2_conv.s1.r40$x5"
                                        #   in Loop: Header=BB6_10 Depth=3
	movq	160(%rsp), %r10                 # 8-byte Reload
	incq	%r10
	movq	168(%rsp), %rdi                 # 8-byte Reload
	incl	%edi
	movl	-72(%rsp), %eax                 # 4-byte Reload
	addl	$8, %eax
	movl	-68(%rsp), %esi                 # 4-byte Reload
	addl	$-8, %esi
	cmpq	128(%rsp), %r10                 # 8-byte Folded Reload
	je	.LBB6_14
.LBB6_10:                               # %"for head2_conv.s1.n.ni.ni1"
                                        #   Parent Loop BB6_2 Depth=1
                                        #     Parent Loop BB6_3 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB6_11 Depth 4
                                        #           Child Loop BB6_20 Depth 5
                                        #           Child Loop BB6_24 Depth 5
	movl	%eax, -72(%rsp)                 # 4-byte Spill
	cltq
	movq	136(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	152(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %r8
	cmpl	$9, %esi
	movl	$8, %r14d
	movl	%esi, -68(%rsp)                 # 4-byte Spill
	cmovll	%esi, %r14d
	movq	144(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %rsi
	addq	-104(%rsp), %rax                # 8-byte Folded Reload
	movq	112(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %r15
	movl	%r14d, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	shrq	$4, %rax
	incq	%rax
	andq	$-2, %rax
	negq	%rax
	movq	%rax, 200(%rsp)                 # 8-byte Spill
	movq	%rdi, 168(%rsp)                 # 8-byte Spill
	leal	(,%rdi,8), %eax
	movl	-128(%rsp), %edx                # 4-byte Reload
	movl	%edx, %edi
	subl	%eax, %edi
	cmpl	$9, %edi
	movl	$8, %eax
	cmovll	%edi, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	shrq	$4, %rax
	incq	%rax
	movq	%rax, 216(%rsp)                 # 8-byte Spill
	cmpl	$9, %edi
	movl	$8, %eax
	cmovgel	%eax, %edi
	leal	(,%r10,8), %eax
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, %ebx
	subl	%eax, %ebx
	movq	-64(%rsp), %rax                 # 8-byte Reload
	movq	%r10, 160(%rsp)                 # 8-byte Spill
	leal	(%rax,%r10,8), %eax
	cltq
	movq	%rax, 192(%rsp)                 # 8-byte Spill
	movl	%edi, %r12d
	andl	$-16, %r12d
	xorl	%r11d, %r11d
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB6_11
	.p2align	4, 0x90
.LBB6_25:                               # %"end for head2_conv.s1.n.ni.nii"
                                        #   in Loop: Header=BB6_11 Depth=4
	incq	%r11
	addq	%r9, %rsi
	addq	%r9, %rcx
	cmpq	$39, %r11
	je	.LBB6_26
.LBB6_11:                               # %"for head2_conv.s1.r40$x4"
                                        #   Parent Loop BB6_2 Depth=1
                                        #     Parent Loop BB6_3 Depth=2
                                        #       Parent Loop BB6_10 Depth=3
                                        # =>      This Loop Header: Depth=4
                                        #           Child Loop BB6_20 Depth 5
                                        #           Child Loop BB6_24 Depth 5
	testl	%ebx, %ebx
	jle	.LBB6_25
# %bb.12:                               # %"for head2_conv.s1.n.ni.nii.preheader"
                                        #   in Loop: Header=BB6_11 Depth=4
	movq	%r11, %rax
	imulq	%rbp, %rax
	addq	232(%rsp), %rax                 # 8-byte Folded Reload
	vmovss	(%rdx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	cmpl	$15, %edi
	ja	.LBB6_17
# %bb.13:                               #   in Loop: Header=BB6_11 Depth=4
	xorl	%eax, %eax
	jmp	.LBB6_24
	.p2align	4, 0x90
.LBB6_17:                               # %vector.ph
                                        #   in Loop: Header=BB6_11 Depth=4
	vbroadcastss	%xmm0, %ymm1
	cmpq	$0, 224(%rsp)                   # 8-byte Folded Reload
	je	.LBB6_18
# %bb.19:                               # %vector.body.preheader
                                        #   in Loop: Header=BB6_11 Depth=4
	movq	200(%rsp), %r10                 # 8-byte Reload
	xorl	%r13d, %r13d
	.p2align	4, 0x90
.LBB6_20:                               # %vector.body
                                        #   Parent Loop BB6_2 Depth=1
                                        #     Parent Loop BB6_3 Depth=2
                                        #       Parent Loop BB6_10 Depth=3
                                        #         Parent Loop BB6_11 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovups	-96(%rsi,%r13,4), %ymm2
	vmovups	-64(%rsi,%r13,4), %ymm3
	vfmadd213ps	-96(%r15,%r13,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%r15,%r13,4), %ymm1, %ymm3 # ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, -96(%r15,%r13,4)
	vmovups	%ymm3, -64(%r15,%r13,4)
	vmovups	-32(%rsi,%r13,4), %ymm2
	vmovups	(%rsi,%r13,4), %ymm3
	vfmadd213ps	-32(%r15,%r13,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	(%r15,%r13,4), %ymm1, %ymm3 # ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, -32(%r15,%r13,4)
	vmovups	%ymm3, (%r15,%r13,4)
	addq	$32, %r13
	addq	$2, %r10
	jne	.LBB6_20
# %bb.21:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB6_11 Depth=4
	testb	$1, 216(%rsp)                   # 1-byte Folded Reload
	je	.LBB6_23
.LBB6_22:                               # %vector.body.epil
                                        #   in Loop: Header=BB6_11 Depth=4
	movq	%r11, %rax
	imulq	176(%rsp), %rax                 # 8-byte Folded Reload
	addq	184(%rsp), %rax                 # 8-byte Folded Reload
	addq	192(%rsp), %r13                 # 8-byte Folded Reload
	movq	-32(%rsp), %rdx                 # 8-byte Reload
	addq	%r13, %rdx
	addq	%rax, %r13
	movq	-112(%rsp), %rax                # 8-byte Reload
	vmovups	(%rax,%r13,4), %ymm2
	vmovups	32(%rax,%r13,4), %ymm3
	movq	-96(%rsp), %rax                 # 8-byte Reload
	vfmadd213ps	(%rax,%rdx,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	32(%rax,%rdx,4), %ymm1, %ymm3 # ymm3 = (ymm1 * ymm3) + mem
	vmovups	%ymm2, (%rax,%rdx,4)
	vmovups	%ymm3, 32(%rax,%rdx,4)
	movq	-40(%rsp), %rdx                 # 8-byte Reload
.LBB6_23:                               # %middle.block
                                        #   in Loop: Header=BB6_11 Depth=4
	movq	%r12, %rax
	cmpq	%rdi, %r12
	movq	208(%rsp), %r13                 # 8-byte Reload
	je	.LBB6_25
	.p2align	4, 0x90
.LBB6_24:                               # %"for head2_conv.s1.n.ni.nii"
                                        #   Parent Loop BB6_2 Depth=1
                                        #     Parent Loop BB6_3 Depth=2
                                        #       Parent Loop BB6_10 Depth=3
                                        #         Parent Loop BB6_11 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovss	(%rcx,%rax,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	(%r8,%rax,4), %xmm0, %xmm1 # xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, (%r8,%rax,4)
	incq	%rax
	cmpq	%rax, %r14
	jne	.LBB6_24
	jmp	.LBB6_25
.LBB6_18:                               #   in Loop: Header=BB6_11 Depth=4
	xorl	%r13d, %r13d
	testb	$1, 216(%rsp)                   # 1-byte Folded Reload
	jne	.LBB6_22
	jmp	.LBB6_23
.LBB6_16:                               # %destructor_block
	xorl	%eax, %eax
	addq	$240, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end6:
	.size	train_cost_model.par_for.head2_conv.s1.n.n.n, .Lfunc_end6-train_cost_model.par_for.head2_conv.s1.n.n.n
                                        # -- End function
	.section	.text.train_cost_model.par_for.head2_relu.s0.c.c.c,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.head2_relu.s0.c.c.c
	.type	train_cost_model.par_for.head2_relu.s0.c.c.c,@function
train_cost_model.par_for.head2_relu.s0.c.c.c: # @train_cost_model.par_for.head2_relu.s0.c.c.c
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
                                        # kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r15
	movslq	4(%rdx), %r9
	movl	16(%rdx), %r11d
	movl	24(%rdx), %ebp
	movl	28(%rdx), %edi
	movq	32(%rdx), %r12
	movq	48(%rdx), %r10
	cmpl	%esi, 8(%rdx)
	movq	%r12, -120(%rsp)                # 8-byte Spill
	movq	%r10, -64(%rsp)                 # 8-byte Spill
	jle	.LBB7_5
# %bb.1:                                # %true_bb
	movl	%r9d, %r14d
	movl	%r9d, -80(%rsp)                 # 4-byte Spill
	movl	12(%rdx), %edx
	movl	%edx, %r8d
	sarl	$3, %r8d
	movl	%esi, %ecx
	sarl	$3, %ecx
	movl	%esi, %eax
	andl	$7, %eax
	leal	(%rax,%rax,2), %eax
	addl	$7, %r11d
	sarl	$3, %r11d
	subl	%ebp, %edi
	leaq	1(%rax), %rbp
	leaq	2(%rax), %rbx
	movq	%rbx, 8(%rsp)                   # 8-byte Spill
	movl	%edx, -24(%rsp)                 # 4-byte Spill
	andl	$-8, %edx
	movl	%edi, -128(%rsp)                # 4-byte Spill
	movl	%edx, 40(%rsp)                  # 4-byte Spill
	subl	%edx, %edi
	movl	%edi, -124(%rsp)                # 4-byte Spill
	imull	%ecx, %r14d
	movl	%r15d, %ebx
	imull	%ecx, %ebx
	imull	$168, %ecx, %ecx
	leal	(%rcx,%rax), %edx
	leal	(%rcx,%rax), %edi
	incl	%edi
	leal	(%rcx,%rax), %r13d
	addl	$2, %r13d
	movl	%r8d, %eax
	andl	$3, %eax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movl	%r8d, %eax
	andl	$-4, %eax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	andl	$7, %esi
	movq	%r9, %rax
	imulq	%rsi, %rax
	leaq	(%rax,%rax,2), %rax
	leaq	(%r10,%rax,4), %rax
	movq	%rax, 88(%rsp)                  # 8-byte Spill
	imull	$168, %r14d, %r14d
	leal	(,%r9,8), %eax
	leal	(%rax,%rax,2), %eax
	movq	%rax, 104(%rsp)                 # 8-byte Spill
	imulq	%r15, %rsi
	leaq	(%rsi,%rsi,2), %rax
	leaq	(%r12,%rax,4), %rax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	imull	$168, %ebx, %ebx
	movl	-80(%rsp), %esi                 # 4-byte Reload
	movl	%esi, %eax
	imull	%edx, %eax
	movq	%rax, -104(%rsp)                # 8-byte Spill
	imull	%r15d, %edx
	movq	%rdx, -88(%rsp)                 # 8-byte Spill
	movq	%rbp, %rcx
	imulq	%r9, %rcx
	imulq	%r15, %rbp
	movl	%r9d, %eax
	imull	%edi, %eax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	imull	%r15d, %edi
	movq	%rdi, -96(%rsp)                 # 8-byte Spill
	movq	8(%rsp), %rax                   # 8-byte Reload
	imulq	%rax, %r9
	imulq	%r15, %rax
	movq	%rax, %rdx
	movl	%esi, %eax
	imull	%r13d, %eax
	movl	%eax, -80(%rsp)                 # 4-byte Spill
	imull	%r15d, %r13d
	movl	%r13d, -56(%rsp)                # 4-byte Spill
	leal	(,%r15,8), %eax
	leal	(%rax,%rax,2), %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	movslq	%r8d, %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	%r11, (%rsp)                    # 8-byte Spill
	movslq	%r11d, %rax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	movq	%r8, 48(%rsp)                   # 8-byte Spill
	leaq	-1(%r8), %rax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	leaq	224(%r10), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	leaq	224(%r12), %rax
	movq	%rax, -40(%rsp)                 # 8-byte Spill
	movq	%rcx, 152(%rsp)                 # 8-byte Spill
	leaq	(%r10,%rcx,4), %rax
	movq	%rax, 144(%rsp)                 # 8-byte Spill
	movq	%rbp, 160(%rsp)                 # 8-byte Spill
	leaq	(%r12,%rbp,4), %rax
	movq	%rax, 136(%rsp)                 # 8-byte Spill
	movq	%r9, 72(%rsp)                   # 8-byte Spill
	leaq	(%r10,%r9,4), %rax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movq	%rdx, 8(%rsp)                   # 8-byte Spill
	leaq	(%r12,%rdx,4), %rax
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	xorl	%esi, %esi
	jmp	.LBB7_2
	.p2align	4, 0x90
.LBB7_132:                              # %"end for head2_relu.s0.n.n2.2"
                                        #   in Loop: Header=BB7_2 Depth=1
	movq	112(%rsp), %rsi                 # 8-byte Reload
	incq	%rsi
	movq	-8(%rsp), %r14                  # 8-byte Reload
	movq	104(%rsp), %rcx                 # 8-byte Reload
	addl	%ecx, %r14d
	movq	-16(%rsp), %rbx                 # 8-byte Reload
	movq	96(%rsp), %rdx                  # 8-byte Reload
	addl	%edx, %ebx
	movq	-104(%rsp), %rax                # 8-byte Reload
	addl	%ecx, %eax
	movq	%rax, -104(%rsp)                # 8-byte Spill
	movq	-88(%rsp), %rax                 # 8-byte Reload
	addl	%edx, %eax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	movq	-112(%rsp), %rax                # 8-byte Reload
	addl	%ecx, %eax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	movq	-96(%rsp), %rax                 # 8-byte Reload
	addl	%edx, %eax
	movq	%rax, -96(%rsp)                 # 8-byte Spill
	leal	(%rcx,%r13), %eax
	movl	%eax, -80(%rsp)                 # 4-byte Spill
	leal	(%r11,%rdx), %eax
	movl	%eax, -56(%rsp)                 # 4-byte Spill
	cmpq	$7, %rsi
	movq	-64(%rsp), %r10                 # 8-byte Reload
	je	.LBB7_86
.LBB7_2:                                # %"for head2_relu.s0.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB7_12 Depth 2
                                        #     Child Loop BB7_15 Depth 2
                                        #     Child Loop BB7_18 Depth 2
                                        #       Child Loop BB7_26 Depth 3
                                        #       Child Loop BB7_29 Depth 3
                                        #       Child Loop BB7_22 Depth 3
                                        #     Child Loop BB7_88 Depth 2
                                        #     Child Loop BB7_91 Depth 2
                                        #     Child Loop BB7_94 Depth 2
                                        #       Child Loop BB7_100 Depth 3
                                        #       Child Loop BB7_103 Depth 3
                                        #       Child Loop BB7_106 Depth 3
                                        #     Child Loop BB7_112 Depth 2
                                        #     Child Loop BB7_115 Depth 2
                                        #     Child Loop BB7_118 Depth 2
                                        #       Child Loop BB7_124 Depth 3
                                        #       Child Loop BB7_127 Depth 3
                                        #       Child Loop BB7_130 Depth 3
	movq	%rsi, 112(%rsp)                 # 8-byte Spill
	movslq	%r14d, %r14
	movslq	%ebx, %rbx
	cmpl	$8, -24(%rsp)                   # 4-byte Folded Reload
	movl	-128(%rsp), %r12d               # 4-byte Reload
	jl	.LBB7_16
# %bb.3:                                # %"for head2_relu.s0.n.n.preheader"
                                        #   in Loop: Header=BB7_2 Depth=1
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r14,4), %rax
	movq	80(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rbx,4), %rcx
	cmpq	$3, 24(%rsp)                    # 8-byte Folded Reload
	jae	.LBB7_11
# %bb.4:                                #   in Loop: Header=BB7_2 Depth=1
	xorl	%edx, %edx
	jmp	.LBB7_13
	.p2align	4, 0x90
.LBB7_11:                               # %"for head2_relu.s0.n.n.preheader1"
                                        #   in Loop: Header=BB7_2 Depth=1
	movl	$96, %esi
	xorl	%edx, %edx
	movq	16(%rsp), %rdi                  # 8-byte Reload
	.p2align	4, 0x90
.LBB7_12:                               # %"for head2_relu.s0.n.n"
                                        #   Parent Loop BB7_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rcx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, -96(%rax,%rsi)
	vmovups	-64(%rcx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, -64(%rax,%rsi)
	vmovups	-32(%rcx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, -32(%rax,%rsi)
	vmovups	(%rcx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, (%rax,%rsi)
	addq	$4, %rdx
	subq	$-128, %rsi
	cmpq	%rdx, %rdi
	jne	.LBB7_12
.LBB7_13:                               # %"end for head2_relu.s0.n.n.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB7_2 Depth=1
	cmpq	$0, -72(%rsp)                   # 8-byte Folded Reload
	je	.LBB7_16
# %bb.14:                               # %"for head2_relu.s0.n.n.epil.preheader"
                                        #   in Loop: Header=BB7_2 Depth=1
	shlq	$5, %rdx
	movq	-72(%rsp), %rsi                 # 8-byte Reload
	.p2align	4, 0x90
.LBB7_15:                               # %"for head2_relu.s0.n.n.epil"
                                        #   Parent Loop BB7_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rcx,%rdx), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, (%rax,%rdx)
	addq	$32, %rdx
	decq	%rsi
	jne	.LBB7_15
.LBB7_16:                               # %"end for head2_relu.s0.n.n"
                                        #   in Loop: Header=BB7_2 Depth=1
	movq	%rbx, -16(%rsp)                 # 8-byte Spill
	movq	%r14, -8(%rsp)                  # 8-byte Spill
	movq	-104(%rsp), %rax                # 8-byte Reload
	cltq
	movq	%rax, -104(%rsp)                # 8-byte Spill
	movq	-88(%rsp), %rax                 # 8-byte Reload
	cltq
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	movq	-48(%rsp), %rax                 # 8-byte Reload
	cmpl	%eax, 48(%rsp)                  # 4-byte Folded Reload
	jge	.LBB7_32
# %bb.17:                               # %"for head2_relu.s0.n.n1.preheader"
                                        #   in Loop: Header=BB7_2 Depth=1
	xorl	%r11d, %r11d
	movl	-124(%rsp), %r8d                # 4-byte Reload
	movl	%r8d, %r14d
	movl	40(%rsp), %edx                  # 4-byte Reload
	movq	32(%rsp), %r15                  # 8-byte Reload
	jmp	.LBB7_18
	.p2align	4, 0x90
.LBB7_31:                               # %"end for head2_relu.s0.n.ni"
                                        #   in Loop: Header=BB7_18 Depth=2
	incq	%r15
	incl	%r11d
	leal	8(%r9), %edx
	addl	$-8, %r8d
	addl	$-8, %r14d
	cmpq	-48(%rsp), %r15                 # 8-byte Folded Reload
	je	.LBB7_32
.LBB7_18:                               # %"for head2_relu.s0.n.n1"
                                        #   Parent Loop BB7_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB7_26 Depth 3
                                        #       Child Loop BB7_29 Depth 3
                                        #       Child Loop BB7_22 Depth 3
	cmpl	$9, %r14d
	movl	$8, %ebx
	cmovll	%r14d, %ebx
	cmpl	$9, %r8d
	movl	$8, %r13d
	cmovll	%r8d, %r13d
	leal	(,%r11,8), %ecx
	movl	-124(%rsp), %eax                # 4-byte Reload
                                        # kill: def $eax killed $eax def $rax
	subl	%ecx, %eax
	cmpl	$9, %eax
	movl	$8, %ecx
	cmovll	%eax, %ecx
	cmpl	$9, %eax
	movl	$8, %esi
	cmovgel	%esi, %eax
	movslq	%edx, %r9
	leal	(,%r15,8), %edx
	movl	%r12d, %esi
	subl	%edx, %esi
	testl	%esi, %esi
	jle	.LBB7_31
# %bb.19:                               # %"for head2_relu.s0.n.ni.preheader"
                                        #   in Loop: Header=BB7_18 Depth=2
	cmpl	$15, %eax
	ja	.LBB7_23
# %bb.20:                               #   in Loop: Header=BB7_18 Depth=2
	xorl	%edi, %edi
	jmp	.LBB7_21
	.p2align	4, 0x90
.LBB7_23:                               # %vector.ph127
                                        #   in Loop: Header=BB7_18 Depth=2
	andl	$-16, %r13d
	addq	$-16, %r13
	shrq	$4, %r13
	andl	$-16, %ecx
	addq	$-16, %rcx
	movq	%rcx, %r12
	shrq	$4, %r12
	incq	%r12
	movl	%eax, %edi
	andl	$-16, %edi
	cmpq	$48, %rcx
	jae	.LBB7_25
# %bb.24:                               #   in Loop: Header=BB7_18 Depth=2
	xorl	%ebp, %ebp
	jmp	.LBB7_27
	.p2align	4, 0x90
.LBB7_25:                               # %vector.ph127.new
                                        #   in Loop: Header=BB7_18 Depth=2
	movq	-104(%rsp), %rcx                # 8-byte Reload
	addq	%r9, %rcx
	movq	-32(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r9), %rcx
	movq	-40(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx,4), %rcx
	leaq	1(%r13), %rsi
	andq	$-4, %rsi
	negq	%rsi
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB7_26:                               # %vector.body122
                                        #   Parent Loop BB7_2 Depth=1
                                        #     Parent Loop BB7_18 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rcx,%rbp,4), %ymm2
	vmovups	-192(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rdx,%rbp,4)
	vmovups	%ymm3, -192(%rdx,%rbp,4)
	vmovups	-160(%rcx,%rbp,4), %ymm2
	vmovups	-128(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rdx,%rbp,4)
	vmovups	%ymm3, -128(%rdx,%rbp,4)
	vmovups	-96(%rcx,%rbp,4), %ymm2
	vmovups	-64(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rdx,%rbp,4)
	vmovups	%ymm3, -64(%rdx,%rbp,4)
	vmovups	-32(%rcx,%rbp,4), %ymm2
	vmovups	(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdx,%rbp,4)
	vmovups	%ymm3, (%rdx,%rbp,4)
	addq	$64, %rbp
	addq	$4, %rsi
	jne	.LBB7_26
.LBB7_27:                               # %middle.block120.unr-lcssa
                                        #   in Loop: Header=BB7_18 Depth=2
	testb	$3, %r12b
	movq	-120(%rsp), %r12                # 8-byte Reload
	je	.LBB7_30
# %bb.28:                               # %vector.body122.epil.preheader
                                        #   in Loop: Header=BB7_18 Depth=2
	incb	%r13b
	movzbl	%r13b, %ecx
	andl	$3, %ecx
	negq	%rcx
	movq	-104(%rsp), %rdx                # 8-byte Reload
	addq	%rbp, %rdx
	addq	%r9, %rdx
	leaq	(%r10,%rdx,4), %rdx
	addq	-88(%rsp), %rbp                 # 8-byte Folded Reload
	addq	%r9, %rbp
	leaq	(%r12,%rbp,4), %rsi
	movl	$32, %ebp
	.p2align	4, 0x90
.LBB7_29:                               # %vector.body122.epil
                                        #   Parent Loop BB7_2 Depth=1
                                        #     Parent Loop BB7_18 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rsi,%rbp), %ymm2
	vmovups	(%rsi,%rbp), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdx,%rbp)
	vmovups	%ymm3, (%rdx,%rbp)
	addq	$64, %rbp
	incq	%rcx
	jne	.LBB7_29
.LBB7_30:                               # %middle.block120
                                        #   in Loop: Header=BB7_18 Depth=2
	cmpq	%rax, %rdi
	movl	-128(%rsp), %r12d               # 4-byte Reload
	je	.LBB7_31
.LBB7_21:                               # %"for head2_relu.s0.n.ni.preheader136"
                                        #   in Loop: Header=BB7_18 Depth=2
	subq	%rdi, %rbx
	movq	-104(%rsp), %rax                # 8-byte Reload
	addq	%rdi, %rax
	addq	%r9, %rax
	leaq	(%r10,%rax,4), %rax
	addq	-88(%rsp), %rdi                 # 8-byte Folded Reload
	addq	%r9, %rdi
	movq	-120(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rdi,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB7_22:                               # %"for head2_relu.s0.n.ni"
                                        #   Parent Loop BB7_2 Depth=1
                                        #     Parent Loop BB7_18 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rdx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm1, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rbx
	jne	.LBB7_22
	jmp	.LBB7_31
	.p2align	4, 0x90
.LBB7_32:                               # %"end for head2_relu.s0.n.n2"
                                        #   in Loop: Header=BB7_2 Depth=1
	cmpl	$7, -24(%rsp)                   # 4-byte Folded Reload
	movq	-8(%rsp), %rbx                  # 8-byte Reload
	movq	-16(%rsp), %r8                  # 8-byte Reload
	jle	.LBB7_92
# %bb.33:                               # %"for head2_relu.s0.n.n.preheader.1"
                                        #   in Loop: Header=BB7_2 Depth=1
	cmpq	$3, 24(%rsp)                    # 8-byte Folded Reload
	jae	.LBB7_87
# %bb.34:                               #   in Loop: Header=BB7_2 Depth=1
	xorl	%eax, %eax
	jmp	.LBB7_89
	.p2align	4, 0x90
.LBB7_87:                               # %"for head2_relu.s0.n.n.1.preheader"
                                        #   in Loop: Header=BB7_2 Depth=1
	movq	144(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rcx
	movq	136(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r8,4), %rdx
	movl	$96, %esi
	xorl	%eax, %eax
	movq	16(%rsp), %rdi                  # 8-byte Reload
	.p2align	4, 0x90
.LBB7_88:                               # %"for head2_relu.s0.n.n.1"
                                        #   Parent Loop BB7_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rdx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, -96(%rcx,%rsi)
	vmovups	-64(%rdx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, -64(%rcx,%rsi)
	vmovups	-32(%rdx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, -32(%rcx,%rsi)
	vmovups	(%rdx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, (%rcx,%rsi)
	addq	$4, %rax
	subq	$-128, %rsi
	cmpq	%rax, %rdi
	jne	.LBB7_88
.LBB7_89:                               # %"end for head2_relu.s0.n.n.1.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB7_2 Depth=1
	cmpq	$0, -72(%rsp)                   # 8-byte Folded Reload
	je	.LBB7_92
# %bb.90:                               # %"for head2_relu.s0.n.n.1.epil.preheader"
                                        #   in Loop: Header=BB7_2 Depth=1
	movq	152(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,8), %rcx
	addq	%rbx, %rcx
	leaq	(%r10,%rcx,4), %rcx
	movq	160(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,8), %rax
	addq	%r8, %rax
	movq	-120(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	xorl	%edx, %edx
	movq	-72(%rsp), %rsi                 # 8-byte Reload
	.p2align	4, 0x90
.LBB7_91:                               # %"for head2_relu.s0.n.n.1.epil"
                                        #   Parent Loop BB7_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rax,%rdx), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, (%rcx,%rdx)
	addq	$32, %rdx
	decq	%rsi
	jne	.LBB7_91
.LBB7_92:                               # %"end for head2_relu.s0.n.n.1"
                                        #   in Loop: Header=BB7_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	cltq
	movq	%rax, -112(%rsp)                # 8-byte Spill
	movq	-96(%rsp), %rax                 # 8-byte Reload
	cltq
	movq	%rax, -96(%rsp)                 # 8-byte Spill
	movq	-48(%rsp), %rax                 # 8-byte Reload
	cmpl	%eax, 48(%rsp)                  # 4-byte Folded Reload
	jge	.LBB7_108
# %bb.93:                               # %"for head2_relu.s0.n.n1.preheader.1"
                                        #   in Loop: Header=BB7_2 Depth=1
	xorl	%r9d, %r9d
	movl	-124(%rsp), %r8d                # 4-byte Reload
	movl	%r8d, %r14d
	movl	40(%rsp), %edx                  # 4-byte Reload
	movq	32(%rsp), %r15                  # 8-byte Reload
	jmp	.LBB7_94
	.p2align	4, 0x90
.LBB7_107:                              # %"end for head2_relu.s0.n.ni.1"
                                        #   in Loop: Header=BB7_94 Depth=2
	incq	%r15
	incl	%r9d
	leal	8(%r11), %edx
	addl	$-8, %r8d
	addl	$-8, %r14d
	cmpq	-48(%rsp), %r15                 # 8-byte Folded Reload
	je	.LBB7_108
.LBB7_94:                               # %"for head2_relu.s0.n.n1.1"
                                        #   Parent Loop BB7_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB7_100 Depth 3
                                        #       Child Loop BB7_103 Depth 3
                                        #       Child Loop BB7_106 Depth 3
	cmpl	$9, %r14d
	movl	$8, %ebx
	cmovll	%r14d, %ebx
	cmpl	$9, %r8d
	movl	$8, %r13d
	cmovll	%r8d, %r13d
	leal	(,%r9,8), %ecx
	movl	-124(%rsp), %eax                # 4-byte Reload
                                        # kill: def $eax killed $eax def $rax
	subl	%ecx, %eax
	cmpl	$9, %eax
	movl	$8, %ecx
	cmovll	%eax, %ecx
	cmpl	$9, %eax
	movl	$8, %esi
	cmovgel	%esi, %eax
	movslq	%edx, %r11
	leal	(,%r15,8), %edx
	movl	%r12d, %esi
	subl	%edx, %esi
	testl	%esi, %esi
	jle	.LBB7_107
# %bb.95:                               # %"for head2_relu.s0.n.ni.preheader.1"
                                        #   in Loop: Header=BB7_94 Depth=2
	cmpl	$16, %eax
	jae	.LBB7_97
# %bb.96:                               #   in Loop: Header=BB7_94 Depth=2
	xorl	%edi, %edi
	jmp	.LBB7_105
	.p2align	4, 0x90
.LBB7_97:                               # %vector.ph111
                                        #   in Loop: Header=BB7_94 Depth=2
	andl	$-16, %r13d
	addq	$-16, %r13
	shrq	$4, %r13
	andl	$-16, %ecx
	addq	$-16, %rcx
	movq	%rcx, %r12
	shrq	$4, %r12
	incq	%r12
	movl	%eax, %edi
	andl	$-16, %edi
	cmpq	$48, %rcx
	jae	.LBB7_99
# %bb.98:                               #   in Loop: Header=BB7_94 Depth=2
	xorl	%ebp, %ebp
	jmp	.LBB7_101
	.p2align	4, 0x90
.LBB7_99:                               # %vector.ph111.new
                                        #   in Loop: Header=BB7_94 Depth=2
	movq	-112(%rsp), %rcx                # 8-byte Reload
	addq	%r11, %rcx
	movq	-32(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	movq	-96(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r11), %rcx
	movq	-40(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx,4), %rcx
	leaq	1(%r13), %rsi
	andq	$-4, %rsi
	negq	%rsi
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB7_100:                              # %vector.body106
                                        #   Parent Loop BB7_2 Depth=1
                                        #     Parent Loop BB7_94 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rcx,%rbp,4), %ymm2
	vmovups	-192(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rdx,%rbp,4)
	vmovups	%ymm3, -192(%rdx,%rbp,4)
	vmovups	-160(%rcx,%rbp,4), %ymm2
	vmovups	-128(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rdx,%rbp,4)
	vmovups	%ymm3, -128(%rdx,%rbp,4)
	vmovups	-96(%rcx,%rbp,4), %ymm2
	vmovups	-64(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rdx,%rbp,4)
	vmovups	%ymm3, -64(%rdx,%rbp,4)
	vmovups	-32(%rcx,%rbp,4), %ymm2
	vmovups	(%rcx,%rbp,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdx,%rbp,4)
	vmovups	%ymm3, (%rdx,%rbp,4)
	addq	$64, %rbp
	addq	$4, %rsi
	jne	.LBB7_100
.LBB7_101:                              # %middle.block104.unr-lcssa
                                        #   in Loop: Header=BB7_94 Depth=2
	testb	$3, %r12b
	movq	-120(%rsp), %r12                # 8-byte Reload
	je	.LBB7_104
# %bb.102:                              # %vector.body106.epil.preheader
                                        #   in Loop: Header=BB7_94 Depth=2
	incb	%r13b
	movzbl	%r13b, %ecx
	andl	$3, %ecx
	negq	%rcx
	movq	-112(%rsp), %rdx                # 8-byte Reload
	addq	%rbp, %rdx
	addq	%r11, %rdx
	leaq	(%r10,%rdx,4), %rdx
	addq	-96(%rsp), %rbp                 # 8-byte Folded Reload
	addq	%r11, %rbp
	leaq	(%r12,%rbp,4), %rsi
	movl	$32, %ebp
	.p2align	4, 0x90
.LBB7_103:                              # %vector.body106.epil
                                        #   Parent Loop BB7_2 Depth=1
                                        #     Parent Loop BB7_94 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rsi,%rbp), %ymm2
	vmovups	(%rsi,%rbp), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdx,%rbp)
	vmovups	%ymm3, (%rdx,%rbp)
	addq	$64, %rbp
	incq	%rcx
	jne	.LBB7_103
.LBB7_104:                              # %middle.block104
                                        #   in Loop: Header=BB7_94 Depth=2
	cmpq	%rax, %rdi
	movl	-128(%rsp), %r12d               # 4-byte Reload
	je	.LBB7_107
.LBB7_105:                              # %"for head2_relu.s0.n.ni.1.preheader"
                                        #   in Loop: Header=BB7_94 Depth=2
	subq	%rdi, %rbx
	movq	-112(%rsp), %rax                # 8-byte Reload
	addq	%rdi, %rax
	addq	%r11, %rax
	leaq	(%r10,%rax,4), %rax
	addq	-96(%rsp), %rdi                 # 8-byte Folded Reload
	addq	%r11, %rdi
	movq	-120(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rdi,4), %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB7_106:                              # %"for head2_relu.s0.n.ni.1"
                                        #   Parent Loop BB7_2 Depth=1
                                        #     Parent Loop BB7_94 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rdx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm1, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rbx
	jne	.LBB7_106
	jmp	.LBB7_107
	.p2align	4, 0x90
.LBB7_108:                              # %"end for head2_relu.s0.n.n2.1"
                                        #   in Loop: Header=BB7_2 Depth=1
	movslq	-80(%rsp), %r13                 # 4-byte Folded Reload
	movslq	-56(%rsp), %r11                 # 4-byte Folded Reload
	cmpl	$8, -24(%rsp)                   # 4-byte Folded Reload
	movq	(%rsp), %r8                     # 8-byte Reload
	movq	-8(%rsp), %rdi                  # 8-byte Reload
	movq	-16(%rsp), %r9                  # 8-byte Reload
	jl	.LBB7_116
# %bb.109:                              # %"for head2_relu.s0.n.n.preheader.2"
                                        #   in Loop: Header=BB7_2 Depth=1
	cmpq	$3, 24(%rsp)                    # 8-byte Folded Reload
	jae	.LBB7_111
# %bb.110:                              #   in Loop: Header=BB7_2 Depth=1
	xorl	%eax, %eax
	jmp	.LBB7_113
	.p2align	4, 0x90
.LBB7_111:                              # %"for head2_relu.s0.n.n.2.preheader"
                                        #   in Loop: Header=BB7_2 Depth=1
	movq	128(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %rcx
	movq	120(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r9,4), %rdx
	movl	$96, %esi
	xorl	%eax, %eax
	movq	16(%rsp), %rbx                  # 8-byte Reload
	.p2align	4, 0x90
.LBB7_112:                              # %"for head2_relu.s0.n.n.2"
                                        #   Parent Loop BB7_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%rdx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, -96(%rcx,%rsi)
	vmovups	-64(%rdx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, -64(%rcx,%rsi)
	vmovups	-32(%rdx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, -32(%rcx,%rsi)
	vmovups	(%rdx,%rsi), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, (%rcx,%rsi)
	addq	$4, %rax
	subq	$-128, %rsi
	cmpq	%rax, %rbx
	jne	.LBB7_112
.LBB7_113:                              # %"end for head2_relu.s0.n.n.2.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB7_2 Depth=1
	cmpq	$0, -72(%rsp)                   # 8-byte Folded Reload
	je	.LBB7_116
# %bb.114:                              # %"for head2_relu.s0.n.n.2.epil.preheader"
                                        #   in Loop: Header=BB7_2 Depth=1
	movq	72(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax,8), %rcx
	addq	%rdi, %rcx
	leaq	(%r10,%rcx,4), %rcx
	movq	8(%rsp), %rdx                   # 8-byte Reload
	leaq	(%rdx,%rax,8), %rax
	addq	%r9, %rax
	movq	-120(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	xorl	%edx, %edx
	movq	-72(%rsp), %rsi                 # 8-byte Reload
	.p2align	4, 0x90
.LBB7_115:                              # %"for head2_relu.s0.n.n.2.epil"
                                        #   Parent Loop BB7_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rax,%rdx), %ymm2
	vmaxps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, (%rcx,%rdx)
	addq	$32, %rdx
	decq	%rsi
	jne	.LBB7_115
.LBB7_116:                              # %"end for head2_relu.s0.n.n.2"
                                        #   in Loop: Header=BB7_2 Depth=1
	cmpl	%r8d, 48(%rsp)                  # 4-byte Folded Reload
	jge	.LBB7_132
# %bb.117:                              # %"for head2_relu.s0.n.n1.preheader.2"
                                        #   in Loop: Header=BB7_2 Depth=1
	xorl	%r8d, %r8d
	movl	-124(%rsp), %r14d               # 4-byte Reload
	movl	%r14d, %ebp
	movl	40(%rsp), %edi                  # 4-byte Reload
	movq	32(%rsp), %r10                  # 8-byte Reload
	movq	%r11, 64(%rsp)                  # 8-byte Spill
	movq	%r13, 56(%rsp)                  # 8-byte Spill
	jmp	.LBB7_118
	.p2align	4, 0x90
.LBB7_131:                              # %"end for head2_relu.s0.n.ni.2"
                                        #   in Loop: Header=BB7_118 Depth=2
	incq	%r10
	incl	%r8d
	leal	8(%rdx), %edi
	addl	$-8, %r14d
	addl	$-8, %ebp
	cmpq	-48(%rsp), %r10                 # 8-byte Folded Reload
	je	.LBB7_132
.LBB7_118:                              # %"for head2_relu.s0.n.n1.2"
                                        #   Parent Loop BB7_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB7_124 Depth 3
                                        #       Child Loop BB7_127 Depth 3
                                        #       Child Loop BB7_130 Depth 3
	cmpl	$9, %ebp
	movl	$8, %ebx
	cmovll	%ebp, %ebx
	cmpl	$9, %r14d
	movl	$8, %r15d
	cmovll	%r14d, %r15d
	leal	(,%r8,8), %ecx
	movl	-124(%rsp), %eax                # 4-byte Reload
                                        # kill: def $eax killed $eax def $rax
	subl	%ecx, %eax
	cmpl	$9, %eax
	movl	$8, %esi
	cmovll	%eax, %esi
	cmpl	$9, %eax
	movl	$8, %ecx
	cmovgel	%ecx, %eax
	movslq	%edi, %rdx
	leal	(,%r10,8), %ecx
	movl	%r12d, %edi
	subl	%ecx, %edi
	testl	%edi, %edi
	jle	.LBB7_131
# %bb.119:                              # %"for head2_relu.s0.n.ni.preheader.2"
                                        #   in Loop: Header=BB7_118 Depth=2
	cmpl	$16, %eax
	jae	.LBB7_121
# %bb.120:                              #   in Loop: Header=BB7_118 Depth=2
	xorl	%r9d, %r9d
	jmp	.LBB7_129
	.p2align	4, 0x90
.LBB7_121:                              # %vector.ph95
                                        #   in Loop: Header=BB7_118 Depth=2
	movl	%ebp, -56(%rsp)                 # 4-byte Spill
	movq	%r8, -80(%rsp)                  # 8-byte Spill
	andl	$-16, %r15d
	addq	$-16, %r15
	shrq	$4, %r15
	andl	$-16, %esi
	addq	$-16, %rsi
	movq	%rsi, %r8
	shrq	$4, %r8
	incq	%r8
	movl	%eax, %r9d
	andl	$-16, %r9d
	cmpq	$48, %rsi
	jae	.LBB7_123
# %bb.122:                              #   in Loop: Header=BB7_118 Depth=2
	xorl	%r12d, %r12d
	jmp	.LBB7_125
	.p2align	4, 0x90
.LBB7_123:                              # %vector.ph95.new
                                        #   in Loop: Header=BB7_118 Depth=2
	movq	%rdx, %rdi
	leaq	(%rdx,%r13), %rcx
	movq	-32(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx,4), %r13
	leaq	(%r11,%rdx), %rcx
	movq	-40(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx,4), %r11
	leaq	1(%r15), %rsi
	andq	$-4, %rsi
	negq	%rsi
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB7_124:                              # %vector.body92
                                        #   Parent Loop BB7_2 Depth=1
                                        #     Parent Loop BB7_118 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-224(%r11,%r12,4), %ymm2
	vmovups	-192(%r11,%r12,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -224(%r13,%r12,4)
	vmovups	%ymm3, -192(%r13,%r12,4)
	vmovups	-160(%r11,%r12,4), %ymm2
	vmovups	-128(%r11,%r12,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -160(%r13,%r12,4)
	vmovups	%ymm3, -128(%r13,%r12,4)
	vmovups	-96(%r11,%r12,4), %ymm2
	vmovups	-64(%r11,%r12,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -96(%r13,%r12,4)
	vmovups	%ymm3, -64(%r13,%r12,4)
	vmovups	-32(%r11,%r12,4), %ymm2
	vmovups	(%r11,%r12,4), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -32(%r13,%r12,4)
	vmovups	%ymm3, (%r13,%r12,4)
	addq	$64, %r12
	addq	$4, %rsi
	jne	.LBB7_124
.LBB7_125:                              # %middle.block90.unr-lcssa
                                        #   in Loop: Header=BB7_118 Depth=2
	testb	$3, %r8b
	movq	64(%rsp), %r11                  # 8-byte Reload
	movq	56(%rsp), %r13                  # 8-byte Reload
	je	.LBB7_128
# %bb.126:                              # %vector.body92.epil.preheader
                                        #   in Loop: Header=BB7_118 Depth=2
	incb	%r15b
	movzbl	%r15b, %esi
	andl	$3, %esi
	negq	%rsi
	leaq	(%r12,%r13), %rcx
	addq	%rdx, %rcx
	movq	-64(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%rcx,4), %rdi
	addq	%r11, %r12
	addq	%rdx, %r12
	movq	-120(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%r12,4), %rcx
	movl	$32, %ebp
	.p2align	4, 0x90
.LBB7_127:                              # %vector.body92.epil
                                        #   Parent Loop BB7_2 Depth=1
                                        #     Parent Loop BB7_118 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rcx,%rbp), %ymm2
	vmovups	(%rcx,%rbp), %ymm3
	vmaxps	%ymm0, %ymm2, %ymm2
	vmaxps	%ymm0, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rdi,%rbp)
	vmovups	%ymm3, (%rdi,%rbp)
	addq	$64, %rbp
	incq	%rsi
	jne	.LBB7_127
.LBB7_128:                              # %middle.block90
                                        #   in Loop: Header=BB7_118 Depth=2
	cmpq	%rax, %r9
	movl	-128(%rsp), %r12d               # 4-byte Reload
	movq	-80(%rsp), %r8                  # 8-byte Reload
	movl	-56(%rsp), %ebp                 # 4-byte Reload
	je	.LBB7_131
.LBB7_129:                              # %"for head2_relu.s0.n.ni.2.preheader"
                                        #   in Loop: Header=BB7_118 Depth=2
	subq	%r9, %rbx
	leaq	(%r9,%r13), %rax
	addq	%rdx, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	addq	%r11, %r9
	addq	%rdx, %r9
	movq	-120(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%r9,4), %rcx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB7_130:                              # %"for head2_relu.s0.n.ni.2"
                                        #   Parent Loop BB7_2 Depth=1
                                        #     Parent Loop BB7_118 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rsi,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm1, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rbx
	jne	.LBB7_130
	jmp	.LBB7_131
.LBB7_5:                                # %false_bb
	movl	%edi, -128(%rsp)                # 4-byte Spill
	movq	%r11, (%rsp)                    # 8-byte Spill
	movl	20(%rdx), %ecx
	movl	%esi, %eax
	sarl	$3, %eax
	movl	%esi, %edx
	andl	$-8, %edx
	subl	%eax, %edx
	movq	%rdx, 40(%rsp)                  # 8-byte Spill
	subl	%edx, %ecx
	jle	.LBB7_86
# %bb.6:                                # %"for head2_relu.s0.w.wi5.preheader"
	movq	%r9, %rbx
	movq	%rbp, %r13
	andl	$7, %esi
	movq	(%rsp), %rdx                    # 8-byte Reload
	addl	$7, %edx
	sarl	$3, %edx
	movq	%rdx, -88(%rsp)                 # 8-byte Spill
	cmpl	$8, %ecx
	movl	$7, %edx
	cmovll	%ecx, %edx
	movq	%rdx, 32(%rsp)                  # 8-byte Spill
	leal	(%rsi,%rsi,2), %ecx
	movl	%r15d, %r9d
	movl	%r15d, %r8d
	movq	%rbx, %rdi
	leal	(,%rbx,8), %ebp
	leal	(%rbp,%rbp,2), %ebp
	movl	%ebp, 24(%rsp)                  # 4-byte Spill
	movl	%ecx, %ecx
	movq	%rcx, %rbp
	imulq	%r15, %rbp
	movq	%rbp, 16(%rsp)                  # 8-byte Spill
	movq	%rcx, %rbp
	imulq	%rbx, %rbp
	movq	%rbp, 104(%rsp)                 # 8-byte Spill
	leaq	1(%rcx), %rbp
	movq	%rbp, %rbx
	imulq	%r15, %rbx
	movq	%rbx, 88(%rsp)                  # 8-byte Spill
	imulq	%rdi, %rbp
	movq	%rbp, 96(%rsp)                  # 8-byte Spill
	addq	$2, %rcx
	imulq	%rcx, %r15
	movq	%r15, 48(%rsp)                  # 8-byte Spill
	imulq	%rdi, %rcx
	movq	%rcx, 112(%rsp)                 # 8-byte Spill
	imull	$168, %eax, %ebp
	leal	(%rsi,%rsi,2), %ebx
	leal	(%rbx,%rbp), %r15d
	movl	%edi, %r11d
	imull	%r15d, %r11d
	leal	(%rbx,%rbp), %r14d
	incl	%r14d
	leal	(%rbx,%rbp), %esi
	addl	$2, %esi
	leal	(,%rdi,8), %edx
	imull	%r9d, %r15d
	movl	%edi, %eax
	imull	%esi, %edi
	movq	%rdi, %rcx
	imull	%r9d, %esi
	leal	(,%r9,8), %edi
	leal	(%rdi,%rdi,2), %edi
	movl	%edi, 80(%rsp)                  # 4-byte Spill
	movl	-128(%rsp), %ebx                # 4-byte Reload
	subl	%r13d, %ebx
	leal	(%rdx,%rdx,2), %edx
	movq	%rdx, 8(%rsp)                   # 8-byte Spill
	imull	%r14d, %eax
	imull	%r8d, %r14d
	leal	(,%r8,8), %edx
	leal	(%rdx,%rdx,2), %edx
	movq	%rdx, 72(%rsp)                  # 8-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	leaq	224(%r10), %rdx
	movq	%rdx, -72(%rsp)                 # 8-byte Spill
	leaq	224(%r12), %rdx
	movq	%rdx, -8(%rsp)                  # 8-byte Spill
	xorl	%edx, %edx
	movq	%rdx, -24(%rsp)                 # 8-byte Spill
	movl	$8, %ebp
	movl	%ebx, -124(%rsp)                # 4-byte Spill
	movq	%r13, -48(%rsp)                 # 8-byte Spill
	jmp	.LBB7_7
	.p2align	4, 0x90
.LBB7_85:                               # %"end for head2_relu.s0.n.n12.2"
                                        #   in Loop: Header=BB7_7 Depth=1
	movq	-24(%rsp), %rdi                 # 8-byte Reload
	incq	%rdi
	movq	8(%rsp), %rcx                   # 8-byte Reload
	movq	-56(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%rcx), %r11d
	movq	72(%rsp), %rdx                  # 8-byte Reload
	movq	-16(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%rdx), %r15d
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%rcx), %eax
	movq	-112(%rsp), %rsi                # 8-byte Reload
	leal	(%rsi,%rdx), %r14d
	movq	-104(%rsp), %rsi                # 8-byte Reload
	leal	(%rsi,%rcx), %ecx
	movq	-96(%rsp), %rsi                 # 8-byte Reload
	addl	%edx, %esi
	movq	%rdi, %rdx
	movq	%rdi, -24(%rsp)                 # 8-byte Spill
	cmpq	32(%rsp), %rdi                  # 8-byte Folded Reload
	je	.LBB7_86
.LBB7_7:                                # %"for head2_relu.s0.w.wi5"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB7_9 Depth 2
                                        #       Child Loop BB7_43 Depth 3
                                        #       Child Loop BB7_46 Depth 3
                                        #       Child Loop BB7_39 Depth 3
                                        #     Child Loop BB7_51 Depth 2
                                        #       Child Loop BB7_59 Depth 3
                                        #       Child Loop BB7_62 Depth 3
                                        #       Child Loop BB7_65 Depth 3
                                        #     Child Loop BB7_69 Depth 2
                                        #       Child Loop BB7_77 Depth 3
                                        #       Child Loop BB7_80 Depth 3
                                        #       Child Loop BB7_83 Depth 3
	movslq	%ecx, %rcx
	movq	%rcx, -104(%rsp)                # 8-byte Spill
	movslq	%esi, %rcx
	movq	%rcx, -96(%rsp)                 # 8-byte Spill
	cltq
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	movslq	%r14d, %rax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	movslq	%r11d, %rax
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	movslq	%r15d, %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	cmpl	$0, (%rsp)                      # 4-byte Folded Reload
	movl	-128(%rsp), %r14d               # 4-byte Reload
	movq	-88(%rsp), %r15                 # 8-byte Reload
	jle	.LBB7_85
# %bb.8:                                # %"for head2_relu.s0.n.n11.preheader"
                                        #   in Loop: Header=BB7_7 Depth=1
	movq	40(%rsp), %rax                  # 8-byte Reload
	movq	-24(%rsp), %rcx                 # 8-byte Reload
	addl	%ecx, %eax
	movl	24(%rsp), %ecx                  # 4-byte Reload
	imull	%eax, %ecx
	imull	80(%rsp), %eax                  # 4-byte Folded Reload
	movslq	%eax, %rdx
	movslq	%ecx, %rcx
	movq	16(%rsp), %rax                  # 8-byte Reload
	movq	%rdx, -32(%rsp)                 # 8-byte Spill
	leaq	(%rax,%rdx), %rax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movq	104(%rsp), %rax                 # 8-byte Reload
	movq	%rcx, -40(%rsp)                 # 8-byte Spill
	addq	%rcx, %rax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movl	%ebx, %esi
	xorl	%ecx, %ecx
	movq	-16(%rsp), %r11                 # 8-byte Reload
	jmp	.LBB7_9
	.p2align	4, 0x90
.LBB7_10:                               # %true_bb15
                                        #   in Loop: Header=BB7_9 Depth=2
	movl	%eax, %eax
	movq	64(%rsp), %rdx                  # 8-byte Reload
	addq	%rax, %rdx
	vmovups	(%r12,%rdx,4), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	addq	56(%rsp), %rax                  # 8-byte Folded Reload
	vmovups	%ymm2, (%r10,%rax,4)
.LBB7_48:                               # %after_bb17
                                        #   in Loop: Header=BB7_9 Depth=2
	incq	%rcx
	leal	8(%r8), %eax
	addl	$-8, %esi
	cmpq	%r15, %rcx
	movq	-64(%rsp), %r10                 # 8-byte Reload
	je	.LBB7_49
.LBB7_9:                                # %"for head2_relu.s0.n.n11"
                                        #   Parent Loop BB7_7 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB7_43 Depth 3
                                        #       Child Loop BB7_46 Depth 3
                                        #       Child Loop BB7_39 Depth 3
	cmpl	$9, %esi
	movl	$8, %r9d
	cmovll	%esi, %r9d
	movl	%eax, %r8d
	leal	(,%rcx,8), %eax
	movl	%ebx, %edi
	subl	%eax, %edi
	cmpl	$9, %edi
	cmovgel	%ebp, %edi
	leal	8(%r13,%rcx,8), %edx
	cmpl	%r14d, %edx
	jle	.LBB7_10
# %bb.35:                               # %false_bb16
                                        #   in Loop: Header=BB7_9 Depth=2
	leal	(%r13,%rcx,8), %eax
	cmpl	%eax, %r14d
	jle	.LBB7_48
# %bb.36:                               # %"for head2_relu.s0.n.ni18.preheader"
                                        #   in Loop: Header=BB7_9 Depth=2
	cmpl	$15, %edi
	ja	.LBB7_40
# %bb.37:                               #   in Loop: Header=BB7_9 Depth=2
	xorl	%r10d, %r10d
	jmp	.LBB7_38
	.p2align	4, 0x90
.LBB7_40:                               # %vector.ph81
                                        #   in Loop: Header=BB7_9 Depth=2
	movl	%r9d, %r13d
	andl	$-16, %r13d
	addq	$-16, %r13
	shrq	$4, %r13
	movl	%edi, %eax
	andl	$-16, %eax
	addq	$-16, %rax
	movq	%rax, %r12
	shrq	$4, %r12
	incq	%r12
	movl	%edi, %r10d
	andl	$-16, %r10d
	cmpq	$48, %rax
	jae	.LBB7_42
# %bb.41:                               #   in Loop: Header=BB7_9 Depth=2
	xorl	%edx, %edx
	jmp	.LBB7_44
.LBB7_42:                               # %vector.ph81.new
                                        #   in Loop: Header=BB7_9 Depth=2
	movq	-56(%rsp), %rax                 # 8-byte Reload
	addq	%r8, %rax
	movq	-72(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %rbp
	leaq	(%r11,%r8), %rax
	movq	-8(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rdx,%rax,4), %rbx
	leaq	1(%r13), %r11
	andq	$-4, %r11
	negq	%r11
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB7_43:                               # %vector.body78
                                        #   Parent Loop BB7_7 Depth=1
                                        #     Parent Loop BB7_9 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rbx,%rdx,4), %ymm2
	vmovups	-192(%rbx,%rdx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rbp,%rdx,4)
	vmovups	%ymm3, -192(%rbp,%rdx,4)
	vmovups	-160(%rbx,%rdx,4), %ymm2
	vmovups	-128(%rbx,%rdx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rbp,%rdx,4)
	vmovups	%ymm3, -128(%rbp,%rdx,4)
	vmovups	-96(%rbx,%rdx,4), %ymm2
	vmovups	-64(%rbx,%rdx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rbp,%rdx,4)
	vmovups	%ymm3, -64(%rbp,%rdx,4)
	vmovups	-32(%rbx,%rdx,4), %ymm2
	vmovups	(%rbx,%rdx,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rbp,%rdx,4)
	vmovups	%ymm3, (%rbp,%rdx,4)
	addq	$64, %rdx
	addq	$4, %r11
	jne	.LBB7_43
.LBB7_44:                               # %middle.block76.unr-lcssa
                                        #   in Loop: Header=BB7_9 Depth=2
	testb	$3, %r12b
	movq	-120(%rsp), %r12                # 8-byte Reload
	movq	-16(%rsp), %r11                 # 8-byte Reload
	je	.LBB7_47
# %bb.45:                               # %vector.body78.epil.preheader
                                        #   in Loop: Header=BB7_9 Depth=2
	incb	%r13b
	movzbl	%r13b, %ebp
	andl	$3, %ebp
	negq	%rbp
	movq	-56(%rsp), %rax                 # 8-byte Reload
	addq	%rdx, %rax
	addq	%r8, %rax
	movq	-64(%rsp), %rbx                 # 8-byte Reload
	leaq	(%rbx,%rax,4), %rbx
	addq	%r11, %rdx
	addq	%r8, %rdx
	leaq	(%r12,%rdx,4), %rdx
	movl	$32, %eax
	.p2align	4, 0x90
.LBB7_46:                               # %vector.body78.epil
                                        #   Parent Loop BB7_7 Depth=1
                                        #     Parent Loop BB7_9 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rdx,%rax), %ymm2
	vmovups	(%rdx,%rax), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rbx,%rax)
	vmovups	%ymm3, (%rbx,%rax)
	addq	$64, %rax
	incq	%rbp
	jne	.LBB7_46
.LBB7_47:                               # %middle.block76
                                        #   in Loop: Header=BB7_9 Depth=2
	cmpq	%rdi, %r10
	movl	-128(%rsp), %r14d               # 4-byte Reload
	movq	-88(%rsp), %r15                 # 8-byte Reload
	movl	$8, %ebp
	movl	-124(%rsp), %ebx                # 4-byte Reload
	movq	-48(%rsp), %r13                 # 8-byte Reload
	je	.LBB7_48
.LBB7_38:                               # %"for head2_relu.s0.n.ni18.preheader137"
                                        #   in Loop: Header=BB7_9 Depth=2
	subq	%r10, %r9
	movq	-56(%rsp), %rax                 # 8-byte Reload
	addq	%r10, %rax
	addq	%r8, %rax
	movq	-64(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	addq	%r11, %r10
	addq	%r8, %r10
	leaq	(%r12,%r10,4), %rax
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB7_39:                               # %"for head2_relu.s0.n.ni18"
                                        #   Parent Loop BB7_7 Depth=1
                                        #     Parent Loop BB7_9 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovss	(%rax,%rdi,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rdx,%rdi,4)
	incq	%rdi
	cmpq	%rdi, %r9
	jne	.LBB7_39
	jmp	.LBB7_48
	.p2align	4, 0x90
.LBB7_49:                               # %"end for head2_relu.s0.n.n12"
                                        #   in Loop: Header=BB7_7 Depth=1
	cmpl	$0, (%rsp)                      # 4-byte Folded Reload
	jle	.LBB7_85
# %bb.50:                               # %"for head2_relu.s0.n.n11.preheader.1"
                                        #   in Loop: Header=BB7_7 Depth=1
	movq	88(%rsp), %rax                  # 8-byte Reload
	movq	-32(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rax,%rcx), %r8
	movq	96(%rsp), %rax                  # 8-byte Reload
	movq	-40(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rax,%rcx), %r9
	xorl	%eax, %eax
	movl	%ebx, %r10d
	xorl	%edx, %edx
	movq	%r8, 64(%rsp)                   # 8-byte Spill
	movq	%r9, 56(%rsp)                   # 8-byte Spill
	jmp	.LBB7_51
	.p2align	4, 0x90
.LBB7_52:                               # %true_bb15.1
                                        #   in Loop: Header=BB7_51 Depth=2
	movl	%ecx, %eax
	leaq	(%r8,%rax), %rcx
	movq	-120(%rsp), %rsi                # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	addq	%r9, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	vmovups	%ymm2, (%rcx,%rax,4)
.LBB7_66:                               # %after_bb17.1
                                        #   in Loop: Header=BB7_51 Depth=2
	incq	%rdx
	leal	8(%r12), %eax
	addl	$-8, %r10d
	cmpq	%r15, %rdx
	je	.LBB7_67
.LBB7_51:                               # %"for head2_relu.s0.n.n11.1"
                                        #   Parent Loop BB7_7 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB7_59 Depth 3
                                        #       Child Loop BB7_62 Depth 3
                                        #       Child Loop BB7_65 Depth 3
	cmpl	$9, %r10d
	movl	$8, %edi
	cmovll	%r10d, %edi
	movl	%eax, %r12d
	leal	(,%rdx,8), %ecx
	movl	%ebx, %eax
	subl	%ecx, %eax
	cmpl	$9, %eax
	cmovgel	%ebp, %eax
	leal	8(%r13,%rdx,8), %esi
	cmpl	%r14d, %esi
	jle	.LBB7_52
# %bb.53:                               # %false_bb16.1
                                        #   in Loop: Header=BB7_51 Depth=2
	leal	(%r13,%rdx,8), %ecx
	cmpl	%ecx, %r14d
	jle	.LBB7_66
# %bb.54:                               # %"for head2_relu.s0.n.ni18.preheader.1"
                                        #   in Loop: Header=BB7_51 Depth=2
	cmpl	$16, %eax
	jae	.LBB7_56
# %bb.55:                               #   in Loop: Header=BB7_51 Depth=2
	xorl	%r11d, %r11d
	jmp	.LBB7_64
	.p2align	4, 0x90
.LBB7_56:                               # %vector.ph67
                                        #   in Loop: Header=BB7_51 Depth=2
	movl	%edi, %r13d
	andl	$-16, %r13d
	addq	$-16, %r13
	shrq	$4, %r13
	movl	%eax, %ecx
	andl	$-16, %ecx
	addq	$-16, %rcx
	movq	%rcx, %r9
	shrq	$4, %r9
	incq	%r9
	movl	%eax, %r11d
	andl	$-16, %r11d
	cmpq	$48, %rcx
	jae	.LBB7_58
# %bb.57:                               #   in Loop: Header=BB7_51 Depth=2
	xorl	%r8d, %r8d
	jmp	.LBB7_60
.LBB7_58:                               # %vector.ph67.new
                                        #   in Loop: Header=BB7_51 Depth=2
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	addq	%r12, %rcx
	movq	-72(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx,4), %rsi
	movq	-112(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%r12), %rcx
	movq	-8(%rsp), %rbp                  # 8-byte Reload
	leaq	(%rbp,%rcx,4), %rbp
	leaq	1(%r13), %rbx
	andq	$-4, %rbx
	negq	%rbx
	xorl	%r8d, %r8d
	.p2align	4, 0x90
.LBB7_59:                               # %vector.body64
                                        #   Parent Loop BB7_7 Depth=1
                                        #     Parent Loop BB7_51 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rbp,%r8,4), %ymm2
	vmovups	-192(%rbp,%r8,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rsi,%r8,4)
	vmovups	%ymm3, -192(%rsi,%r8,4)
	vmovups	-160(%rbp,%r8,4), %ymm2
	vmovups	-128(%rbp,%r8,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rsi,%r8,4)
	vmovups	%ymm3, -128(%rsi,%r8,4)
	vmovups	-96(%rbp,%r8,4), %ymm2
	vmovups	-64(%rbp,%r8,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rsi,%r8,4)
	vmovups	%ymm3, -64(%rsi,%r8,4)
	vmovups	-32(%rbp,%r8,4), %ymm2
	vmovups	(%rbp,%r8,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rsi,%r8,4)
	vmovups	%ymm3, (%rsi,%r8,4)
	addq	$64, %r8
	addq	$4, %rbx
	jne	.LBB7_59
.LBB7_60:                               # %middle.block62.unr-lcssa
                                        #   in Loop: Header=BB7_51 Depth=2
	testb	$3, %r9b
	je	.LBB7_63
# %bb.61:                               # %vector.body64.epil.preheader
                                        #   in Loop: Header=BB7_51 Depth=2
	incb	%r13b
	movzbl	%r13b, %esi
	andl	$3, %esi
	negq	%rsi
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	addq	%r8, %rcx
	addq	%r12, %rcx
	movq	-64(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rcx,4), %rbp
	addq	-112(%rsp), %r8                 # 8-byte Folded Reload
	addq	%r12, %r8
	movq	-120(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%r8,4), %rbx
	movl	$32, %ecx
	.p2align	4, 0x90
.LBB7_62:                               # %vector.body64.epil
                                        #   Parent Loop BB7_7 Depth=1
                                        #     Parent Loop BB7_51 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rbx,%rcx), %ymm2
	vmovups	(%rbx,%rcx), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rbp,%rcx)
	vmovups	%ymm3, (%rbp,%rcx)
	addq	$64, %rcx
	incq	%rsi
	jne	.LBB7_62
.LBB7_63:                               # %middle.block62
                                        #   in Loop: Header=BB7_51 Depth=2
	cmpq	%rax, %r11
	movl	-128(%rsp), %r14d               # 4-byte Reload
	movq	-88(%rsp), %r15                 # 8-byte Reload
	movl	$8, %ebp
	movl	-124(%rsp), %ebx                # 4-byte Reload
	movq	-48(%rsp), %r13                 # 8-byte Reload
	movq	64(%rsp), %r8                   # 8-byte Reload
	movq	56(%rsp), %r9                   # 8-byte Reload
	je	.LBB7_66
.LBB7_64:                               # %"for head2_relu.s0.n.ni18.1.preheader"
                                        #   in Loop: Header=BB7_51 Depth=2
	subq	%r11, %rdi
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addq	%r11, %rax
	addq	%r12, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	addq	-112(%rsp), %r11                # 8-byte Folded Reload
	addq	%r12, %r11
	movq	-120(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%r11,4), %rcx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB7_65:                               # %"for head2_relu.s0.n.ni18.1"
                                        #   Parent Loop BB7_7 Depth=1
                                        #     Parent Loop BB7_51 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rsi,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rdi
	jne	.LBB7_65
	jmp	.LBB7_66
	.p2align	4, 0x90
.LBB7_67:                               # %"end for head2_relu.s0.n.n12.1"
                                        #   in Loop: Header=BB7_7 Depth=1
	cmpl	$0, (%rsp)                      # 4-byte Folded Reload
	movq	-120(%rsp), %r12                # 8-byte Reload
	movq	-64(%rsp), %r10                 # 8-byte Reload
	jle	.LBB7_85
# %bb.68:                               # %"for head2_relu.s0.n.n11.preheader.2"
                                        #   in Loop: Header=BB7_7 Depth=1
	movq	-32(%rsp), %rax                 # 8-byte Reload
	addq	48(%rsp), %rax                  # 8-byte Folded Reload
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	-40(%rsp), %rax                 # 8-byte Reload
	addq	112(%rsp), %rax                 # 8-byte Folded Reload
	movq	%rax, -40(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movl	%ebx, %r11d
	xorl	%edx, %edx
	jmp	.LBB7_69
	.p2align	4, 0x90
.LBB7_70:                               # %true_bb15.2
                                        #   in Loop: Header=BB7_69 Depth=2
	movl	%ecx, %eax
	movq	-32(%rsp), %rcx                 # 8-byte Reload
	addq	%rax, %rcx
	vmovups	(%r12,%rcx,4), %ymm2
	vmaxps	%ymm1, %ymm2, %ymm2
	addq	-40(%rsp), %rax                 # 8-byte Folded Reload
	vmovups	%ymm2, (%r10,%rax,4)
.LBB7_84:                               # %after_bb17.2
                                        #   in Loop: Header=BB7_69 Depth=2
	incq	%rdx
	leal	8(%r8), %eax
	addl	$-8, %r11d
	cmpq	%r15, %rdx
	movl	$8, %ebp
	movl	-124(%rsp), %ebx                # 4-byte Reload
	movq	-48(%rsp), %r13                 # 8-byte Reload
	je	.LBB7_85
.LBB7_69:                               # %"for head2_relu.s0.n.n11.2"
                                        #   Parent Loop BB7_7 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB7_77 Depth 3
                                        #       Child Loop BB7_80 Depth 3
                                        #       Child Loop BB7_83 Depth 3
	cmpl	$9, %r11d
	movl	$8, %edi
	cmovll	%r11d, %edi
	movl	%eax, %r8d
	leal	(,%rdx,8), %ecx
	movl	%ebx, %eax
	subl	%ecx, %eax
	cmpl	$9, %eax
	cmovgel	%ebp, %eax
	leal	8(%r13,%rdx,8), %esi
	cmpl	%r14d, %esi
	jle	.LBB7_70
# %bb.71:                               # %false_bb16.2
                                        #   in Loop: Header=BB7_69 Depth=2
	leal	(%r13,%rdx,8), %ecx
	cmpl	%ecx, %r14d
	jle	.LBB7_84
# %bb.72:                               # %"for head2_relu.s0.n.ni18.preheader.2"
                                        #   in Loop: Header=BB7_69 Depth=2
	cmpl	$16, %eax
	jae	.LBB7_74
# %bb.73:                               #   in Loop: Header=BB7_69 Depth=2
	xorl	%r13d, %r13d
	jmp	.LBB7_82
	.p2align	4, 0x90
.LBB7_74:                               # %vector.ph
                                        #   in Loop: Header=BB7_69 Depth=2
	movl	%edi, %r12d
	andl	$-16, %r12d
	addq	$-16, %r12
	shrq	$4, %r12
	movl	%eax, %ecx
	andl	$-16, %ecx
	addq	$-16, %rcx
	movq	%rcx, %r9
	shrq	$4, %r9
	incq	%r9
	movl	%eax, %r13d
	andl	$-16, %r13d
	cmpq	$48, %rcx
	jae	.LBB7_76
# %bb.75:                               #   in Loop: Header=BB7_69 Depth=2
	xorl	%esi, %esi
	jmp	.LBB7_78
.LBB7_76:                               # %vector.ph.new
                                        #   in Loop: Header=BB7_69 Depth=2
	movq	-104(%rsp), %rcx                # 8-byte Reload
	addq	%r8, %rcx
	movq	-72(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx,4), %rbp
	movq	-96(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r8), %rcx
	movq	-8(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rcx,4), %rbx
	leaq	1(%r12), %rcx
	andq	$-4, %rcx
	negq	%rcx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB7_77:                               # %vector.body
                                        #   Parent Loop BB7_7 Depth=1
                                        #     Parent Loop BB7_69 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rbx,%rsi,4), %ymm2
	vmovups	-192(%rbx,%rsi,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -224(%rbp,%rsi,4)
	vmovups	%ymm3, -192(%rbp,%rsi,4)
	vmovups	-160(%rbx,%rsi,4), %ymm2
	vmovups	-128(%rbx,%rsi,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -160(%rbp,%rsi,4)
	vmovups	%ymm3, -128(%rbp,%rsi,4)
	vmovups	-96(%rbx,%rsi,4), %ymm2
	vmovups	-64(%rbx,%rsi,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -96(%rbp,%rsi,4)
	vmovups	%ymm3, -64(%rbp,%rsi,4)
	vmovups	-32(%rbx,%rsi,4), %ymm2
	vmovups	(%rbx,%rsi,4), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rbp,%rsi,4)
	vmovups	%ymm3, (%rbp,%rsi,4)
	addq	$64, %rsi
	addq	$4, %rcx
	jne	.LBB7_77
.LBB7_78:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB7_69 Depth=2
	testb	$3, %r9b
	je	.LBB7_81
# %bb.79:                               # %vector.body.epil.preheader
                                        #   in Loop: Header=BB7_69 Depth=2
	incb	%r12b
	movzbl	%r12b, %ecx
	andl	$3, %ecx
	negq	%rcx
	movq	-104(%rsp), %rbp                # 8-byte Reload
	addq	%rsi, %rbp
	addq	%r8, %rbp
	leaq	(%r10,%rbp,4), %rbp
	addq	-96(%rsp), %rsi                 # 8-byte Folded Reload
	addq	%r8, %rsi
	movq	-120(%rsp), %rbx                # 8-byte Reload
	leaq	(%rbx,%rsi,4), %rsi
	movl	$32, %ebx
	.p2align	4, 0x90
.LBB7_80:                               # %vector.body.epil
                                        #   Parent Loop BB7_7 Depth=1
                                        #     Parent Loop BB7_69 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-32(%rsi,%rbx), %ymm2
	vmovups	(%rsi,%rbx), %ymm3
	vmaxps	%ymm1, %ymm2, %ymm2
	vmaxps	%ymm1, %ymm3, %ymm3
	vmovups	%ymm2, -32(%rbp,%rbx)
	vmovups	%ymm3, (%rbp,%rbx)
	addq	$64, %rbx
	incq	%rcx
	jne	.LBB7_80
.LBB7_81:                               # %middle.block
                                        #   in Loop: Header=BB7_69 Depth=2
	cmpq	%rax, %r13
	movl	-128(%rsp), %r14d               # 4-byte Reload
	movq	-120(%rsp), %r12                # 8-byte Reload
	movq	-88(%rsp), %r15                 # 8-byte Reload
	je	.LBB7_84
.LBB7_82:                               # %"for head2_relu.s0.n.ni18.2.preheader"
                                        #   in Loop: Header=BB7_69 Depth=2
	subq	%r13, %rdi
	movq	-104(%rsp), %rax                # 8-byte Reload
	addq	%r13, %rax
	addq	%r8, %rax
	leaq	(%r10,%rax,4), %rax
	addq	-96(%rsp), %r13                 # 8-byte Folded Reload
	addq	%r8, %r13
	leaq	(%r12,%r13,4), %rcx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB7_83:                               # %"for head2_relu.s0.n.ni18.2"
                                        #   Parent Loop BB7_7 Depth=1
                                        #     Parent Loop BB7_69 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx,%rsi,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rdi
	jne	.LBB7_83
	jmp	.LBB7_84
.LBB7_86:                               # %destructor_block
	xorl	%eax, %eax
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end7:
	.size	train_cost_model.par_for.head2_relu.s0.c.c.c, .Lfunc_end7-train_cost_model.par_for.head2_relu.s0.c.c.c
                                        # -- End function
	.section	.text.train_cost_model.par_for.conv1_stage2.s0.c.c.c,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.conv1_stage2.s0.c.c.c
	.type	train_cost_model.par_for.conv1_stage2.s0.c.c.c,@function
train_cost_model.par_for.conv1_stage2.s0.c.c.c: # @train_cost_model.par_for.conv1_stage2.s0.c.c.c
	.cfi_startproc
# %bb.0:                                # %entry
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$232, %rsp
	.cfi_def_cfa_offset 288
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
                                        # kill: def $esi killed $esi def $rsi
	movl	(%rdx), %eax
	movl	%eax, 4(%rsp)                   # 4-byte Spill
	movl	4(%rdx), %eax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movl	16(%rdx), %eax
	movl	%eax, 60(%rsp)                  # 4-byte Spill
	movl	20(%rdx), %eax
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	movl	28(%rdx), %ecx
	movslq	32(%rdx), %rax
	movq	%rax, 152(%rsp)                 # 8-byte Spill
	movl	36(%rdx), %eax
	movl	%eax, 20(%rsp)                  # 4-byte Spill
	movl	%esi, %ebp
	andl	$-4, %ebp
	movl	%ecx, 212(%rsp)                 # 4-byte Spill
	movl	%ecx, %eax
	movq	%rbp, %rcx
	movq	%rbp, 136(%rsp)                 # 8-byte Spill
	subl	%ebp, %eax
	cmpl	$5, %eax
	movl	$4, %r13d
	movl	%eax, %r14d
	cmovll	%eax, %r13d
	movl	%r13d, %eax
	sarl	$31, %eax
	andnl	%r13d, %eax, %ecx
	movq	%rcx, %rax
	shlq	$5, %rax
	movq	40(%rdx), %r15
	movq	56(%rdx), %rbp
	movq	%rbp, 88(%rsp)                  # 8-byte Spill
	movq	72(%rdx), %rbp
	movq	88(%rdx), %r12
	movq	%rsi, %rbx
	movq	%rsi, 40(%rsp)                  # 8-byte Spill
	cmpl	%esi, 8(%rdx)
	movq	%rdi, 184(%rsp)                 # 8-byte Spill
	jle	.LBB8_105
# %bb.1:                                # %true_bb
	cmpl	$67108864, %ecx                 # imm = 0x4000000
	jae	.LBB8_282
# %bb.2:                                # %"assert succeeded"
	movl	12(%rdx), %ecx
	movl	%ecx, 144(%rsp)                 # 4-byte Spill
	orq	$4, %rax
	movq	%rax, %rsi
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB8_283
# %bb.3:                                # %"assert succeeded2"
	movq	%rax, %r10
	movq	40(%rsp), %rax                  # 8-byte Reload
	leal	(,%rax,8), %eax
	andl	$24, %eax
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	testl	%r14d, %r14d
	jle	.LBB8_75
# %bb.4:                                # %"for conv1_stage1.s0.w.preheader"
	movq	%rbp, %r11
	movq	136(%rsp), %rcx                 # 8-byte Reload
	addl	%ecx, %r13d
	movl	48(%rsp), %eax                  # 4-byte Reload
	vmovups	(%r15,%rax,4), %ymm0
	movslq	%ecx, %r9
	movslq	%r13d, %r8
	movq	%r8, %rdx
	subq	%r9, %rdx
	movq	%r9, %rsi
	notq	%rsi
	addq	%r8, %rsi
	movq	%rdx, %rdi
	movq	%r9, %rcx
	andq	$7, %rdi
	je	.LBB8_8
# %bb.5:                                # %"for conv1_stage1.s0.w.prol.preheader"
	negq	%rdi
	xorl	%ebp, %ebp
	movq	%r10, %rcx
	.p2align	4, 0x90
.LBB8_6:                                # %"for conv1_stage1.s0.w.prol"
                                        # =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, (%rcx)
	decq	%rbp
	addq	$32, %rcx
	cmpq	%rbp, %rdi
	jne	.LBB8_6
# %bb.7:                                # %"for conv1_stage1.s0.w.prol.loopexit.loopexit"
	movq	%r9, %rcx
	subq	%rbp, %rcx
.LBB8_8:                                # %"for conv1_stage1.s0.w.prol.loopexit"
	cmpq	$7, %rsi
	jb	.LBB8_11
# %bb.9:                                # %"for conv1_stage1.s0.w.preheader3"
	movq	%rcx, %rdi
	subq	%r8, %rdi
	shlq	$5, %rcx
	movq	%r9, %rbp
	shlq	$5, %rbp
	subq	%rbp, %rcx
	leaq	(%rcx,%r10), %rbp
	addq	$224, %rbp
	.p2align	4, 0x90
.LBB8_10:                               # %"for conv1_stage1.s0.w"
                                        # =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, -224(%rbp)
	vmovaps	%ymm0, -192(%rbp)
	vmovaps	%ymm0, -160(%rbp)
	vmovaps	%ymm0, -128(%rbp)
	vmovaps	%ymm0, -96(%rbp)
	vmovaps	%ymm0, -64(%rbp)
	vmovaps	%ymm0, -32(%rbp)
	vmovaps	%ymm0, (%rbp)
	addq	$256, %rbp                      # imm = 0x100
	addq	$8, %rdi
	jne	.LBB8_10
.LBB8_11:                               # %"end for conv1_stage1.s0.w"
	testl	%r14d, %r14d
	jle	.LBB8_75
# %bb.12:                               # %"for conv1_stage1.s1.w.preheader"
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdx, %rdi
	movq	%r9, %rax
	andq	$3, %rdi
	je	.LBB8_16
# %bb.13:                               # %"for conv1_stage1.s1.w.prol.preheader"
	movq	%r9, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	negq	%rdi
	xorl	%ecx, %ecx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_14:                               # %"for conv1_stage1.s1.w.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd213ps	(%r10,%rcx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%r10,%rcx)
	decq	%rbp
	addq	$32, %rcx
	cmpq	%rbp, %rdi
	jne	.LBB8_14
# %bb.15:                               # %"for conv1_stage1.s1.w.prol.loopexit.loopexit"
	movq	%r9, %rax
	subq	%rbp, %rax
.LBB8_16:                               # %"for conv1_stage1.s1.w.prol.loopexit"
	cmpq	$3, %rsi
	jb	.LBB8_19
# %bb.17:                               # %"for conv1_stage1.s1.w.preheader2"
	movq	%rax, %rbp
	subq	%r8, %rbp
	shlq	$5, %rax
	addq	$96, %rax
	movq	%r9, %rcx
	shlq	$5, %rcx
	movq	%r10, %rdi
	subq	%rcx, %rdi
	.p2align	4, 0x90
.LBB8_18:                               # %"for conv1_stage1.s1.w"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rdi,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdi,%rax)
	vbroadcastss	-64(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rdi,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdi,%rax)
	vbroadcastss	-32(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rdi,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdi,%rax)
	vbroadcastss	(%r12,%rax), %ymm1
	vfmadd213ps	(%rdi,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdi,%rax)
	subq	$-128, %rax
	addq	$4, %rbp
	jne	.LBB8_18
.LBB8_19:                               # %"end for conv1_stage1.s1.w"
	testl	%r14d, %r14d
	jle	.LBB8_75
# %bb.20:                               # %"for conv1_stage1.s1.w.preheader.1"
	movq	48(%rsp), %rax                  # 8-byte Reload
	movq	8(%rsp), %rcx                   # 8-byte Reload
	addl	%ecx, %eax
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdx, %rdi
	movq	%r9, %rax
	andq	$3, %rdi
	je	.LBB8_24
# %bb.21:                               # %"for conv1_stage1.s1.w.1.prol.preheader"
	movq	%r9, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$4, %rax
	negq	%rdi
	xorl	%ecx, %ecx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_22:                               # %"for conv1_stage1.s1.w.1.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd213ps	(%r10,%rcx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%r10,%rcx)
	decq	%rbp
	addq	$32, %rcx
	cmpq	%rbp, %rdi
	jne	.LBB8_22
# %bb.23:                               # %"for conv1_stage1.s1.w.1.prol.loopexit.loopexit"
	movq	%r9, %rax
	subq	%rbp, %rax
.LBB8_24:                               # %"for conv1_stage1.s1.w.1.prol.loopexit"
	cmpq	$3, %rsi
	jb	.LBB8_27
# %bb.25:                               # %"for conv1_stage1.s1.w.1.preheader"
	movq	%rax, %rbp
	subq	%r8, %rbp
	shlq	$5, %rax
	movq	%r9, %rcx
	shlq	$5, %rcx
	movq	%r10, %rbx
	subq	%rcx, %rbx
	addq	$96, %rbx
	.p2align	4, 0x90
.LBB8_26:                               # %"for conv1_stage1.s1.w.1"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	4(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rbx,%rax)
	vbroadcastss	36(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rbx,%rax)
	vbroadcastss	68(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rbx,%rax)
	vbroadcastss	100(%r12,%rax), %ymm1
	vfmadd213ps	(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rax)
	subq	$-128, %rax
	addq	$4, %rbp
	jne	.LBB8_26
.LBB8_27:                               # %"end for conv1_stage1.s1.w.1"
	testl	%r14d, %r14d
	jle	.LBB8_75
# %bb.28:                               # %"for conv1_stage1.s1.w.preheader.2"
	movq	48(%rsp), %rax                  # 8-byte Reload
	movq	8(%rsp), %rcx                   # 8-byte Reload
	leal	(%rax,%rcx,2), %eax
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdx, %rdi
	movq	%r9, %rax
	andq	$3, %rdi
	je	.LBB8_32
# %bb.29:                               # %"for conv1_stage1.s1.w.2.prol.preheader"
	movq	%r9, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$8, %rax
	negq	%rdi
	xorl	%ecx, %ecx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_30:                               # %"for conv1_stage1.s1.w.2.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd213ps	(%r10,%rcx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%r10,%rcx)
	decq	%rbp
	addq	$32, %rcx
	cmpq	%rbp, %rdi
	jne	.LBB8_30
# %bb.31:                               # %"for conv1_stage1.s1.w.2.prol.loopexit.loopexit"
	movq	%r9, %rax
	subq	%rbp, %rax
.LBB8_32:                               # %"for conv1_stage1.s1.w.2.prol.loopexit"
	cmpq	$3, %rsi
	jb	.LBB8_35
# %bb.33:                               # %"for conv1_stage1.s1.w.2.preheader"
	movq	%rax, %rbp
	subq	%r8, %rbp
	shlq	$5, %rax
	movq	%r9, %rcx
	shlq	$5, %rcx
	movq	%r10, %rbx
	subq	%rcx, %rbx
	addq	$96, %rbx
	.p2align	4, 0x90
.LBB8_34:                               # %"for conv1_stage1.s1.w.2"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	8(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rbx,%rax)
	vbroadcastss	40(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rbx,%rax)
	vbroadcastss	72(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rbx,%rax)
	vbroadcastss	104(%r12,%rax), %ymm1
	vfmadd213ps	(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rax)
	subq	$-128, %rax
	addq	$4, %rbp
	jne	.LBB8_34
.LBB8_35:                               # %"end for conv1_stage1.s1.w.2"
	testl	%r14d, %r14d
	jle	.LBB8_75
# %bb.36:                               # %"for conv1_stage1.s1.w.preheader.3"
	movq	8(%rsp), %rax                   # 8-byte Reload
	leal	(%rax,%rax,2), %eax
	addl	48(%rsp), %eax                  # 4-byte Folded Reload
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdx, %rdi
	movq	%r9, %rax
	andq	$3, %rdi
	je	.LBB8_40
# %bb.37:                               # %"for conv1_stage1.s1.w.3.prol.preheader"
	movq	%r9, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$12, %rax
	negq	%rdi
	xorl	%ecx, %ecx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_38:                               # %"for conv1_stage1.s1.w.3.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd213ps	(%r10,%rcx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%r10,%rcx)
	decq	%rbp
	addq	$32, %rcx
	cmpq	%rbp, %rdi
	jne	.LBB8_38
# %bb.39:                               # %"for conv1_stage1.s1.w.3.prol.loopexit.loopexit"
	movq	%r9, %rax
	subq	%rbp, %rax
.LBB8_40:                               # %"for conv1_stage1.s1.w.3.prol.loopexit"
	cmpq	$3, %rsi
	jb	.LBB8_43
# %bb.41:                               # %"for conv1_stage1.s1.w.3.preheader"
	movq	%rax, %rbp
	subq	%r8, %rbp
	shlq	$5, %rax
	movq	%r9, %rcx
	shlq	$5, %rcx
	movq	%r10, %rbx
	subq	%rcx, %rbx
	addq	$96, %rbx
	.p2align	4, 0x90
.LBB8_42:                               # %"for conv1_stage1.s1.w.3"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	12(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rbx,%rax)
	vbroadcastss	44(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rbx,%rax)
	vbroadcastss	76(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rbx,%rax)
	vbroadcastss	108(%r12,%rax), %ymm1
	vfmadd213ps	(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rax)
	subq	$-128, %rax
	addq	$4, %rbp
	jne	.LBB8_42
.LBB8_43:                               # %"end for conv1_stage1.s1.w.3"
	testl	%r14d, %r14d
	jle	.LBB8_75
# %bb.44:                               # %"for conv1_stage1.s1.w.preheader.4"
	movq	48(%rsp), %rax                  # 8-byte Reload
	movq	8(%rsp), %rcx                   # 8-byte Reload
	leal	(%rax,%rcx,4), %eax
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdx, %rdi
	movq	%r9, %rax
	andq	$3, %rdi
	je	.LBB8_48
# %bb.45:                               # %"for conv1_stage1.s1.w.4.prol.preheader"
	movq	%r9, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$16, %rax
	negq	%rdi
	xorl	%ecx, %ecx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_46:                               # %"for conv1_stage1.s1.w.4.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd213ps	(%r10,%rcx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%r10,%rcx)
	decq	%rbp
	addq	$32, %rcx
	cmpq	%rbp, %rdi
	jne	.LBB8_46
# %bb.47:                               # %"for conv1_stage1.s1.w.4.prol.loopexit.loopexit"
	movq	%r9, %rax
	subq	%rbp, %rax
.LBB8_48:                               # %"for conv1_stage1.s1.w.4.prol.loopexit"
	cmpq	$3, %rsi
	jb	.LBB8_51
# %bb.49:                               # %"for conv1_stage1.s1.w.4.preheader"
	movq	%rax, %rbp
	subq	%r8, %rbp
	shlq	$5, %rax
	movq	%r9, %rcx
	shlq	$5, %rcx
	movq	%r10, %rbx
	subq	%rcx, %rbx
	addq	$96, %rbx
	.p2align	4, 0x90
.LBB8_50:                               # %"for conv1_stage1.s1.w.4"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	16(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rbx,%rax)
	vbroadcastss	48(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rbx,%rax)
	vbroadcastss	80(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rbx,%rax)
	vbroadcastss	112(%r12,%rax), %ymm1
	vfmadd213ps	(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rax)
	subq	$-128, %rax
	addq	$4, %rbp
	jne	.LBB8_50
.LBB8_51:                               # %"end for conv1_stage1.s1.w.4"
	testl	%r14d, %r14d
	jle	.LBB8_75
# %bb.52:                               # %"for conv1_stage1.s1.w.preheader.5"
	movq	8(%rsp), %rax                   # 8-byte Reload
	leal	(%rax,%rax,4), %eax
	addl	48(%rsp), %eax                  # 4-byte Folded Reload
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdx, %rdi
	movq	%r9, %rax
	andq	$3, %rdi
	je	.LBB8_56
# %bb.53:                               # %"for conv1_stage1.s1.w.5.prol.preheader"
	movq	%r9, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$20, %rax
	negq	%rdi
	xorl	%ecx, %ecx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_54:                               # %"for conv1_stage1.s1.w.5.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd213ps	(%r10,%rcx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%r10,%rcx)
	decq	%rbp
	addq	$32, %rcx
	cmpq	%rbp, %rdi
	jne	.LBB8_54
# %bb.55:                               # %"for conv1_stage1.s1.w.5.prol.loopexit.loopexit"
	movq	%r9, %rax
	subq	%rbp, %rax
.LBB8_56:                               # %"for conv1_stage1.s1.w.5.prol.loopexit"
	cmpq	$3, %rsi
	jb	.LBB8_59
# %bb.57:                               # %"for conv1_stage1.s1.w.5.preheader"
	movq	%rax, %rbp
	subq	%r8, %rbp
	shlq	$5, %rax
	movq	%r9, %rcx
	shlq	$5, %rcx
	movq	%r10, %rbx
	subq	%rcx, %rbx
	addq	$96, %rbx
	.p2align	4, 0x90
.LBB8_58:                               # %"for conv1_stage1.s1.w.5"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	20(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rbx,%rax)
	vbroadcastss	52(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rbx,%rax)
	vbroadcastss	84(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rbx,%rax)
	vbroadcastss	116(%r12,%rax), %ymm1
	vfmadd213ps	(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rax)
	subq	$-128, %rax
	addq	$4, %rbp
	jne	.LBB8_58
.LBB8_59:                               # %"end for conv1_stage1.s1.w.5"
	testl	%r14d, %r14d
	jle	.LBB8_75
# %bb.60:                               # %"for conv1_stage1.s1.w.preheader.6"
	movq	8(%rsp), %rax                   # 8-byte Reload
	leal	(%rax,%rax,2), %eax
	movq	48(%rsp), %rcx                  # 8-byte Reload
	leal	(%rcx,%rax,2), %eax
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdx, %rdi
	movq	%r9, %rax
	andq	$3, %rdi
	je	.LBB8_64
# %bb.61:                               # %"for conv1_stage1.s1.w.6.prol.preheader"
	movq	%r9, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$24, %rax
	negq	%rdi
	xorl	%ecx, %ecx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_62:                               # %"for conv1_stage1.s1.w.6.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd213ps	(%r10,%rcx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%r10,%rcx)
	decq	%rbp
	addq	$32, %rcx
	cmpq	%rbp, %rdi
	jne	.LBB8_62
# %bb.63:                               # %"for conv1_stage1.s1.w.6.prol.loopexit.loopexit"
	movq	%r9, %rax
	subq	%rbp, %rax
.LBB8_64:                               # %"for conv1_stage1.s1.w.6.prol.loopexit"
	cmpq	$3, %rsi
	jb	.LBB8_67
# %bb.65:                               # %"for conv1_stage1.s1.w.6.preheader"
	movq	%rax, %rbp
	subq	%r8, %rbp
	shlq	$5, %rax
	movq	%r9, %rcx
	shlq	$5, %rcx
	movq	%r10, %rbx
	subq	%rcx, %rbx
	addq	$96, %rbx
	.p2align	4, 0x90
.LBB8_66:                               # %"for conv1_stage1.s1.w.6"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	24(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rbx,%rax)
	vbroadcastss	56(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rbx,%rax)
	vbroadcastss	88(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rbx,%rax)
	vbroadcastss	120(%r12,%rax), %ymm1
	vfmadd213ps	(%rbx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rax)
	subq	$-128, %rax
	addq	$4, %rbp
	jne	.LBB8_66
.LBB8_67:                               # %"end for conv1_stage1.s1.w.6"
	testl	%r14d, %r14d
	jle	.LBB8_75
# %bb.68:                               # %"for conv1_stage1.s1.w.preheader.7"
	movq	8(%rsp), %rcx                   # 8-byte Reload
	leal	(,%rcx,8), %eax
	subl	%ecx, %eax
	addl	48(%rsp), %eax                  # 4-byte Folded Reload
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%r9, %rax
	andq	$3, %rdx
	je	.LBB8_72
# %bb.69:                               # %"for conv1_stage1.s1.w.7.prol.preheader"
	movq	%r9, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$28, %rax
	negq	%rdx
	xorl	%ecx, %ecx
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB8_70:                               # %"for conv1_stage1.s1.w.7.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd213ps	(%r10,%rcx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%r10,%rcx)
	decq	%rdi
	addq	$32, %rcx
	cmpq	%rdi, %rdx
	jne	.LBB8_70
# %bb.71:                               # %"for conv1_stage1.s1.w.7.prol.loopexit.loopexit"
	movq	%r9, %rax
	subq	%rdi, %rax
.LBB8_72:                               # %"for conv1_stage1.s1.w.7.prol.loopexit"
	cmpq	$3, %rsi
	jb	.LBB8_75
# %bb.73:                               # %"for conv1_stage1.s1.w.7.preheader"
	movq	%rax, %rsi
	subq	%r8, %rsi
	shlq	$5, %rax
	shlq	$5, %r9
	movq	%r10, %rdx
	subq	%r9, %rdx
	addq	$96, %rdx
	.p2align	4, 0x90
.LBB8_74:                               # %"for conv1_stage1.s1.w.7"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	28(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx,%rax)
	vbroadcastss	60(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx,%rax)
	vbroadcastss	92(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx,%rax)
	vbroadcastss	124(%r12,%rax), %ymm1
	vfmadd213ps	(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx,%rax)
	subq	$-128, %rax
	addq	$4, %rsi
	jne	.LBB8_74
.LBB8_75:                               # %"end for conv1_stage1.s1.w.7"
	movq	%r10, 80(%rsp)                  # 8-byte Spill
	movl	144(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, %edx
	sarl	$3, %edx
	movq	112(%rsp), %rax                 # 8-byte Reload
	addl	$7, %eax
	sarl	$3, %eax
	movslq	%edx, %rsi
	movq	%rsi, 128(%rsp)                 # 8-byte Spill
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	cltq
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movl	20(%rsp), %r15d                 # 4-byte Reload
	movq	152(%rsp), %r9                  # 8-byte Reload
	subl	%r9d, %r15d
	andl	$-8, %ecx
	subl	%ecx, %r15d
	leaq	-1(%rdx), %rax
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	movl	%edx, %eax
	andl	$7, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movq	%rdx, 104(%rsp)                 # 8-byte Spill
	movl	%edx, %esi
	andl	$-8, %esi
	movq	88(%rsp), %r8                   # 8-byte Reload
	leaq	224(%r8), %rax
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	movq	40(%rsp), %r14                  # 8-byte Reload
	shll	$5, %r14d
	movq	%r14, 40(%rsp)                  # 8-byte Spill
                                        # kill: def $r14d killed $r14d killed $r14 def $r14
	andl	$-128, %r14d
	movq	48(%rsp), %rax                  # 8-byte Reload
	movq	%r14, 192(%rsp)                 # 8-byte Spill
	leal	(%r14,%rax), %edi
	imull	4(%rsp), %edi                   # 4-byte Folded Reload
	leaq	992(%r8), %rax
	movq	%rax, 168(%rsp)                 # 8-byte Spill
	leal	(%r9,%rdi), %edx
	movl	60(%rsp), %eax                  # 4-byte Reload
	subl	%eax, %edi
	movl	%ecx, 136(%rsp)                 # 4-byte Spill
	addl	%ecx, %edx
	subl	%eax, %edx
	movl	%edx, (%rsp)                    # 4-byte Spill
	leaq	96(%r8), %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	xorl	%r10d, %r10d
	movl	$8, %r12d
	movq	%rsi, 96(%rsp)                  # 8-byte Spill
	jmp	.LBB8_76
	.p2align	4, 0x90
.LBB8_100:                              # %"end for conv1_stage2.s0.n.n4"
                                        #   in Loop: Header=BB8_76 Depth=1
	incq	%r10
	movl	4(%rsp), %eax                   # 4-byte Reload
	movq	24(%rsp), %rdi                  # 8-byte Reload
	addl	%eax, %edi
	addl	%eax, (%rsp)                    # 4-byte Folded Spill
	cmpq	$8, %r10
	movl	20(%rsp), %eax                  # 4-byte Reload
	movq	88(%rsp), %r8                   # 8-byte Reload
	movq	96(%rsp), %rsi                  # 8-byte Reload
	je	.LBB8_101
.LBB8_76:                               # %"for conv1_stage2.s0.c.ci"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB8_80 Depth 2
                                        #     Child Loop BB8_83 Depth 2
                                        #     Child Loop BB8_86 Depth 2
                                        #       Child Loop BB8_94 Depth 3
                                        #       Child Loop BB8_97 Depth 3
                                        #       Child Loop BB8_90 Depth 3
	cmpl	$8, 144(%rsp)                   # 4-byte Folded Reload
	jl	.LBB8_84
# %bb.77:                               # %"for conv1_stage2.s0.n.n.preheader"
                                        #   in Loop: Header=BB8_76 Depth=1
	movslq	%edi, %rax
	movq	80(%rsp), %rcx                  # 8-byte Reload
	vbroadcastss	(%rcx,%r10,4), %ymm0
	cmpq	$7, 120(%rsp)                   # 8-byte Folded Reload
	jae	.LBB8_79
# %bb.78:                               #   in Loop: Header=BB8_76 Depth=1
	xorl	%ecx, %ecx
	jmp	.LBB8_81
	.p2align	4, 0x90
.LBB8_79:                               # %"for conv1_stage2.s0.n.n.preheader1"
                                        #   in Loop: Header=BB8_76 Depth=1
	leaq	(%r9,%rax), %rcx
	movq	160(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB8_80:                               # %"for conv1_stage2.s0.n.n"
                                        #   Parent Loop BB8_76 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      # imm = 0x100
	cmpq	%rcx, %rsi
	jne	.LBB8_80
.LBB8_81:                               # %"end for conv1_stage2.s0.n.n.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB8_76 Depth=1
	cmpq	$0, 64(%rsp)                    # 8-byte Folded Reload
	je	.LBB8_84
# %bb.82:                               # %"for conv1_stage2.s0.n.n.epil.preheader"
                                        #   in Loop: Header=BB8_76 Depth=1
	leaq	(%r9,%rcx,8), %rcx
	addq	%rax, %rcx
	leaq	(%r8,%rcx,4), %rax
	movq	64(%rsp), %rcx                  # 8-byte Reload
	.p2align	4, 0x90
.LBB8_83:                               # %"for conv1_stage2.s0.n.n.epil"
                                        #   Parent Loop BB8_76 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax)
	addq	$32, %rax
	decq	%rcx
	jne	.LBB8_83
.LBB8_84:                               # %"end for conv1_stage2.s0.n.n"
                                        #   in Loop: Header=BB8_76 Depth=1
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	movq	104(%rsp), %rax                 # 8-byte Reload
	cmpl	112(%rsp), %eax                 # 4-byte Folded Reload
	jge	.LBB8_100
# %bb.85:                               # %"for conv1_stage2.s0.n.n3.preheader"
                                        #   in Loop: Header=BB8_76 Depth=1
	xorl	%ecx, %ecx
	movl	%r15d, %esi
	movl	%r15d, %r8d
	movl	(%rsp), %eax                    # 4-byte Reload
	movq	128(%rsp), %rdx                 # 8-byte Reload
	movq	%r10, 72(%rsp)                  # 8-byte Spill
	jmp	.LBB8_86
	.p2align	4, 0x90
.LBB8_99:                               # %"end for conv1_stage2.s0.n.ni"
                                        #   in Loop: Header=BB8_86 Depth=2
	incq	%rdx
	incl	%ecx
	leal	8(%rdi), %eax
	addl	$-8, %r8d
	addl	$-8, %esi
	cmpq	8(%rsp), %rdx                   # 8-byte Folded Reload
	je	.LBB8_100
.LBB8_86:                               # %"for conv1_stage2.s0.n.n3"
                                        #   Parent Loop BB8_76 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB8_94 Depth 3
                                        #       Child Loop BB8_97 Depth 3
                                        #       Child Loop BB8_90 Depth 3
	cmpl	$9, %esi
	movl	$8, %r11d
	cmovll	%esi, %r11d
	cmpl	$9, %r8d
	movl	$8, %ebx
	cmovll	%r8d, %ebx
	movslq	%eax, %rdi
	leal	(,%rcx,8), %eax
	movl	%r15d, %ebp
	subl	%eax, %ebp
	cmpl	$9, %ebp
	cmovgel	%r12d, %ebp
	leal	(%r9,%rdx,8), %eax
	cmpl	%eax, 20(%rsp)                  # 4-byte Folded Reload
	jle	.LBB8_99
# %bb.87:                               # %"for conv1_stage2.s0.n.ni.preheader"
                                        #   in Loop: Header=BB8_86 Depth=2
	movq	80(%rsp), %rax                  # 8-byte Reload
	movl	(%rax,%r10,4), %r13d
	cmpl	$31, %ebp
	ja	.LBB8_91
# %bb.88:                               #   in Loop: Header=BB8_86 Depth=2
	xorl	%r14d, %r14d
	jmp	.LBB8_89
	.p2align	4, 0x90
.LBB8_91:                               # %vector.ph203
                                        #   in Loop: Header=BB8_86 Depth=2
	movl	%r15d, %r10d
	andl	$-32, %ebx
	addq	$-32, %rbx
	shrq	$5, %rbx
	movl	%ebp, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r9
	shrq	$5, %r9
	incq	%r9
	movl	%ebp, %r14d
	andl	$-32, %r14d
	vmovd	%r13d, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	cmpq	$224, %rax
	jae	.LBB8_93
# %bb.92:                               #   in Loop: Header=BB8_86 Depth=2
	xorl	%r15d, %r15d
	jmp	.LBB8_95
	.p2align	4, 0x90
.LBB8_93:                               # %vector.ph203.new
                                        #   in Loop: Header=BB8_86 Depth=2
	movq	168(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %rax
	leaq	1(%rbx), %r12
	andq	$-8, %r12
	negq	%r12
	xorl	%r15d, %r15d
	.p2align	4, 0x90
.LBB8_94:                               # %vector.body200
                                        #   Parent Loop BB8_76 Depth=1
                                        #     Parent Loop BB8_86 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	%ymm0, -992(%rax,%r15,4)
	vmovdqu	%ymm0, -960(%rax,%r15,4)
	vmovdqu	%ymm0, -928(%rax,%r15,4)
	vmovdqu	%ymm0, -896(%rax,%r15,4)
	vmovdqu	%ymm0, -864(%rax,%r15,4)
	vmovdqu	%ymm0, -832(%rax,%r15,4)
	vmovdqu	%ymm0, -800(%rax,%r15,4)
	vmovdqu	%ymm0, -768(%rax,%r15,4)
	vmovdqu	%ymm0, -736(%rax,%r15,4)
	vmovdqu	%ymm0, -704(%rax,%r15,4)
	vmovdqu	%ymm0, -672(%rax,%r15,4)
	vmovdqu	%ymm0, -640(%rax,%r15,4)
	vmovdqu	%ymm0, -608(%rax,%r15,4)
	vmovdqu	%ymm0, -576(%rax,%r15,4)
	vmovdqu	%ymm0, -544(%rax,%r15,4)
	vmovdqu	%ymm0, -512(%rax,%r15,4)
	vmovdqu	%ymm0, -480(%rax,%r15,4)
	vmovdqu	%ymm0, -448(%rax,%r15,4)
	vmovdqu	%ymm0, -416(%rax,%r15,4)
	vmovdqu	%ymm0, -384(%rax,%r15,4)
	vmovdqu	%ymm0, -352(%rax,%r15,4)
	vmovdqu	%ymm0, -320(%rax,%r15,4)
	vmovdqu	%ymm0, -288(%rax,%r15,4)
	vmovdqu	%ymm0, -256(%rax,%r15,4)
	vmovdqu	%ymm0, -224(%rax,%r15,4)
	vmovdqu	%ymm0, -192(%rax,%r15,4)
	vmovdqu	%ymm0, -160(%rax,%r15,4)
	vmovdqu	%ymm0, -128(%rax,%r15,4)
	vmovdqu	%ymm0, -96(%rax,%r15,4)
	vmovdqu	%ymm0, -64(%rax,%r15,4)
	vmovdqu	%ymm0, -32(%rax,%r15,4)
	vmovdqu	%ymm0, (%rax,%r15,4)
	addq	$256, %r15                      # imm = 0x100
	addq	$8, %r12
	jne	.LBB8_94
.LBB8_95:                               # %middle.block198.unr-lcssa
                                        #   in Loop: Header=BB8_86 Depth=2
	testb	$7, %r9b
	je	.LBB8_98
# %bb.96:                               # %vector.body200.epil.preheader
                                        #   in Loop: Header=BB8_86 Depth=2
	incb	%bl
	movzbl	%bl, %eax
	andl	$7, %eax
	negq	%rax
	addq	%rdi, %r15
	movq	32(%rsp), %rbx                  # 8-byte Reload
	leaq	(%rbx,%r15,4), %rbx
	.p2align	4, 0x90
.LBB8_97:                               # %vector.body200.epil
                                        #   Parent Loop BB8_76 Depth=1
                                        #     Parent Loop BB8_86 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	%ymm0, -96(%rbx)
	vmovdqu	%ymm0, -64(%rbx)
	vmovdqu	%ymm0, -32(%rbx)
	vmovdqu	%ymm0, (%rbx)
	subq	$-128, %rbx
	incq	%rax
	jne	.LBB8_97
.LBB8_98:                               # %middle.block198
                                        #   in Loop: Header=BB8_86 Depth=2
	cmpq	%rbp, %r14
	movq	152(%rsp), %r9                  # 8-byte Reload
	movl	%r10d, %r15d
	movq	72(%rsp), %r10                  # 8-byte Reload
	movl	$8, %r12d
	je	.LBB8_99
.LBB8_89:                               # %"for conv1_stage2.s0.n.ni.preheader284"
                                        #   in Loop: Header=BB8_86 Depth=2
	subq	%r14, %r11
	movq	%rdi, %rax
	addq	%r14, %rax
	movq	88(%rsp), %rbx                  # 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_90:                               # %"for conv1_stage2.s0.n.ni"
                                        #   Parent Loop BB8_76 Depth=1
                                        #     Parent Loop BB8_86 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%r13d, (%rax,%rbp,4)
	incq	%rbp
	cmpq	%rbp, %r11
	jne	.LBB8_90
	jmp	.LBB8_99
.LBB8_105:                              # %false_bb
	cmpl	$67108864, %ecx                 # imm = 0x4000000
	jae	.LBB8_282
# %bb.106:                              # %"assert succeeded8"
	movl	24(%rdx), %ebx
	orq	$4, %rax
	movq	%rax, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	testq	%rax, %rax
	je	.LBB8_283
# %bb.107:                              # %"assert succeeded10"
	movl	%ebx, 80(%rsp)                  # 4-byte Spill
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, %ecx
	sarl	$2, %ecx
	movq	%rcx, 200(%rsp)                 # 8-byte Spill
	leal	(,%rax,8), %eax
	andl	$24, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	testl	%r14d, %r14d
	jle	.LBB8_179
# %bb.108:                              # %"for conv1_stage1.s0.w13.preheader"
	movq	%rbp, %r11
	movq	136(%rsp), %rcx                 # 8-byte Reload
	addl	%ecx, %r13d
	movl	24(%rsp), %eax                  # 4-byte Reload
	vmovups	(%r15,%rax,4), %ymm0
	movslq	%ecx, %r10
	movslq	%r13d, %r8
	movq	%r8, %rdi
	subq	%r10, %rdi
	movq	%r10, %r9
	notq	%r9
	addq	%r8, %r9
	movq	%rdi, %rcx
	movq	%r10, %rdx
	andq	$7, %rcx
	movq	32(%rsp), %rbx                  # 8-byte Reload
	je	.LBB8_112
# %bb.109:                              # %"for conv1_stage1.s0.w13.prol.preheader"
	negq	%rcx
	xorl	%ebp, %ebp
	movq	%rbx, %rdx
	.p2align	4, 0x90
.LBB8_110:                              # %"for conv1_stage1.s0.w13.prol"
                                        # =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, (%rdx)
	decq	%rbp
	addq	$32, %rdx
	cmpq	%rbp, %rcx
	jne	.LBB8_110
# %bb.111:                              # %"for conv1_stage1.s0.w13.prol.loopexit.loopexit"
	movq	%r10, %rdx
	subq	%rbp, %rdx
.LBB8_112:                              # %"for conv1_stage1.s0.w13.prol.loopexit"
	cmpq	$7, %r9
	jb	.LBB8_115
# %bb.113:                              # %"for conv1_stage1.s0.w13.preheader5"
	movq	%rdx, %rcx
	subq	%r8, %rcx
	shlq	$5, %rdx
	movq	%r10, %rsi
	shlq	$5, %rsi
	subq	%rsi, %rdx
	addq	%rbx, %rdx
	addq	$224, %rdx
	.p2align	4, 0x90
.LBB8_114:                              # %"for conv1_stage1.s0.w13"
                                        # =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, -224(%rdx)
	vmovaps	%ymm0, -192(%rdx)
	vmovaps	%ymm0, -160(%rdx)
	vmovaps	%ymm0, -128(%rdx)
	vmovaps	%ymm0, -96(%rdx)
	vmovaps	%ymm0, -64(%rdx)
	vmovaps	%ymm0, -32(%rdx)
	vmovaps	%ymm0, (%rdx)
	addq	$256, %rdx                      # imm = 0x100
	addq	$8, %rcx
	jne	.LBB8_114
.LBB8_115:                              # %"end for conv1_stage1.s0.w14"
	testl	%r14d, %r14d
	jle	.LBB8_179
# %bb.116:                              # %"for conv1_stage1.s1.w19.preheader"
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdi, %rcx
	movq	%r10, %rax
	andq	$3, %rcx
	movq	32(%rsp), %rbx                  # 8-byte Reload
	je	.LBB8_120
# %bb.117:                              # %"for conv1_stage1.s1.w19.prol.preheader"
	movq	%r10, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	negq	%rcx
	xorl	%edx, %edx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_118:                              # %"for conv1_stage1.s1.w19.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rdx), %ymm1
	vfmadd213ps	(%rbx,%rdx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rdx)
	decq	%rbp
	addq	$32, %rdx
	cmpq	%rbp, %rcx
	jne	.LBB8_118
# %bb.119:                              # %"for conv1_stage1.s1.w19.prol.loopexit.loopexit"
	movq	%r10, %rax
	subq	%rbp, %rax
.LBB8_120:                              # %"for conv1_stage1.s1.w19.prol.loopexit"
	cmpq	$3, %r9
	jb	.LBB8_123
# %bb.121:                              # %"for conv1_stage1.s1.w19.preheader4"
	movq	%rax, %rcx
	subq	%r8, %rcx
	shlq	$5, %rax
	addq	$96, %rax
	movq	%r10, %rsi
	shlq	$5, %rsi
	movq	%rbx, %rdx
	subq	%rsi, %rdx
	.p2align	4, 0x90
.LBB8_122:                              # %"for conv1_stage1.s1.w19"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	-96(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx,%rax)
	vbroadcastss	-64(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx,%rax)
	vbroadcastss	-32(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx,%rax)
	vbroadcastss	(%r12,%rax), %ymm1
	vfmadd213ps	(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx,%rax)
	subq	$-128, %rax
	addq	$4, %rcx
	jne	.LBB8_122
.LBB8_123:                              # %"end for conv1_stage1.s1.w20"
	testl	%r14d, %r14d
	jle	.LBB8_179
# %bb.124:                              # %"for conv1_stage1.s1.w19.preheader.1"
	movq	8(%rsp), %rax                   # 8-byte Reload
	movq	24(%rsp), %rcx                  # 8-byte Reload
	addl	%ecx, %eax
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdi, %rcx
	movq	%r10, %rax
	andq	$3, %rcx
	movq	32(%rsp), %rbx                  # 8-byte Reload
	je	.LBB8_128
# %bb.125:                              # %"for conv1_stage1.s1.w19.1.prol.preheader"
	movq	%r10, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$4, %rax
	negq	%rcx
	xorl	%edx, %edx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_126:                              # %"for conv1_stage1.s1.w19.1.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rdx), %ymm1
	vfmadd213ps	(%rbx,%rdx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rdx)
	decq	%rbp
	addq	$32, %rdx
	cmpq	%rbp, %rcx
	jne	.LBB8_126
# %bb.127:                              # %"for conv1_stage1.s1.w19.1.prol.loopexit.loopexit"
	movq	%r10, %rax
	subq	%rbp, %rax
.LBB8_128:                              # %"for conv1_stage1.s1.w19.1.prol.loopexit"
	cmpq	$3, %r9
	jb	.LBB8_131
# %bb.129:                              # %"for conv1_stage1.s1.w19.1.preheader"
	movq	%rax, %rcx
	subq	%r8, %rcx
	shlq	$5, %rax
	movq	%r10, %rsi
	shlq	$5, %rsi
	movq	%rbx, %rdx
	subq	%rsi, %rdx
	addq	$96, %rdx
	.p2align	4, 0x90
.LBB8_130:                              # %"for conv1_stage1.s1.w19.1"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	4(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx,%rax)
	vbroadcastss	36(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx,%rax)
	vbroadcastss	68(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx,%rax)
	vbroadcastss	100(%r12,%rax), %ymm1
	vfmadd213ps	(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx,%rax)
	subq	$-128, %rax
	addq	$4, %rcx
	jne	.LBB8_130
.LBB8_131:                              # %"end for conv1_stage1.s1.w20.1"
	testl	%r14d, %r14d
	jle	.LBB8_179
# %bb.132:                              # %"for conv1_stage1.s1.w19.preheader.2"
	movq	8(%rsp), %rax                   # 8-byte Reload
	movq	24(%rsp), %rcx                  # 8-byte Reload
	leal	(%rcx,%rax,2), %eax
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdi, %rcx
	movq	%r10, %rax
	andq	$3, %rcx
	movq	32(%rsp), %rbx                  # 8-byte Reload
	je	.LBB8_136
# %bb.133:                              # %"for conv1_stage1.s1.w19.2.prol.preheader"
	movq	%r10, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$8, %rax
	negq	%rcx
	xorl	%edx, %edx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_134:                              # %"for conv1_stage1.s1.w19.2.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rdx), %ymm1
	vfmadd213ps	(%rbx,%rdx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rdx)
	decq	%rbp
	addq	$32, %rdx
	cmpq	%rbp, %rcx
	jne	.LBB8_134
# %bb.135:                              # %"for conv1_stage1.s1.w19.2.prol.loopexit.loopexit"
	movq	%r10, %rax
	subq	%rbp, %rax
.LBB8_136:                              # %"for conv1_stage1.s1.w19.2.prol.loopexit"
	cmpq	$3, %r9
	jb	.LBB8_139
# %bb.137:                              # %"for conv1_stage1.s1.w19.2.preheader"
	movq	%rax, %rcx
	subq	%r8, %rcx
	shlq	$5, %rax
	movq	%r10, %rsi
	shlq	$5, %rsi
	movq	%rbx, %rdx
	subq	%rsi, %rdx
	addq	$96, %rdx
	.p2align	4, 0x90
.LBB8_138:                              # %"for conv1_stage1.s1.w19.2"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	8(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx,%rax)
	vbroadcastss	40(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx,%rax)
	vbroadcastss	72(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx,%rax)
	vbroadcastss	104(%r12,%rax), %ymm1
	vfmadd213ps	(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx,%rax)
	subq	$-128, %rax
	addq	$4, %rcx
	jne	.LBB8_138
.LBB8_139:                              # %"end for conv1_stage1.s1.w20.2"
	testl	%r14d, %r14d
	jle	.LBB8_179
# %bb.140:                              # %"for conv1_stage1.s1.w19.preheader.3"
	movq	8(%rsp), %rax                   # 8-byte Reload
	leal	(%rax,%rax,2), %eax
	addl	24(%rsp), %eax                  # 4-byte Folded Reload
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdi, %rcx
	movq	%r10, %rax
	andq	$3, %rcx
	movq	32(%rsp), %rbx                  # 8-byte Reload
	je	.LBB8_144
# %bb.141:                              # %"for conv1_stage1.s1.w19.3.prol.preheader"
	movq	%r10, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$12, %rax
	negq	%rcx
	xorl	%edx, %edx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_142:                              # %"for conv1_stage1.s1.w19.3.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rdx), %ymm1
	vfmadd213ps	(%rbx,%rdx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rdx)
	decq	%rbp
	addq	$32, %rdx
	cmpq	%rbp, %rcx
	jne	.LBB8_142
# %bb.143:                              # %"for conv1_stage1.s1.w19.3.prol.loopexit.loopexit"
	movq	%r10, %rax
	subq	%rbp, %rax
.LBB8_144:                              # %"for conv1_stage1.s1.w19.3.prol.loopexit"
	cmpq	$3, %r9
	jb	.LBB8_147
# %bb.145:                              # %"for conv1_stage1.s1.w19.3.preheader"
	movq	%rax, %rcx
	subq	%r8, %rcx
	shlq	$5, %rax
	movq	%r10, %rsi
	shlq	$5, %rsi
	movq	%rbx, %rdx
	subq	%rsi, %rdx
	addq	$96, %rdx
	.p2align	4, 0x90
.LBB8_146:                              # %"for conv1_stage1.s1.w19.3"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	12(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx,%rax)
	vbroadcastss	44(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx,%rax)
	vbroadcastss	76(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx,%rax)
	vbroadcastss	108(%r12,%rax), %ymm1
	vfmadd213ps	(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx,%rax)
	subq	$-128, %rax
	addq	$4, %rcx
	jne	.LBB8_146
.LBB8_147:                              # %"end for conv1_stage1.s1.w20.3"
	testl	%r14d, %r14d
	jle	.LBB8_179
# %bb.148:                              # %"for conv1_stage1.s1.w19.preheader.4"
	movq	8(%rsp), %rax                   # 8-byte Reload
	movq	24(%rsp), %rcx                  # 8-byte Reload
	leal	(%rcx,%rax,4), %eax
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdi, %rcx
	movq	%r10, %rax
	andq	$3, %rcx
	movq	32(%rsp), %rbx                  # 8-byte Reload
	je	.LBB8_152
# %bb.149:                              # %"for conv1_stage1.s1.w19.4.prol.preheader"
	movq	%r10, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$16, %rax
	negq	%rcx
	xorl	%edx, %edx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_150:                              # %"for conv1_stage1.s1.w19.4.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rdx), %ymm1
	vfmadd213ps	(%rbx,%rdx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rdx)
	decq	%rbp
	addq	$32, %rdx
	cmpq	%rbp, %rcx
	jne	.LBB8_150
# %bb.151:                              # %"for conv1_stage1.s1.w19.4.prol.loopexit.loopexit"
	movq	%r10, %rax
	subq	%rbp, %rax
.LBB8_152:                              # %"for conv1_stage1.s1.w19.4.prol.loopexit"
	cmpq	$3, %r9
	jb	.LBB8_155
# %bb.153:                              # %"for conv1_stage1.s1.w19.4.preheader"
	movq	%rax, %rcx
	subq	%r8, %rcx
	shlq	$5, %rax
	movq	%r10, %rsi
	shlq	$5, %rsi
	movq	%rbx, %rdx
	subq	%rsi, %rdx
	addq	$96, %rdx
	.p2align	4, 0x90
.LBB8_154:                              # %"for conv1_stage1.s1.w19.4"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	16(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx,%rax)
	vbroadcastss	48(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx,%rax)
	vbroadcastss	80(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx,%rax)
	vbroadcastss	112(%r12,%rax), %ymm1
	vfmadd213ps	(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx,%rax)
	subq	$-128, %rax
	addq	$4, %rcx
	jne	.LBB8_154
.LBB8_155:                              # %"end for conv1_stage1.s1.w20.4"
	testl	%r14d, %r14d
	jle	.LBB8_179
# %bb.156:                              # %"for conv1_stage1.s1.w19.preheader.5"
	movq	8(%rsp), %rax                   # 8-byte Reload
	leal	(%rax,%rax,4), %eax
	addl	24(%rsp), %eax                  # 4-byte Folded Reload
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdi, %rcx
	movq	%r10, %rax
	andq	$3, %rcx
	movq	32(%rsp), %rbx                  # 8-byte Reload
	je	.LBB8_160
# %bb.157:                              # %"for conv1_stage1.s1.w19.5.prol.preheader"
	movq	%r10, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$20, %rax
	negq	%rcx
	xorl	%edx, %edx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_158:                              # %"for conv1_stage1.s1.w19.5.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rdx), %ymm1
	vfmadd213ps	(%rbx,%rdx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rdx)
	decq	%rbp
	addq	$32, %rdx
	cmpq	%rbp, %rcx
	jne	.LBB8_158
# %bb.159:                              # %"for conv1_stage1.s1.w19.5.prol.loopexit.loopexit"
	movq	%r10, %rax
	subq	%rbp, %rax
.LBB8_160:                              # %"for conv1_stage1.s1.w19.5.prol.loopexit"
	cmpq	$3, %r9
	jb	.LBB8_163
# %bb.161:                              # %"for conv1_stage1.s1.w19.5.preheader"
	movq	%rax, %rcx
	subq	%r8, %rcx
	shlq	$5, %rax
	movq	%r10, %rsi
	shlq	$5, %rsi
	movq	%rbx, %rdx
	subq	%rsi, %rdx
	addq	$96, %rdx
	.p2align	4, 0x90
.LBB8_162:                              # %"for conv1_stage1.s1.w19.5"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	20(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx,%rax)
	vbroadcastss	52(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx,%rax)
	vbroadcastss	84(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx,%rax)
	vbroadcastss	116(%r12,%rax), %ymm1
	vfmadd213ps	(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx,%rax)
	subq	$-128, %rax
	addq	$4, %rcx
	jne	.LBB8_162
.LBB8_163:                              # %"end for conv1_stage1.s1.w20.5"
	testl	%r14d, %r14d
	jle	.LBB8_179
# %bb.164:                              # %"for conv1_stage1.s1.w19.preheader.6"
	movq	8(%rsp), %rax                   # 8-byte Reload
	leal	(%rax,%rax,2), %eax
	movq	24(%rsp), %rcx                  # 8-byte Reload
	leal	(%rcx,%rax,2), %eax
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rdi, %rcx
	movq	%r10, %rax
	andq	$3, %rcx
	movq	32(%rsp), %rbx                  # 8-byte Reload
	je	.LBB8_168
# %bb.165:                              # %"for conv1_stage1.s1.w19.6.prol.preheader"
	movq	%r10, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$24, %rax
	negq	%rcx
	xorl	%edx, %edx
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB8_166:                              # %"for conv1_stage1.s1.w19.6.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rdx), %ymm1
	vfmadd213ps	(%rbx,%rdx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rdx)
	decq	%rbp
	addq	$32, %rdx
	cmpq	%rbp, %rcx
	jne	.LBB8_166
# %bb.167:                              # %"for conv1_stage1.s1.w19.6.prol.loopexit.loopexit"
	movq	%r10, %rax
	subq	%rbp, %rax
.LBB8_168:                              # %"for conv1_stage1.s1.w19.6.prol.loopexit"
	cmpq	$3, %r9
	jb	.LBB8_171
# %bb.169:                              # %"for conv1_stage1.s1.w19.6.preheader"
	movq	%rax, %rcx
	subq	%r8, %rcx
	shlq	$5, %rax
	movq	%r10, %rsi
	shlq	$5, %rsi
	movq	%rbx, %rdx
	subq	%rsi, %rdx
	addq	$96, %rdx
	.p2align	4, 0x90
.LBB8_170:                              # %"for conv1_stage1.s1.w19.6"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	24(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx,%rax)
	vbroadcastss	56(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx,%rax)
	vbroadcastss	88(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx,%rax)
	vbroadcastss	120(%r12,%rax), %ymm1
	vfmadd213ps	(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx,%rax)
	subq	$-128, %rax
	addq	$4, %rcx
	jne	.LBB8_170
.LBB8_171:                              # %"end for conv1_stage1.s1.w20.6"
	testl	%r14d, %r14d
	jle	.LBB8_179
# %bb.172:                              # %"for conv1_stage1.s1.w19.preheader.7"
	movq	8(%rsp), %rcx                   # 8-byte Reload
	leal	(,%rcx,8), %eax
	subl	%ecx, %eax
	addl	24(%rsp), %eax                  # 4-byte Folded Reload
	cltq
	vmovups	(%r11,%rax,4), %ymm0
	movq	%r10, %rax
	andq	$3, %rdi
	je	.LBB8_176
# %bb.173:                              # %"for conv1_stage1.s1.w19.7.prol.preheader"
	movq	%r10, %rax
	shlq	$5, %rax
	addq	%r12, %rax
	addq	$28, %rax
	negq	%rdi
	xorl	%ecx, %ecx
	xorl	%edx, %edx
	movq	32(%rsp), %rsi                  # 8-byte Reload
	.p2align	4, 0x90
.LBB8_174:                              # %"for conv1_stage1.s1.w19.7.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax,%rcx), %ymm1
	vfmadd213ps	(%rsi,%rcx), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rsi,%rcx)
	decq	%rdx
	addq	$32, %rcx
	cmpq	%rdx, %rdi
	jne	.LBB8_174
# %bb.175:                              # %"for conv1_stage1.s1.w19.7.prol.loopexit.loopexit"
	movq	%r10, %rax
	subq	%rdx, %rax
.LBB8_176:                              # %"for conv1_stage1.s1.w19.7.prol.loopexit"
	cmpq	$3, %r9
	jb	.LBB8_179
# %bb.177:                              # %"for conv1_stage1.s1.w19.7.preheader"
	movq	%rax, %rcx
	subq	%r8, %rcx
	shlq	$5, %rax
	shlq	$5, %r10
	movq	32(%rsp), %rdx                  # 8-byte Reload
	subq	%r10, %rdx
	addq	$96, %rdx
	.p2align	4, 0x90
.LBB8_178:                              # %"for conv1_stage1.s1.w19.7"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	28(%r12,%rax), %ymm1
	vfmadd213ps	-96(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdx,%rax)
	vbroadcastss	60(%r12,%rax), %ymm1
	vfmadd213ps	-64(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdx,%rax)
	vbroadcastss	92(%r12,%rax), %ymm1
	vfmadd213ps	-32(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdx,%rax)
	vbroadcastss	124(%r12,%rax), %ymm1
	vfmadd213ps	(%rdx,%rax), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdx,%rax)
	subq	$-128, %rax
	addq	$4, %rcx
	jne	.LBB8_178
.LBB8_179:                              # %"end for conv1_stage1.s1.w20.7"
	movq	200(%rsp), %rax                 # 8-byte Reload
	addl	%eax, %eax
	movq	%rax, 200(%rsp)                 # 8-byte Spill
	movl	80(%rsp), %ecx                  # 4-byte Reload
	subl	%eax, %ecx
	cmpl	$2, %ecx
	movl	$1, %eax
	cmovll	%ecx, %eax
	testl	%eax, %eax
	movq	152(%rsp), %rdx                 # 8-byte Reload
	movl	20(%rsp), %ebx                  # 4-byte Reload
	movq	88(%rsp), %r8                   # 8-byte Reload
	movq	32(%rsp), %rsi                  # 8-byte Reload
	js	.LBB8_188
# %bb.180:                              # %"for conv1_stage2.s0.w.wi.wi23.preheader"
	movq	112(%rsp), %rcx                 # 8-byte Reload
	leal	7(%rcx), %r9d
	sarl	$3, %r9d
	movl	4(%rsp), %ecx                   # 4-byte Reload
	movl	%ecx, %edi
	shll	$5, %edi
	movl	%eax, %eax
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	movl	%ebx, %r13d
	subl	%edx, %r13d
	leaq	992(%r8), %rax
	movq	%rax, 144(%rsp)                 # 8-byte Spill
	movq	40(%rsp), %r14                  # 8-byte Reload
	shll	$5, %r14d
	andl	$-128, %r14d
	addl	24(%rsp), %r14d                 # 4-byte Folded Reload
	imull	%ecx, %r14d
	addl	%edx, %r14d
	subl	60(%rsp), %r14d                 # 4-byte Folded Reload
	movq	%r14, 40(%rsp)                  # 8-byte Spill
	shll	$6, %ecx
	movl	%ecx, 208(%rsp)                 # 4-byte Spill
	leaq	96(%r8), %rax
	movq	%rax, 104(%rsp)                 # 8-byte Spill
	xorl	%ecx, %ecx
	movq	%r9, 72(%rsp)                   # 8-byte Spill
	movl	%edi, 192(%rsp)                 # 4-byte Spill
	movl	%r13d, 168(%rsp)                # 4-byte Spill
	jmp	.LBB8_181
	.p2align	4, 0x90
.LBB8_191:                              # %"end for conv1_stage2.s0.w.wi.wii27"
                                        #   in Loop: Header=BB8_181 Depth=1
	movq	216(%rsp), %rbp                 # 8-byte Reload
	leaq	1(%rbp), %rax
	movq	40(%rsp), %rcx                  # 8-byte Reload
	addl	208(%rsp), %ecx                 # 4-byte Folded Reload
	movq	%rcx, 40(%rsp)                  # 8-byte Spill
	cmpq	224(%rsp), %rbp                 # 8-byte Folded Reload
	movq	%rax, %rcx
	je	.LBB8_188
.LBB8_181:                              # %"for conv1_stage2.s0.w.wi.wi23"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB8_183 Depth 2
                                        #       Child Loop BB8_184 Depth 3
                                        #         Child Loop BB8_186 Depth 4
                                        #           Child Loop BB8_200 Depth 5
                                        #           Child Loop BB8_203 Depth 5
                                        #           Child Loop BB8_196 Depth 5
	movq	200(%rsp), %rax                 # 8-byte Reload
	movq	%rcx, 216(%rsp)                 # 8-byte Spill
	addl	%eax, %ecx
	addl	%ecx, %ecx
	movl	212(%rsp), %eax                 # 4-byte Reload
	subl	%ecx, %eax
	jle	.LBB8_191
# %bb.182:                              # %"for conv1_stage2.s0.w.wi.wii26.preheader"
                                        #   in Loop: Header=BB8_181 Depth=1
	xorl	%ecx, %ecx
	cmpl	$1, %eax
	sete	%cl
	movl	$2, %eax
	subq	%rcx, %rax
	movq	%rax, 176(%rsp)                 # 8-byte Spill
	movq	216(%rsp), %rax                 # 8-byte Reload
	addl	%eax, %eax
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	%eax, %ecx
	xorl	%ebp, %ebp
	jmp	.LBB8_183
	.p2align	4, 0x90
.LBB8_190:                              # %"end for conv1_stage2.s0.c.ci31"
                                        #   in Loop: Header=BB8_183 Depth=2
	movq	96(%rsp), %rbp                  # 8-byte Reload
	incq	%rbp
	movl	192(%rsp), %edi                 # 4-byte Reload
	movl	64(%rsp), %ecx                  # 4-byte Reload
	addl	%edi, %ecx
	cmpq	176(%rsp), %rbp                 # 8-byte Folded Reload
	je	.LBB8_191
.LBB8_183:                              # %"for conv1_stage2.s0.w.wi.wii26"
                                        #   Parent Loop BB8_181 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB8_184 Depth 3
                                        #         Child Loop BB8_186 Depth 4
                                        #           Child Loop BB8_200 Depth 5
                                        #           Child Loop BB8_203 Depth 5
                                        #           Child Loop BB8_196 Depth 5
	movq	48(%rsp), %rax                  # 8-byte Reload
	movq	%rbp, 96(%rsp)                  # 8-byte Spill
	addl	%eax, %ebp
	movq	136(%rsp), %rax                 # 8-byte Reload
	addl	%ebp, %eax
	shll	$3, %ebp
	movq	%rbp, 160(%rsp)                 # 8-byte Spill
	imull	%edi, %eax
	subl	60(%rsp), %eax                  # 4-byte Folded Reload
	movl	%eax, (%rsp)                    # 4-byte Spill
	movl	%ecx, 64(%rsp)                  # 4-byte Spill
	movl	%ecx, 128(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	jmp	.LBB8_184
	.p2align	4, 0x90
.LBB8_206:                              # %"end for conv1_stage2.s0.n.n34"
                                        #   in Loop: Header=BB8_184 Depth=3
	movq	120(%rsp), %rcx                 # 8-byte Reload
	incq	%rcx
	movl	128(%rsp), %eax                 # 4-byte Reload
	addl	4(%rsp), %eax                   # 4-byte Folded Reload
	movl	%eax, 128(%rsp)                 # 4-byte Spill
	movq	%rcx, %rax
	movq	%rcx, 120(%rsp)                 # 8-byte Spill
	cmpq	$8, %rcx
	je	.LBB8_190
.LBB8_184:                              # %"for conv1_stage2.s0.c.ci30"
                                        #   Parent Loop BB8_181 Depth=1
                                        #     Parent Loop BB8_183 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB8_186 Depth 4
                                        #           Child Loop BB8_200 Depth 5
                                        #           Child Loop BB8_203 Depth 5
                                        #           Child Loop BB8_196 Depth 5
	cmpl	$0, 112(%rsp)                   # 4-byte Folded Reload
	jle	.LBB8_206
# %bb.185:                              # %"for conv1_stage2.s0.n.n33.preheader"
                                        #   in Loop: Header=BB8_184 Depth=3
	movq	24(%rsp), %rax                  # 8-byte Reload
	movq	120(%rsp), %rdi                 # 8-byte Reload
	addl	%edi, %eax
	imull	4(%rsp), %eax                   # 4-byte Folded Reload
	addl	(%rsp), %eax                    # 4-byte Folded Reload
	movq	160(%rsp), %rcx                 # 8-byte Reload
	addl	%edi, %ecx
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	cltq
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	movl	%r13d, %ebp
	movl	128(%rsp), %r10d                # 4-byte Reload
	xorl	%r14d, %r14d
	jmp	.LBB8_186
	.p2align	4, 0x90
.LBB8_187:                              # %true_bb37
                                        #   in Loop: Header=BB8_186 Depth=4
	movq	8(%rsp), %rcx                   # 8-byte Reload
	vbroadcastss	(%rsi,%rcx,4), %ymm0
	cltq
	addq	80(%rsp), %rax                  # 8-byte Folded Reload
	vmovups	%ymm0, (%r8,%rax,4)
.LBB8_205:                              # %after_bb39
                                        #   in Loop: Header=BB8_186 Depth=4
	incq	%r14
	addl	$8, %r10d
	addl	$-8, %ebp
	cmpq	%r9, %r14
	je	.LBB8_206
.LBB8_186:                              # %"for conv1_stage2.s0.n.n33"
                                        #   Parent Loop BB8_181 Depth=1
                                        #     Parent Loop BB8_183 Depth=2
                                        #       Parent Loop BB8_184 Depth=3
                                        # =>      This Loop Header: Depth=4
                                        #           Child Loop BB8_200 Depth 5
                                        #           Child Loop BB8_203 Depth 5
                                        #           Child Loop BB8_196 Depth 5
	cmpl	$9, %ebp
	movl	$8, %ecx
	cmovll	%ebp, %ecx
	leal	(,%r14,8), %eax
	movl	%r13d, %r11d
	subl	%eax, %r11d
	cmpl	$9, %r11d
	movl	$8, %eax
	cmovgel	%eax, %r11d
	leal	(%rdx,%r14,8), %eax
	leal	(%rdx,%r14,8), %edi
	addl	$8, %edi
	cmpl	%ebx, %edi
	jle	.LBB8_187
# %bb.192:                              # %false_bb38
                                        #   in Loop: Header=BB8_186 Depth=4
	cmpl	%eax, %ebx
	jle	.LBB8_205
# %bb.193:                              # %"for conv1_stage2.s0.n.ni40.preheader"
                                        #   in Loop: Header=BB8_186 Depth=4
	movslq	%r10d, %r15
	movq	8(%rsp), %rax                   # 8-byte Reload
	movl	(%rsi,%rax,4), %edi
	cmpl	$31, %r11d
	ja	.LBB8_197
# %bb.194:                              #   in Loop: Header=BB8_186 Depth=4
	xorl	%r12d, %r12d
	jmp	.LBB8_195
	.p2align	4, 0x90
.LBB8_197:                              # %vector.ph
                                        #   in Loop: Header=BB8_186 Depth=4
	movl	%ecx, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	shrq	$5, %rax
	movl	%r11d, %esi
	andl	$-32, %esi
	addq	$-32, %rsi
	movq	%rsi, %r13
	shrq	$5, %r13
	incq	%r13
	movl	%r11d, %r12d
	andl	$-32, %r12d
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	cmpq	$224, %rsi
	jae	.LBB8_199
# %bb.198:                              #   in Loop: Header=BB8_186 Depth=4
	xorl	%r8d, %r8d
	jmp	.LBB8_201
.LBB8_199:                              # %vector.ph.new
                                        #   in Loop: Header=BB8_186 Depth=4
	movq	144(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%r15,4), %r9
	leaq	1(%rax), %rsi
	andq	$-8, %rsi
	negq	%rsi
	xorl	%r8d, %r8d
	.p2align	4, 0x90
.LBB8_200:                              # %vector.body
                                        #   Parent Loop BB8_181 Depth=1
                                        #     Parent Loop BB8_183 Depth=2
                                        #       Parent Loop BB8_184 Depth=3
                                        #         Parent Loop BB8_186 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovdqu	%ymm0, -992(%r9,%r8,4)
	vmovdqu	%ymm0, -960(%r9,%r8,4)
	vmovdqu	%ymm0, -928(%r9,%r8,4)
	vmovdqu	%ymm0, -896(%r9,%r8,4)
	vmovdqu	%ymm0, -864(%r9,%r8,4)
	vmovdqu	%ymm0, -832(%r9,%r8,4)
	vmovdqu	%ymm0, -800(%r9,%r8,4)
	vmovdqu	%ymm0, -768(%r9,%r8,4)
	vmovdqu	%ymm0, -736(%r9,%r8,4)
	vmovdqu	%ymm0, -704(%r9,%r8,4)
	vmovdqu	%ymm0, -672(%r9,%r8,4)
	vmovdqu	%ymm0, -640(%r9,%r8,4)
	vmovdqu	%ymm0, -608(%r9,%r8,4)
	vmovdqu	%ymm0, -576(%r9,%r8,4)
	vmovdqu	%ymm0, -544(%r9,%r8,4)
	vmovdqu	%ymm0, -512(%r9,%r8,4)
	vmovdqu	%ymm0, -480(%r9,%r8,4)
	vmovdqu	%ymm0, -448(%r9,%r8,4)
	vmovdqu	%ymm0, -416(%r9,%r8,4)
	vmovdqu	%ymm0, -384(%r9,%r8,4)
	vmovdqu	%ymm0, -352(%r9,%r8,4)
	vmovdqu	%ymm0, -320(%r9,%r8,4)
	vmovdqu	%ymm0, -288(%r9,%r8,4)
	vmovdqu	%ymm0, -256(%r9,%r8,4)
	vmovdqu	%ymm0, -224(%r9,%r8,4)
	vmovdqu	%ymm0, -192(%r9,%r8,4)
	vmovdqu	%ymm0, -160(%r9,%r8,4)
	vmovdqu	%ymm0, -128(%r9,%r8,4)
	vmovdqu	%ymm0, -96(%r9,%r8,4)
	vmovdqu	%ymm0, -64(%r9,%r8,4)
	vmovdqu	%ymm0, -32(%r9,%r8,4)
	vmovdqu	%ymm0, (%r9,%r8,4)
	addq	$256, %r8                       # imm = 0x100
	addq	$8, %rsi
	jne	.LBB8_200
.LBB8_201:                              # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB8_186 Depth=4
	testb	$7, %r13b
	je	.LBB8_204
# %bb.202:                              # %vector.body.epil.preheader
                                        #   in Loop: Header=BB8_186 Depth=4
	incb	%al
	movzbl	%al, %eax
	andl	$7, %eax
	negq	%rax
	addq	%r15, %r8
	movq	104(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%r8,4), %rsi
	.p2align	4, 0x90
.LBB8_203:                              # %vector.body.epil
                                        #   Parent Loop BB8_181 Depth=1
                                        #     Parent Loop BB8_183 Depth=2
                                        #       Parent Loop BB8_184 Depth=3
                                        #         Parent Loop BB8_186 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovdqu	%ymm0, -96(%rsi)
	vmovdqu	%ymm0, -64(%rsi)
	vmovdqu	%ymm0, -32(%rsi)
	vmovdqu	%ymm0, (%rsi)
	subq	$-128, %rsi
	incq	%rax
	jne	.LBB8_203
.LBB8_204:                              # %middle.block
                                        #   in Loop: Header=BB8_186 Depth=4
	cmpq	%r11, %r12
	movq	152(%rsp), %rdx                 # 8-byte Reload
	movl	20(%rsp), %ebx                  # 4-byte Reload
	movq	88(%rsp), %r8                   # 8-byte Reload
	movq	32(%rsp), %rsi                  # 8-byte Reload
	movq	72(%rsp), %r9                   # 8-byte Reload
	movl	168(%rsp), %r13d                # 4-byte Reload
	je	.LBB8_205
.LBB8_195:                              # %"for conv1_stage2.s0.n.ni40.preheader285"
                                        #   in Loop: Header=BB8_186 Depth=4
	leaq	(%r8,%r15,4), %rax
	.p2align	4, 0x90
.LBB8_196:                              # %"for conv1_stage2.s0.n.ni40"
                                        #   Parent Loop BB8_181 Depth=1
                                        #     Parent Loop BB8_183 Depth=2
                                        #       Parent Loop BB8_184 Depth=3
                                        #         Parent Loop BB8_186 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	movl	%edi, (%rax,%r12,4)
	incq	%r12
	cmpq	%r12, %rcx
	jne	.LBB8_196
	jmp	.LBB8_205
.LBB8_101:                              # %"end for conv1_stage2.s0.c.ci"
	movl	%eax, %edi
	subl	%r9d, %edi
	movl	%edi, %r11d
	subl	136(%rsp), %r11d                # 4-byte Folded Reload
	movq	104(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, %eax
	andl	$7, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movl	%edx, %ebp
	andl	$-8, %ebp
	movq	48(%rsp), %rax                  # 8-byte Reload
	movq	192(%rsp), %rcx                 # 8-byte Reload
	leal	(%rcx,%rax), %ebx
	addl	$32, %ebx
	imull	4(%rsp), %ebx                   # 4-byte Folded Reload
	leal	(%r9,%rbx), %r10d
	movl	60(%rsp), %eax                  # 4-byte Reload
	subl	%eax, %ebx
	leal	(,%rdx,8), %ecx
	leal	(%r10,%rdx,8), %edx
	subl	%eax, %edx
	movl	%edx, (%rsp)                    # 4-byte Spill
	subl	%ecx, %edi
	movl	%edi, 96(%rsp)                  # 4-byte Spill
	xorl	%esi, %esi
	movl	$8, %r14d
	movq	80(%rsp), %r10                  # 8-byte Reload
	movq	%rbp, 176(%rsp)                 # 8-byte Spill
	jmp	.LBB8_102
	.p2align	4, 0x90
.LBB8_228:                              # %"end for conv1_stage2.s0.n.n4.1"
                                        #   in Loop: Header=BB8_102 Depth=1
	incq	%rsi
	movl	4(%rsp), %eax                   # 4-byte Reload
	movq	24(%rsp), %rbx                  # 8-byte Reload
	addl	%eax, %ebx
	addl	%eax, (%rsp)                    # 4-byte Folded Spill
	cmpq	$8, %rsi
	movl	20(%rsp), %eax                  # 4-byte Reload
	movq	88(%rsp), %r8                   # 8-byte Reload
	movq	176(%rsp), %rbp                 # 8-byte Reload
	je	.LBB8_229
.LBB8_102:                              # %"for conv1_stage2.s0.c.ci.1"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB8_208 Depth 2
                                        #     Child Loop BB8_211 Depth 2
                                        #     Child Loop BB8_214 Depth 2
                                        #       Child Loop BB8_220 Depth 3
                                        #       Child Loop BB8_223 Depth 3
                                        #       Child Loop BB8_226 Depth 3
	cmpl	$8, 144(%rsp)                   # 4-byte Folded Reload
	jl	.LBB8_212
# %bb.103:                              # %"for conv1_stage2.s0.n.n.preheader.1"
                                        #   in Loop: Header=BB8_102 Depth=1
	movslq	%ebx, %rax
	vbroadcastss	32(%r10,%rsi,4), %ymm0
	cmpq	$7, 120(%rsp)                   # 8-byte Folded Reload
	jae	.LBB8_207
# %bb.104:                              #   in Loop: Header=BB8_102 Depth=1
	xorl	%ecx, %ecx
	jmp	.LBB8_209
	.p2align	4, 0x90
.LBB8_207:                              # %"for conv1_stage2.s0.n.n.1.preheader"
                                        #   in Loop: Header=BB8_102 Depth=1
	leaq	(%r9,%rax), %rcx
	movq	160(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB8_208:                              # %"for conv1_stage2.s0.n.n.1"
                                        #   Parent Loop BB8_102 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      # imm = 0x100
	cmpq	%rcx, %rbp
	jne	.LBB8_208
.LBB8_209:                              # %"end for conv1_stage2.s0.n.n.1.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB8_102 Depth=1
	cmpq	$0, 64(%rsp)                    # 8-byte Folded Reload
	je	.LBB8_212
# %bb.210:                              # %"for conv1_stage2.s0.n.n.1.epil.preheader"
                                        #   in Loop: Header=BB8_102 Depth=1
	leaq	(%r9,%rcx,8), %rcx
	addq	%rax, %rcx
	leaq	(%r8,%rcx,4), %rax
	movq	64(%rsp), %rcx                  # 8-byte Reload
	.p2align	4, 0x90
.LBB8_211:                              # %"for conv1_stage2.s0.n.n.1.epil"
                                        #   Parent Loop BB8_102 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax)
	addq	$32, %rax
	decq	%rcx
	jne	.LBB8_211
.LBB8_212:                              # %"end for conv1_stage2.s0.n.n.1"
                                        #   in Loop: Header=BB8_102 Depth=1
	movq	%rbx, 24(%rsp)                  # 8-byte Spill
	movq	104(%rsp), %rax                 # 8-byte Reload
	cmpl	112(%rsp), %eax                 # 4-byte Folded Reload
	jge	.LBB8_228
# %bb.213:                              # %"for conv1_stage2.s0.n.n3.preheader.1"
                                        #   in Loop: Header=BB8_102 Depth=1
	xorl	%r12d, %r12d
	movl	96(%rsp), %r15d                 # 4-byte Reload
	movl	%r11d, %edx
	movl	(%rsp), %eax                    # 4-byte Reload
	movq	128(%rsp), %r8                  # 8-byte Reload
	movq	%rsi, 72(%rsp)                  # 8-byte Spill
	jmp	.LBB8_214
	.p2align	4, 0x90
.LBB8_227:                              # %"end for conv1_stage2.s0.n.ni.1"
                                        #   in Loop: Header=BB8_214 Depth=2
	incq	%r8
	incl	%r12d
	leal	8(%r13), %eax
	addl	$-8, %edx
	addl	$-8, %r15d
	cmpq	8(%rsp), %r8                    # 8-byte Folded Reload
	movq	152(%rsp), %r9                  # 8-byte Reload
	je	.LBB8_228
.LBB8_214:                              # %"for conv1_stage2.s0.n.n3.1"
                                        #   Parent Loop BB8_102 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB8_220 Depth 3
                                        #       Child Loop BB8_223 Depth 3
                                        #       Child Loop BB8_226 Depth 3
	cmpl	$9, %r15d
	movl	$8, %ebp
	cmovll	%r15d, %ebp
	cmpl	$9, %edx
	movl	$8, %ebx
	cmovll	%edx, %ebx
	movslq	%eax, %r13
	leal	(,%r12,8), %eax
	movl	%r11d, %ecx
	subl	%eax, %ecx
	cmpl	$9, %ecx
	cmovgel	%r14d, %ecx
	leal	(%r9,%r8,8), %eax
	cmpl	%eax, 20(%rsp)                  # 4-byte Folded Reload
	jle	.LBB8_227
# %bb.215:                              # %"for conv1_stage2.s0.n.ni.preheader.1"
                                        #   in Loop: Header=BB8_214 Depth=2
	movl	32(%r10,%rsi,4), %edi
	cmpl	$32, %ecx
	jae	.LBB8_217
# %bb.216:                              #   in Loop: Header=BB8_214 Depth=2
	xorl	%r9d, %r9d
	jmp	.LBB8_225
	.p2align	4, 0x90
.LBB8_217:                              # %vector.ph225
                                        #   in Loop: Header=BB8_214 Depth=2
	andl	$-32, %ebx
	addq	$-32, %rbx
	shrq	$5, %rbx
	movl	%ecx, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r10
	shrq	$5, %r10
	incq	%r10
	movl	%ecx, %r9d
	andl	$-32, %r9d
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	cmpq	$224, %rax
	jae	.LBB8_219
# %bb.218:                              #   in Loop: Header=BB8_214 Depth=2
	xorl	%eax, %eax
	jmp	.LBB8_221
	.p2align	4, 0x90
.LBB8_219:                              # %vector.ph225.new
                                        #   in Loop: Header=BB8_214 Depth=2
	movq	168(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r13,4), %rsi
	leaq	1(%rbx), %r14
	andq	$-8, %r14
	negq	%r14
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB8_220:                              # %vector.body220
                                        #   Parent Loop BB8_102 Depth=1
                                        #     Parent Loop BB8_214 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	%ymm0, -992(%rsi,%rax,4)
	vmovdqu	%ymm0, -960(%rsi,%rax,4)
	vmovdqu	%ymm0, -928(%rsi,%rax,4)
	vmovdqu	%ymm0, -896(%rsi,%rax,4)
	vmovdqu	%ymm0, -864(%rsi,%rax,4)
	vmovdqu	%ymm0, -832(%rsi,%rax,4)
	vmovdqu	%ymm0, -800(%rsi,%rax,4)
	vmovdqu	%ymm0, -768(%rsi,%rax,4)
	vmovdqu	%ymm0, -736(%rsi,%rax,4)
	vmovdqu	%ymm0, -704(%rsi,%rax,4)
	vmovdqu	%ymm0, -672(%rsi,%rax,4)
	vmovdqu	%ymm0, -640(%rsi,%rax,4)
	vmovdqu	%ymm0, -608(%rsi,%rax,4)
	vmovdqu	%ymm0, -576(%rsi,%rax,4)
	vmovdqu	%ymm0, -544(%rsi,%rax,4)
	vmovdqu	%ymm0, -512(%rsi,%rax,4)
	vmovdqu	%ymm0, -480(%rsi,%rax,4)
	vmovdqu	%ymm0, -448(%rsi,%rax,4)
	vmovdqu	%ymm0, -416(%rsi,%rax,4)
	vmovdqu	%ymm0, -384(%rsi,%rax,4)
	vmovdqu	%ymm0, -352(%rsi,%rax,4)
	vmovdqu	%ymm0, -320(%rsi,%rax,4)
	vmovdqu	%ymm0, -288(%rsi,%rax,4)
	vmovdqu	%ymm0, -256(%rsi,%rax,4)
	vmovdqu	%ymm0, -224(%rsi,%rax,4)
	vmovdqu	%ymm0, -192(%rsi,%rax,4)
	vmovdqu	%ymm0, -160(%rsi,%rax,4)
	vmovdqu	%ymm0, -128(%rsi,%rax,4)
	vmovdqu	%ymm0, -96(%rsi,%rax,4)
	vmovdqu	%ymm0, -64(%rsi,%rax,4)
	vmovdqu	%ymm0, -32(%rsi,%rax,4)
	vmovdqu	%ymm0, (%rsi,%rax,4)
	addq	$256, %rax                      # imm = 0x100
	addq	$8, %r14
	jne	.LBB8_220
.LBB8_221:                              # %middle.block218.unr-lcssa
                                        #   in Loop: Header=BB8_214 Depth=2
	testb	$7, %r10b
	je	.LBB8_224
# %bb.222:                              # %vector.body220.epil.preheader
                                        #   in Loop: Header=BB8_214 Depth=2
	incb	%bl
	movzbl	%bl, %esi
	andl	$7, %esi
	negq	%rsi
	addq	%r13, %rax
	movq	32(%rsp), %rbx                  # 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	.p2align	4, 0x90
.LBB8_223:                              # %vector.body220.epil
                                        #   Parent Loop BB8_102 Depth=1
                                        #     Parent Loop BB8_214 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	%ymm0, -96(%rax)
	vmovdqu	%ymm0, -64(%rax)
	vmovdqu	%ymm0, -32(%rax)
	vmovdqu	%ymm0, (%rax)
	subq	$-128, %rax
	incq	%rsi
	jne	.LBB8_223
.LBB8_224:                              # %middle.block218
                                        #   in Loop: Header=BB8_214 Depth=2
	cmpq	%rcx, %r9
	movq	80(%rsp), %r10                  # 8-byte Reload
	movq	72(%rsp), %rsi                  # 8-byte Reload
	movl	$8, %r14d
	je	.LBB8_227
.LBB8_225:                              # %"for conv1_stage2.s0.n.ni.1.preheader"
                                        #   in Loop: Header=BB8_214 Depth=2
	subq	%r9, %rbp
	movq	%r13, %rax
	addq	%r9, %rax
	movq	88(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB8_226:                              # %"for conv1_stage2.s0.n.ni.1"
                                        #   Parent Loop BB8_102 Depth=1
                                        #     Parent Loop BB8_214 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%edi, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rbp
	jne	.LBB8_226
	jmp	.LBB8_227
.LBB8_188:                              # %call_destructor.exit74
	movq	184(%rsp), %rdi                 # 8-byte Reload
	jmp	.LBB8_189
.LBB8_229:                              # %"end for conv1_stage2.s0.c.ci.1"
	movl	%eax, %edi
	subl	%r9d, %edi
	movl	%edi, %eax
	subl	136(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, 72(%rsp)                  # 4-byte Spill
	movq	104(%rsp), %rbp                 # 8-byte Reload
	movl	%ebp, %eax
	andl	$7, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movl	%ebp, %ebx
	andl	$-8, %ebx
	movq	48(%rsp), %rax                  # 8-byte Reload
	movq	192(%rsp), %rcx                 # 8-byte Reload
	leal	(%rcx,%rax), %esi
	addl	$64, %esi
	imull	4(%rsp), %esi                   # 4-byte Folded Reload
	leal	(%r9,%rsi), %eax
	movl	60(%rsp), %edx                  # 4-byte Reload
	subl	%edx, %esi
	leal	(,%rbp,8), %ecx
	leal	(%rax,%rbp,8), %eax
	subl	%edx, %eax
	movl	%eax, (%rsp)                    # 4-byte Spill
	subl	%ecx, %edi
	movl	%edi, 96(%rsp)                  # 4-byte Spill
	xorl	%r11d, %r11d
	movq	%rbx, 176(%rsp)                 # 8-byte Spill
	jmp	.LBB8_230
	.p2align	4, 0x90
.LBB8_254:                              # %"end for conv1_stage2.s0.n.n4.1154"
                                        #   in Loop: Header=BB8_230 Depth=1
	incq	%r11
	movl	4(%rsp), %eax                   # 4-byte Reload
	movq	24(%rsp), %rsi                  # 8-byte Reload
	addl	%eax, %esi
	addl	%eax, (%rsp)                    # 4-byte Folded Spill
	cmpq	$8, %r11
	movl	20(%rsp), %ebp                  # 4-byte Reload
	movq	88(%rsp), %r8                   # 8-byte Reload
	movq	176(%rsp), %rbx                 # 8-byte Reload
	je	.LBB8_255
.LBB8_230:                              # %"for conv1_stage2.s0.c.ci.1127"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB8_234 Depth 2
                                        #     Child Loop BB8_237 Depth 2
                                        #     Child Loop BB8_240 Depth 2
                                        #       Child Loop BB8_246 Depth 3
                                        #       Child Loop BB8_249 Depth 3
                                        #       Child Loop BB8_252 Depth 3
	cmpl	$8, 144(%rsp)                   # 4-byte Folded Reload
	jl	.LBB8_238
# %bb.231:                              # %"for conv1_stage2.s0.n.n.preheader.1128"
                                        #   in Loop: Header=BB8_230 Depth=1
	movslq	%esi, %rax
	vbroadcastss	64(%r10,%r11,4), %ymm0
	cmpq	$7, 120(%rsp)                   # 8-byte Folded Reload
	jae	.LBB8_233
# %bb.232:                              #   in Loop: Header=BB8_230 Depth=1
	xorl	%ecx, %ecx
	jmp	.LBB8_235
	.p2align	4, 0x90
.LBB8_233:                              # %"for conv1_stage2.s0.n.n.1132.preheader"
                                        #   in Loop: Header=BB8_230 Depth=1
	leaq	(%r9,%rax), %rcx
	movq	160(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB8_234:                              # %"for conv1_stage2.s0.n.n.1132"
                                        #   Parent Loop BB8_230 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      # imm = 0x100
	cmpq	%rcx, %rbx
	jne	.LBB8_234
.LBB8_235:                              # %"end for conv1_stage2.s0.n.n.1134.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB8_230 Depth=1
	cmpq	$0, 64(%rsp)                    # 8-byte Folded Reload
	je	.LBB8_238
# %bb.236:                              # %"for conv1_stage2.s0.n.n.1132.epil.preheader"
                                        #   in Loop: Header=BB8_230 Depth=1
	leaq	(%r9,%rcx,8), %rcx
	addq	%rax, %rcx
	leaq	(%r8,%rcx,4), %rax
	movq	64(%rsp), %rcx                  # 8-byte Reload
	.p2align	4, 0x90
.LBB8_237:                              # %"for conv1_stage2.s0.n.n.1132.epil"
                                        #   Parent Loop BB8_230 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax)
	addq	$32, %rax
	decq	%rcx
	jne	.LBB8_237
.LBB8_238:                              # %"end for conv1_stage2.s0.n.n.1134"
                                        #   in Loop: Header=BB8_230 Depth=1
	movq	%rsi, 24(%rsp)                  # 8-byte Spill
	movq	104(%rsp), %rax                 # 8-byte Reload
	cmpl	112(%rsp), %eax                 # 4-byte Folded Reload
	jge	.LBB8_254
# %bb.239:                              # %"for conv1_stage2.s0.n.n3.preheader.1136"
                                        #   in Loop: Header=BB8_230 Depth=1
	xorl	%r12d, %r12d
	movl	96(%rsp), %r15d                 # 4-byte Reload
	movl	72(%rsp), %edx                  # 4-byte Reload
	movl	(%rsp), %eax                    # 4-byte Reload
	movq	128(%rsp), %r8                  # 8-byte Reload
	jmp	.LBB8_240
	.p2align	4, 0x90
.LBB8_253:                              # %"end for conv1_stage2.s0.n.ni.1150"
                                        #   in Loop: Header=BB8_240 Depth=2
	incq	%r8
	incl	%r12d
	leal	8(%r13), %eax
	addl	$-8, %edx
	addl	$-8, %r15d
	cmpq	8(%rsp), %r8                    # 8-byte Folded Reload
	je	.LBB8_254
.LBB8_240:                              # %"for conv1_stage2.s0.n.n3.1141"
                                        #   Parent Loop BB8_230 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB8_246 Depth 3
                                        #       Child Loop BB8_249 Depth 3
                                        #       Child Loop BB8_252 Depth 3
	cmpl	$9, %r15d
	movl	$8, %ecx
	cmovll	%r15d, %ecx
	cmpl	$9, %edx
	movl	$8, %ebx
	cmovll	%edx, %ebx
	movslq	%eax, %r13
	leal	(,%r12,8), %eax
	movl	72(%rsp), %esi                  # 4-byte Reload
                                        # kill: def $esi killed $esi def $rsi
	subl	%eax, %esi
	cmpl	$9, %esi
	movl	$8, %eax
	cmovgel	%eax, %esi
	leal	(%r9,%r8,8), %eax
	cmpl	%eax, 20(%rsp)                  # 4-byte Folded Reload
	jle	.LBB8_253
# %bb.241:                              # %"for conv1_stage2.s0.n.ni.preheader.1143"
                                        #   in Loop: Header=BB8_240 Depth=2
	movl	64(%r10,%r11,4), %edi
	cmpl	$32, %esi
	jae	.LBB8_243
# %bb.242:                              #   in Loop: Header=BB8_240 Depth=2
	xorl	%r14d, %r14d
	jmp	.LBB8_251
	.p2align	4, 0x90
.LBB8_243:                              # %vector.ph247
                                        #   in Loop: Header=BB8_240 Depth=2
	andl	$-32, %ebx
	addq	$-32, %rbx
	shrq	$5, %rbx
	movl	%esi, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r9
	shrq	$5, %r9
	incq	%r9
	movl	%esi, %r14d
	andl	$-32, %r14d
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	cmpq	$224, %rax
	jae	.LBB8_245
# %bb.244:                              #   in Loop: Header=BB8_240 Depth=2
	xorl	%eax, %eax
	jmp	.LBB8_247
	.p2align	4, 0x90
.LBB8_245:                              # %vector.ph247.new
                                        #   in Loop: Header=BB8_240 Depth=2
	movq	168(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r13,4), %rbp
	leaq	1(%rbx), %r10
	andq	$-8, %r10
	negq	%r10
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB8_246:                              # %vector.body242
                                        #   Parent Loop BB8_230 Depth=1
                                        #     Parent Loop BB8_240 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	%ymm0, -992(%rbp,%rax,4)
	vmovdqu	%ymm0, -960(%rbp,%rax,4)
	vmovdqu	%ymm0, -928(%rbp,%rax,4)
	vmovdqu	%ymm0, -896(%rbp,%rax,4)
	vmovdqu	%ymm0, -864(%rbp,%rax,4)
	vmovdqu	%ymm0, -832(%rbp,%rax,4)
	vmovdqu	%ymm0, -800(%rbp,%rax,4)
	vmovdqu	%ymm0, -768(%rbp,%rax,4)
	vmovdqu	%ymm0, -736(%rbp,%rax,4)
	vmovdqu	%ymm0, -704(%rbp,%rax,4)
	vmovdqu	%ymm0, -672(%rbp,%rax,4)
	vmovdqu	%ymm0, -640(%rbp,%rax,4)
	vmovdqu	%ymm0, -608(%rbp,%rax,4)
	vmovdqu	%ymm0, -576(%rbp,%rax,4)
	vmovdqu	%ymm0, -544(%rbp,%rax,4)
	vmovdqu	%ymm0, -512(%rbp,%rax,4)
	vmovdqu	%ymm0, -480(%rbp,%rax,4)
	vmovdqu	%ymm0, -448(%rbp,%rax,4)
	vmovdqu	%ymm0, -416(%rbp,%rax,4)
	vmovdqu	%ymm0, -384(%rbp,%rax,4)
	vmovdqu	%ymm0, -352(%rbp,%rax,4)
	vmovdqu	%ymm0, -320(%rbp,%rax,4)
	vmovdqu	%ymm0, -288(%rbp,%rax,4)
	vmovdqu	%ymm0, -256(%rbp,%rax,4)
	vmovdqu	%ymm0, -224(%rbp,%rax,4)
	vmovdqu	%ymm0, -192(%rbp,%rax,4)
	vmovdqu	%ymm0, -160(%rbp,%rax,4)
	vmovdqu	%ymm0, -128(%rbp,%rax,4)
	vmovdqu	%ymm0, -96(%rbp,%rax,4)
	vmovdqu	%ymm0, -64(%rbp,%rax,4)
	vmovdqu	%ymm0, -32(%rbp,%rax,4)
	vmovdqu	%ymm0, (%rbp,%rax,4)
	addq	$256, %rax                      # imm = 0x100
	addq	$8, %r10
	jne	.LBB8_246
.LBB8_247:                              # %middle.block240.unr-lcssa
                                        #   in Loop: Header=BB8_240 Depth=2
	testb	$7, %r9b
	movq	80(%rsp), %r10                  # 8-byte Reload
	je	.LBB8_250
# %bb.248:                              # %vector.body242.epil.preheader
                                        #   in Loop: Header=BB8_240 Depth=2
	incb	%bl
	movzbl	%bl, %ebp
	andl	$7, %ebp
	negq	%rbp
	addq	%r13, %rax
	movq	32(%rsp), %rbx                  # 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	.p2align	4, 0x90
.LBB8_249:                              # %vector.body242.epil
                                        #   Parent Loop BB8_230 Depth=1
                                        #     Parent Loop BB8_240 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	%ymm0, -96(%rax)
	vmovdqu	%ymm0, -64(%rax)
	vmovdqu	%ymm0, -32(%rax)
	vmovdqu	%ymm0, (%rax)
	subq	$-128, %rax
	incq	%rbp
	jne	.LBB8_249
.LBB8_250:                              # %middle.block240
                                        #   in Loop: Header=BB8_240 Depth=2
	cmpq	%rsi, %r14
	movq	152(%rsp), %r9                  # 8-byte Reload
	je	.LBB8_253
.LBB8_251:                              # %"for conv1_stage2.s0.n.ni.1147.preheader"
                                        #   in Loop: Header=BB8_240 Depth=2
	subq	%r14, %rcx
	movq	%r13, %rax
	addq	%r14, %rax
	movq	88(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rax,4), %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB8_252:                              # %"for conv1_stage2.s0.n.ni.1147"
                                        #   Parent Loop BB8_230 Depth=1
                                        #     Parent Loop BB8_240 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%edi, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rcx
	jne	.LBB8_252
	jmp	.LBB8_253
.LBB8_255:                              # %"end for conv1_stage2.s0.c.ci.1155"
	movl	%ebp, %edi
	subl	%r9d, %edi
	movl	%edi, %eax
	subl	136(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, 72(%rsp)                  # 4-byte Spill
	movq	104(%rsp), %rsi                 # 8-byte Reload
	movl	%esi, %eax
	andl	$7, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movl	%esi, %ebx
	andl	$-8, %ebx
	movq	40(%rsp), %rcx                  # 8-byte Reload
	addl	48(%rsp), %ecx                  # 4-byte Folded Reload
	orl	$96, %ecx
	imull	4(%rsp), %ecx                   # 4-byte Folded Reload
	leal	(%r9,%rcx), %eax
	movl	60(%rsp), %edx                  # 4-byte Reload
	subl	%edx, %ecx
	movq	%rcx, 40(%rsp)                  # 8-byte Spill
	leal	(,%rsi,8), %ecx
	leal	(%rax,%rsi,8), %eax
	subl	%edx, %eax
	movl	%eax, (%rsp)                    # 4-byte Spill
	subl	%ecx, %edi
	movl	%edi, 64(%rsp)                  # 4-byte Spill
	xorl	%r11d, %r11d
	movq	%rbx, 96(%rsp)                  # 8-byte Spill
	jmp	.LBB8_256
	.p2align	4, 0x90
.LBB8_280:                              # %"end for conv1_stage2.s0.n.n4.1.1"
                                        #   in Loop: Header=BB8_256 Depth=1
	incq	%r11
	movq	40(%rsp), %rax                  # 8-byte Reload
	movl	4(%rsp), %ecx                   # 4-byte Reload
	addl	%ecx, %eax
	movq	%rax, 40(%rsp)                  # 8-byte Spill
	addl	%ecx, (%rsp)                    # 4-byte Folded Spill
	cmpq	$8, %r11
	movq	80(%rsp), %r10                  # 8-byte Reload
	movq	96(%rsp), %rbx                  # 8-byte Reload
	je	.LBB8_281
.LBB8_256:                              # %"for conv1_stage2.s0.c.ci.1.1"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB8_260 Depth 2
                                        #     Child Loop BB8_263 Depth 2
                                        #     Child Loop BB8_266 Depth 2
                                        #       Child Loop BB8_272 Depth 3
                                        #       Child Loop BB8_275 Depth 3
                                        #       Child Loop BB8_278 Depth 3
	cmpl	$8, 144(%rsp)                   # 4-byte Folded Reload
	jl	.LBB8_264
# %bb.257:                              # %"for conv1_stage2.s0.n.n.preheader.1.1"
                                        #   in Loop: Header=BB8_256 Depth=1
	movslq	40(%rsp), %rax                  # 4-byte Folded Reload
	vbroadcastss	96(%r10,%r11,4), %ymm0
	cmpq	$7, 120(%rsp)                   # 8-byte Folded Reload
	jae	.LBB8_259
# %bb.258:                              #   in Loop: Header=BB8_256 Depth=1
	xorl	%ecx, %ecx
	jmp	.LBB8_261
	.p2align	4, 0x90
.LBB8_259:                              # %"for conv1_stage2.s0.n.n.1.1.preheader"
                                        #   in Loop: Header=BB8_256 Depth=1
	leaq	(%r9,%rax), %rcx
	movq	160(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rcx,4), %rdx
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB8_260:                              # %"for conv1_stage2.s0.n.n.1.1"
                                        #   Parent Loop BB8_256 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, -224(%rdx)
	vmovups	%ymm0, -192(%rdx)
	vmovups	%ymm0, -160(%rdx)
	vmovups	%ymm0, -128(%rdx)
	vmovups	%ymm0, -96(%rdx)
	vmovups	%ymm0, -64(%rdx)
	vmovups	%ymm0, -32(%rdx)
	vmovups	%ymm0, (%rdx)
	addq	$8, %rcx
	addq	$256, %rdx                      # imm = 0x100
	cmpq	%rcx, %rbx
	jne	.LBB8_260
.LBB8_261:                              # %"end for conv1_stage2.s0.n.n.1.1.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB8_256 Depth=1
	cmpq	$0, 24(%rsp)                    # 8-byte Folded Reload
	je	.LBB8_264
# %bb.262:                              # %"for conv1_stage2.s0.n.n.1.1.epil.preheader"
                                        #   in Loop: Header=BB8_256 Depth=1
	leaq	(%r9,%rcx,8), %rcx
	addq	%rax, %rcx
	leaq	(%r8,%rcx,4), %rax
	movq	24(%rsp), %rcx                  # 8-byte Reload
	.p2align	4, 0x90
.LBB8_263:                              # %"for conv1_stage2.s0.n.n.1.1.epil"
                                        #   Parent Loop BB8_256 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	%ymm0, (%rax)
	addq	$32, %rax
	decq	%rcx
	jne	.LBB8_263
.LBB8_264:                              # %"end for conv1_stage2.s0.n.n.1.1"
                                        #   in Loop: Header=BB8_256 Depth=1
	movq	104(%rsp), %rax                 # 8-byte Reload
	cmpl	112(%rsp), %eax                 # 4-byte Folded Reload
	jge	.LBB8_280
# %bb.265:                              # %"for conv1_stage2.s0.n.n3.preheader.1.1"
                                        #   in Loop: Header=BB8_256 Depth=1
	xorl	%r12d, %r12d
	movl	64(%rsp), %r15d                 # 4-byte Reload
	movl	72(%rsp), %edx                  # 4-byte Reload
	movl	(%rsp), %eax                    # 4-byte Reload
	movq	128(%rsp), %r10                 # 8-byte Reload
	jmp	.LBB8_266
	.p2align	4, 0x90
.LBB8_279:                              # %"end for conv1_stage2.s0.n.ni.1.1"
                                        #   in Loop: Header=BB8_266 Depth=2
	incq	%r10
	incl	%r12d
	leal	8(%r13), %eax
	addl	$-8, %edx
	addl	$-8, %r15d
	cmpq	8(%rsp), %r10                   # 8-byte Folded Reload
	je	.LBB8_280
.LBB8_266:                              # %"for conv1_stage2.s0.n.n3.1.1"
                                        #   Parent Loop BB8_256 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB8_272 Depth 3
                                        #       Child Loop BB8_275 Depth 3
                                        #       Child Loop BB8_278 Depth 3
	cmpl	$9, %r15d
	movl	$8, %ecx
	cmovll	%r15d, %ecx
	cmpl	$9, %edx
	movl	$8, %ebx
	cmovll	%edx, %ebx
	movslq	%eax, %r13
	leal	(,%r12,8), %eax
	movl	72(%rsp), %esi                  # 4-byte Reload
                                        # kill: def $esi killed $esi def $rsi
	subl	%eax, %esi
	cmpl	$9, %esi
	movl	$8, %eax
	cmovgel	%eax, %esi
	leal	(%r9,%r10,8), %eax
	cmpl	%eax, %ebp
	jle	.LBB8_279
# %bb.267:                              # %"for conv1_stage2.s0.n.ni.preheader.1.1"
                                        #   in Loop: Header=BB8_266 Depth=2
	movq	80(%rsp), %rax                  # 8-byte Reload
	movl	96(%rax,%r11,4), %edi
	cmpl	$32, %esi
	jae	.LBB8_269
# %bb.268:                              #   in Loop: Header=BB8_266 Depth=2
	xorl	%r14d, %r14d
	jmp	.LBB8_277
	.p2align	4, 0x90
.LBB8_269:                              # %vector.ph269
                                        #   in Loop: Header=BB8_266 Depth=2
	andl	$-32, %ebx
	addq	$-32, %rbx
	shrq	$5, %rbx
	movl	%esi, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, %r8
	shrq	$5, %r8
	incq	%r8
	movl	%esi, %r14d
	andl	$-32, %r14d
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	cmpq	$224, %rax
	jae	.LBB8_271
# %bb.270:                              #   in Loop: Header=BB8_266 Depth=2
	xorl	%eax, %eax
	jmp	.LBB8_273
	.p2align	4, 0x90
.LBB8_271:                              # %vector.ph269.new
                                        #   in Loop: Header=BB8_266 Depth=2
	movq	168(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r13,4), %rbp
	leaq	1(%rbx), %r9
	andq	$-8, %r9
	negq	%r9
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB8_272:                              # %vector.body264
                                        #   Parent Loop BB8_256 Depth=1
                                        #     Parent Loop BB8_266 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	%ymm0, -992(%rbp,%rax,4)
	vmovdqu	%ymm0, -960(%rbp,%rax,4)
	vmovdqu	%ymm0, -928(%rbp,%rax,4)
	vmovdqu	%ymm0, -896(%rbp,%rax,4)
	vmovdqu	%ymm0, -864(%rbp,%rax,4)
	vmovdqu	%ymm0, -832(%rbp,%rax,4)
	vmovdqu	%ymm0, -800(%rbp,%rax,4)
	vmovdqu	%ymm0, -768(%rbp,%rax,4)
	vmovdqu	%ymm0, -736(%rbp,%rax,4)
	vmovdqu	%ymm0, -704(%rbp,%rax,4)
	vmovdqu	%ymm0, -672(%rbp,%rax,4)
	vmovdqu	%ymm0, -640(%rbp,%rax,4)
	vmovdqu	%ymm0, -608(%rbp,%rax,4)
	vmovdqu	%ymm0, -576(%rbp,%rax,4)
	vmovdqu	%ymm0, -544(%rbp,%rax,4)
	vmovdqu	%ymm0, -512(%rbp,%rax,4)
	vmovdqu	%ymm0, -480(%rbp,%rax,4)
	vmovdqu	%ymm0, -448(%rbp,%rax,4)
	vmovdqu	%ymm0, -416(%rbp,%rax,4)
	vmovdqu	%ymm0, -384(%rbp,%rax,4)
	vmovdqu	%ymm0, -352(%rbp,%rax,4)
	vmovdqu	%ymm0, -320(%rbp,%rax,4)
	vmovdqu	%ymm0, -288(%rbp,%rax,4)
	vmovdqu	%ymm0, -256(%rbp,%rax,4)
	vmovdqu	%ymm0, -224(%rbp,%rax,4)
	vmovdqu	%ymm0, -192(%rbp,%rax,4)
	vmovdqu	%ymm0, -160(%rbp,%rax,4)
	vmovdqu	%ymm0, -128(%rbp,%rax,4)
	vmovdqu	%ymm0, -96(%rbp,%rax,4)
	vmovdqu	%ymm0, -64(%rbp,%rax,4)
	vmovdqu	%ymm0, -32(%rbp,%rax,4)
	vmovdqu	%ymm0, (%rbp,%rax,4)
	addq	$256, %rax                      # imm = 0x100
	addq	$8, %r9
	jne	.LBB8_272
.LBB8_273:                              # %middle.block262.unr-lcssa
                                        #   in Loop: Header=BB8_266 Depth=2
	testb	$7, %r8b
	je	.LBB8_276
# %bb.274:                              # %vector.body264.epil.preheader
                                        #   in Loop: Header=BB8_266 Depth=2
	incb	%bl
	movzbl	%bl, %ebp
	andl	$7, %ebp
	negq	%rbp
	addq	%r13, %rax
	movq	32(%rsp), %rbx                  # 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	.p2align	4, 0x90
.LBB8_275:                              # %vector.body264.epil
                                        #   Parent Loop BB8_256 Depth=1
                                        #     Parent Loop BB8_266 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovdqu	%ymm0, -96(%rax)
	vmovdqu	%ymm0, -64(%rax)
	vmovdqu	%ymm0, -32(%rax)
	vmovdqu	%ymm0, (%rax)
	subq	$-128, %rax
	incq	%rbp
	jne	.LBB8_275
.LBB8_276:                              # %middle.block262
                                        #   in Loop: Header=BB8_266 Depth=2
	cmpq	%rsi, %r14
	movq	152(%rsp), %r9                  # 8-byte Reload
	movl	20(%rsp), %ebp                  # 4-byte Reload
	movq	88(%rsp), %r8                   # 8-byte Reload
	je	.LBB8_279
.LBB8_277:                              # %"for conv1_stage2.s0.n.ni.1.1.preheader"
                                        #   in Loop: Header=BB8_266 Depth=2
	subq	%r14, %rcx
	movq	%r13, %rax
	addq	%r14, %rax
	leaq	(%r8,%rax,4), %rax
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB8_278:                              # %"for conv1_stage2.s0.n.ni.1.1"
                                        #   Parent Loop BB8_256 Depth=1
                                        #     Parent Loop BB8_266 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	movl	%edi, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %rcx
	jne	.LBB8_278
	jmp	.LBB8_279
.LBB8_281:                              # %"end for conv1_stage2.s0.c.ci.1.1"
	movq	184(%rsp), %rdi                 # 8-byte Reload
	movq	%r10, %rsi
.LBB8_189:                              # %call_destructor.exit72
	vzeroupper
	callq	halide_free@PLT
	xorl	%eax, %eax
	addq	$232, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB8_282:                              # %"assert failed"
	.cfi_def_cfa_offset 288
	leaq	.Lstr.127(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	movq	%rax, %rdx
	addq	$232, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	halide_error_buffer_allocation_too_large@PLT # TAILCALL
.LBB8_283:                              # %"assert failed1"
	.cfi_def_cfa_offset 288
	movq	184(%rsp), %rdi                 # 8-byte Reload
	addq	$232, %rsp
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	halide_error_out_of_memory@PLT  # TAILCALL
.Lfunc_end8:
	.size	train_cost_model.par_for.conv1_stage2.s0.c.c.c, .Lfunc_end8-train_cost_model.par_for.conv1_stage2.s0.c.c.c
	.cfi_endproc
                                        # -- End function
	.section	.text.train_cost_model.par_for.conv1_stage2.s1.c.c.c,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.conv1_stage2.s1.c.c.c
	.type	train_cost_model.par_for.conv1_stage2.s1.c.c.c,@function
train_cost_model.par_for.conv1_stage2.s1.c.c.c: # @train_cost_model.par_for.conv1_stage2.s1.c.c.c
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$808, %rsp                      # imm = 0x328
                                        # kill: def $esi killed $esi def $rsi
	movl	(%rdx), %edi
	movslq	4(%rdx), %r14
	movslq	8(%rdx), %r8
	movl	20(%rdx), %ebx
	movl	24(%rdx), %ecx
	movl	32(%rdx), %r10d
	movl	36(%rdx), %r15d
	movq	40(%rdx), %r11
	movq	56(%rdx), %rax
	movq	%rax, -8(%rsp)                  # 8-byte Spill
	movq	72(%rdx), %r9
	cmpl	%esi, 12(%rdx)
	movl	%edi, -112(%rsp)                # 4-byte Spill
	movq	%r14, -16(%rsp)                 # 8-byte Spill
	movq	%r8, -56(%rsp)                  # 8-byte Spill
	movq	%r10, 112(%rsp)                 # 8-byte Spill
	movq	%r11, -104(%rsp)                # 8-byte Spill
	movq	%r9, -88(%rsp)                  # 8-byte Spill
	movl	%ebx, -92(%rsp)                 # 4-byte Spill
	movl	%r15d, -108(%rsp)               # 4-byte Spill
	jle	.LBB9_12
# %bb.1:                                # %true_bb
	movl	16(%rdx), %eax
	movl	%eax, -48(%rsp)                 # 4-byte Spill
                                        # kill: def $eax killed $eax def $rax
	sarl	$3, %eax
	movq	%rax, (%rsp)                    # 8-byte Spill
	movl	%esi, %edx
	sarl	$2, %edx
	addl	%edx, %edx
	shll	$3, %esi
	andl	$24, %esi
	addl	$7, %ecx
	sarl	$3, %ecx
	leal	(,%r8,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, 132(%rsp)                 # 4-byte Spill
	movl	%edi, %eax
	shll	$5, %eax
	movl	%eax, 128(%rsp)                 # 4-byte Spill
	leaq	(%r14,%r14), %r12
	leaq	(%r12,%r12,4), %rax
	movq	%rax, 424(%rsp)                 # 8-byte Spill
	leaq	(,%r14,4), %r13
	leaq	(,%r13,2), %rax
	addq	%r13, %rax
	movq	%rax, 416(%rsp)                 # 8-byte Spill
	movq	%r14, %rax
	shlq	$4, %rax
	movq	%rax, 192(%rsp)                 # 8-byte Spill
	subq	%r14, %rax
	subq	%r14, %rax
	movq	%rax, 408(%rsp)                 # 8-byte Spill
	leaq	(,%r8,8), %rax
	movq	%rax, 400(%rsp)                 # 8-byte Spill
	subq	%r8, %rax
	movq	%rax, 392(%rsp)                 # 8-byte Spill
	leaq	(%r12,%r12,8), %rax
	movq	%rax, 384(%rsp)                 # 8-byte Spill
	leaq	(,%r13,4), %rax
	addq	%r13, %rax
	movq	%rax, 376(%rsp)                 # 8-byte Spill
	movq	%r8, %rbp
	shlq	$4, %rbp
	movq	%rbp, 184(%rsp)                 # 8-byte Spill
	subq	%r8, %rbp
	subq	%r8, %rbp
	movq	%rbp, 368(%rsp)                 # 8-byte Spill
	leaq	(%r14,%r14,2), %rax
	leaq	(%r14,%rax,4), %rbp
	movq	%rbp, 352(%rsp)                 # 8-byte Spill
	shlq	$3, %rax
	subq	%r14, %rax
	movq	%rax, 360(%rsp)                 # 8-byte Spill
	leaq	(%r14,%r14,4), %rax
	leaq	(%r14,%rax,2), %rbp
	movq	%rbp, 344(%rsp)                 # 8-byte Spill
	leaq	(%rax,%rax,2), %rbp
	movq	%rbp, 336(%rsp)                 # 8-byte Spill
	leaq	(%r14,%rax,4), %rbp
	movq	%rbp, 176(%rsp)                 # 8-byte Spill
	leaq	(%rax,%rax,4), %rax
	movq	%rax, 168(%rsp)                 # 8-byte Spill
	movq	%r14, %r13
	shlq	$5, %r13
	subq	%r14, %r13
	movq	%r13, 328(%rsp)                 # 8-byte Spill
	subq	%r14, %r13
	movq	%r13, 320(%rsp)                 # 8-byte Spill
	leaq	(%r8,%r8,2), %rax
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	leaq	(,%rax,8), %r13
	subq	%r8, %r13
	movq	%r13, 312(%rsp)                 # 8-byte Spill
	subl	%r10d, %r15d
	movl	-48(%rsp), %eax                 # 4-byte Reload
	movl	%eax, %r13d
	andl	$-8, %r13d
	subl	%r13d, %r15d
	movl	%r15d, -32(%rsp)                # 4-byte Spill
	movl	%r10d, %eax
	subl	%ebx, %eax
	movq	%rax, 208(%rsp)                 # 8-byte Spill
	movq	%r13, 216(%rsp)                 # 8-byte Spill
	leal	(%r10,%r13), %eax
	subl	%ebx, %eax
	movl	%eax, 124(%rsp)                 # 4-byte Spill
	movl	%edx, %eax
	orl	$1, %eax
	movl	%eax, 120(%rsp)                 # 4-byte Spill
	movslq	(%rsp), %rax                    # 4-byte Folded Reload
	movq	%rax, 304(%rsp)                 # 8-byte Spill
	leaq	(,%r14,8), %rax
	movq	%rax, 152(%rsp)                 # 8-byte Spill
	leaq	(%r14,%r14,8), %r15
	leaq	(%r8,%r8), %rax
	leaq	(,%r8,4), %r13
	leaq	(%r8,%r8,4), %r10
	leaq	(%rax,%rax,2), %r11
	movq	%r11, -64(%rsp)                 # 8-byte Spill
	leaq	(%r8,%r8,8), %r11
	movq	192(%rsp), %rbp                 # 8-byte Reload
	leaq	(%r14,%rbp), %r12
	movq	%r12, 280(%rsp)                 # 8-byte Spill
	leaq	(%rax,%rax,4), %rbp
	movq	%rbp, -40(%rsp)                 # 8-byte Spill
	leaq	(%r8,%r10,2), %rbp
	movq	%rbp, 80(%rsp)                  # 8-byte Spill
	leaq	(%r14,%r15,2), %rbp
	movq	%rbp, 272(%rsp)                 # 8-byte Spill
	leaq	(%r13,%r13,2), %rbp
	movq	%rbp, 72(%rsp)                  # 8-byte Spill
	movq	160(%rsp), %rbp                 # 8-byte Reload
	leaq	(%r8,%rbp,4), %rbp
	movq	%rbp, 64(%rsp)                  # 8-byte Spill
	movq	176(%rsp), %rbp                 # 8-byte Reload
	leaq	(%r14,%rbp), %rbp
	movq	%rbp, 264(%rsp)                 # 8-byte Spill
	leaq	(%r10,%r10,2), %rbp
	movq	%rbp, 56(%rsp)                  # 8-byte Spill
	movq	152(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rbp,2), %rbp
	movq	%rbp, 256(%rsp)                 # 8-byte Spill
	movq	184(%rsp), %rbp                 # 8-byte Reload
	leaq	(%r8,%rbp), %rbp
	movq	%rbp, 48(%rsp)                  # 8-byte Spill
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	leaq	(%rax,%rax,8), %rax
	movq	%rax, 40(%rsp)                  # 8-byte Spill
	movq	168(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r14), %rax
	movq	%rax, 248(%rsp)                 # 8-byte Spill
	movq	%r11, -72(%rsp)                 # 8-byte Spill
	leaq	(%r8,%r11,2), %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	%r15, 296(%rsp)                 # 8-byte Spill
	leaq	(%r15,%r15,2), %rax
	leaq	(%r13,%r13,4), %r15
	movq	%rax, 240(%rsp)                 # 8-byte Spill
	leaq	(%rax,%r14), %rax
	leaq	(%r8,%r10,4), %rbp
	movq	%rax, 232(%rsp)                 # 8-byte Spill
	leaq	(%r14,%rax), %rax
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	movq	%rbp, 472(%rsp)                 # 8-byte Spill
	leaq	(%r8,%rbp), %rax
	movq	%rax, 464(%rsp)                 # 8-byte Spill
	movslq	%ecx, %rax
	movq	%rax, 456(%rsp)                 # 8-byte Spill
	leaq	224(%r9), %rax
	movq	%rax, 448(%rsp)                 # 8-byte Spill
	movq	-104(%rsp), %rax                # 8-byte Reload
	addq	$224, %rax
	movq	%rax, 440(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 144(%rsp)                 # 8-byte Spill
	movq	%r10, 88(%rsp)                  # 8-byte Spill
	movq	%r15, 24(%rsp)                  # 8-byte Spill
	movq	%rsi, 16(%rsp)                  # 8-byte Spill
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	.p2align	4, 0x90
.LBB9_2:                                # %"for conv1_stage2.s1.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB9_3 Depth 2
                                        #       Child Loop BB9_5 Depth 3
                                        #       Child Loop BB9_8 Depth 3
                                        #         Child Loop BB9_9 Depth 4
                                        #           Child Loop BB9_25 Depth 5
                                        #           Child Loop BB9_29 Depth 5
	movl	132(%rsp), %eax                 # 4-byte Reload
	movl	%eax, %ebp
	imull	%edx, %ebp
	movl	128(%rsp), %eax                 # 4-byte Reload
	imull	%edx, %eax
	subl	%ebx, %eax
	movl	%eax, 140(%rsp)                 # 4-byte Spill
	shll	$5, %edx
	orl	%esi, %edx
	imull	%edi, %edx
	movq	208(%rsp), %rax                 # 8-byte Reload
	addl	%edx, %eax
	movl	%eax, -44(%rsp)                 # 4-byte Spill
	movq	216(%rsp), %rax                 # 8-byte Reload
	movq	%rbp, -24(%rsp)                 # 8-byte Spill
	addl	%ebp, %eax
	movl	%eax, 136(%rsp)                 # 4-byte Spill
	addl	124(%rsp), %edx                 # 4-byte Folded Reload
	xorl	%eax, %eax
	jmp	.LBB9_3
	.p2align	4, 0x90
.LBB9_32:                               # %"end for conv1_stage2.s1.n.n2"
                                        #   in Loop: Header=BB9_3 Depth=2
	movq	432(%rsp), %rax                 # 8-byte Reload
	incq	%rax
	movl	-112(%rsp), %edi                # 4-byte Reload
	addl	%edi, -44(%rsp)                 # 4-byte Folded Spill
	movq	200(%rsp), %rdx                 # 8-byte Reload
	addl	%edi, %edx
	cmpq	$8, %rax
	movq	16(%rsp), %rsi                  # 8-byte Reload
	je	.LBB9_20
.LBB9_3:                                # %"for conv1_stage2.s1.c.ci"
                                        #   Parent Loop BB9_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB9_5 Depth 3
                                        #       Child Loop BB9_8 Depth 3
                                        #         Child Loop BB9_9 Depth 4
                                        #           Child Loop BB9_25 Depth 5
                                        #           Child Loop BB9_29 Depth 5
	movq	%rdx, 200(%rsp)                 # 8-byte Spill
	movq	%rax, 432(%rsp)                 # 8-byte Spill
	addq	%rsi, %rax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	cmpl	$8, -48(%rsp)                   # 4-byte Folded Reload
	movq	-104(%rsp), %r15                # 8-byte Reload
	jl	.LBB9_6
# %bb.4:                                # %"for conv1_stage2.s1.n.n.preheader"
                                        #   in Loop: Header=BB9_3 Depth=2
	movq	152(%rsp), %rax                 # 8-byte Reload
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	addq	%rcx, %rax
	movq	-8(%rsp), %rsi                  # 8-byte Reload
	vbroadcastss	(%rsi,%rax,4), %ymm0
	vmovups	%ymm0, 768(%rsp)                # 32-byte Spill
	movq	296(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm0
	vmovups	%ymm0, 736(%rsp)                # 32-byte Spill
	movq	424(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm0
	vmovups	%ymm0, 704(%rsp)                # 32-byte Spill
	movq	344(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm0
	vmovups	%ymm0, 672(%rsp)                # 32-byte Spill
	movq	416(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm0
	vmovups	%ymm0, 640(%rsp)                # 32-byte Spill
	movq	352(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm0
	vmovups	%ymm0, 608(%rsp)                # 32-byte Spill
	movq	408(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm0
	vmovups	%ymm0, 576(%rsp)                # 32-byte Spill
	movq	336(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm0
	vmovups	%ymm0, 544(%rsp)                # 32-byte Spill
	movq	192(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm0
	vmovups	%ymm0, 512(%rsp)                # 32-byte Spill
	movq	280(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm0
	vmovups	%ymm0, 480(%rsp)                # 32-byte Spill
	movq	384(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm10
	movq	272(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm11
	movq	376(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm12
	movq	176(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm13
	movq	264(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm14
	movq	360(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	vbroadcastss	(%rsi,%rax,4), %ymm15
	movq	256(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %r8
	movq	168(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %r9
	movq	248(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %r10
	movq	240(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %r11
	movq	232(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %r12
	movq	224(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rbp
	movq	320(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rdi
	movq	328(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %r14
	movl	-44(%rsp), %eax                 # 4-byte Reload
                                        # kill: def $eax killed $eax def $rax
	movq	-24(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx def $rcx
	movq	(%rsp), %rdx                    # 8-byte Reload
	vbroadcastss	(%rsi,%r8,4), %ymm0
	movq	392(%rsp), %r8                  # 8-byte Reload
	vbroadcastss	(%rsi,%r9,4), %ymm1
	vbroadcastss	(%rsi,%r10,4), %ymm2
	vbroadcastss	(%rsi,%r11,4), %ymm3
	movq	160(%rsp), %r11                 # 8-byte Reload
	movq	368(%rsp), %r10                 # 8-byte Reload
	movq	-88(%rsp), %r9                  # 8-byte Reload
	vbroadcastss	(%rsi,%r12,4), %ymm4
	movq	312(%rsp), %r12                 # 8-byte Reload
	vbroadcastss	(%rsi,%rbp,4), %ymm5
	movq	-56(%rsp), %rbx                 # 8-byte Reload
	vbroadcastss	(%rsi,%rdi,4), %ymm6
	movq	288(%rsp), %rdi                 # 8-byte Reload
	movq	184(%rsp), %rbp                 # 8-byte Reload
	vbroadcastss	(%rsi,%r14,4), %ymm7
	movq	400(%rsp), %r14                 # 8-byte Reload
	.p2align	4, 0x90
.LBB9_5:                                # %"for conv1_stage2.s1.n.n"
                                        #   Parent Loop BB9_2 Depth=1
                                        #     Parent Loop BB9_3 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	cltq
	movslq	%ecx, %rcx
	vmovups	(%r9,%rcx,4), %ymm8
	vmovups	768(%rsp), %ymm9                # 32-byte Reload
	vfmadd213ps	(%r15,%rax,4), %ymm9, %ymm8 # ymm8 = (ymm9 * ymm8) + mem
	leaq	(%rbx,%rcx), %rsi
	vmovups	736(%rsp), %ymm9                # 32-byte Reload
	vfmadd231ps	(%r9,%rsi,4), %ymm9, %ymm8 # ymm8 = (ymm9 * mem) + ymm8
	leaq	(%rdi,%rcx), %rsi
	vmovups	704(%rsp), %ymm9                # 32-byte Reload
	vfmadd231ps	(%r9,%rsi,4), %ymm9, %ymm8 # ymm8 = (ymm9 * mem) + ymm8
	leaq	(%r11,%rcx), %rsi
	vmovups	672(%rsp), %ymm9                # 32-byte Reload
	vfmadd231ps	(%r9,%rsi,4), %ymm9, %ymm8 # ymm8 = (ymm9 * mem) + ymm8
	leaq	(%r13,%rcx), %rsi
	vmovups	640(%rsp), %ymm9                # 32-byte Reload
	vfmadd231ps	(%r9,%rsi,4), %ymm9, %ymm8 # ymm8 = (ymm9 * mem) + ymm8
	movq	88(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vmovups	608(%rsp), %ymm9                # 32-byte Reload
	vfmadd231ps	(%r9,%rsi,4), %ymm9, %ymm8 # ymm8 = (ymm9 * mem) + ymm8
	movq	-64(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vmovups	576(%rsp), %ymm9                # 32-byte Reload
	vfmadd231ps	(%r9,%rsi,4), %ymm9, %ymm8 # ymm8 = (ymm9 * mem) + ymm8
	leaq	(%r8,%rcx), %rsi
	vmovups	544(%rsp), %ymm9                # 32-byte Reload
	vfmadd231ps	(%r9,%rsi,4), %ymm9, %ymm8 # ymm8 = (ymm9 * mem) + ymm8
	leaq	(%r14,%rcx), %rsi
	vmovups	512(%rsp), %ymm9                # 32-byte Reload
	vfmadd231ps	(%r9,%rsi,4), %ymm9, %ymm8 # ymm8 = (ymm9 * mem) + ymm8
	movq	-72(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vmovups	480(%rsp), %ymm9                # 32-byte Reload
	vfmadd231ps	(%r9,%rsi,4), %ymm9, %ymm8 # ymm8 = (ymm9 * mem) + ymm8
	movq	-40(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm10, %ymm8 # ymm8 = (ymm10 * mem) + ymm8
	movq	80(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm11, %ymm8 # ymm8 = (ymm11 * mem) + ymm8
	movq	72(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm12, %ymm8 # ymm8 = (ymm12 * mem) + ymm8
	movq	64(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm13, %ymm8 # ymm8 = (ymm13 * mem) + ymm8
	leaq	(%r10,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm14, %ymm8 # ymm8 = (ymm14 * mem) + ymm8
	movq	56(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm15, %ymm8 # ymm8 = (ymm15 * mem) + ymm8
	leaq	(%rbp,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm0, %ymm8 # ymm8 = (ymm0 * mem) + ymm8
	movq	48(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm1, %ymm8 # ymm8 = (ymm1 * mem) + ymm8
	movq	40(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm2, %ymm8 # ymm8 = (ymm2 * mem) + ymm8
	movq	32(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm3, %ymm8 # ymm8 = (ymm3 * mem) + ymm8
	movq	24(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm4, %ymm8 # ymm8 = (ymm4 * mem) + ymm8
	movq	472(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm5, %ymm8 # ymm8 = (ymm5 * mem) + ymm8
	movq	464(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm6, %ymm8 # ymm8 = (ymm6 * mem) + ymm8
	leaq	(%r12,%rcx), %rsi
	vfmadd231ps	(%r9,%rsi,4), %ymm7, %ymm8 # ymm8 = (ymm7 * mem) + ymm8
	vmovups	%ymm8, (%r15,%rax,4)
	addl	$8, %ecx
	addl	$8, %eax
	decq	%rdx
	jne	.LBB9_5
.LBB9_6:                                # %"end for conv1_stage2.s1.n.n"
                                        #   in Loop: Header=BB9_3 Depth=2
	movq	8(%rsp), %rax                   # 8-byte Reload
	cmpl	%eax, (%rsp)                    # 4-byte Folded Reload
	movq	-16(%rsp), %rdi                 # 8-byte Reload
	jge	.LBB9_32
# %bb.7:                                # %"for conv1_stage2.s1.n.n1.preheader"
                                        #   in Loop: Header=BB9_3 Depth=2
	movl	-112(%rsp), %eax                # 4-byte Reload
	imull	-80(%rsp), %eax                 # 4-byte Folded Reload
	addl	140(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, 96(%rsp)                  # 4-byte Spill
	xorl	%esi, %esi
	movl	-32(%rsp), %eax                 # 4-byte Reload
	movl	%eax, %ebp
	movl	%eax, %ebx
	movq	200(%rsp), %rax                 # 8-byte Reload
	movl	%eax, %ecx
	movl	136(%rsp), %eax                 # 4-byte Reload
	movq	304(%rsp), %r11                 # 8-byte Reload
	jmp	.LBB9_8
	.p2align	4, 0x90
.LBB9_31:                               # %"end for conv1_stage2.s1.r63$x6"
                                        #   in Loop: Header=BB9_8 Depth=3
	movq	104(%rsp), %r11                 # 8-byte Reload
	incq	%r11
	movq	608(%rsp), %rsi                 # 8-byte Reload
	incl	%esi
	movl	480(%rsp), %eax                 # 4-byte Reload
	addl	$8, %eax
	movl	512(%rsp), %ecx                 # 4-byte Reload
	addl	$8, %ecx
	movl	544(%rsp), %ebx                 # 4-byte Reload
	addl	$-8, %ebx
	movl	576(%rsp), %ebp                 # 4-byte Reload
	addl	$-8, %ebp
	cmpq	456(%rsp), %r11                 # 8-byte Folded Reload
	movq	-88(%rsp), %r9                  # 8-byte Reload
	je	.LBB9_32
.LBB9_8:                                # %"for conv1_stage2.s1.n.n1"
                                        #   Parent Loop BB9_2 Depth=1
                                        #     Parent Loop BB9_3 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB9_9 Depth 4
                                        #           Child Loop BB9_25 Depth 5
                                        #           Child Loop BB9_29 Depth 5
	movl	%eax, 480(%rsp)                 # 4-byte Spill
	cltq
	leaq	(%r9,%rax,4), %r12
	movl	%ecx, 512(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rcx
	movq	-104(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%rcx,4), %r9
	cmpl	$9, %ebp
	movl	$8, %r8d
	movl	%ebp, 576(%rsp)                 # 4-byte Spill
	cmovll	%ebp, %r8d
	movq	448(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	cmpl	$9, %ebx
	movl	$8, %ebp
	movl	%ebx, 544(%rsp)                 # 4-byte Spill
	cmovll	%ebx, %ebp
	movq	440(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %r14
	andl	$-32, %ebp
	addq	$-32, %rbp
	shrq	$5, %rbp
	incq	%rbp
	andq	$-2, %rbp
	negq	%rbp
	movq	%rbp, 704(%rsp)                 # 8-byte Spill
	movq	%rsi, 608(%rsp)                 # 8-byte Spill
	leal	(,%rsi,8), %eax
	movl	-32(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, %ebp
	subl	%eax, %ebp
	cmpl	$9, %ebp
	movl	$8, %eax
	cmovgel	%eax, %ebp
	movl	%ebp, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, 768(%rsp)                 # 8-byte Spill
	shrq	$5, %rax
	incq	%rax
	movq	%rax, 736(%rsp)                 # 8-byte Spill
	movq	112(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%r11,8), %ecx
	movl	-108(%rsp), %eax                # 4-byte Reload
	subl	%ecx, %eax
	addl	96(%rsp), %ecx                  # 4-byte Folded Reload
	movq	-24(%rsp), %rsi                 # 8-byte Reload
	movq	%r11, 104(%rsp)                 # 8-byte Spill
	leal	(%rsi,%r11,8), %esi
	movslq	%ecx, %rcx
	movq	%rcx, 672(%rsp)                 # 8-byte Spill
	movslq	%esi, %rcx
	movq	%rcx, 640(%rsp)                 # 8-byte Spill
	movl	%ebp, %ecx
	andl	$-32, %ecx
	xorl	%r11d, %r11d
	jmp	.LBB9_9
	.p2align	4, 0x90
.LBB9_30:                               # %"end for conv1_stage2.s1.n.ni"
                                        #   in Loop: Header=BB9_9 Depth=4
	incq	%r11
	addq	%r13, %rdx
	addq	%r13, %r12
	cmpq	$24, %r11
	je	.LBB9_31
.LBB9_9:                                # %"for conv1_stage2.s1.r63$x5"
                                        #   Parent Loop BB9_2 Depth=1
                                        #     Parent Loop BB9_3 Depth=2
                                        #       Parent Loop BB9_8 Depth=3
                                        # =>      This Loop Header: Depth=4
                                        #           Child Loop BB9_25 Depth 5
                                        #           Child Loop BB9_29 Depth 5
	testl	%eax, %eax
	jle	.LBB9_30
# %bb.10:                               # %"for conv1_stage2.s1.n.ni.preheader"
                                        #   in Loop: Header=BB9_9 Depth=4
	leaq	8(%r11), %rsi
	imulq	%rdi, %rsi
	addq	-80(%rsp), %rsi                 # 8-byte Folded Reload
	movq	-8(%rsp), %rbx                  # 8-byte Reload
	vmovss	(%rbx,%rsi,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	cmpl	$31, %ebp
	ja	.LBB9_22
# %bb.11:                               #   in Loop: Header=BB9_9 Depth=4
	xorl	%esi, %esi
	jmp	.LBB9_29
	.p2align	4, 0x90
.LBB9_22:                               # %vector.ph95
                                        #   in Loop: Header=BB9_9 Depth=4
	vbroadcastss	%xmm0, %ymm1
	cmpq	$0, 768(%rsp)                   # 8-byte Folded Reload
	je	.LBB9_23
# %bb.24:                               # %vector.body92.preheader
                                        #   in Loop: Header=BB9_9 Depth=4
	movq	704(%rsp), %r10                 # 8-byte Reload
	xorl	%r15d, %r15d
	.p2align	4, 0x90
.LBB9_25:                               # %vector.body92
                                        #   Parent Loop BB9_2 Depth=1
                                        #     Parent Loop BB9_3 Depth=2
                                        #       Parent Loop BB9_8 Depth=3
                                        #         Parent Loop BB9_9 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovups	-224(%rdx,%r15,4), %ymm2
	vmovups	-192(%rdx,%r15,4), %ymm3
	vmovups	-160(%rdx,%r15,4), %ymm4
	vmovups	-128(%rdx,%r15,4), %ymm5
	vfmadd213ps	-224(%r14,%r15,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-192(%r14,%r15,4), %ymm1, %ymm3 # ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-160(%r14,%r15,4), %ymm1, %ymm4 # ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-128(%r14,%r15,4), %ymm1, %ymm5 # ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -224(%r14,%r15,4)
	vmovups	%ymm3, -192(%r14,%r15,4)
	vmovups	%ymm4, -160(%r14,%r15,4)
	vmovups	%ymm5, -128(%r14,%r15,4)
	vmovups	-96(%rdx,%r15,4), %ymm2
	vmovups	-64(%rdx,%r15,4), %ymm3
	vmovups	-32(%rdx,%r15,4), %ymm4
	vmovups	(%rdx,%r15,4), %ymm5
	vfmadd213ps	-96(%r14,%r15,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%r14,%r15,4), %ymm1, %ymm3 # ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%r14,%r15,4), %ymm1, %ymm4 # ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%r14,%r15,4), %ymm1, %ymm5 # ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%r14,%r15,4)
	vmovups	%ymm3, -64(%r14,%r15,4)
	vmovups	%ymm4, -32(%r14,%r15,4)
	vmovups	%ymm5, (%r14,%r15,4)
	addq	$64, %r15
	addq	$2, %r10
	jne	.LBB9_25
# %bb.26:                               # %middle.block90.unr-lcssa
                                        #   in Loop: Header=BB9_9 Depth=4
	testb	$1, 736(%rsp)                   # 1-byte Folded Reload
	je	.LBB9_28
.LBB9_27:                               # %vector.body92.epil
                                        #   in Loop: Header=BB9_9 Depth=4
	movq	%r11, %rsi
	imulq	-56(%rsp), %rsi                 # 8-byte Folded Reload
	addq	640(%rsp), %rsi                 # 8-byte Folded Reload
	movq	672(%rsp), %rdi                 # 8-byte Reload
	addq	%r15, %rdi
	addq	%r15, %rsi
	movq	-88(%rsp), %rbx                 # 8-byte Reload
	vmovups	(%rbx,%rsi,4), %ymm2
	vmovups	32(%rbx,%rsi,4), %ymm3
	vmovups	64(%rbx,%rsi,4), %ymm4
	vmovups	96(%rbx,%rsi,4), %ymm5
	movq	-104(%rsp), %rsi                # 8-byte Reload
	vfmadd213ps	(%rsi,%rdi,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	32(%rsi,%rdi,4), %ymm1, %ymm3 # ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	64(%rsi,%rdi,4), %ymm1, %ymm4 # ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	96(%rsi,%rdi,4), %ymm1, %ymm5 # ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, (%rsi,%rdi,4)
	vmovups	%ymm3, 32(%rsi,%rdi,4)
	vmovups	%ymm4, 64(%rsi,%rdi,4)
	vmovups	%ymm5, 96(%rsi,%rdi,4)
	movq	-16(%rsp), %rdi                 # 8-byte Reload
.LBB9_28:                               # %middle.block90
                                        #   in Loop: Header=BB9_9 Depth=4
	movq	%rcx, %rsi
	cmpq	%rbp, %rcx
	je	.LBB9_30
	.p2align	4, 0x90
.LBB9_29:                               # %"for conv1_stage2.s1.n.ni"
                                        #   Parent Loop BB9_2 Depth=1
                                        #     Parent Loop BB9_3 Depth=2
                                        #       Parent Loop BB9_8 Depth=3
                                        #         Parent Loop BB9_9 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovss	(%r12,%rsi,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	(%r9,%rsi,4), %xmm0, %xmm1 # xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, (%r9,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %r8
	jne	.LBB9_29
	jmp	.LBB9_30
.LBB9_23:                               #   in Loop: Header=BB9_9 Depth=4
	xorl	%r15d, %r15d
	testb	$1, 736(%rsp)                   # 1-byte Folded Reload
	jne	.LBB9_27
	jmp	.LBB9_28
	.p2align	4, 0x90
.LBB9_20:                               # %"end for conv1_stage2.s1.c.ci"
                                        #   in Loop: Header=BB9_2 Depth=1
	testb	$1, 144(%rsp)                   # 1-byte Folded Reload
	jne	.LBB9_35
# %bb.21:                               # %"end for conv1_stage2.s1.c.ci.for conv1_stage2.s1.w.wi_crit_edge"
                                        #   in Loop: Header=BB9_2 Depth=1
	movb	$1, %al
	movq	%rax, 144(%rsp)                 # 8-byte Spill
	movl	120(%rsp), %eax                 # 4-byte Reload
	movl	%eax, %edx
	movl	-92(%rsp), %ebx                 # 4-byte Reload
	jmp	.LBB9_2
.LBB9_12:                               # %false_bb
	movl	28(%rdx), %eax
	movl	%esi, %edx
	sarl	$2, %edx
	leal	(%rdx,%rdx), %ebx
	movq	%rbx, %rbp
	movq	%rbx, 56(%rsp)                  # 8-byte Spill
	subl	%ebx, %eax
	cmpl	$3, %eax
	movl	$2, %ebp
	cmovll	%eax, %ebp
	movl	%ebp, 48(%rsp)                  # 4-byte Spill
	testl	%eax, %eax
	jle	.LBB9_35
# %bb.13:                               # %"for conv1_stage2.s1.w.wi8.preheader"
	movq	%r11, %rbp
	shll	$3, %esi
	andl	$24, %esi
	leal	7(%rcx), %eax
	sarl	$3, %eax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	-56(%rsp), %rbx                 # 8-byte Reload
	leal	(,%rbx,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, 40(%rsp)                  # 4-byte Spill
	movl	-108(%rsp), %eax                # 4-byte Reload
	movq	112(%rsp), %r8                  # 8-byte Reload
	subl	%r8d, %eax
	movl	%eax, 480(%rsp)                 # 4-byte Spill
	leaq	224(%r9), %rax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movl	%ebx, %eax
	imull	%edx, %eax
	shll	$4, %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, -40(%rsp)                 # 4-byte Spill
	leal	(,%rbx,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, 32(%rsp)                  # 4-byte Spill
	leaq	(,%rbx,4), %r10
	leaq	224(%r11), %rax
	movq	%rax, 88(%rsp)                  # 8-byte Spill
	shll	$6, %edx
	orl	%esi, %edx
	imull	%edi, %edx
	addl	%r8d, %edx
	subl	-92(%rsp), %edx                 # 4-byte Folded Reload
	shll	$5, %edi
	movl	%edi, 24(%rsp)                  # 4-byte Spill
	xorl	%edi, %edi
	movq	%rsi, 16(%rsp)                  # 8-byte Spill
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	jmp	.LBB9_14
	.p2align	4, 0x90
.LBB9_34:                               # %"end for conv1_stage2.s1.c.ci12"
                                        #   in Loop: Header=BB9_14 Depth=1
	movq	64(%rsp), %rdi                  # 8-byte Reload
	incl	%edi
	movl	32(%rsp), %eax                  # 4-byte Reload
	addl	%eax, -40(%rsp)                 # 4-byte Folded Spill
	movq	72(%rsp), %rdx                  # 8-byte Reload
	addl	24(%rsp), %edx                  # 4-byte Folded Reload
	cmpl	48(%rsp), %edi                  # 4-byte Folded Reload
	je	.LBB9_35
.LBB9_14:                               # %"for conv1_stage2.s1.w.wi8"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB9_15 Depth 2
                                        #       Child Loop BB9_17 Depth 3
                                        #         Child Loop BB9_18 Depth 4
                                        #           Child Loop BB9_43 Depth 5
                                        #           Child Loop BB9_39 Depth 5
	movq	56(%rsp), %rax                  # 8-byte Reload
	movq	%rdi, 64(%rsp)                  # 8-byte Spill
	leal	(%rdi,%rax), %esi
	movl	40(%rsp), %eax                  # 4-byte Reload
                                        # kill: def $eax killed $eax def $rax
	imull	%esi, %eax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	shll	$5, %esi
	movq	%rsi, 80(%rsp)                  # 8-byte Spill
	movq	%rdx, 72(%rsp)                  # 8-byte Spill
	movl	%edx, %eax
	movl	%edx, -64(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	-16(%rsp), %rdi                 # 8-byte Reload
	jmp	.LBB9_15
	.p2align	4, 0x90
.LBB9_33:                               # %"end for conv1_stage2.s1.n.n15"
                                        #   in Loop: Header=BB9_15 Depth=2
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	incq	%rcx
	movl	-64(%rsp), %eax                 # 4-byte Reload
	addl	-112(%rsp), %eax                # 4-byte Folded Reload
	movl	%eax, -64(%rsp)                 # 4-byte Spill
	movq	%rcx, %rax
	movq	%rcx, -72(%rsp)                 # 8-byte Spill
	cmpq	$8, %rcx
	movq	8(%rsp), %rcx                   # 8-byte Reload
	je	.LBB9_34
.LBB9_15:                               # %"for conv1_stage2.s1.c.ci11"
                                        #   Parent Loop BB9_14 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB9_17 Depth 3
                                        #         Child Loop BB9_18 Depth 4
                                        #           Child Loop BB9_43 Depth 5
                                        #           Child Loop BB9_39 Depth 5
	testl	%ecx, %ecx
	jle	.LBB9_33
# %bb.16:                               # %"for conv1_stage2.s1.n.n14.preheader"
                                        #   in Loop: Header=BB9_15 Depth=2
	movq	16(%rsp), %rax                  # 8-byte Reload
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %r8
	movq	80(%rsp), %rax                  # 8-byte Reload
	addl	%r8d, %eax
	imull	-112(%rsp), %eax                # 4-byte Folded Reload
	movl	%eax, 104(%rsp)                 # 4-byte Spill
	movl	480(%rsp), %edx                 # 4-byte Reload
	movl	-64(%rsp), %ecx                 # 4-byte Reload
	movl	-40(%rsp), %eax                 # 4-byte Reload
	xorl	%ebx, %ebx
	jmp	.LBB9_17
	.p2align	4, 0x90
.LBB9_48:                               # %"end for conv1_stage2.s1.r63$x19"
                                        #   in Loop: Header=BB9_17 Depth=3
	movq	512(%rsp), %rbx                 # 8-byte Reload
	incq	%rbx
	movl	544(%rsp), %eax                 # 4-byte Reload
	addl	$8, %eax
	movl	576(%rsp), %ecx                 # 4-byte Reload
	addl	$8, %ecx
	movl	608(%rsp), %edx                 # 4-byte Reload
	addl	$-8, %edx
	cmpq	-32(%rsp), %rbx                 # 8-byte Folded Reload
	movq	-88(%rsp), %r9                  # 8-byte Reload
	je	.LBB9_33
.LBB9_17:                               # %"for conv1_stage2.s1.n.n14"
                                        #   Parent Loop BB9_14 Depth=1
                                        #     Parent Loop BB9_15 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB9_18 Depth 4
                                        #           Child Loop BB9_43 Depth 5
                                        #           Child Loop BB9_39 Depth 5
	movl	%eax, 544(%rsp)                 # 4-byte Spill
	cltq
	leaq	(%r9,%rax,4), %rsi
	movl	%ecx, 576(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rcx
	cmpl	$9, %edx
	movl	$8, %r13d
	movl	%edx, 608(%rsp)                 # 4-byte Spill
	cmovll	%edx, %r13d
	leaq	(%rbp,%rcx,4), %r15
	movq	96(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rdx,%rax,4), %r11
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rcx,4), %r9
	movl	%r13d, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	shrq	$5, %rax
	incq	%rax
	andq	$-2, %rax
	negq	%rax
	movq	%rax, 640(%rsp)                 # 8-byte Spill
	leal	(,%rbx,8), %eax
	movl	480(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, %r14d
	subl	%eax, %r14d
	cmpl	$9, %r14d
	movl	$8, %eax
	cmovgel	%eax, %r14d
	movl	%r14d, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	movq	%rax, 736(%rsp)                 # 8-byte Spill
	shrq	$5, %rax
	incq	%rax
	movq	%rax, 704(%rsp)                 # 8-byte Spill
	movq	112(%rsp), %rcx                 # 8-byte Reload
	leal	(%rcx,%rbx,8), %eax
	leal	(%rcx,%rbx,8), %r12d
	addl	$8, %r12d
	movl	-108(%rsp), %ecx                # 4-byte Reload
	subl	%eax, %ecx
	movl	%ecx, 768(%rsp)                 # 4-byte Spill
	subl	-92(%rsp), %eax                 # 4-byte Folded Reload
	addl	104(%rsp), %eax                 # 4-byte Folded Reload
	movq	-24(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, 512(%rsp)                 # 8-byte Spill
	leal	(%rcx,%rbx,8), %edx
	movslq	%eax, %rcx
	movslq	%edx, %rax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	movl	%r14d, %eax
	andl	$-32, %eax
	movq	%rax, 672(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	jmp	.LBB9_18
	.p2align	4, 0x90
.LBB9_19:                               # %true_bb21
                                        #   in Loop: Header=BB9_18 Depth=4
	movq	%rax, %rdx
	imulq	-56(%rsp), %rdx                 # 8-byte Folded Reload
	addq	-80(%rsp), %rdx                 # 8-byte Folded Reload
	movq	-88(%rsp), %rbx                 # 8-byte Reload
	vmovups	(%rbx,%rdx,4), %ymm0
	leaq	8(%rax), %rdx
	imulq	%rdi, %rdx
	addq	%r8, %rdx
	movq	-8(%rsp), %rbx                  # 8-byte Reload
	vbroadcastss	(%rbx,%rdx,4), %ymm1
	vfmadd213ps	(%rbp,%rcx,4), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovups	%ymm1, (%rbp,%rcx,4)
.LBB9_47:                               # %after_bb23
                                        #   in Loop: Header=BB9_18 Depth=4
	incq	%rax
	addq	%r10, %r11
	addq	%r10, %rsi
	cmpq	$24, %rax
	je	.LBB9_48
.LBB9_18:                               # %"for conv1_stage2.s1.r63$x18"
                                        #   Parent Loop BB9_14 Depth=1
                                        #     Parent Loop BB9_15 Depth=2
                                        #       Parent Loop BB9_17 Depth=3
                                        # =>      This Loop Header: Depth=4
                                        #           Child Loop BB9_43 Depth 5
                                        #           Child Loop BB9_39 Depth 5
	cmpl	-108(%rsp), %r12d               # 4-byte Folded Reload
	jle	.LBB9_19
# %bb.36:                               # %false_bb22
                                        #   in Loop: Header=BB9_18 Depth=4
	cmpl	$0, 768(%rsp)                   # 4-byte Folded Reload
	jle	.LBB9_47
# %bb.37:                               # %"for conv1_stage2.s1.n.ni24.preheader"
                                        #   in Loop: Header=BB9_18 Depth=4
	leaq	8(%rax), %rdx
	imulq	%rdi, %rdx
	addq	%r8, %rdx
	movq	-8(%rsp), %rbp                  # 8-byte Reload
	vmovss	(%rbp,%rdx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	cmpl	$31, %r14d
	ja	.LBB9_40
# %bb.38:                               #   in Loop: Header=BB9_18 Depth=4
	xorl	%edx, %edx
	movq	-104(%rsp), %rbp                # 8-byte Reload
	jmp	.LBB9_39
	.p2align	4, 0x90
.LBB9_40:                               # %vector.ph
                                        #   in Loop: Header=BB9_18 Depth=4
	vbroadcastss	%xmm0, %ymm1
	cmpq	$0, 736(%rsp)                   # 8-byte Folded Reload
	je	.LBB9_41
# %bb.42:                               # %vector.body.preheader
                                        #   in Loop: Header=BB9_18 Depth=4
	movq	640(%rsp), %rdx                 # 8-byte Reload
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB9_43:                               # %vector.body
                                        #   Parent Loop BB9_14 Depth=1
                                        #     Parent Loop BB9_15 Depth=2
                                        #       Parent Loop BB9_17 Depth=3
                                        #         Parent Loop BB9_18 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovups	-224(%r11,%rbp,4), %ymm2
	vmovups	-192(%r11,%rbp,4), %ymm3
	vmovups	-160(%r11,%rbp,4), %ymm4
	vmovups	-128(%r11,%rbp,4), %ymm5
	vfmadd213ps	-224(%r9,%rbp,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-192(%r9,%rbp,4), %ymm1, %ymm3 # ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-160(%r9,%rbp,4), %ymm1, %ymm4 # ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	-128(%r9,%rbp,4), %ymm1, %ymm5 # ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -224(%r9,%rbp,4)
	vmovups	%ymm3, -192(%r9,%rbp,4)
	vmovups	%ymm4, -160(%r9,%rbp,4)
	vmovups	%ymm5, -128(%r9,%rbp,4)
	vmovups	-96(%r11,%rbp,4), %ymm2
	vmovups	-64(%r11,%rbp,4), %ymm3
	vmovups	-32(%r11,%rbp,4), %ymm4
	vmovups	(%r11,%rbp,4), %ymm5
	vfmadd213ps	-96(%r9,%rbp,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	-64(%r9,%rbp,4), %ymm1, %ymm3 # ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	-32(%r9,%rbp,4), %ymm1, %ymm4 # ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	(%r9,%rbp,4), %ymm1, %ymm5 # ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, -96(%r9,%rbp,4)
	vmovups	%ymm3, -64(%r9,%rbp,4)
	vmovups	%ymm4, -32(%r9,%rbp,4)
	vmovups	%ymm5, (%r9,%rbp,4)
	addq	$64, %rbp
	addq	$2, %rdx
	jne	.LBB9_43
# %bb.44:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB9_18 Depth=4
	testb	$1, 704(%rsp)                   # 1-byte Folded Reload
	je	.LBB9_46
.LBB9_45:                               # %vector.body.epil
                                        #   in Loop: Header=BB9_18 Depth=4
	movq	%rax, %rdx
	imulq	-56(%rsp), %rdx                 # 8-byte Folded Reload
	addq	-80(%rsp), %rdx                 # 8-byte Folded Reload
	leaq	(%rcx,%rbp), %rdi
	addq	%rbp, %rdx
	movq	-88(%rsp), %rbx                 # 8-byte Reload
	vmovups	(%rbx,%rdx,4), %ymm2
	vmovups	32(%rbx,%rdx,4), %ymm3
	vmovups	64(%rbx,%rdx,4), %ymm4
	vmovups	96(%rbx,%rdx,4), %ymm5
	movq	-104(%rsp), %rdx                # 8-byte Reload
	vfmadd213ps	(%rdx,%rdi,4), %ymm1, %ymm2 # ymm2 = (ymm1 * ymm2) + mem
	vfmadd213ps	32(%rdx,%rdi,4), %ymm1, %ymm3 # ymm3 = (ymm1 * ymm3) + mem
	vfmadd213ps	64(%rdx,%rdi,4), %ymm1, %ymm4 # ymm4 = (ymm1 * ymm4) + mem
	vfmadd213ps	96(%rdx,%rdi,4), %ymm1, %ymm5 # ymm5 = (ymm1 * ymm5) + mem
	vmovups	%ymm2, (%rdx,%rdi,4)
	vmovups	%ymm3, 32(%rdx,%rdi,4)
	vmovups	%ymm4, 64(%rdx,%rdi,4)
	vmovups	%ymm5, 96(%rdx,%rdi,4)
	movq	-16(%rsp), %rdi                 # 8-byte Reload
.LBB9_46:                               # %middle.block
                                        #   in Loop: Header=BB9_18 Depth=4
	movq	672(%rsp), %rbx                 # 8-byte Reload
	movq	%rbx, %rdx
	cmpq	%r14, %rbx
	movq	-104(%rsp), %rbp                # 8-byte Reload
	je	.LBB9_47
	.p2align	4, 0x90
.LBB9_39:                               # %"for conv1_stage2.s1.n.ni24"
                                        #   Parent Loop BB9_14 Depth=1
                                        #     Parent Loop BB9_15 Depth=2
                                        #       Parent Loop BB9_17 Depth=3
                                        #         Parent Loop BB9_18 Depth=4
                                        # =>        This Inner Loop Header: Depth=5
	vmovss	(%rsi,%rdx,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	(%r15,%rdx,4), %xmm0, %xmm1 # xmm1 = (xmm0 * xmm1) + mem
	vmovss	%xmm1, (%r15,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %r13
	jne	.LBB9_39
	jmp	.LBB9_47
.LBB9_41:                               #   in Loop: Header=BB9_18 Depth=4
	xorl	%ebp, %ebp
	testb	$1, 704(%rsp)                   # 1-byte Folded Reload
	jne	.LBB9_45
	jmp	.LBB9_46
.LBB9_35:                               # %destructor_block
	xorl	%eax, %eax
	addq	$808, %rsp                      # imm = 0x328
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end9:
	.size	train_cost_model.par_for.conv1_stage2.s1.c.c.c, .Lfunc_end9-train_cost_model.par_for.conv1_stage2.s1.c.c.c
                                        # -- End function
	.section	.text.train_cost_model.par_for.f7.s0.n.n,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.f7.s0.n.n
	.type	train_cost_model.par_for.f7.s0.n.n,@function
train_cost_model.par_for.f7.s0.n.n:     # @train_cost_model.par_for.f7.s0.n.n
# %bb.0:                                # %entry
	pushq	%rax
	movslq	4(%rdx), %rdi
	movq	16(%rdx), %rax
	movslq	%esi, %rcx
	leaq	(%rdi,%rcx,8), %rsi
	cmpl	%ecx, (%rdx)
	jle	.LBB10_2
# %bb.1:                                # %true_bb
	subq	%rdi, %rsi
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%ymm0, (%rax,%rsi,4)
.LBB10_4:                               # %destructor_block
	xorl	%eax, %eax
	popq	%rcx
	vzeroupper
	retq
.LBB10_2:                               # %false_bb
	movl	8(%rdx), %edx
	subl	%esi, %edx
	jle	.LBB10_4
# %bb.3:                                # %"for f7.s0.n.ni.preheader"
	shlq	$3, %rcx
	cmpl	$9, %edx
	movl	$8, %esi
	cmovll	%edx, %esi
	movslq	%ecx, %rcx
	leaq	(%rax,%rcx,4), %rdi
	decl	%esi
	leaq	4(,%rsi,4), %rdx
	xorl	%esi, %esi
	callq	memset@PLT
	xorl	%eax, %eax
	popq	%rcx
	retq
.Lfunc_end10:
	.size	train_cost_model.par_for.f7.s0.n.n, .Lfunc_end10-train_cost_model.par_for.f7.s0.n.n
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.f7.s1.n.n
.LCPI11_0:
	.long	0x3f800000                      # float 1
.LCPI11_1:
	.long	0x45800000                      # float 4096
.LCPI11_2:
	.long	0x40000000                      # float 2
.LCPI11_3:
	.long	0xbf800000                      # float -1
.LCPI11_4:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4
.LCPI11_5:
	.long	24                              # 0x18
	.long	22                              # 0x16
	.long	25                              # 0x19
	.long	9                               # 0x9
.LCPI11_6:
	.long	33                              # 0x21
	.long	27                              # 0x1b
	.long	28                              # 0x1c
	.long	26                              # 0x1a
	.section	.text.train_cost_model.par_for.f7.s1.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.f7.s1.n.n,@function
train_cost_model.par_for.f7.s1.n.n:     # @train_cost_model.par_for.f7.s1.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1128, %rsp                     # imm = 0x468
                                        # kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %rax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	movl	4(%rdx), %eax
	movl	8(%rdx), %r13d
	movslq	12(%rdx), %rcx
	movq	%rcx, 208(%rsp)                 # 8-byte Spill
	movl	20(%rdx), %r10d
	movl	24(%rdx), %r9d
	movslq	28(%rdx), %r8
	vmovss	32(%rdx), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	vmovaps	%xmm0, 720(%rsp)                # 16-byte Spill
	movq	40(%rdx), %rcx
	movq	56(%rdx), %r11
	movq	72(%rdx), %rdi
	movq	%rdi, (%rsp)                    # 8-byte Spill
	leal	(,%rsi,8), %edi
	movl	%edi, 640(%rsp)                 # 4-byte Spill
	leal	(%r10,%rsi,8), %r14d
	cmpl	%esi, 16(%rdx)
	movq	%rax, 200(%rsp)                 # 8-byte Spill
	movq	%r11, -40(%rsp)                 # 8-byte Spill
	jle	.LBB11_5
# %bb.1:                                # %true_bb
	testl	%eax, %eax
	jle	.LBB11_41
# %bb.2:                                # %"for f7.s1.r77$x.preheader"
	leal	(%r13,%r13), %ebx
	leal	(%rbx,%rbx,4), %eax
	subl	%r9d, %eax
	addl	%r14d, %eax
	movl	%eax, 608(%rsp)                 # 4-byte Spill
	leal	(%r13,%r13,4), %eax
	movl	%r14d, %r12d
	movl	%r9d, %ebp
	movq	%r8, %r11
	movq	%r8, 480(%rsp)                  # 8-byte Spill
	leal	(%r13,%rax,2), %r15d
	leal	(,%r13,4), %edx
	movq	%r10, %r9
	leal	(%rdx,%rdx,2), %r10d
	movl	%r13d, %edx
	shll	$4, %edx
	addl	%r13d, %edx
	leal	(%rbx,%rbx,8), %edi
	subl	%ebp, %edi
	addl	%r14d, %edi
	movl	%edi, 256(%rsp)                 # 4-byte Spill
	leal	(%r13,%r13,8), %edi
	leal	(%r13,%rdi,2), %ebx
	subl	%ebp, %ebx
	addl	%r14d, %ebx
	movl	%ebx, 224(%rsp)                 # 4-byte Spill
	leal	(%r13,%rax,4), %r8d
	vmovd	%r13d, %xmm1
	vmovd	%ebp, %xmm2
	vmovd	%r14d, %xmm3
	leal	(%rdi,%rdi,2), %eax
	addl	%r13d, %eax
	addl	%r13d, %eax
	vmovd	%r10d, %xmm0
	movl	%r13d, %edi
	vpinsrd	$1, %eax, %xmm0, %xmm0
	vpinsrd	$2, %r15d, %xmm0, %xmm0
	shll	$5, %edi
	movl	%edi, %ebx
	leal	(%rdi,%r13,2), %r10d
	movl	%edi, %eax
	subl	%r13d, %eax
	vmovd	%eax, %xmm5
	subl	%r13d, %eax
	subl	%ebp, %eax
	addl	%r14d, %eax
	movl	%eax, 352(%rsp)                 # 4-byte Spill
	subl	%ebp, %ebx
	vpinsrd	$3, %r10d, %xmm0, %xmm4
	vpinsrd	$1, %r13d, %xmm5, %xmm0
	vpinsrd	$2, %r8d, %xmm0, %xmm0
	vpinsrd	$3, %edx, %xmm0, %xmm5
	addl	%r14d, %ebx
	movl	%ebx, 576(%rsp)                 # 4-byte Spill
	subl	%ebp, %r12d
	movl	%r12d, 320(%rsp)                # 4-byte Spill
	movq	-48(%rsp), %rbx                 # 8-byte Reload
	leaq	(%rbx,%rbx,2), %rdx
	leaq	(,%rdx,8), %rax
	movq	%rdx, %r10
	movq	%rdx, 288(%rsp)                 # 8-byte Spill
	subq	%rbx, %rax
	movq	%rax, %r8
	movq	%rbx, %rbp
	shlq	$4, %rbp
	movq	%rbp, %rax
	subq	%rbx, %rax
	subq	%rbx, %rax
	leal	(%r9,%rsi,8), %edx
	movslq	%edx, %rdi
	leaq	(,%rdi,4), %rsi
	leaq	(,%r11,4), %rdx
	subq	%rdx, %rsi
	movq	%rsi, -32(%rsp)                 # 8-byte Spill
	leaq	(,%rbx,8), %rdx
	subq	%rbx, %rdx
	leaq	(%rbx,%rbp), %r12
	addq	%rdi, %rbp
	movq	%rbp, -56(%rsp)                 # 8-byte Spill
	leaq	(%rbx,%rbx,4), %rbp
	leaq	(,%rbp,4), %rsi
	addq	%rbp, %rsi
	leaq	(%rsi,%rbx), %r9
	addq	%rdi, %rsi
	addq	%rdi, %r9
	addq	%rdi, %rdx
	leaq	(%rbx,%rbp,2), %r11
	addq	%rdi, %r11
	leaq	(%rbx,%r10,4), %r14
	addq	%rdi, %r14
	addq	%rdi, %rax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	leaq	(,%rbp,2), %r15
	addq	%rbp, %r15
	addq	%rdi, %r15
	leaq	(%rbx,%rbp,4), %r10
	leaq	(%rbx,%r10), %r13
	addq	%rdi, %r10
	leaq	(%rdi,%rbp), %rax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	leaq	(%rdi,%rbp,2), %rax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	leaq	(%rdi,%rbp,4), %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	leaq	(%rbx,%rbx,8), %rbp
	leaq	(%rbx,%rbp,2), %rax
	addq	%rdi, %rax
	movq	%rax, -96(%rsp)                 # 8-byte Spill
	leaq	(,%rbp,2), %rax
	addq	%rbp, %rax
	movq	%rax, 448(%rsp)                 # 8-byte Spill
	leaq	(%rbp,%rdi), %rax
	movq	%rax, -8(%rsp)                  # 8-byte Spill
	leaq	(%rdi,%rbp,2), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	addq	%rdi, %r12
	movq	%r12, 24(%rsp)                  # 8-byte Spill
	addq	%rdi, %r13
	movq	%r13, -72(%rsp)                 # 8-byte Spill
	addq	%rdi, %r8
	movq	%r8, 512(%rsp)                  # 8-byte Spill
	leaq	(%rdi,%rbx,8), %r8
	movq	%rbx, 544(%rsp)                 # 8-byte Spill
	leaq	(%rdi,%rbx,2), %rax
	addq	%rdi, %rbx
	movq	288(%rsp), %r12                 # 8-byte Reload
	leaq	(%rdi,%r12,2), %r13
	leaq	(%rdi,%r12,4), %rbp
	movq	%rbp, -16(%rsp)                 # 8-byte Spill
	leaq	(%r12,%rdi), %rbp
	movq	%rbp, -24(%rsp)                 # 8-byte Spill
	leaq	(%rdi,%r12,8), %rbp
	movq	%rbp, 288(%rsp)                 # 8-byte Spill
	movq	448(%rsp), %r12                 # 8-byte Reload
	addq	%rdi, %r12
	movq	480(%rsp), %rdi                 # 8-byte Reload
	subq	%rdi, %r8
	movq	%r8, 144(%rsp)                  # 8-byte Spill
	subq	%rdi, -56(%rsp)                 # 8-byte Folded Spill
	subq	%rdi, %rax
	movq	%rax, 136(%rsp)                 # 8-byte Spill
	subq	%rdi, %rsi
	movq	%rsi, 184(%rsp)                 # 8-byte Spill
	subq	%rdi, %r9
	movq	%r9, 176(%rsp)                  # 8-byte Spill
	subq	%rdi, %r13
	movq	%r13, 128(%rsp)                 # 8-byte Spill
	subq	%rdi, -80(%rsp)                 # 8-byte Folded Spill
	subq	%rdi, %rdx
	movq	%rdx, 192(%rsp)                 # 8-byte Spill
	subq	%rdi, -8(%rsp)                  # 8-byte Folded Spill
	subq	%rdi, -88(%rsp)                 # 8-byte Folded Spill
	subq	%rdi, %r11
	movq	%r11, 168(%rsp)                 # 8-byte Spill
	subq	%rdi, -16(%rsp)                 # 8-byte Folded Spill
	subq	%rdi, %r14
	movq	%r14, 160(%rsp)                 # 8-byte Spill
	subq	%rdi, -64(%rsp)                 # 8-byte Folded Spill
	subq	%rdi, %r15
	movq	%r15, 152(%rsp)                 # 8-byte Spill
	subq	%rdi, %r10
	movq	%r10, 88(%rsp)                  # 8-byte Spill
	subq	%rdi, 16(%rsp)                  # 8-byte Folded Spill
	subq	%rdi, -96(%rsp)                 # 8-byte Folded Spill
	subq	%rdi, 8(%rsp)                   # 8-byte Folded Spill
	subq	%rdi, 24(%rsp)                  # 8-byte Folded Spill
	subq	%rdi, -72(%rsp)                 # 8-byte Folded Spill
	movq	512(%rsp), %r15                 # 8-byte Reload
	subq	%rdi, %r15
	subq	%rdi, -24(%rsp)                 # 8-byte Folded Spill
	subq	%rdi, %rbx
	movq	%rbx, -48(%rsp)                 # 8-byte Spill
	movq	288(%rsp), %r13                 # 8-byte Reload
	subq	%rdi, %r13
	movq	%r12, %r14
	subq	%rdi, %r14
	vpbroadcastd	%xmm1, %xmm0
	vpmulld	.LCPI11_5(%rip), %xmm0, %xmm1
	vpbroadcastd	%xmm2, %ymm2
	vpsubd	%xmm2, %xmm1, %xmm1
	vpmulld	.LCPI11_6(%rip), %xmm0, %xmm0
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%xmm3, %xmm1, %xmm1
	vpsubd	%xmm2, %xmm0, %xmm0
	vpaddd	%xmm3, %xmm0, %xmm0
	vinserti128	$1, %xmm4, %ymm5, %ymm4
	vpsubd	%ymm2, %ymm4, %ymm2
	vpaddd	%ymm3, %ymm2, %ymm2
	movslq	320(%rsp), %r8                  # 4-byte Folded Reload
	vpmovsxdq	%xmm1, %ymm1
	vmovdqu	%ymm1, 1056(%rsp)               # 32-byte Spill
	movslq	608(%rsp), %r9                  # 4-byte Folded Reload
	vbroadcastss	720(%rsp), %ymm1        # 16-byte Folded Reload
	vmovups	%ymm1, 1024(%rsp)               # 32-byte Spill
	movslq	352(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	576(%rsp), %rsi                 # 4-byte Folded Reload
	movslq	640(%rsp), %rbx                 # 4-byte Folded Reload
	vextracti128	$1, %ymm2, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vmovdqu	%ymm1, 992(%rsp)                # 32-byte Spill
	vpmovsxdq	%xmm2, %ymm1
	vmovdqu	%ymm1, 960(%rsp)                # 32-byte Spill
	vpmovsxdq	%xmm0, %ymm0
	vmovdqu	%ymm0, 768(%rsp)                # 32-byte Spill
	movslq	224(%rsp), %rax                 # 4-byte Folded Reload
	movslq	256(%rsp), %rdi                 # 4-byte Folded Reload
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	movq	%rbx, 80(%rsp)                  # 8-byte Spill
	vmovups	(%rdx,%rbx,4), %ymm5
	movq	544(%rsp), %r12                 # 8-byte Reload
	shlq	$7, %r12
	movq	(%rsp), %rbx                    # 8-byte Reload
	leaq	(%rbx,%rdi,4), %rdx
	movq	%rdx, 120(%rsp)                 # 8-byte Spill
	leaq	(%rbx,%rax,4), %rax
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	leaq	(%rbx,%rsi,4), %rax
	movq	%rax, 104(%rsp)                 # 8-byte Spill
	leaq	(%rbx,%rbp,4), %rax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	leaq	(%rbx,%r9,4), %r11
	leaq	(%rbx,%r8,4), %r10
	vbroadcastss	.LCPI11_0(%rip), %ymm0  # ymm0 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmovups	%ymm0, 928(%rsp)                # 32-byte Spill
	vxorps	%xmm7, %xmm7, %xmm7
	vbroadcastss	.LCPI11_1(%rip), %ymm0  # ymm0 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vmovups	%ymm0, 896(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI11_2(%rip), %ymm0  # ymm0 = [2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0]
	vmovups	%ymm0, 864(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI11_3(%rip), %ymm0  # ymm0 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vmovups	%ymm0, 832(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI11_4(%rip), %ymm0  # ymm0 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vmovups	%ymm0, 800(%rsp)                # 32-byte Spill
	xorl	%esi, %esi
	xorl	%edi, %edi
	jmp	.LBB11_3
	.p2align	4, 0x90
.LBB11_13:                              # %"for f7.s1.r77$x"
                                        #   in Loop: Header=BB11_3 Depth=1
	movq	88(%rsp), %rax                  # 8-byte Reload
.LBB11_14:                              # %"for f7.s1.r77$x"
                                        #   in Loop: Header=BB11_3 Depth=1
	vmovups	(%rcx,%rax,4), %ymm3
	vmaxps	%ymm7, %ymm3, %ymm3
	vmovups	288(%rsp), %ymm5                # 32-byte Reload
	vmovups	512(%rsp), %ymm9                # 32-byte Reload
	vmovups	480(%rsp), %ymm10               # 32-byte Reload
	vmovups	448(%rsp), %ymm12               # 32-byte Reload
	vmovups	672(%rsp), %ymm13               # 32-byte Reload
	vblendvps	%ymm6, %ymm2, %ymm3, %ymm2
	vfmadd132ps	(%rbx,%r9,4), %ymm1, %ymm2 # ymm2 = (ymm2 * mem) + ymm1
	vfmadd213ps	%ymm15, %ymm14, %ymm2   # ymm2 = (ymm14 * ymm2) + ymm15
	vfmadd213ps	%ymm2, %ymm4, %ymm11    # ymm11 = (ymm4 * ymm11) + ymm2
	movq	128(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm0
	vmaxps	%ymm7, %ymm0, %ymm0
	movq	120(%rsp), %rax                 # 8-byte Reload
	vmulps	(%rax,%rsi,4), %ymm14, %ymm1
	vmulps	%ymm1, %ymm0, %ymm0
	movq	-80(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm1
	movq	112(%rsp), %rax                 # 8-byte Reload
	vmulps	(%rax,%rsi,4), %ymm14, %ymm2
	vmaxps	%ymm7, %ymm1, %ymm1
	vfmadd231ps	%ymm2, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm2) + ymm0
	vmovdqu	640(%rsp), %ymm1                # 32-byte Reload
	vpaddq	768(%rsp), %ymm1, %ymm1         # 32-byte Folded Reload
	movq	192(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm2
	vmaxps	%ymm7, %ymm2, %ymm2
	vextracti128	$1, %ymm1, %xmm3
	vpextrq	$1, %xmm3, %rax
	vmulps	(%rbx,%rax,4), %ymm10, %ymm4
	vfmadd231ps	%ymm4, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm4) + ymm0
	vmovq	%xmm3, %rax
	movq	144(%rsp), %rbp                 # 8-byte Reload
	vmovups	(%rcx,%rbp,4), %ymm2
	vmaxps	%ymm7, %ymm2, %ymm2
	vmovups	608(%rsp), %ymm4                # 32-byte Reload
	vmulps	(%rbx,%rax,4), %ymm4, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	vpextrq	$1, %xmm1, %rax
	movq	-8(%rsp), %rbp                  # 8-byte Reload
	vmovups	(%rcx,%rbp,4), %ymm2
	vmaxps	%ymm7, %ymm2, %ymm2
	vmulps	(%rbx,%rax,4), %ymm10, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	movq	-88(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm2
	vmaxps	%ymm7, %ymm2, %ymm2
	vmulps	%ymm4, %ymm12, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	movq	168(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm2
	vmaxps	%ymm7, %ymm2, %ymm2
	vmulps	%ymm12, %ymm10, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	movq	-16(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm2
	vmaxps	%ymm7, %ymm2, %ymm2
	vmovups	576(%rsp), %ymm12               # 32-byte Reload
	vmulps	%ymm4, %ymm12, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	vmulps	%ymm12, %ymm10, %ymm2
	movq	160(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm3
	vmaxps	%ymm7, %ymm3, %ymm3
	vfmadd231ps	%ymm2, %ymm3, %ymm0     # ymm0 = (ymm3 * ymm2) + ymm0
	movq	-64(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm2
	vmovq	%xmm1, %rax
	vmulps	(%rbx,%rax,4), %ymm9, %ymm1
	vmaxps	%ymm7, %ymm2, %ymm2
	vfmadd231ps	%ymm1, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm1) + ymm0
	vmovdqa	544(%rsp), %xmm1                # 16-byte Reload
	vpextrq	$1, %xmm1, %rax
	vmulps	(%rbx,%rax,4), %ymm9, %ymm1
	movq	152(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm2
	vmaxps	%ymm7, %ymm2, %ymm2
	vfmadd231ps	%ymm1, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm1) + ymm0
	vmovups	352(%rsp), %ymm1                # 32-byte Reload
	vfmadd132ps	864(%rsp), %ymm1, %ymm11 # 32-byte Folded Reload
                                        # ymm11 = (ymm11 * mem) + ymm1
	vaddps	%ymm0, %ymm11, %ymm0
	vmovups	224(%rsp), %ymm1                # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm14, %ymm1    # ymm1 = (ymm14 * ymm1) + ymm0
	movq	184(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm0
	vmaxps	%ymm7, %ymm0, %ymm0
	vandps	%ymm0, %ymm6, %ymm0
	vfmadd213ps	%ymm1, %ymm13, %ymm0    # ymm0 = (ymm13 * ymm0) + ymm1
	vmovups	320(%rsp), %ymm1                # 32-byte Reload
	vaddps	832(%rsp), %ymm1, %ymm1         # 32-byte Folded Reload
	vmulps	%ymm1, %ymm13, %ymm1
	movq	176(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm2
	vmaxps	%ymm7, %ymm2, %ymm2
	vfmadd231ps	%ymm1, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm1) + ymm0
	vmovups	256(%rsp), %ymm1                # 32-byte Reload
	vfmadd132ps	(%rbx,%r8,4), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vfmadd231ps	800(%rsp), %ymm1, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm1 * mem) + ymm5
	incq	%rdi
	addq	%r12, %rcx
	addq	208(%rsp), %rsi                 # 8-byte Folded Reload
	cmpq	%rdi, %rdx
	je	.LBB11_15
.LBB11_3:                               # %"for f7.s1.r77$x"
                                        # =>This Inner Loop Header: Depth=1
	vmovups	%ymm5, 288(%rsp)                # 32-byte Spill
	vmovq	%rsi, %xmm0
	vpbroadcastq	%xmm0, %ymm2
	vpaddq	1056(%rsp), %ymm2, %ymm0        # 32-byte Folded Reload
	vmovq	%xmm0, %rdx
	vmovups	(%rbx,%rdx,4), %ymm15
	vpextrq	$1, %xmm0, %rdx
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %rbx
	vpextrq	$1, %xmm0, %rax
	movq	(%rsp), %rbp                    # 8-byte Reload
	vmovups	(%rbp,%rbx,4), %ymm6
	vmovups	%ymm6, 608(%rsp)                # 32-byte Spill
	movq	(%rsp), %rbx                    # 8-byte Reload
	vmovups	(%rbx,%rax,4), %ymm11
	vpaddq	960(%rsp), %ymm2, %ymm3         # 32-byte Folded Reload
	vmovq	%xmm3, %rax
	vmovups	(%rbx,%rax,4), %ymm0
	vmovups	%ymm0, 448(%rsp)                # 32-byte Spill
	vmulps	(%r11,%rsi,4), %ymm11, %ymm0
	vmovups	928(%rsp), %ymm12               # 32-byte Reload
	vmovups	%ymm0, 256(%rsp)                # 32-byte Spill
	vmaxps	%ymm12, %ymm0, %ymm0
	vmovups	%ymm0, 512(%rsp)                # 32-byte Spill
	vdivps	1024(%rsp), %ymm0, %ymm4        # 32-byte Folded Reload
	movq	96(%rsp), %rax                  # 8-byte Reload
	vmovups	(%rax,%rsi,4), %ymm1
	vmovdqu	%ymm2, 640(%rsp)                # 32-byte Spill
	vpaddq	992(%rsp), %ymm2, %ymm0         # 32-byte Folded Reload
	vmulps	(%rbx,%rdx,4), %ymm15, %ymm2
	vextracti128	$1, %ymm3, %xmm5
	vmovdqa	%xmm5, 224(%rsp)                # 16-byte Spill
	vpextrq	$1, %xmm5, %rax
	movq	-32(%rsp), %rdx                 # 8-byte Reload
	vmovups	(%rcx,%rdx), %ymm5
	vmaxps	%ymm7, %ymm5, %ymm5
	movq	-48(%rsp), %rdx                 # 8-byte Reload
	vmovups	(%rcx,%rdx,4), %ymm8
	vmaxps	%ymm7, %ymm8, %ymm8
	vmulps	%ymm6, %ymm8, %ymm8
	vfmadd231ps	%ymm5, %ymm2, %ymm8     # ymm8 = (ymm2 * ymm5) + ymm8
	movq	136(%rsp), %rdx                 # 8-byte Reload
	vmovups	(%rcx,%rdx,4), %ymm5
	vmaxps	%ymm7, %ymm5, %ymm5
	movq	-24(%rsp), %rdx                 # 8-byte Reload
	vmovups	(%rcx,%rdx,4), %ymm9
	vmaxps	%ymm7, %ymm9, %ymm9
	vmulps	%ymm6, %ymm9, %ymm9
	vfmadd231ps	%ymm5, %ymm2, %ymm9     # ymm9 = (ymm2 * ymm5) + ymm9
	vcmpeqps	(%rbx,%rax,4), %ymm7, %ymm5
	vmaxps	%ymm12, %ymm1, %ymm2
	vmovups	896(%rsp), %ymm1                # 32-byte Reload
	vdivps	%ymm2, %ymm1, %ymm1
	vmovq	%xmm0, %rax
	vminps	%ymm11, %ymm1, %ymm1
	vmulps	(%rbx,%rax,4), %ymm1, %ymm1
	vmovups	%ymm1, 544(%rsp)                # 32-byte Spill
	vpextrq	$1, %xmm3, %rax
	vblendvps	%ymm5, %ymm8, %ymm9, %ymm1
	vmovups	(%rbx,%rax,4), %ymm3
	vmovups	%ymm3, 672(%rsp)                # 32-byte Spill
	vroundps	$2, %ymm4, %ymm3
	vmulps	%ymm1, %ymm3, %ymm10
	vmaxps	%ymm12, %ymm4, %ymm13
	movq	-72(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm1
	vmaxps	%ymm7, %ymm1, %ymm1
	vmovups	%ymm15, 480(%rsp)               # 32-byte Spill
	vaddps	%ymm6, %ymm15, %ymm3
	vmulps	%ymm1, %ymm3, %ymm1
	vdivps	%ymm2, %ymm1, %ymm15
	movq	-56(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm1
	vmaxps	%ymm7, %ymm1, %ymm1
	vmovups	(%r10,%rsi,4), %ymm14
	vmovaps	%ymm11, %ymm8
	vcmpltps	%ymm11, %ymm12, %ymm6
	movq	104(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rax,%rsi,4), %ymm3
	vmovups	(%rcx,%r14,4), %ymm4
	vmovups	(%rcx,%r13,4), %ymm12
	vmovups	(%rcx,%r15,4), %ymm9
	testq	%rdi, %rdi
	vmovups	%ymm3, 576(%rsp)                # 32-byte Spill
	je	.LBB11_4
# %bb.10:                               # %"for f7.s1.r77$x"
                                        #   in Loop: Header=BB11_3 Depth=1
	movq	8(%rsp), %rax                   # 8-byte Reload
	jmp	.LBB11_11
	.p2align	4, 0x90
.LBB11_4:                               #   in Loop: Header=BB11_3 Depth=1
	movq	24(%rsp), %rax                  # 8-byte Reload
.LBB11_11:                              # %"for f7.s1.r77$x"
                                        #   in Loop: Header=BB11_3 Depth=1
	vmovups	(%rcx,%rax,4), %ymm3
	vmaxps	%ymm7, %ymm3, %ymm3
	movq	200(%rsp), %rdx                 # 8-byte Reload
	vmovups	256(%rsp), %ymm11               # 32-byte Reload
	vmovdqa	224(%rsp), %xmm5                # 16-byte Reload
	vmovq	%xmm5, %r8
	vmaxps	%ymm7, %ymm4, %ymm4
	vmovups	%ymm4, 256(%rsp)                # 32-byte Spill
	vmaxps	%ymm7, %ymm12, %ymm12
	vdivps	%ymm13, %ymm10, %ymm10
	vmaxps	%ymm7, %ymm9, %ymm4
	vmulps	544(%rsp), %ymm11, %ymm11       # 32-byte Folded Reload
	vmovaps	%ymm6, %ymm9
	vandps	%ymm6, %ymm15, %ymm15
	vblendvps	%ymm6, %ymm1, %ymm3, %ymm1
	vpextrq	$1, %xmm0, %rax
	vmulps	(%rbx,%rax,4), %ymm8, %ymm3
	vmulps	%ymm1, %ymm3, %ymm1
	vdivps	%ymm2, %ymm1, %ymm1
	vextracti128	$1, %ymm0, %xmm0
	vmovq	%xmm0, %r9
	movq	-96(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm2
	vmaxps	%ymm7, %ymm2, %ymm2
	vmovups	%ymm12, 224(%rsp)               # 32-byte Spill
	vmovups	%ymm10, 352(%rsp)               # 32-byte Spill
	vmovups	%ymm8, 320(%rsp)                # 32-byte Spill
	vmovdqa	%xmm0, 544(%rsp)                # 16-byte Spill
	jne	.LBB11_13
# %bb.12:                               #   in Loop: Header=BB11_3 Depth=1
	movq	16(%rsp), %rax                  # 8-byte Reload
	jmp	.LBB11_14
.LBB11_5:                               # %false_bb
	testl	%eax, %eax
	jle	.LBB11_41
# %bb.6:                                # %"for f7.s1.r77$x2.preheader"
	movl	36(%rdx), %eax
	subl	%r14d, %eax
	movl	%r14d, %ebp
	subl	%r8d, %ebp
	cmpl	$9, %eax
	movl	$8, %edi
	cmovll	%eax, %edi
	xorl	%edx, %edx
	movl	%eax, 220(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	movl	$0, %eax
	movq	%rax, 512(%rsp)                 # 8-byte Spill
	cmovlel	%edx, %edi
	movq	%rdi, %r8
	movq	%rdi, 72(%rsp)                  # 8-byte Spill
	leal	(%r13,%r13,4), %edx
	leal	(%r13,%rdx,2), %eax
	subl	%r9d, %eax
	addl	%r14d, %eax
	movl	%eax, 608(%rsp)                 # 4-byte Spill
	leal	(,%r13,4), %eax
	leal	(%rax,%rax,2), %eax
	subl	%r9d, %eax
	addl	%r14d, %eax
	movl	%eax, 256(%rsp)                 # 4-byte Spill
	movl	%r13d, %eax
	shll	$4, %eax
	addl	%r13d, %eax
	subl	%r9d, %eax
	addl	%r14d, %eax
	movl	%eax, 576(%rsp)                 # 4-byte Spill
	leal	(%r13,%r13), %eax
	leal	(%rax,%rax,8), %eax
	subl	%r9d, %eax
	addl	%r14d, %eax
	movl	%eax, 224(%rsp)                 # 4-byte Spill
	leal	(%r13,%r13,8), %edi
	leal	(%r13,%rdi,2), %eax
	movq	%rdi, -88(%rsp)                 # 8-byte Spill
	subl	%r9d, %eax
	addl	%r14d, %eax
	movl	%eax, 352(%rsp)                 # 4-byte Spill
	movq	%rdx, -64(%rsp)                 # 8-byte Spill
	leal	(%r13,%rdx,4), %eax
	movl	%eax, -80(%rsp)                 # 4-byte Spill
	subl	%r9d, %eax
	addl	%r14d, %eax
	movl	%eax, 544(%rsp)                 # 4-byte Spill
	leal	(%rdx,%rdx,4), %eax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	addl	%r13d, %eax
	subl	%r9d, %eax
	addl	%r14d, %eax
	movl	%eax, 320(%rsp)                 # 4-byte Spill
	leal	(%rdi,%rdi,2), %eax
	movl	%eax, %edx
	subl	%r9d, %edx
	addl	%r14d, %edx
	movl	%edx, 480(%rsp)                 # 4-byte Spill
	addl	%r13d, %eax
	leal	(%rax,%r13), %edx
	subl	%r9d, %eax
	addl	%r14d, %eax
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	subl	%r9d, %edx
	addl	%r14d, %edx
	movl	%edx, 448(%rsp)                 # 4-byte Spill
	movl	%r13d, %edi
	shll	$5, %edi
	leal	(%rdi,%r13), %eax
	subl	%r9d, %eax
	addl	%r14d, %eax
	movl	%eax, 672(%rsp)                 # 4-byte Spill
	leal	(%rdi,%r13,2), %eax
	subl	%r9d, %eax
	addl	%r14d, %eax
	movl	%eax, -56(%rsp)                 # 4-byte Spill
	movq	-48(%rsp), %r15                 # 8-byte Reload
	leaq	(%r15,%r15,2), %rax
	movq	%rax, 768(%rsp)                 # 8-byte Spill
	leaq	(,%rax,8), %rdx
	subq	%r15, %rdx
	movq	%rdx, 928(%rsp)                 # 8-byte Spill
	movq	%r15, %rdx
	shlq	$4, %rdx
	leaq	(%r15,%rdx), %rbx
	movq	%rbx, 864(%rsp)                 # 8-byte Spill
	subq	%r15, %rdx
	subq	%r15, %rdx
	movq	%rdx, 896(%rsp)                 # 8-byte Spill
	leaq	(,%r15,8), %rdx
	leaq	(%rdx,%rdx,2), %rbx
	movq	%rbx, 800(%rsp)                 # 8-byte Spill
	subq	%r15, %rdx
	movq	%rdx, 832(%rsp)                 # 8-byte Spill
	leaq	-1(%r8), %r14
	movq	%rsi, %r8
	movq	%rbp, -32(%rsp)                 # 8-byte Spill
	leal	(%r14,%rbp), %r12d
	cmpl	%ebp, %r12d
	setl	-96(%rsp)                       # 1-byte Folded Spill
	shrq	$32, %r14
	setne	-97(%rsp)                       # 1-byte Folded Spill
	movl	%r13d, %ebp
	addl	-80(%rsp), %ebp                 # 4-byte Folded Reload
	leal	(%r10,%r13), %ebx
	leal	(%rbx,%rsi,8), %ebx
	leal	(%r10,%rdi), %r12d
	subl	%r13d, %edi
	leal	(%r10,%rdi), %edx
	movq	%r10, %rax
	leal	(%rdx,%rsi,8), %r11d
	leal	(%r12,%rsi,8), %esi
	subl	%r13d, %edi
	addl	%eax, %edi
	leal	(%rdi,%r8,8), %edi
	movq	-64(%rsp), %rdx                 # 8-byte Reload
	leal	(%r10,%rdx,2), %edx
	leal	(%rdx,%r8,8), %r12d
	movq	-88(%rsp), %rdx                 # 8-byte Reload
	addl	%eax, %edx
	leal	(%rdx,%r8,8), %r10d
	movq	-72(%rsp), %rdx                 # 8-byte Reload
	addl	%eax, %edx
	leal	(%rdx,%r8,8), %r14d
	addl	%eax, %ebp
	leal	(%rbp,%r8,8), %ebp
	leal	(%r13,%r13,2), %edx
	leal	(%rax,%rdx,8), %edx
	leal	(%rdx,%r8,8), %edx
	leal	(%rax,%r8,8), %r13d
	subl	%r9d, %ebx
	movl	%ebx, 68(%rsp)                  # 4-byte Spill
	subl	%r9d, %r11d
	movl	%r11d, 64(%rsp)                 # 4-byte Spill
	subl	%r9d, %esi
	movl	%esi, 60(%rsp)                  # 4-byte Spill
	subl	%r9d, %edi
	movl	%edi, 56(%rsp)                  # 4-byte Spill
	subl	%r9d, %r12d
	movl	%r12d, 52(%rsp)                 # 4-byte Spill
	subl	%r9d, %r10d
	movl	%r10d, 48(%rsp)                 # 4-byte Spill
	subl	%r9d, %r14d
	movl	%r14d, 44(%rsp)                 # 4-byte Spill
	subl	%r9d, %ebp
	movl	%ebp, 40(%rsp)                  # 4-byte Spill
	subl	%r9d, %edx
	movl	%edx, 36(%rsp)                  # 4-byte Spill
	subl	%r9d, %r13d
	movl	%r13d, 32(%rsp)                 # 4-byte Spill
	movslq	640(%rsp), %rdx                 # 4-byte Folded Reload
	movq	-40(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %rax
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	movb	-97(%rsp), %al                  # 1-byte Reload
	orb	-96(%rsp), %al                  # 1-byte Folded Reload
	movb	%al, -97(%rsp)                  # 1-byte Spill
	movslq	544(%rsp), %r14                 # 4-byte Folded Reload
	movslq	576(%rsp), %r8                  # 4-byte Folded Reload
	movslq	256(%rsp), %r13                 # 4-byte Folded Reload
	movslq	448(%rsp), %r11                 # 4-byte Folded Reload
	movslq	608(%rsp), %r12                 # 4-byte Folded Reload
	movslq	-56(%rsp), %r9                  # 4-byte Folded Reload
	movslq	672(%rsp), %r10                 # 4-byte Folded Reload
	leaq	(,%r15,4), %rdi
	leaq	(%rdi,%rdi,4), %rax
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	leaq	(%rdi,%rdi,2), %rax
	movq	%rax, 104(%rsp)                 # 8-byte Spill
	movslq	480(%rsp), %rdi                 # 4-byte Folded Reload
	movslq	288(%rsp), %rax                 # 4-byte Folded Reload
	movslq	320(%rsp), %rbp                 # 4-byte Folded Reload
	movslq	352(%rsp), %rbx                 # 4-byte Folded Reload
	leaq	(%r15,%r15), %rdx
	leaq	(%rdx,%rdx,8), %rsi
	movq	%rsi, 96(%rsp)                  # 8-byte Spill
	leaq	(%rdx,%rdx,4), %rsi
	movq	%rsi, 24(%rsp)                  # 8-byte Spill
	leaq	(%rdx,%rdx,2), %rdx
	movq	%rdx, 16(%rsp)                  # 8-byte Spill
	movslq	224(%rsp), %rdx                 # 4-byte Folded Reload
	movq	72(%rsp), %rsi                  # 8-byte Reload
                                        # kill: def $esi killed $esi killed $rsi def $rsi
	andl	$2147483640, %esi               # imm = 0x7FFFFFF8
	movq	%rsi, 736(%rsp)                 # 8-byte Spill
	vbroadcastss	720(%rsp), %ymm0        # 16-byte Folded Reload
	vmovups	%ymm0, 1088(%rsp)               # 32-byte Spill
	movq	(%rsp), %rsi                    # 8-byte Reload
	leaq	(%rsi,%rdx,4), %rdx
	movq	%rdx, 480(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%rbx,4), %rdx
	movq	%rdx, 448(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%rbp,4), %rdx
	movq	%rdx, 672(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%rax,4), %rax
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%rdi,4), %rax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%r10,4), %rax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%r9,4), %rax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%r12,4), %rax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%r11,4), %rax
	movq	%rax, -96(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%r13,4), %rax
	movq	%rax, -8(%rsp)                  # 8-byte Spill
	leaq	(%rsi,%r8,4), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%r14,4), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movl	-32(%rsp), %eax                 # 4-byte Reload
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	vmovss	.LCPI11_0(%rip), %xmm13         # xmm13 = mem[0],zero,zero,zero
	leaq	(%r15,%r15,8), %rax
	leaq	(%rax,%rax,2), %rdx
	movq	%rdx, -40(%rsp)                 # 8-byte Spill
	leaq	(%r15,%r15,4), %rdx
	leaq	(%r15,%rdx,4), %rsi
	movq	%rsi, 440(%rsp)                 # 8-byte Spill
	leaq	(%r15,%rsi), %rsi
	movq	%rsi, 432(%rsp)                 # 8-byte Spill
	movq	%rax, 88(%rsp)                  # 8-byte Spill
	leaq	(%r15,%rax,2), %rax
	movq	%rax, 424(%rsp)                 # 8-byte Spill
	leaq	(%rdx,%rdx,2), %rax
	movq	%rax, 416(%rsp)                 # 8-byte Spill
	movq	768(%rsp), %rax                 # 8-byte Reload
	leaq	(%r15,%rax,4), %rax
	movq	%rax, 408(%rsp)                 # 8-byte Spill
	leaq	(%r15,%rdx,2), %rax
	movq	%rax, 400(%rsp)                 # 8-byte Spill
	movq	%rdx, 80(%rsp)                  # 8-byte Spill
	leaq	(%rdx,%rdx,4), %rax
	movq	%rax, 392(%rsp)                 # 8-byte Spill
	leaq	(%rax,%r15), %rax
	movq	%rax, 384(%rsp)                 # 8-byte Spill
	movq	208(%rsp), %rax                 # 8-byte Reload
	leaq	(,%rax,4), %rax
	movq	%rax, 744(%rsp)                 # 8-byte Spill
	jmp	.LBB11_7
	.p2align	4, 0x90
.LBB11_40:                              # %"end for f7.s1.n.ni"
                                        #   in Loop: Header=BB11_7 Depth=1
	movq	512(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movq	744(%rsp), %rax                 # 8-byte Reload
	addq	%rax, 480(%rsp)                 # 8-byte Folded Spill
	addq	%rax, 448(%rsp)                 # 8-byte Folded Spill
	addq	%rax, 672(%rsp)                 # 8-byte Folded Spill
	addq	%rax, -56(%rsp)                 # 8-byte Folded Spill
	addq	%rax, -64(%rsp)                 # 8-byte Folded Spill
	addq	%rax, -72(%rsp)                 # 8-byte Folded Spill
	addq	%rax, -80(%rsp)                 # 8-byte Folded Spill
	addq	%rax, -88(%rsp)                 # 8-byte Folded Spill
	addq	%rax, -96(%rsp)                 # 8-byte Folded Spill
	addq	%rax, -8(%rsp)                  # 8-byte Folded Spill
	addq	%rax, -16(%rsp)                 # 8-byte Folded Spill
	addq	%rax, -24(%rsp)                 # 8-byte Folded Spill
	movq	208(%rsp), %rax                 # 8-byte Reload
	addl	%eax, 68(%rsp)                  # 4-byte Folded Spill
	addl	%eax, 64(%rsp)                  # 4-byte Folded Spill
	addl	%eax, 60(%rsp)                  # 4-byte Folded Spill
	addl	%eax, 56(%rsp)                  # 4-byte Folded Spill
	addl	%eax, 52(%rsp)                  # 4-byte Folded Spill
	addl	%eax, 48(%rsp)                  # 4-byte Folded Spill
	addl	%eax, 44(%rsp)                  # 4-byte Folded Spill
	addl	%eax, 40(%rsp)                  # 4-byte Folded Spill
	addl	%eax, 36(%rsp)                  # 4-byte Folded Spill
	addl	%eax, 32(%rsp)                  # 4-byte Folded Spill
	movq	%rdx, %rax
	movq	%rdx, 512(%rsp)                 # 8-byte Spill
	cmpq	200(%rsp), %rdx                 # 8-byte Folded Reload
	je	.LBB11_41
.LBB11_7:                               # %"for f7.s1.r77$x2"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB11_17 Depth 2
                                        #     Child Loop BB11_25 Depth 2
	cmpl	$0, 220(%rsp)                   # 4-byte Folded Reload
	jle	.LBB11_40
# %bb.8:                                # %"for f7.s1.n.ni.preheader"
                                        #   in Loop: Header=BB11_7 Depth=1
	movslq	68(%rsp), %rax                  # 4-byte Folded Reload
	movq	(%rsp), %rdi                    # 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movslq	64(%rsp), %rax                  # 4-byte Folded Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 192(%rsp)                 # 8-byte Spill
	movslq	60(%rsp), %rax                  # 4-byte Folded Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 184(%rsp)                 # 8-byte Spill
	movslq	56(%rsp), %rax                  # 4-byte Folded Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 176(%rsp)                 # 8-byte Spill
	movslq	52(%rsp), %rax                  # 4-byte Folded Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 168(%rsp)                 # 8-byte Spill
	movslq	48(%rsp), %rax                  # 4-byte Folded Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	movslq	44(%rsp), %rax                  # 4-byte Folded Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 152(%rsp)                 # 8-byte Spill
	movslq	40(%rsp), %rax                  # 4-byte Folded Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 144(%rsp)                 # 8-byte Spill
	movslq	36(%rsp), %rax                  # 4-byte Folded Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 136(%rsp)                 # 8-byte Spill
	cmpl	$8, 72(%rsp)                    # 4-byte Folded Reload
	setb	%r8b
	movq	512(%rsp), %rdx                 # 8-byte Reload
	movq	-48(%rsp), %rsi                 # 8-byte Reload
	imulq	%rsi, %rdx
	movq	%rdx, %rbp
	leaq	(%rsi,%rdx,2), %rbx
	leaq	(%rsi,%rdx,4), %rax
	shlq	$5, %rdx
	movq	%rdx, 992(%rsp)                 # 8-byte Spill
	shlq	$4, %rbp
	addq	%rsi, %rbp
	addq	%rbp, %rbp
	movq	%rbp, 128(%rsp)                 # 8-byte Spill
	shlq	$4, %rbx
	movq	%rbx, 1056(%rsp)                # 8-byte Spill
	shlq	$3, %rax
	movq	%rax, 1024(%rsp)                # 8-byte Spill
	orb	-97(%rsp), %r8b                 # 1-byte Folded Reload
	movslq	32(%rsp), %rax                  # 4-byte Folded Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 960(%rsp)                 # 8-byte Spill
	je	.LBB11_16
# %bb.9:                                #   in Loop: Header=BB11_7 Depth=1
	xorl	%r15d, %r15d
	jmp	.LBB11_25
	.p2align	4, 0x90
.LBB11_16:                              # %vector.body.preheader
                                        #   in Loop: Header=BB11_7 Depth=1
	xorl	%ebx, %ebx
	movq	736(%rsp), %r14                 # 8-byte Reload
	vxorps	%xmm15, %xmm15, %xmm15
	jmp	.LBB11_17
	.p2align	4, 0x90
.LBB11_22:                              # %vector.body
                                        #   in Loop: Header=BB11_17 Depth=2
	vmovups	(%rcx,%rsi,4), %ymm2
.LBB11_23:                              # %vector.body
                                        #   in Loop: Header=BB11_17 Depth=2
	vmaxps	%ymm15, %ymm2, %ymm2
	vmovaps	%ymm1, %ymm8
	vmovups	576(%rsp), %ymm1                # 32-byte Reload
	vmovaps	%ymm6, %ymm13
	vmovaps	%ymm0, %ymm6
	vmovups	352(%rsp), %ymm0                # 32-byte Reload
	vblendvps	%ymm3, %ymm9, %ymm2, %ymm2
	movq	-88(%rsp), %rdx                 # 8-byte Reload
	vfmadd132ps	(%rdx,%rbx,4), %ymm10, %ymm2 # ymm2 = (ymm2 * mem) + ymm10
	vfmadd213ps	%ymm5, %ymm1, %ymm2     # ymm2 = (ymm1 * ymm2) + ymm5
	vfmadd213ps	%ymm2, %ymm11, %ymm12   # ymm12 = (ymm11 * ymm12) + ymm2
	vbroadcastss	.LCPI11_2(%rip), %ymm11 # ymm11 = [2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0]
	vfmadd213ps	%ymm14, %ymm12, %ymm11  # ymm11 = (ymm12 * ymm11) + ymm14
	movq	80(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmovups	(%rcx,%rdx,4), %ymm2
	movq	16(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmovups	(%rcx,%rdx,4), %ymm5
	movq	832(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmaxps	%ymm15, %ymm2, %ymm2
	vmaxps	%ymm15, %ymm5, %ymm5
	movq	480(%rsp), %rsi                 # 8-byte Reload
	vmulps	(%rsi,%rbx,4), %ymm1, %ymm9
	vmulps	%ymm5, %ymm9, %ymm5
	movq	448(%rsp), %rsi                 # 8-byte Reload
	vmulps	(%rsi,%rbx,4), %ymm1, %ymm9
	vfmadd231ps	%ymm9, %ymm2, %ymm5     # ymm5 = (ymm2 * ymm9) + ymm5
	vmovups	(%rcx,%rdx,4), %ymm2
	addq	1024(%rsp), %r11                # 8-byte Folded Reload
	vmaxps	%ymm15, %ymm2, %ymm2
	movq	672(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%rdx,%rbx,4), %ymm4, %ymm9
	vfmadd231ps	%ymm9, %ymm2, %ymm5     # ymm5 = (ymm2 * ymm9) + ymm5
	vmovups	(%rcx,%r11,4), %ymm2
	movq	88(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmaxps	%ymm15, %ymm2, %ymm2
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	vmulps	(%rsi,%rbx,4), %ymm7, %ymm9
	vfmadd231ps	%ymm9, %ymm2, %ymm5     # ymm5 = (ymm2 * ymm9) + ymm5
	vmovups	(%rcx,%rdx,4), %ymm2
	movq	24(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmaxps	%ymm15, %ymm2, %ymm2
	movq	-64(%rsp), %rsi                 # 8-byte Reload
	vmulps	(%rsi,%rbx,4), %ymm4, %ymm9
	vfmadd231ps	%ymm9, %ymm2, %ymm5     # ymm5 = (ymm2 * ymm9) + ymm5
	vmovups	(%rcx,%rdx,4), %ymm2
	vmaxps	%ymm15, %ymm2, %ymm2
	vmovups	256(%rsp), %ymm10               # 32-byte Reload
	vmulps	%ymm7, %ymm10, %ymm9
	vfmadd231ps	%ymm9, %ymm2, %ymm5     # ymm5 = (ymm2 * ymm9) + ymm5
	movq	400(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmovups	(%rcx,%rdx,4), %ymm2
	vmaxps	%ymm15, %ymm2, %ymm2
	vmulps	%ymm4, %ymm10, %ymm9
	vfmadd231ps	%ymm9, %ymm2, %ymm5     # ymm5 = (ymm2 * ymm9) + ymm5
	movq	104(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmovups	(%rcx,%rdx,4), %ymm2
	vmaxps	%ymm15, %ymm2, %ymm2
	vmulps	%ymm0, %ymm7, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm5     # ymm5 = (ymm2 * ymm7) + ymm5
	movq	408(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmulps	%ymm0, %ymm4, %ymm2
	vmovups	(%rcx,%rdx,4), %ymm4
	movq	896(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmaxps	%ymm15, %ymm4, %ymm4
	vfmadd231ps	%ymm2, %ymm4, %ymm5     # ymm5 = (ymm4 * ymm2) + ymm5
	vmovups	(%rcx,%rdx,4), %ymm2
	movq	-72(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%rdx,%rbx,4), %ymm8, %ymm4
	vmaxps	%ymm15, %ymm2, %ymm2
	vfmadd231ps	%ymm4, %ymm2, %ymm5     # ymm5 = (ymm2 * ymm4) + ymm5
	movq	416(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmovups	(%rcx,%rdx,4), %ymm2
	vmaxps	%ymm15, %ymm2, %ymm2
	movq	-80(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%rdx,%rbx,4), %ymm8, %ymm4
	vfmadd231ps	%ymm4, %ymm2, %ymm5     # ymm5 = (ymm2 * ymm4) + ymm5
	vaddps	%ymm5, %ymm11, %ymm2
	vfmadd213ps	%ymm2, %ymm1, %ymm6     # ymm6 = (ymm1 * ymm6) + ymm2
	movq	384(%rsp), %rdx                 # 8-byte Reload
	addq	%rax, %rdx
	vbroadcastss	.LCPI11_3(%rip), %ymm1  # ymm1 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vaddps	640(%rsp), %ymm1, %ymm0         # 32-byte Folded Reload
	addq	392(%rsp), %rax                 # 8-byte Folded Reload
	vmovups	(%rcx,%rax,4), %ymm1
	vmaxps	%ymm15, %ymm1, %ymm1
	vandps	%ymm1, %ymm3, %ymm1
	vmovups	608(%rsp), %ymm2                # 32-byte Reload
	vfmadd213ps	%ymm6, %ymm2, %ymm1     # ymm1 = (ymm2 * ymm1) + ymm6
	vmulps	%ymm2, %ymm0, %ymm0
	vmovups	(%rcx,%rdx,4), %ymm2
	vmaxps	%ymm15, %ymm2, %ymm2
	vfmadd231ps	%ymm0, %ymm2, %ymm1     # ymm1 = (ymm2 * ymm0) + ymm1
	movq	-24(%rsp), %rax                 # 8-byte Reload
	vfmadd132ps	(%rax,%rbx,4), %ymm1, %ymm13 # ymm13 = (ymm13 * mem) + ymm1
	vbroadcastss	.LCPI11_4(%rip), %ymm0  # ymm0 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	120(%rsp), %rax                 # 8-byte Reload
	vfmadd213ps	(%rax,%rbx,4), %ymm13, %ymm0 # ymm0 = (ymm13 * ymm0) + mem
	vmovups	%ymm0, (%rax,%rbx,4)
	addq	$8, %rbx
	cmpq	%rbx, %r14
	je	.LBB11_24
.LBB11_17:                              # %vector.body
                                        #   Parent Loop BB11_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	8(%rsp), %rax                   # 8-byte Reload
	addl	%ebx, %eax
	movslq	%eax, %r11
	movq	992(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r11), %rax
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	movq	800(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rax,%rsi), %rsi
	movq	-48(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rax,%rdi), %r10
	movq	128(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%r11), %r8
	movq	768(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rax,%rdi), %r9
	vmovups	(%rcx,%rdx,4), %ymm0
	vmovups	%ymm0, 320(%rsp)                # 32-byte Spill
	movq	928(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmovups	(%rcx,%rsi,4), %ymm0
	vmovups	%ymm0, 224(%rsp)                # 32-byte Spill
	movq	432(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rax,%rsi), %rsi
	vmovups	(%rcx,%r10,4), %ymm2
	movq	1056(%rsp), %rdi                # 8-byte Reload
	leaq	(%rdi,%r11), %rdi
	vmovups	(%rcx,%r8,4), %ymm3
	movq	864(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rax,%rbp), %r8
	vmovups	(%rcx,%r9,4), %ymm5
	movq	96(%rsp), %rbp                  # 8-byte Reload
	leaq	(%rax,%rbp), %r10
	vmovups	(%rcx,%rdx,4), %ymm11
	movq	424(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx), %rdx
	vmovups	(%rcx,%rsi,4), %ymm9
	movq	112(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rax,%rsi), %r9
	vmovups	(%rcx,%rdi,4), %ymm10
	movq	440(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rax,%rsi), %rsi
	movq	960(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%rdi,%rbx,4), %ymm0
	vmovups	%ymm0, 576(%rsp)                # 32-byte Spill
	movq	136(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%rdi,%rbx,4), %ymm4
	movq	152(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%rdi,%rbx,4), %ymm7
	movq	160(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%rdi,%rbx,4), %ymm0
	movq	168(%rsp), %rdi                 # 8-byte Reload
	vmulps	(%rdi,%rbx,4), %ymm0, %ymm12
	vbroadcastss	.LCPI11_0(%rip), %ymm6  # ymm6 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	movq	144(%rsp), %rdi                 # 8-byte Reload
	vmulps	(%rdi,%rbx,4), %ymm4, %ymm13
	vmovups	(%rcx,%rax,4), %ymm14
	vmaxps	%ymm15, %ymm14, %ymm14
	vmaxps	%ymm15, %ymm2, %ymm2
	vmulps	%ymm2, %ymm7, %ymm2
	vfmadd231ps	%ymm14, %ymm13, %ymm2   # ymm2 = (ymm13 * ymm14) + ymm2
	vmaxps	%ymm6, %ymm12, %ymm8
	vmaxps	%ymm15, %ymm3, %ymm3
	vmaxps	%ymm15, %ymm5, %ymm5
	vmulps	%ymm5, %ymm7, %ymm5
	vfmadd231ps	%ymm3, %ymm13, %ymm5    # ymm5 = (ymm13 * ymm3) + ymm5
	vmovaps	%ymm8, %ymm1
	vdivps	1088(%rsp), %ymm8, %ymm3        # 32-byte Folded Reload
	movq	-16(%rsp), %rdi                 # 8-byte Reload
	vcmpeqps	(%rdi,%rbx,4), %ymm15, %ymm13
	vblendvps	%ymm13, %ymm2, %ymm5, %ymm2
	movq	176(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%rdi,%rbx,4), %ymm5
	vmaxps	%ymm6, %ymm5, %ymm13
	vroundps	$10, %ymm3, %ymm5
	vmulps	%ymm2, %ymm5, %ymm14
	vxorps	%xmm8, %xmm8, %xmm8
	vmaxps	%ymm6, %ymm3, %ymm15
	vmaxps	%ymm8, %ymm9, %ymm2
	vaddps	%ymm7, %ymm4, %ymm3
	vmulps	%ymm2, %ymm3, %ymm2
	vbroadcastss	.LCPI11_1(%rip), %ymm3  # ymm3 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vdivps	%ymm13, %ymm3, %ymm5
	vcmpltps	%ymm0, %ymm6, %ymm3
	vmovups	%ymm0, 640(%rsp)                # 32-byte Spill
	vminps	%ymm0, %ymm5, %ymm5
	movq	-8(%rsp), %rdi                  # 8-byte Reload
	vmulps	(%rdi,%rbx,4), %ymm5, %ymm5
	vdivps	%ymm13, %ymm2, %ymm9
	vmaxps	%ymm8, %ymm10, %ymm10
	movq	184(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%rdi,%rbx,4), %ymm0
	movq	192(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%rdi,%rbx,4), %ymm6
	movq	-32(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%rdi,%rbx,4), %ymm2
	cmpq	$0, 512(%rsp)                   # 8-byte Folded Reload
	vmovups	%ymm2, 608(%rsp)                # 32-byte Spill
	vmovups	%ymm0, 352(%rsp)                # 32-byte Spill
	vmovups	%ymm6, 256(%rsp)                # 32-byte Spill
	je	.LBB11_18
# %bb.19:                               # %vector.body
                                        #   in Loop: Header=BB11_17 Depth=2
	vmovups	(%rcx,%r10,4), %ymm2
	jmp	.LBB11_20
	.p2align	4, 0x90
.LBB11_18:                              #   in Loop: Header=BB11_17 Depth=2
	vmovups	(%rcx,%r8,4), %ymm2
.LBB11_20:                              # %vector.body
                                        #   in Loop: Header=BB11_17 Depth=2
	vmaxps	%ymm8, %ymm2, %ymm2
	vmovups	320(%rsp), %ymm0                # 32-byte Reload
	vmaxps	%ymm8, %ymm0, %ymm6
	vmovups	224(%rsp), %ymm0                # 32-byte Reload
	vmaxps	%ymm8, %ymm0, %ymm0
	vdivps	%ymm15, %ymm14, %ymm14
	vmaxps	%ymm8, %ymm11, %ymm11
	vmovaps	%ymm3, %ymm15
	vblendvps	%ymm3, %ymm10, %ymm2, %ymm2
	movq	-96(%rsp), %rdi                 # 8-byte Reload
	vmovups	640(%rsp), %ymm10               # 32-byte Reload
	vmulps	(%rdi,%rbx,4), %ymm10, %ymm10
	vmulps	%ymm2, %ymm10, %ymm2
	vdivps	%ymm13, %ymm2, %ymm10
	vmulps	%ymm5, %ymm12, %ymm12
	vandps	%ymm3, %ymm9, %ymm5
	vmovups	(%rcx,%rdx,4), %ymm2
	vmaxps	%ymm8, %ymm2, %ymm9
	vxorps	%xmm15, %xmm15, %xmm15
	jne	.LBB11_22
# %bb.21:                               #   in Loop: Header=BB11_17 Depth=2
	vmovups	(%rcx,%r9,4), %ymm2
	jmp	.LBB11_23
	.p2align	4, 0x90
.LBB11_24:                              # %middle.block
                                        #   in Loop: Header=BB11_7 Depth=1
	movq	%r14, %r15
	cmpq	72(%rsp), %r14                  # 8-byte Folded Reload
	vmovss	.LCPI11_0(%rip), %xmm13         # xmm13 = mem[0],zero,zero,zero
	jne	.LBB11_25
	jmp	.LBB11_40
	.p2align	4, 0x90
.LBB11_39:                              # %"for f7.s1.n.ni"
                                        #   in Loop: Header=BB11_25 Depth=2
	vmovss	352(%rsp), %xmm8                # 4-byte Reload
                                        # xmm8 = mem[0],zero,zero,zero
	vmovss	320(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmovss	544(%rsp), %xmm15               # 4-byte Reload
                                        # xmm15 = mem[0],zero,zero,zero
	vmovaps	%xmm11, %xmm6
	vmovaps	%xmm7, %xmm11
	vmovaps	%xmm1, %xmm7
	movq	-88(%rsp), %rdx                 # 8-byte Reload
	vfmadd132ss	(%rdx,%r15,4), %xmm0, %xmm14 # xmm14 = (xmm14 * mem) + xmm0
	vfmadd213ss	%xmm12, %xmm8, %xmm14   # xmm14 = (xmm8 * xmm14) + xmm12
	vfmadd213ss	%xmm14, %xmm2, %xmm10   # xmm10 = (xmm2 * xmm10) + xmm14
	movq	80(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rbx,%rdx), %rdx
	vmovss	(%rcx,%rdx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	movq	16(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rbx,%rdx), %rdx
	vmovss	(%rcx,%rdx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	movq	832(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rbx,%rdx), %rdx
	vmaxss	%xmm5, %xmm0, %xmm0
	vmaxss	%xmm5, %xmm2, %xmm2
	movq	480(%rsp), %rsi                 # 8-byte Reload
	vmulss	(%rsi,%r15,4), %xmm8, %xmm4
	vmulss	%xmm4, %xmm2, %xmm2
	movq	448(%rsp), %rsi                 # 8-byte Reload
	vmulss	(%rsi,%r15,4), %xmm8, %xmm4
	vfmadd231ss	%xmm4, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm4) + xmm2
	vmovss	(%rcx,%rdx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	addq	1024(%rsp), %rax                # 8-byte Folded Reload
	movq	672(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%rdx,%r15,4), %xmm6, %xmm4
	vmaxss	%xmm5, %xmm0, %xmm0
	vfmadd231ss	%xmm4, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm4) + xmm2
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rbx,%rax), %rax
	vmaxss	%xmm5, %xmm0, %xmm0
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%rdx,%r15,4), %xmm3, %xmm4
	vfmadd231ss	%xmm4, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm4) + xmm2
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	movq	24(%rsp), %rax                  # 8-byte Reload
	leaq	(%rbx,%rax), %rax
	movq	-64(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%rdx,%r15,4), %xmm6, %xmm4
	vmaxss	%xmm5, %xmm0, %xmm0
	vfmadd231ss	%xmm4, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm4) + xmm2
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm0, %xmm0
	vmovss	608(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vmulss	%xmm1, %xmm3, %xmm4
	vfmadd231ss	%xmm4, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm4) + xmm2
	movq	400(%rsp), %rax                 # 8-byte Reload
	leaq	(%rbx,%rax), %rax
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm1, %xmm6, %xmm4
	vfmadd231ss	%xmm4, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm4) + xmm2
	movq	104(%rsp), %rax                 # 8-byte Reload
	leaq	(%rbx,%rax), %rax
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm3, %xmm15, %xmm4
	vfmadd231ss	%xmm4, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm4) + xmm2
	movq	408(%rsp), %rax                 # 8-byte Reload
	leaq	(%rbx,%rax), %rax
	vmulss	%xmm6, %xmm15, %xmm0
	vmovss	(%rcx,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	movq	896(%rsp), %rax                 # 8-byte Reload
	leaq	(%rbx,%rax), %rax
	vmaxss	%xmm5, %xmm4, %xmm4
	vfmadd231ss	%xmm0, %xmm4, %xmm2     # xmm2 = (xmm4 * xmm0) + xmm2
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm0, %xmm0
	movq	-72(%rsp), %rax                 # 8-byte Reload
	vmulss	(%rax,%r15,4), %xmm7, %xmm4
	vfmadd231ss	%xmm4, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm4) + xmm2
	movq	416(%rsp), %rax                 # 8-byte Reload
	leaq	(%rbx,%rax), %rax
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm0, %xmm0
	movq	-80(%rsp), %rax                 # 8-byte Reload
	vmulss	(%rax,%r15,4), %xmm7, %xmm4
	vfmadd231ss	%xmm4, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm4) + xmm2
	vmovss	224(%rsp), %xmm0                # 4-byte Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	.LCPI11_2(%rip), %xmm10, %xmm0 # xmm0 = (xmm10 * mem) + xmm0
	vaddss	%xmm2, %xmm0, %xmm0
	vmovss	256(%rsp), %xmm1                # 4-byte Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm0, %xmm8, %xmm1     # xmm1 = (xmm8 * xmm1) + xmm0
	movq	384(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	addq	392(%rsp), %rbx                 # 8-byte Folded Reload
	vmovss	(%rcx,%rbx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm0, %xmm0
	vmaxss	%xmm5, %xmm2, %xmm2
	vcmpltss	%xmm11, %xmm13, %xmm4
	vandps	%xmm2, %xmm4, %xmm2
	vmovss	640(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm1, %xmm4, %xmm2     # xmm2 = (xmm4 * xmm2) + xmm1
	vaddss	.LCPI11_3(%rip), %xmm11, %xmm1
	vmulss	%xmm4, %xmm1, %xmm1
	vfmadd231ss	%xmm1, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm1) + xmm2
	movq	-24(%rsp), %rax                 # 8-byte Reload
	vfmadd132ss	(%rax,%r15,4), %xmm2, %xmm9 # xmm9 = (xmm9 * mem) + xmm2
	movq	120(%rsp), %rax                 # 8-byte Reload
	vmovss	.LCPI11_4(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vfmadd213ss	(%rax,%r15,4), %xmm0, %xmm9 # xmm9 = (xmm0 * xmm9) + mem
	vmovss	%xmm9, (%rax,%r15,4)
	incq	%r15
	cmpq	%r15, 72(%rsp)                  # 8-byte Folded Reload
	je	.LBB11_40
.LBB11_25:                              # %"for f7.s1.n.ni"
                                        #   Parent Loop BB11_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	8(%rsp), %rax                   # 8-byte Reload
	addl	%r15d, %eax
	cltq
	movq	992(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax), %rbx
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rbx,%rdx), %r8
	movq	800(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rbx,%rdx), %r11
	movq	-48(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rbx,%rdx), %r10
	movq	128(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax), %r14
	movq	768(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rbx,%rdx), %rdi
	movq	928(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rbx,%rdx), %r12
	movq	432(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rbx,%rdx), %rdx
	movq	%rdx, 752(%rsp)                 # 8-byte Spill
	movq	1056(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%rax), %rdx
	movq	%rdx, 256(%rsp)                 # 8-byte Spill
	movq	864(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rbx,%rsi), %rbp
	movq	96(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rbx,%rsi), %r13
	movq	424(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rbx,%rdx), %rdx
	movq	%rdx, 760(%rsp)                 # 8-byte Spill
	movq	112(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rbx,%rsi), %rsi
	movq	440(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rbx,%rdx), %r9
	movq	960(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rdx,%r15,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	movq	136(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rdx,%r15,4), %xmm15           # xmm15 = mem[0],zero,zero,zero
	movq	152(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rdx,%r15,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	movq	160(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rdx,%r15,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movq	168(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rdx,%r15,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmulss	%xmm6, %xmm7, %xmm0
	vmaxss	%xmm13, %xmm0, %xmm4
	movq	176(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rdx,%r15,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vxorps	%xmm5, %xmm5, %xmm5
	cmpq	$0, 512(%rsp)                   # 8-byte Folded Reload
	vmovss	%xmm1, 352(%rsp)                # 4-byte Spill
	je	.LBB11_26
# %bb.27:                               # %"for f7.s1.n.ni"
                                        #   in Loop: Header=BB11_25 Depth=2
	vmovss	(%rcx,%r13,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	jmp	.LBB11_28
	.p2align	4, 0x90
.LBB11_26:                              #   in Loop: Header=BB11_25 Depth=2
	vmovss	(%rcx,%rbp,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
.LBB11_28:                              # %"for f7.s1.n.ni"
                                        #   in Loop: Header=BB11_25 Depth=2
	vmaxss	%xmm5, %xmm0, %xmm0
	vdivss	720(%rsp), %xmm4, %xmm11        # 16-byte Folded Reload
	vmovss	%xmm4, 320(%rsp)                # 4-byte Spill
	vmovss	%xmm6, 288(%rsp)                # 4-byte Spill
	vmovss	%xmm0, 224(%rsp)                # 4-byte Spill
	je	.LBB11_29
# %bb.30:                               # %"for f7.s1.n.ni"
                                        #   in Loop: Header=BB11_25 Depth=2
	vmovss	(%rcx,%r9,4), %xmm0             # xmm0 = mem[0],zero,zero,zero
	jmp	.LBB11_31
	.p2align	4, 0x90
.LBB11_29:                              #   in Loop: Header=BB11_25 Depth=2
	vmovss	(%rcx,%rsi,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
.LBB11_31:                              # %"for f7.s1.n.ni"
                                        #   in Loop: Header=BB11_25 Depth=2
	vmaxss	%xmm5, %xmm0, %xmm14
	vucomiss	%xmm7, %xmm13
	movq	184(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rdx,%r15,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 544(%rsp)                # 4-byte Spill
	movq	192(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rdx,%r15,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovss	%xmm0, 608(%rsp)                # 4-byte Spill
	movq	-32(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rdx,%r15,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	movq	144(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%rdx,%r15,4), %xmm15, %xmm0
	vmaxss	%xmm13, %xmm2, %xmm8
	vmovss	(%rcx,%r8,4), %xmm10            # xmm10 = mem[0],zero,zero,zero
	vmovss	(%rcx,%r11,4), %xmm9            # xmm9 = mem[0],zero,zero,zero
	vmovss	(%rcx,%rbx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm2, %xmm2
	vmovss	(%rcx,%r10,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm4, %xmm4
	vmulss	%xmm4, %xmm3, %xmm4
	vfmadd231ss	%xmm2, %xmm0, %xmm4     # xmm4 = (xmm0 * xmm2) + xmm4
	vmovss	(%rcx,%r14,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm2, %xmm2
	vmovss	(%rcx,%rdi,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm6, %xmm6
	vmulss	%xmm6, %xmm3, %xmm6
	vfmadd231ss	%xmm2, %xmm0, %xmm6     # xmm6 = (xmm0 * xmm2) + xmm6
	movq	-16(%rsp), %rsi                 # 8-byte Reload
	vcmpeqss	(%rsi,%r15,4), %xmm5, %xmm0
	vblendvps	%xmm0, %xmm4, %xmm6, %xmm4
	vroundss	$10, %xmm11, %xmm11, %xmm6
	vmovss	(%rcx,%r12,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmovss	.LCPI11_1(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm8, %xmm0, %xmm0
	vminss	%xmm7, %xmm0, %xmm0
	movq	-8(%rsp), %rdx                  # 8-byte Reload
	vmulss	(%rdx,%r15,4), %xmm0, %xmm0
	vmovss	%xmm1, 640(%rsp)                # 4-byte Spill
	vmovss	%xmm2, 576(%rsp)                # 4-byte Spill
	jb	.LBB11_32
# %bb.33:                               # %"for f7.s1.n.ni"
                                        #   in Loop: Header=BB11_25 Depth=2
	vxorps	%xmm12, %xmm12, %xmm12
	jmp	.LBB11_34
	.p2align	4, 0x90
.LBB11_32:                              #   in Loop: Header=BB11_25 Depth=2
	movq	752(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rcx,%rdx,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm1, %xmm1
	vaddss	%xmm3, %xmm15, %xmm2
	vmulss	%xmm1, %xmm2, %xmm1
	vdivss	%xmm8, %xmm1, %xmm12
.LBB11_34:                              # %"for f7.s1.n.ni"
                                        #   in Loop: Header=BB11_25 Depth=2
	vmaxss	.LCPI11_0(%rip), %xmm11, %xmm2
	vmulss	%xmm4, %xmm6, %xmm4
	vmulss	%xmm0, %xmm7, %xmm0
	vmovaps	%xmm15, %xmm11
	jb	.LBB11_35
# %bb.36:                               # %"for f7.s1.n.ni"
                                        #   in Loop: Header=BB11_25 Depth=2
	vmovss	.LCPI11_0(%rip), %xmm13         # xmm13 = mem[0],zero,zero,zero
	vmovss	288(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
	vmovss	224(%rsp), %xmm15               # 4-byte Reload
                                        # xmm15 = mem[0],zero,zero,zero
	jmp	.LBB11_37
	.p2align	4, 0x90
.LBB11_35:                              #   in Loop: Header=BB11_25 Depth=2
	movq	256(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rcx,%rdx,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm1, %xmm15
	vmovss	.LCPI11_0(%rip), %xmm13         # xmm13 = mem[0],zero,zero,zero
	vmovss	288(%rsp), %xmm6                # 4-byte Reload
                                        # xmm6 = mem[0],zero,zero,zero
.LBB11_37:                              # %"for f7.s1.n.ni"
                                        #   in Loop: Header=BB11_25 Depth=2
	vmovaps	%xmm9, %xmm1
	vmaxss	%xmm5, %xmm10, %xmm9
	vmaxss	%xmm5, %xmm1, %xmm1
	vdivss	%xmm2, %xmm4, %xmm4
	vmovss	576(%rsp), %xmm2                # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm2, %xmm2
	vmulss	%xmm0, %xmm6, %xmm10
	movq	-96(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%rdx,%r15,4), %xmm7, %xmm0
	vmulss	%xmm0, %xmm15, %xmm0
	vdivss	%xmm8, %xmm0, %xmm0
	vmovss	%xmm1, 256(%rsp)                # 4-byte Spill
	vmovss	%xmm4, 224(%rsp)                # 4-byte Spill
	jae	.LBB11_39
# %bb.38:                               #   in Loop: Header=BB11_25 Depth=2
	movq	760(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rcx,%rdx,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmaxss	%xmm5, %xmm4, %xmm14
	jmp	.LBB11_39
.LBB11_15:                              # %destructor_block.loopexit
	movq	-40(%rsp), %rax                 # 8-byte Reload
	movq	80(%rsp), %rcx                  # 8-byte Reload
	vmovups	%ymm5, (%rax,%rcx,4)
.LBB11_41:                              # %destructor_block
	xorl	%eax, %eax
	addq	$1128, %rsp                     # imm = 0x468
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end11:
	.size	train_cost_model.par_for.f7.s1.n.n, .Lfunc_end11-train_cost_model.par_for.f7.s1.n.n
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.f6_0_d_def__.s0.n.n.n
.LCPI12_0:
	.long	0x3f800000                      # float 1
.LCPI12_1:
	.long	0x2edbe6ff                      # float 1.00000001E-10
.LCPI12_3:
	.long	0xc0000000                      # float -2
.LCPI12_4:
	.long	8                               # 0x8
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5
.LCPI12_2:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.text.train_cost_model.par_for.f6_0_d_def__.s0.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.f6_0_d_def__.s0.n.n.n,@function
train_cost_model.par_for.f6_0_d_def__.s0.n.n.n: # @train_cost_model.par_for.f6_0_d_def__.s0.n.n.n
	.cfi_startproc
# %bb.0:                                # %entry
	pushq	%rbp
	.cfi_def_cfa_offset 16
	pushq	%r15
	.cfi_def_cfa_offset 24
	pushq	%r14
	.cfi_def_cfa_offset 32
	pushq	%r13
	.cfi_def_cfa_offset 40
	pushq	%r12
	.cfi_def_cfa_offset 48
	pushq	%rbx
	.cfi_def_cfa_offset 56
	subq	$360, %rsp                      # imm = 0x168
	.cfi_def_cfa_offset 416
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	.cfi_offset %rbp, -16
	movq	%rdx, %rcx
	movl	%esi, %r8d
	movl	(%rdx), %r12d
	movl	16(%rdx), %ebx
	movl	%esi, %r15d
	sarl	$31, %r15d
	xorl	%ebp, %ebp
	testl	%ebx, %ebx
	sete	%bpl
	movl	%ebp, %esi
	negl	%esi
	subl	%r15d, %r8d
	orl	%ebx, %esi
	movl	%r8d, %eax
	cltd
	idivl	%esi
	movl	%ebx, %r14d
	sarl	$31, %r14d
	movl	%r14d, %esi
	xorl	%ebx, %esi
	movl	%r14d, 192(%rsp)                # 4-byte Spill
	notl	%r14d
	addl	%r14d, %esi
	andl	%r15d, %esi
	addl	%edx, %esi
	addl	%ebp, %ebx
	movl	%r8d, %eax
	cltd
	idivl	%ebx
                                        # kill: def $eax killed $eax def $rax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	decl	%ebp
	andl	%ebp, %esi
	leal	(,%rsi,8), %edx
	movq	%rsi, 48(%rsp)                  # 8-byte Spill
	leal	8(,%rsi,8), %eax
	cmpl	%eax, %r12d
	movl	%r12d, %esi
	movl	%eax, 164(%rsp)                 # 4-byte Spill
	cmovgl	%eax, %esi
	movq	%rsi, 168(%rsp)                 # 8-byte Spill
	movl	%esi, %eax
	movq	%rdx, 80(%rsp)                  # 8-byte Spill
	subl	%edx, %eax
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %eax
	leaq	(,%rax,4), %rdx
	cmpl	$536870912, %eax                # imm = 0x20000000
	jae	.LBB12_56
# %bb.1:                                # %"assert succeeded"
	movl	%ebp, 56(%rsp)                  # 4-byte Spill
	movl	8(%rcx), %eax
	movl	%eax, 156(%rsp)                 # 4-byte Spill
	movslq	4(%rcx), %rax
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	movl	12(%rcx), %eax
	movl	%eax, 44(%rsp)                  # 4-byte Spill
	movslq	20(%rcx), %rax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movslq	24(%rcx), %rax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movslq	28(%rcx), %rax
	movq	%rax, 88(%rsp)                  # 8-byte Spill
	movq	32(%rcx), %rax
	movq	%rax, 104(%rsp)                 # 8-byte Spill
	movq	48(%rcx), %rax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	64(%rcx), %rax
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	movq	80(%rcx), %r13
	addq	$4, %rdx
	movq	%rdi, %rbx
	movq	%rdx, %rsi
	callq	halide_malloc@PLT
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	testq	%rax, %rax
	je	.LBB12_57
# %bb.2:                                # %"assert succeeded2"
	movq	%rbx, 224(%rsp)                 # 8-byte Spill
	subl	192(%rsp), %r14d                # 4-byte Folded Reload
	andl	%r15d, %r14d
	movq	64(%rsp), %rax                  # 8-byte Reload
	addl	%r14d, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movq	168(%rsp), %r15                 # 8-byte Reload
	leal	7(%r15), %r9d
	movl	%r15d, %r8d
	sarl	$3, %r8d
	subl	48(%rsp), %r8d                  # 4-byte Folded Reload
	movl	%r8d, %ecx
	sarl	$31, %ecx
	andnl	%r8d, %ecx, %eax
	vmovd	%r12d, %xmm10
	movq	%rax, %rbp
	movl	%eax, %r11d
	testl	%r8d, %r8d
	jle	.LBB12_5
# %bb.3:                                # %"for f7_1_d_def__.s0.n.n.preheader"
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	vmovss	.LCPI12_0(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	128(%rsp), %rax                 # 8-byte Reload
	vdivss	(%r13,%rax,4), %xmm0, %xmm0
	vbroadcastss	%xmm0, %ymm0
	vpbroadcastd	%xmm10, %ymm2
	movq	120(%rsp), %rax                 # 8-byte Reload
	vpbroadcastd	(%rax), %ymm3
	movq	80(%rsp), %rax                  # 8-byte Reload
	leaq	1(%rax), %r10
	movq	32(%rsp), %rax                  # 8-byte Reload
	shlq	$3, %rcx
	movq	%rcx, %rdx
	subq	88(%rsp), %rdx                  # 8-byte Folded Reload
	leaq	(,%rdx,4), %rdi
	addq	%r13, %rdi
	subq	96(%rsp), %rcx                  # 8-byte Folded Reload
	movq	24(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rdx,%rcx,4), %rbx
	xorl	%edx, %edx
	vbroadcastss	.LCPI12_1(%rip), %ymm4  # ymm4 = [1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10]
	vmovdqa	.LCPI12_2(%rip), %ymm5          # ymm5 = [0,1,2,3,4,5,6,7]
	vbroadcastss	.LCPI12_0(%rip), %ymm6  # ymm6 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI12_3(%rip), %ymm7  # ymm7 = [-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0]
	movq	%r11, %rcx
	.p2align	4, 0x90
.LBB12_4:                               # %"for f7_1_d_def__.s0.n.n"
                                        # =>This Inner Loop Header: Depth=1
	vmulps	(%rbx,%rdx,4), %ymm0, %ymm8
	vmaxps	%ymm4, %ymm8, %ymm9
	leal	(%r10,%rdx), %esi
	vmovd	%esi, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vdivps	%ymm9, %ymm6, %ymm11
	vpaddd	%ymm5, %ymm1, %ymm1
	vpcmpgtd	%ymm2, %ymm1, %ymm1
	vmulps	(%rdi,%rdx,4), %ymm0, %ymm12
	vpandn	%ymm3, %ymm1, %ymm1
	vdivps	%ymm12, %ymm6, %ymm12
	vmulps	%ymm7, %ymm1, %ymm1
	vsubps	%ymm12, %ymm11, %ymm11
	vmulps	%ymm1, %ymm11, %ymm1
	vmulps	%ymm9, %ymm9, %ymm9
	vmulps	%ymm1, %ymm0, %ymm1
	vdivps	%ymm9, %ymm1, %ymm1
	vcmpleps	%ymm8, %ymm4, %ymm8
	vandps	%ymm1, %ymm8, %ymm1
	vmovaps	%ymm1, (%rax,%rdx,4)
	addq	$8, %rdx
	decq	%rcx
	jne	.LBB12_4
.LBB12_5:                               # %"end for f7_1_d_def__.s0.n.n"
	movq	64(%rsp), %rax                  # 8-byte Reload
	andl	56(%rsp), %eax                  # 4-byte Folded Reload
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movq	32(%rsp), %rsi                  # 8-byte Reload
	leal	-1(%r15), %ecx
	sarl	$3, %ecx
	movq	48(%rsp), %rax                  # 8-byte Reload
	subl	%eax, %ecx
	incl	%ecx
	cmpl	%ecx, %r8d
	cmovgel	%r8d, %ecx
	sarl	$3, %r9d
	subl	%eax, %r9d
	cmpl	%ecx, %r9d
	cmovgl	%ecx, %r9d
	cmpl	%r8d, %r9d
	cmovll	%r8d, %r9d
	movl	%r9d, %ecx
	sarl	$31, %ecx
	andnl	%r9d, %ecx, %ecx
	movslq	80(%rsp), %rax                  # 4-byte Folded Reload
	movq	%rax, 184(%rsp)                 # 8-byte Spill
	movq	%rbp, %r9
	cmpl	%ecx, %r9d
	movq	%r12, 176(%rsp)                 # 8-byte Spill
	movq	96(%rsp), %rbx                  # 8-byte Reload
	movq	88(%rsp), %rdi                  # 8-byte Reload
	movq	24(%rsp), %rbp                  # 8-byte Reload
	jge	.LBB12_21
# %bb.6:                                # %"for f7_1_d_def__.s0.n.n3.preheader"
	movq	%r11, %r10
	leal	-1(%r12), %eax
	movl	%eax, 76(%rsp)                  # 4-byte Spill
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm12
	vmovd	%ebx, %xmm0
	vpbroadcastd	%xmm0, %ymm15
	vpbroadcastd	%xmm10, %ymm11
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqu	%ymm0, 320(%rsp)                # 32-byte Spill
	movl	%ecx, %eax
	movq	%rax, 264(%rsp)                 # 8-byte Spill
	addl	48(%rsp), %r9d                  # 4-byte Folded Reload
	leal	(,%r9,8), %eax
	movl	%r15d, %edx
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	subl	%eax, %edx
	vmovq	%rbx, %xmm0
	vpbroadcastq	%xmm0, %ymm13
	vmovq	%rdi, %xmm0
	vpbroadcastq	%xmm0, %ymm8
	movq	184(%rsp), %rax                 # 8-byte Reload
	leaq	(,%rax,4), %rcx
	movq	%rsi, %rax
	subq	%rcx, %rax
	movq	%rax, 256(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	vmovss	.LCPI12_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmovss	.LCPI12_1(%rip), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmovss	.LCPI12_3(%rip), %xmm9          # xmm9 = mem[0],zero,zero,zero
	vpxor	%xmm10, %xmm10, %xmm10
	movl	%edx, 160(%rsp)                 # 4-byte Spill
	vmovdqu	%ymm15, 288(%rsp)               # 32-byte Spill
	jmp	.LBB12_7
	.p2align	4, 0x90
.LBB12_8:                               # %true_bb
                                        #   in Loop: Header=BB12_7 Depth=1
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	.LCPI12_2(%rip), %ymm2          # ymm2 = [0,1,2,3,4,5,6,7]
	vpor	%ymm2, %ymm0, %ymm0
	vpminsd	%ymm0, %ymm12, %ymm0
	vmovdqa	%ymm13, %ymm5
	vpmaxsd	%ymm10, %ymm0, %ymm13
	vpsubd	%ymm15, %ymm13, %ymm0
	vmovd	%xmm0, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 192(%rsp)                 # 8-byte Spill
	vpextrd	$1, %xmm0, %edx
	movq	%r10, %r8
	movslq	%edx, %r10
	vpextrd	$2, %xmm0, %ebx
	vpextrd	$3, %xmm0, %r14d
	movslq	%ebx, %r11
	vextracti128	$1, %ymm0, %xmm0
	vmovd	%xmm0, %edx
	movslq	%edx, %rdx
	vpextrd	$1, %xmm0, %ebx
	vpextrd	$2, %xmm0, %ecx
	movslq	%ebx, %rbx
	movslq	%ecx, %rcx
	movq	%r9, %rbp
	vpextrd	$3, %xmm0, %r9d
	movslq	%r9d, %r9
	movq	24(%rsp), %rdi                  # 8-byte Reload
	vmovss	(%rdi,%rdx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	movq	24(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, (%rdx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	24(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$32, (%rdx,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	24(%rsp), %rcx                  # 8-byte Reload
	vinsertps	$48, (%rcx,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%r14d, %rcx
	movq	24(%rsp), %rdx                  # 8-byte Reload
	movq	192(%rsp), %rdi                 # 8-byte Reload
	vmovss	(%rdx,%rdi,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	movq	24(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, (%rdx,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	24(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$32, (%rdx,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	24(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$48, (%rdx,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	128(%rsp), %rcx                 # 8-byte Reload
	vdivss	(%r13,%rcx,4), %xmm4, %xmm1
	vbroadcastss	%xmm1, %ymm9
	vmulps	%ymm0, %ymm9, %ymm10
	vbroadcastss	.LCPI12_1(%rip), %ymm3  # ymm3 = [1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10]
	vmaxps	%ymm3, %ymm10, %ymm14
	orl	$1, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqa	%ymm8, %ymm6
	vmovdqa	%ymm11, %ymm4
	vpaddd	%ymm2, %ymm0, %ymm8
	movq	120(%rsp), %rax                 # 8-byte Reload
	vpbroadcastd	(%rax), %ymm11
	vbroadcastss	.LCPI12_0(%rip), %ymm0  # ymm0 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vpsubd	320(%rsp), %ymm13, %ymm2        # 32-byte Folded Reload
	vmovd	%xmm2, %ecx
	movslq	%ecx, %r9
	vpextrd	$1, %xmm2, %edx
	movslq	%edx, %r10
	vpextrd	$2, %xmm2, %eax
	vpextrd	$3, %xmm2, %r14d
	movslq	%eax, %r11
	vextracti128	$1, %ymm2, %xmm2
	vmovd	%xmm2, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm2, %edx
	vpextrd	$2, %xmm2, %eax
	movslq	%edx, %rdx
	cltq
	vpextrd	$3, %xmm2, %ebx
	movslq	%ebx, %rbx
	vmovss	(%r13,%rcx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r13,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r13,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movslq	%r14d, %rcx
	vmovss	(%r13,%r9,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	movq	%rbp, %r9
	movq	24(%rsp), %rbp                  # 8-byte Reload
	movq	88(%rsp), %rdi                  # 8-byte Reload
	movq	96(%rsp), %rbx                  # 8-byte Reload
	vinsertps	$16, (%r13,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	%r8, %r10
	vinsertps	$32, (%r13,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r13,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vdivps	%ymm14, %ymm0, %ymm2
	vmulps	%ymm1, %ymm9, %ymm1
	vdivps	%ymm1, %ymm0, %ymm0
	vpcmpgtd	%ymm4, %ymm8, %ymm1
	vmovdqa	%ymm5, %ymm13
	vmovdqa	%ymm6, %ymm8
	vpandn	%ymm11, %ymm1, %ymm1
	vmovdqa	%ymm4, %ymm11
	vsubps	%ymm0, %ymm2, %ymm0
	vbroadcastss	.LCPI12_3(%rip), %ymm2  # ymm2 = [-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0]
	vmulps	%ymm2, %ymm1, %ymm1
	vmulps	%ymm14, %ymm14, %ymm2
	vmulps	%ymm1, %ymm9, %ymm1
	vmovss	.LCPI12_3(%rip), %xmm9          # xmm9 = mem[0],zero,zero,zero
	vmovss	.LCPI12_1(%rip), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmovss	.LCPI12_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmulps	%ymm1, %ymm0, %ymm0
	vdivps	%ymm2, %ymm0, %ymm0
	vcmpleps	%ymm10, %ymm3, %ymm1
	vxorps	%xmm10, %xmm10, %xmm10
	vandps	%ymm0, %ymm1, %ymm0
	movq	%r8, %rcx
	shlq	$5, %rcx
	vmovaps	%ymm0, (%rsi,%rcx)
.LBB12_20:                              # %after_bb
                                        #   in Loop: Header=BB12_7 Depth=1
	incq	%r10
	movq	144(%rsp), %rax                 # 8-byte Reload
	incl	%eax
	movl	136(%rsp), %edx                 # 4-byte Reload
	addl	$-8, %edx
	incl	%r9d
	addq	$8, 56(%rsp)                    # 8-byte Folded Spill
	cmpq	264(%rsp), %r10                 # 8-byte Folded Reload
	movq	32(%rsp), %rsi                  # 8-byte Reload
	je	.LBB12_21
.LBB12_7:                               # %"for f7_1_d_def__.s0.n.n3"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB12_18 Depth 2
                                        #     Child Loop BB12_12 Depth 2
	cmpl	$9, %edx
	movl	$8, %r8d
	movl	%edx, 136(%rsp)                 # 4-byte Spill
	cmovll	%edx, %r8d
	movq	%rax, 144(%rsp)                 # 8-byte Spill
	leal	(,%rax,8), %ecx
	movl	160(%rsp), %eax                 # 4-byte Reload
	movl	%eax, %r14d
	subl	%ecx, %r14d
	cmpl	$9, %r14d
	movl	$8, %eax
	cmovgel	%eax, %r14d
	movq	48(%rsp), %rax                  # 8-byte Reload
	leal	(%rax,%r10), %ecx
	leal	8(,%rcx,8), %edx
	movl	%ecx, %eax
	shll	$3, %eax
	cmpl	%r15d, %edx
	jle	.LBB12_8
# %bb.9:                                # %false_bb
                                        #   in Loop: Header=BB12_7 Depth=1
	cmpl	%eax, %r15d
	movl	76(%rsp), %esi                  # 4-byte Reload
	jle	.LBB12_20
# %bb.10:                               # %"for f7_1_d_def__.s0.n.ni.preheader"
                                        #   in Loop: Header=BB12_7 Depth=1
	movslq	%r9d, %r11
	shlq	$5, %r11
	addq	256(%rsp), %r11                 # 8-byte Folded Reload
	movq	128(%rsp), %rcx                 # 8-byte Reload
	vdivss	(%r13,%rcx,4), %xmm4, %xmm3
	movq	120(%rsp), %rcx                 # 8-byte Reload
	vmovd	(%rcx), %xmm14                  # xmm14 = mem[0],zero,zero,zero
	cmpl	$7, %r14d
	ja	.LBB12_17
# %bb.11:                               #   in Loop: Header=BB12_7 Depth=1
	xorl	%r14d, %r14d
	jmp	.LBB12_12
	.p2align	4, 0x90
.LBB12_17:                              # %vector.ph
                                        #   in Loop: Header=BB12_7 Depth=1
	movq	%r10, 240(%rsp)                 # 8-byte Spill
	movq	%r9, 248(%rsp)                  # 8-byte Spill
	movl	%r8d, %r9d
	andl	$-8, %r9d
	movq	%r14, 232(%rsp)                 # 8-byte Spill
                                        # kill: def $r14d killed $r14d killed $r14 def $r14
	andl	$-8, %r14d
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqu	%ymm0, 192(%rsp)                # 32-byte Spill
	vmovdqa	%xmm14, %xmm0
	vbroadcastss	%xmm3, %ymm14
	vmovdqa	%xmm0, 272(%rsp)                # 16-byte Spill
	vpbroadcastd	%xmm0, %ymm15
	xorl	%esi, %esi
	vmovdqa	.LCPI12_2(%rip), %ymm9          # ymm9 = [0,1,2,3,4,5,6,7]
	vmovdqa	%ymm13, %ymm4
	vmovdqa	%ymm8, %ymm5
	vpxor	%xmm13, %xmm13, %xmm13
	vmovdqa	%ymm11, %ymm10
	.p2align	4, 0x90
.LBB12_18:                              # %vector.body
                                        #   Parent Loop BB12_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vpaddd	192(%rsp), %ymm9, %ymm8         # 32-byte Folded Reload
	vpminsd	%ymm8, %ymm12, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpmovsxdq	%xmm0, %ymm0
	vpmovsxdq	%xmm1, %ymm1
	vpcmpgtq	%ymm13, %ymm0, %ymm2
	vmovdqa	%ymm12, %ymm6
	vpand	%ymm0, %ymm2, %ymm12
	vpcmpgtq	%ymm13, %ymm1, %ymm0
	vpand	%ymm1, %ymm0, %ymm0
	vpsubq	%ymm4, %ymm0, %ymm1
	vpsubq	%ymm4, %ymm12, %ymm2
	vpextrq	$1, %xmm2, %rcx
	vmovq	%xmm2, %rdx
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm2, %r12
	vmovq	%xmm1, %r15
	vpextrq	$1, %xmm1, %r10
	vextracti128	$1, %ymm1, %xmm1
	vmovss	(%rbp,%rdx,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbp,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrq	$1, %xmm2, %rcx
	vmovq	%xmm1, %rdx
	vbroadcastss	.LCPI12_1(%rip), %ymm11 # ymm11 = [1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10,1.00000001E-10]
	vpextrq	$1, %xmm1, %rax
	vpcmpgtd	%ymm8, %ymm10, %ymm1
	vpand	%ymm1, %ymm15, %ymm1
	vpsubq	%ymm5, %ymm0, %ymm0
	vmovss	(%rbp,%r15,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vpsubq	%ymm5, %ymm12, %ymm8
	vmovdqa	%ymm6, %ymm12
	vmovq	%xmm8, %r15
	vinsertps	$32, (%rbp,%r12,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrq	$1, %xmm8, %rbx
	vinsertps	$16, (%rbp,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vextracti128	$1, %ymm8, %xmm6
	vmovq	%xmm6, %rdi
	vinsertps	$48, (%rbp,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vpextrq	$1, %xmm6, %rcx
	vinsertps	$32, (%rbp,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrq	$1, %xmm0, %rdx
	vinsertps	$48, (%rbp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovq	%xmm0, %rax
	vextracti128	$1, %ymm0, %xmm0
	vmovss	(%r13,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmovq	%xmm0, %rax
	vinsertps	$16, (%r13,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrq	$1, %xmm0, %rdx
	vinsertps	$32, (%r13,%rax,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r13,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r13,%r15,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertf128	$1, %xmm2, %ymm7, %ymm2
	vbroadcastss	.LCPI12_0(%rip), %ymm7  # ymm7 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vinsertps	$32, (%r13,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmulps	%ymm2, %ymm14, %ymm2
	vinsertps	$48, (%r13,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm6, %ymm0
	vmaxps	%ymm11, %ymm2, %ymm6
	vmulps	%ymm0, %ymm14, %ymm0
	vdivps	%ymm0, %ymm7, %ymm0
	vdivps	%ymm6, %ymm7, %ymm7
	vsubps	%ymm0, %ymm7, %ymm0
	vbroadcastss	.LCPI12_3(%rip), %ymm7  # ymm7 = [-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0,-2.0E+0]
	vmulps	%ymm7, %ymm1, %ymm1
	vmulps	%ymm1, %ymm0, %ymm0
	vmulps	%ymm6, %ymm6, %ymm1
	vmulps	%ymm0, %ymm14, %ymm0
	vdivps	%ymm1, %ymm0, %ymm0
	vcmpleps	%ymm2, %ymm11, %ymm1
	vandps	%ymm0, %ymm1, %ymm0
	vmovups	%ymm0, (%r11,%rsi,4)
	vpbroadcastd	.LCPI12_4(%rip), %ymm0  # ymm0 = [8,8,8,8,8,8,8,8]
	vpaddd	%ymm0, %ymm9, %ymm9
	addq	$8, %rsi
	cmpq	%rsi, %r9
	jne	.LBB12_18
# %bb.19:                               # %middle.block
                                        #   in Loop: Header=BB12_7 Depth=1
	cmpq	232(%rsp), %r14                 # 8-byte Folded Reload
	movq	176(%rsp), %r12                 # 8-byte Reload
	movq	168(%rsp), %r15                 # 8-byte Reload
	movq	96(%rsp), %rbx                  # 8-byte Reload
	movq	88(%rsp), %rdi                  # 8-byte Reload
	movq	248(%rsp), %r9                  # 8-byte Reload
	movq	240(%rsp), %r10                 # 8-byte Reload
	movl	76(%rsp), %esi                  # 4-byte Reload
	vmovdqu	288(%rsp), %ymm15               # 32-byte Reload
	vmovdqa	%ymm10, %ymm11
	vmovdqa	%ymm4, %ymm13
	vmovdqa	%ymm5, %ymm8
	vmovss	.LCPI12_0(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmovss	.LCPI12_1(%rip), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmovss	.LCPI12_3(%rip), %xmm9          # xmm9 = mem[0],zero,zero,zero
	vpxor	%xmm10, %xmm10, %xmm10
	vmovdqa	272(%rsp), %xmm14               # 16-byte Reload
	jne	.LBB12_12
	jmp	.LBB12_20
	.p2align	4, 0x90
.LBB12_16:                              # %select.end
                                        #   in Loop: Header=BB12_12 Depth=2
	vmovss	%xmm2, (%r11,%r14,4)
	incq	%r14
	cmpq	%r14, %r8
	je	.LBB12_20
.LBB12_12:                              # %"for f7_1_d_def__.s0.n.ni"
                                        #   Parent Loop BB12_7 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	56(%rsp), %rax                  # 8-byte Reload
	addl	%r14d, %eax
	cmpl	%eax, %esi
	movl	%esi, %ecx
	cmovgl	%eax, %ecx
	movslq	%ecx, %rcx
	movq	%rcx, %rdx
	sarq	$63, %rdx
	andnq	%rcx, %rdx, %rdx
	movq	%rdx, %rcx
	subq	%rbx, %rcx
	vmulss	(%rbp,%rcx,4), %xmm3, %xmm1
	vmaxss	%xmm5, %xmm1, %xmm0
	vucomiss	%xmm5, %xmm1
	setb	%cl
	vxorps	%xmm2, %xmm2, %xmm2
	vmovdqa	%xmm14, %xmm1
	cmpl	%r12d, %eax
	jl	.LBB12_14
# %bb.13:                               # %"for f7_1_d_def__.s0.n.ni"
                                        #   in Loop: Header=BB12_12 Depth=2
	vpxor	%xmm1, %xmm1, %xmm1
.LBB12_14:                              # %"for f7_1_d_def__.s0.n.ni"
                                        #   in Loop: Header=BB12_12 Depth=2
	vdivss	%xmm0, %xmm4, %xmm6
	subq	%rdi, %rdx
	vmulss	(%r13,%rdx,4), %xmm3, %xmm7
	vdivss	%xmm7, %xmm4, %xmm7
	vsubss	%xmm7, %xmm6, %xmm6
	vmulss	%xmm6, %xmm1, %xmm1
	vmulss	%xmm1, %xmm9, %xmm1
	testb	$1, %cl
	jne	.LBB12_16
# %bb.15:                               # %select.false.sink
                                        #   in Loop: Header=BB12_12 Depth=2
	vmulss	%xmm0, %xmm0, %xmm0
	vmulss	%xmm1, %xmm3, %xmm1
	vdivss	%xmm0, %xmm1, %xmm2
	jmp	.LBB12_16
.LBB12_21:                              # %"consume f7_1_d_def__"
	movq	64(%rsp), %r11                  # 8-byte Reload
	addl	%r11d, %r11d
	movl	44(%rsp), %ecx                  # 4-byte Reload
	subl	%r11d, %ecx
	jle	.LBB12_53
# %bb.22:                               # %"for f6_0_d_def__.s0.w.wi.preheader"
	movl	%r11d, %edx
	subl	156(%rsp), %edx                 # 4-byte Folded Reload
	movl	%r12d, %r13d
	subl	80(%rsp), %r13d                 # 4-byte Folded Reload
	xorl	%eax, %eax
	cmpl	$1, %ecx
	sete	%al
	movl	$2, %r10d
	subq	%rax, %r10
	movq	%rdx, 136(%rsp)                 # 8-byte Spill
	movslq	%edx, %r9
	cmpl	$9, %r13d
	movl	$8, %ebp
	cmovll	%r13d, %ebp
	xorl	%ecx, %ecx
	testl	%r13d, %r13d
	cmovlel	%ecx, %ebp
	movl	%ebp, %r15d
	andl	$2147483616, %r15d              # imm = 0x7FFFFFE0
	leaq	-32(%r15), %rax
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	movq	%rax, %rbx
	shrq	$5, %rbx
	incq	%rbx
	movq	104(%rsp), %rax                 # 8-byte Reload
	addq	$224, %rax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movl	%r9d, %eax
	imull	112(%rsp), %eax                 # 4-byte Folded Reload
	movq	48(%rsp), %rdx                  # 8-byte Reload
	leal	(%rax,%rdx,8), %edi
	movq	32(%rsp), %rsi                  # 8-byte Reload
	movq	%rbx, %rax
	movq	%rbx, 192(%rsp)                 # 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 144(%rsp)                 # 8-byte Spill
	jmp	.LBB12_23
	.p2align	4, 0x90
.LBB12_29:                              # %true_bb6
                                        #   in Loop: Header=BB12_23 Depth=1
	leaq	(%rcx,%r9), %rax
	imulq	112(%rsp), %rax                 # 8-byte Folded Reload
	addq	184(%rsp), %rax                 # 8-byte Folded Reload
	movq	104(%rsp), %rdx                 # 8-byte Reload
	vmovups	%ymm0, (%rdx,%rax,4)
	movq	32(%rsp), %rsi                  # 8-byte Reload
.LBB12_52:                              # %after_bb8
                                        #   in Loop: Header=BB12_23 Depth=1
	incq	%rcx
	addl	112(%rsp), %edi                 # 4-byte Folded Reload
	cmpq	%r10, %rcx
	je	.LBB12_53
.LBB12_23:                              # %"for f6_0_d_def__.s0.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB12_39 Depth 2
                                        #     Child Loop BB12_34 Depth 2
	leal	(%r11,%rcx), %edx
	cmpl	%r12d, 164(%rsp)                # 4-byte Folded Reload
	jle	.LBB12_24
# %bb.30:                               # %false_bb7
                                        #   in Loop: Header=BB12_23 Depth=1
	testl	%r13d, %r13d
	jle	.LBB12_52
# %bb.31:                               # %"for f6_0_d_def__.s0.n.ni.preheader"
                                        #   in Loop: Header=BB12_23 Depth=1
	movslq	%edi, %r14
	testl	%edx, %edx
	setns	%al
	cmpl	44(%rsp), %edx                  # 4-byte Folded Reload
	setl	%dl
	andb	%al, %dl
	cmpl	$31, %ebp
	ja	.LBB12_36
# %bb.32:                               #   in Loop: Header=BB12_23 Depth=1
	xorl	%ebx, %ebx
	jmp	.LBB12_33
	.p2align	4, 0x90
.LBB12_24:                              # %true_bb6
                                        #   in Loop: Header=BB12_23 Depth=1
	cmpl	44(%rsp), %edx                  # 4-byte Folded Reload
	jl	.LBB12_25
# %bb.26:                               # %true_bb6
                                        #   in Loop: Header=BB12_23 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	testl	%edx, %edx
	jns	.LBB12_29
	jmp	.LBB12_28
	.p2align	4, 0x90
.LBB12_25:                              #   in Loop: Header=BB12_23 Depth=1
	vmovaps	(%rsi), %ymm0
	testl	%edx, %edx
	jns	.LBB12_29
.LBB12_28:                              # %true_bb6
                                        #   in Loop: Header=BB12_23 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	jmp	.LBB12_29
	.p2align	4, 0x90
.LBB12_36:                              # %vector.ph49
                                        #   in Loop: Header=BB12_23 Depth=1
	cmpq	$0, 56(%rsp)                    # 8-byte Folded Reload
	je	.LBB12_37
# %bb.38:                               # %vector.body47.preheader
                                        #   in Loop: Header=BB12_23 Depth=1
	movq	24(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r14,4), %rbx
	movq	144(%rsp), %r12                 # 8-byte Reload
	xorl	%r8d, %r8d
	movq	32(%rsp), %rsi                  # 8-byte Reload
	jmp	.LBB12_39
	.p2align	4, 0x90
.LBB12_44:                              # %vector.body47
                                        #   in Loop: Header=BB12_39 Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
	vpxor	%xmm2, %xmm2, %xmm2
	vpxor	%xmm3, %xmm3, %xmm3
.LBB12_45:                              # %vector.body47
                                        #   in Loop: Header=BB12_39 Depth=2
	vmovups	%ymm0, -96(%rbx,%r8,4)
	vmovups	%ymm1, -64(%rbx,%r8,4)
	vmovdqu	%ymm2, -32(%rbx,%r8,4)
	vmovdqu	%ymm3, (%rbx,%r8,4)
	addq	$64, %r8
	addq	$2, %r12
	je	.LBB12_46
.LBB12_39:                              # %vector.body47
                                        #   Parent Loop BB12_23 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vxorps	%xmm0, %xmm0, %xmm0
	testb	%dl, %dl
	jne	.LBB12_40
# %bb.41:                               # %vector.body47
                                        #   in Loop: Header=BB12_39 Depth=2
	vxorps	%xmm1, %xmm1, %xmm1
	vpxor	%xmm2, %xmm2, %xmm2
	vpxor	%xmm3, %xmm3, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	jmp	.LBB12_42
	.p2align	4, 0x90
.LBB12_40:                              #   in Loop: Header=BB12_39 Depth=2
	vmovups	(%rsi,%r8,4), %ymm1
	vmovdqu	32(%rsi,%r8,4), %ymm2
	vmovdqu	64(%rsi,%r8,4), %ymm3
	vmovups	96(%rsi,%r8,4), %ymm4
.LBB12_42:                              # %vector.body47
                                        #   in Loop: Header=BB12_39 Depth=2
	vmovups	%ymm1, -224(%rbx,%r8,4)
	vmovdqu	%ymm2, -192(%rbx,%r8,4)
	vmovdqu	%ymm3, -160(%rbx,%r8,4)
	vmovups	%ymm4, -128(%rbx,%r8,4)
	je	.LBB12_44
# %bb.43:                               #   in Loop: Header=BB12_39 Depth=2
	vmovups	128(%rsi,%r8,4), %ymm0
	vmovups	160(%rsi,%r8,4), %ymm1
	vmovdqu	192(%rsi,%r8,4), %ymm2
	vmovdqu	224(%rsi,%r8,4), %ymm3
	jmp	.LBB12_45
.LBB12_37:                              #   in Loop: Header=BB12_23 Depth=1
	xorl	%r8d, %r8d
	movq	32(%rsp), %rsi                  # 8-byte Reload
.LBB12_46:                              # %middle.block45.unr-lcssa
                                        #   in Loop: Header=BB12_23 Depth=1
	testb	$1, 192(%rsp)                   # 1-byte Folded Reload
	movq	176(%rsp), %r12                 # 8-byte Reload
	je	.LBB12_51
# %bb.47:                               # %vector.body47.epil
                                        #   in Loop: Header=BB12_23 Depth=1
	movq	136(%rsp), %rax                 # 8-byte Reload
	addl	%ecx, %eax
	imull	112(%rsp), %eax                 # 4-byte Folded Reload
	addl	80(%rsp), %eax                  # 4-byte Folded Reload
	movslq	%eax, %rbx
	testb	%dl, %dl
	jne	.LBB12_48
# %bb.49:                               # %vector.body47.epil
                                        #   in Loop: Header=BB12_23 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	vpxor	%xmm2, %xmm2, %xmm2
	vpxor	%xmm3, %xmm3, %xmm3
	jmp	.LBB12_50
.LBB12_48:                              #   in Loop: Header=BB12_23 Depth=1
	movq	32(%rsp), %rax                  # 8-byte Reload
	vmovups	(%rax,%r8,4), %ymm0
	vmovups	32(%rax,%r8,4), %ymm1
	vmovdqu	64(%rax,%r8,4), %ymm2
	vmovdqu	96(%rax,%r8,4), %ymm3
.LBB12_50:                              # %vector.body47.epil
                                        #   in Loop: Header=BB12_23 Depth=1
	addq	%rbx, %r8
	movq	104(%rsp), %rax                 # 8-byte Reload
	vmovups	%ymm0, (%rax,%r8,4)
	vmovups	%ymm1, 32(%rax,%r8,4)
	vmovdqu	%ymm2, 64(%rax,%r8,4)
	vmovdqu	%ymm3, 96(%rax,%r8,4)
	movq	32(%rsp), %rsi                  # 8-byte Reload
.LBB12_51:                              # %middle.block45
                                        #   in Loop: Header=BB12_23 Depth=1
	movq	%r15, %rbx
	cmpq	%rbp, %r15
	je	.LBB12_52
.LBB12_33:                              # %"for f6_0_d_def__.s0.n.ni.preheader59"
                                        #   in Loop: Header=BB12_23 Depth=1
	movq	104(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r14,4), %rax
	movq	32(%rsp), %rsi                  # 8-byte Reload
	jmp	.LBB12_34
	.p2align	4, 0x90
.LBB12_54:                              # %"for f6_0_d_def__.s0.n.ni"
                                        #   in Loop: Header=BB12_34 Depth=2
	vxorps	%xmm0, %xmm0, %xmm0
.LBB12_55:                              # %"for f6_0_d_def__.s0.n.ni"
                                        #   in Loop: Header=BB12_34 Depth=2
	vmovss	%xmm0, (%rax,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rbp
	je	.LBB12_52
.LBB12_34:                              # %"for f6_0_d_def__.s0.n.ni"
                                        #   Parent Loop BB12_23 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	testb	%dl, %dl
	je	.LBB12_54
# %bb.35:                               #   in Loop: Header=BB12_34 Depth=2
	vmovss	(%rsi,%rbx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	jmp	.LBB12_55
.LBB12_53:                              # %call_destructor.exit20
	movq	224(%rsp), %rdi                 # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	xorl	%eax, %eax
	addq	$360, %rsp                      # imm = 0x168
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	retq
.LBB12_56:                              # %"assert failed"
	.cfi_def_cfa_offset 416
	leaq	.Lstr.130(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	addq	$360, %rsp                      # imm = 0x168
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	halide_error_buffer_allocation_too_large@PLT # TAILCALL
.LBB12_57:                              # %"assert failed1"
	.cfi_def_cfa_offset 416
	movq	%rbx, %rdi
	addq	$360, %rsp                      # imm = 0x168
	.cfi_def_cfa_offset 56
	popq	%rbx
	.cfi_def_cfa_offset 48
	popq	%r12
	.cfi_def_cfa_offset 40
	popq	%r13
	.cfi_def_cfa_offset 32
	popq	%r14
	.cfi_def_cfa_offset 24
	popq	%r15
	.cfi_def_cfa_offset 16
	popq	%rbp
	.cfi_def_cfa_offset 8
	jmp	halide_error_out_of_memory@PLT  # TAILCALL
.Lfunc_end12:
	.size	train_cost_model.par_for.f6_0_d_def__.s0.n.n.n, .Lfunc_end12-train_cost_model.par_for.f6_0_d_def__.s0.n.n.n
	.cfi_endproc
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n
.LCPI13_0:
	.zero	32
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$152, %rsp
	movq	%rdx, %rcx
	movl	%esi, %edi
	movl	4(%rdx), %r9d
	movl	12(%rdx), %r11d
	movl	%esi, %r10d
	sarl	$31, %r10d
	xorl	%esi, %esi
	testl	%r11d, %r11d
	sete	%sil
	movl	%esi, %ebp
	negl	%ebp
	movl	%r11d, %ebx
	sarl	$31, %ebx
	subl	%r10d, %edi
	orl	%r11d, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r8d
	movl	%ebx, %r15d
	leal	(%r11,%rsi), %r14d
	movl	%edi, %eax
	cltd
	idivl	%r14d
	notl	%r15d
	decl	%esi
	movl	%r15d, %edi
	subl	%ebx, %edi
	andl	%r10d, %edi
	addl	%eax, %edi
	andl	%esi, %edi
	leal	(%rdi,%rdi), %r12d
	subl	%r12d, %r9d
	jle	.LBB13_21
# %bb.1:                                # %"for relu1_0_d_def__.s0.w.wi.preheader"
	movl	(%rcx), %edx
	movslq	8(%rcx), %rbp
	movl	16(%rcx), %r14d
	movq	24(%rcx), %rcx
	movq	%rcx, 24(%rsp)                  # 8-byte Spill
	xorl	%r11d, %ebx
	addl	%r15d, %ebx
	andl	%r10d, %ebx
	addl	%r8d, %ebx
	andl	%esi, %ebx
	leal	(,%rbx,8), %r8d
	subl	%r14d, %r12d
	movl	%edx, %esi
	subl	%r8d, %esi
	leal	8(,%rbx,8), %eax
	xorl	%ecx, %ecx
	cmpl	$1, %r9d
	sete	%cl
	movl	$2, %ebx
	subq	%rcx, %rbx
	movq	%rbx, 80(%rsp)                  # 8-byte Spill
	cmpl	$9, %esi
	movl	$8, %ecx
	cmovll	%esi, %ecx
	movl	%ebp, %ebx
	shll	$5, %ebx
	movl	%ebx, 44(%rsp)                  # 4-byte Spill
	imull	%ebp, %r12d
	shll	$5, %r12d
	decl	%ecx
	movl	%esi, 16(%rsp)                  # 4-byte Spill
	testl	%esi, %esi
	movl	$-1, %esi
	cmovgl	%ecx, %esi
	leaq	4(,%rsi,4), %rcx
	movq	%rcx, 32(%rsp)                  # 8-byte Spill
	leaq	(,%rbp,4), %rcx
	leaq	(%rcx,%rcx,2), %rcx
	movq	%rcx, 72(%rsp)                  # 8-byte Spill
	addl	%r8d, %r12d
	movq	%rbp, %rcx
	shlq	$4, %rcx
	movq	%rcx, 144(%rsp)                 # 8-byte Spill
	shll	$6, %edi
	shll	$5, %r14d
	subl	%r14d, %edi
	leal	3(%rdi), %r14d
	imull	%ebp, %r14d
	addl	%r8d, %r14d
	leal	(,%rbp,4), %ecx
	movl	%ecx, 60(%rsp)                  # 4-byte Spill
	leaq	(,%rbp,8), %rcx
	movq	%rcx, 64(%rsp)                  # 8-byte Spill
	leal	2(%rdi), %esi
	imull	%ebp, %esi
	addl	%r8d, %esi
	orl	$1, %edi
	movq	%rbp, 88(%rsp)                  # 8-byte Spill
	imull	%ebp, %edi
	addl	%r8d, %edi
	xorl	%ebp, %ebp
	movl	%edx, 20(%rsp)                  # 4-byte Spill
	movl	%eax, 12(%rsp)                  # 4-byte Spill
	jmp	.LBB13_2
	.p2align	4, 0x90
.LBB13_20:                              # %"end for relu1_0_d_def__.s0.c"
                                        #   in Loop: Header=BB13_2 Depth=1
	movq	96(%rsp), %rbp                  # 8-byte Reload
	incq	%rbp
	movl	56(%rsp), %r12d                 # 4-byte Reload
	movl	44(%rsp), %ecx                  # 4-byte Reload
	addl	%ecx, %r12d
	movl	52(%rsp), %r14d                 # 4-byte Reload
	addl	%ecx, %r14d
	movl	48(%rsp), %esi                  # 4-byte Reload
	addl	%ecx, %esi
	movq	104(%rsp), %rdi                 # 8-byte Reload
	addl	%ecx, %edi
	cmpq	80(%rsp), %rbp                  # 8-byte Folded Reload
	je	.LBB13_21
.LBB13_2:                               # %"for relu1_0_d_def__.s0.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB13_3 Depth 2
	movq	%rbp, 96(%rsp)                  # 8-byte Spill
	movslq	%r12d, %rbp
	leaq	(,%rbp,4), %rcx
	movq	%rcx, 128(%rsp)                 # 8-byte Spill
	movq	72(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rbp,4), %rcx
	movq	%rcx, 120(%rsp)                 # 8-byte Spill
	movq	64(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rbp,4), %rcx
	movq	%rcx, 112(%rsp)                 # 8-byte Spill
	addq	88(%rsp), %rbp                  # 8-byte Folded Reload
	movq	%rbp, 136(%rsp)                 # 8-byte Spill
	movl	$32, %r13d
	movl	%r12d, 56(%rsp)                 # 4-byte Spill
	movq	%rdi, 104(%rsp)                 # 8-byte Spill
	movl	%edi, %ebx
	movl	%esi, 48(%rsp)                  # 4-byte Spill
	movl	%esi, %ebp
	movl	%r14d, 52(%rsp)                 # 4-byte Spill
	movq	24(%rsp), %r15                  # 8-byte Reload
	jmp	.LBB13_3
	.p2align	4, 0x90
.LBB13_16:                              # %true_bb.3
                                        #   in Loop: Header=BB13_3 Depth=2
	vxorps	%xmm0, %xmm0, %xmm0
	movq	120(%rsp), %rcx                 # 8-byte Reload
	vmovups	%ymm0, (%r15,%rcx)
.LBB13_19:                              # %after_bb.3
                                        #   in Loop: Header=BB13_3 Depth=2
	addq	144(%rsp), %r15                 # 8-byte Folded Reload
	movl	60(%rsp), %ecx                  # 4-byte Reload
	addl	%ecx, %r14d
	addl	%ecx, %ebp
	addl	%ecx, %ebx
	addl	%ecx, %r12d
	addq	$-4, %r13
	je	.LBB13_20
.LBB13_3:                               # %"for relu1_0_d_def__.s0.c"
                                        #   Parent Loop BB13_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpl	%edx, %eax
	jle	.LBB13_4
# %bb.5:                                # %false_bb
                                        #   in Loop: Header=BB13_3 Depth=2
	cmpl	$0, 16(%rsp)                    # 4-byte Folded Reload
	jle	.LBB13_7
# %bb.6:                                # %"for relu1_0_d_def__.s0.n.ni.preheader"
                                        #   in Loop: Header=BB13_3 Depth=2
	movslq	%r12d, %rax
	movq	24(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vzeroupper
	callq	memset@PLT
	movl	12(%rsp), %eax                  # 4-byte Reload
	movl	20(%rsp), %edx                  # 4-byte Reload
.LBB13_7:                               # %after_bb
                                        #   in Loop: Header=BB13_3 Depth=2
	cmpl	%edx, %eax
	jle	.LBB13_8
.LBB13_9:                               # %false_bb.1
                                        #   in Loop: Header=BB13_3 Depth=2
	cmpl	$0, 16(%rsp)                    # 4-byte Folded Reload
	jle	.LBB13_11
# %bb.10:                               # %"for relu1_0_d_def__.s0.n.ni.preheader.1"
                                        #   in Loop: Header=BB13_3 Depth=2
	movslq	%ebx, %rax
	movq	24(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vzeroupper
	callq	memset@PLT
	movl	12(%rsp), %eax                  # 4-byte Reload
	movl	20(%rsp), %edx                  # 4-byte Reload
.LBB13_11:                              # %after_bb.1
                                        #   in Loop: Header=BB13_3 Depth=2
	cmpl	%edx, %eax
	jle	.LBB13_12
.LBB13_13:                              # %false_bb.2
                                        #   in Loop: Header=BB13_3 Depth=2
	cmpl	$0, 16(%rsp)                    # 4-byte Folded Reload
	jle	.LBB13_15
# %bb.14:                               # %"for relu1_0_d_def__.s0.n.ni.preheader.2"
                                        #   in Loop: Header=BB13_3 Depth=2
	movslq	%ebp, %rax
	movq	24(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vzeroupper
	callq	memset@PLT
	movl	12(%rsp), %eax                  # 4-byte Reload
	movl	20(%rsp), %edx                  # 4-byte Reload
.LBB13_15:                              # %after_bb.2
                                        #   in Loop: Header=BB13_3 Depth=2
	cmpl	%edx, %eax
	jle	.LBB13_16
	jmp	.LBB13_17
	.p2align	4, 0x90
.LBB13_4:                               # %true_bb
                                        #   in Loop: Header=BB13_3 Depth=2
	vxorps	%xmm0, %xmm0, %xmm0
	movq	128(%rsp), %rcx                 # 8-byte Reload
	vmovups	%ymm0, (%r15,%rcx)
	cmpl	%edx, %eax
	jg	.LBB13_9
.LBB13_8:                               # %true_bb.1
                                        #   in Loop: Header=BB13_3 Depth=2
	vxorps	%xmm0, %xmm0, %xmm0
	movq	136(%rsp), %rcx                 # 8-byte Reload
	vmovups	%ymm0, (%r15,%rcx,4)
	cmpl	%edx, %eax
	jg	.LBB13_13
.LBB13_12:                              # %true_bb.2
                                        #   in Loop: Header=BB13_3 Depth=2
	vxorps	%xmm0, %xmm0, %xmm0
	movq	112(%rsp), %rcx                 # 8-byte Reload
	vmovups	%ymm0, (%r15,%rcx)
	cmpl	%edx, %eax
	jle	.LBB13_16
.LBB13_17:                              # %false_bb.3
                                        #   in Loop: Header=BB13_3 Depth=2
	cmpl	$0, 16(%rsp)                    # 4-byte Folded Reload
	jle	.LBB13_19
# %bb.18:                               # %"for relu1_0_d_def__.s0.n.ni.preheader.3"
                                        #   in Loop: Header=BB13_3 Depth=2
	movslq	%r14d, %rax
	movq	24(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdi
	xorl	%esi, %esi
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vzeroupper
	callq	memset@PLT
	movl	12(%rsp), %eax                  # 4-byte Reload
	movl	20(%rsp), %edx                  # 4-byte Reload
	jmp	.LBB13_19
.LBB13_21:                              # %destructor_block
	xorl	%eax, %eax
	addq	$152, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end13:
	.size	train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n, .Lfunc_end13-train_cost_model.par_for.relu1_0_d_def__.s0.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n
.LCPI14_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI14_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$112, %rsp
	movl	%esi, %r8d
	movl	(%rdx), %eax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movl	8(%rdx), %r14d
	movslq	4(%rdx), %r11
	movl	12(%rdx), %r10d
	movl	16(%rdx), %r15d
	movslq	20(%rdx), %rax
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	movl	24(%rdx), %esi
	movl	28(%rdx), %r9d
	movl	32(%rdx), %eax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	40(%rdx), %rax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	movq	56(%rdx), %rax
	movq	%rax, -128(%rsp)                # 8-byte Spill
	movq	72(%rdx), %r12
	movl	%r8d, %r13d
	sarl	$31, %r13d
	xorl	%ecx, %ecx
	testl	%esi, %esi
	sete	%cl
	movl	%ecx, %edi
	negl	%edi
	subl	%r13d, %r8d
	orl	%esi, %edi
	movl	%r8d, %eax
	cltd
	idivl	%edi
	movl	%esi, %eax
	sarl	$31, %eax
	movl	%eax, %ebp
	xorl	%esi, %ebp
	movl	%eax, %edi
	notl	%edi
	addl	%edi, %ebp
	andl	%r13d, %ebp
	addl	%edx, %ebp
	subl	%eax, %edi
	andl	%r13d, %edi
	addl	%ecx, %esi
	decl	%ecx
	andl	%ecx, %ebp
	movl	%r8d, %eax
	cltd
	idivl	%esi
	movq	%r11, %rsi
	addl	%eax, %edi
	andl	%ecx, %edi
	movl	%edi, %eax
	sarl	$31, %eax
	andl	%edi, %eax
	cmpl	$-2, %eax
	movl	$-1, %ecx
	cmovgl	%eax, %ecx
	addl	%ecx, %ecx
	movl	%ecx, 36(%rsp)                  # 4-byte Spill
	movl	%ecx, %r11d
	negl	%r11d
	leal	(%rdi,%rdi), %eax
	subl	%eax, %r10d
	cmpl	$3, %r10d
	movl	$2, %edx
	cmovll	%r10d, %edx
	cmpl	%r11d, %edx
	movl	%r11d, %ecx
	movl	%edx, 28(%rsp)                  # 4-byte Spill
	cmovgel	%edx, %ecx
	movl	%ecx, 32(%rsp)                  # 4-byte Spill
	cmpl	%r11d, %r10d
	cmovgl	%r11d, %r10d
	movl	%eax, %ecx
	subl	%r14d, %ecx
	movq	%rax, -80(%rsp)                 # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	subl	%r9d, %eax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movq	%r15, 56(%rsp)                  # 8-byte Spill
	leal	(%r15,%r15,8), %eax
	leal	(%rax,%rax,2), %eax
	leal	(%rax,%rbp,8), %eax
	movl	%eax, -104(%rsp)                # 4-byte Spill
	leal	(,%rbp,8), %edx
	vmovd	%edx, %xmm0
	movq	%rbp, 48(%rsp)                  # 8-byte Spill
	leal	8(,%rbp,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	movq	%rcx, 40(%rsp)                  # 8-byte Spill
	movslq	%ecx, %rax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	movl	%edx, -108(%rsp)                # 4-byte Spill
	movslq	%edx, %rax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	testl	%r10d, %r10d
	movq	%rsi, -88(%rsp)                 # 8-byte Spill
	movq	%r12, (%rsp)                    # 8-byte Spill
	jle	.LBB14_4
# %bb.1:                                # %"for relu1_0_d_def__.s6.w.wi.preheader"
	movq	56(%rsp), %r14                  # 8-byte Reload
	movl	%r14d, %ebx
	shll	$5, %ebx
	vpbroadcastd	%xmm0, %ymm1
	vpor	.LCPI14_0(%rip), %ymm1, %ymm1
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	leal	-1(%rcx), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	movl	%ecx, %eax
	subl	-108(%rsp), %eax                # 4-byte Folded Reload
	cmpl	$9, %eax
	movl	$8, %ecx
	cmovll	%eax, %ecx
	xorl	%ebp, %ebp
	movl	%eax, -8(%rsp)                  # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%ebp, %ecx
	vpminsd	%ymm1, %ymm2, %ymm1
	movl	%r10d, %eax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movl	%ecx, %eax
	andl	$2147483640, %eax               # imm = 0x7FFFFFF8
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	addq	$-8, %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	%rax, %rdx
	shrq	$3, %rdx
	incq	%rdx
	movl	%edx, %r15d
	andl	$3, %r15d
	movq	-120(%rsp), %rax                # 8-byte Reload
	addq	$96, %rax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movl	%esi, %eax
	imull	40(%rsp), %eax                  # 4-byte Folded Reload
	movq	48(%rsp), %r10                  # 8-byte Reload
	leal	(%rax,%r10,8), %r8d
	leaq	96(%r12), %rax
	movq	%rax, 88(%rsp)                  # 8-byte Spill
	movq	-72(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%r10,8), %eax
	movl	%eax, -16(%rsp)                 # 4-byte Spill
	shll	$6, %edi
	shll	$5, %r9d
	subl	%r9d, %edi
	orl	$27, %edi
	imull	%r14d, %edi
	leal	(%rdi,%r10,8), %r9d
	andq	$-4, %rdx
	negq	%rdx
	movq	%rdx, 104(%rsp)                 # 8-byte Spill
	movq	%r15, -40(%rsp)                 # 8-byte Spill
	negq	%r15
	movq	%r15, -48(%rsp)                 # 8-byte Spill
	vmovss	.LCPI14_1(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI14_1(%rip), %ymm3  # ymm3 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-128(%rsp), %rax                # 8-byte Reload
	leaq	96(%rax), %rax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	movq	-80(%rsp), %rax                 # 8-byte Reload
	movl	%eax, %r10d
	movl	%ebx, -96(%rsp)                 # 4-byte Spill
	jmp	.LBB14_2
	.p2align	4, 0x90
.LBB14_3:                               # %true_bb
                                        #   in Loop: Header=BB14_2 Depth=1
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%ebp, %eax
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %eax
	imull	-56(%rsp), %eax                 # 4-byte Folded Reload
	addl	-72(%rsp), %eax                 # 4-byte Folded Reload
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm1, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vmovss	(%r12,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %eax
	cltq
	vinsertps	$16, (%r12,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%r12,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%r12,%rax,4), %xmm6, %xmm5 # xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %eax
	cltq
	vmovss	(%r12,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %eax
	cltq
	vinsertps	$16, (%r12,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%r12,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%r12,%rax,4), %xmm6, %xmm4 # xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm4, %ymm4
	movq	72(%rsp), %rax                  # 8-byte Reload
	addq	%rbp, %rax
	imulq	%rsi, %rax
	addq	64(%rsp), %rax                  # 8-byte Folded Reload
	movq	-120(%rsp), %rdx                # 8-byte Reload
	vmulps	(%rdx,%rax,4), %ymm4, %ymm4
	movq	16(%rsp), %rax                  # 8-byte Reload
	addl	%ebp, %eax
	imull	%ebx, %eax
	addl	-104(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-128(%rsp), %rdx                # 8-byte Reload
	vfmadd213ps	(%rdx,%rax,4), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, (%rdx,%rax,4)
.LBB14_20:                              # %after_bb
                                        #   in Loop: Header=BB14_2 Depth=1
	incq	%rbp
	addl	%esi, %r8d
	incl	%r10d
	addl	%ebx, %r9d
	cmpq	8(%rsp), %rbp                   # 8-byte Folded Reload
	je	.LBB14_4
.LBB14_2:                               # %"for relu1_0_d_def__.s6.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB14_15 Depth 2
                                        #     Child Loop BB14_18 Depth 2
                                        #     Child Loop BB14_11 Depth 2
	movq	-64(%rsp), %rax                 # 8-byte Reload
	cmpl	%eax, -100(%rsp)                # 4-byte Folded Reload
	jle	.LBB14_3
# %bb.8:                                # %false_bb
                                        #   in Loop: Header=BB14_2 Depth=1
	cmpl	$0, -8(%rsp)                    # 4-byte Folded Reload
	jle	.LBB14_20
# %bb.9:                                # %"for relu1_0_d_def__.s6.n.ni.preheader"
                                        #   in Loop: Header=BB14_2 Depth=1
	movq	%rsi, %r14
	movl	%r10d, %eax
	sarl	$31, %eax
	andnl	%r10d, %eax, %edx
	imull	-56(%rsp), %edx                 # 4-byte Folded Reload
	addl	-16(%rsp), %edx                 # 4-byte Folded Reload
	movslq	%r8d, %rax
	movq	-120(%rsp), %rdi                # 8-byte Reload
	leaq	(%rdi,%rax,4), %rdi
	movslq	%edx, %rbx
	leaq	(%r12,%rbx,4), %r13
	movslq	%r9d, %rdx
	movq	-128(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx,4), %r15
	cmpl	$7, %ecx
	ja	.LBB14_12
# %bb.10:                               #   in Loop: Header=BB14_2 Depth=1
	xorl	%eax, %eax
	movq	%r14, %rsi
	movl	-96(%rsp), %ebx                 # 4-byte Reload
	jmp	.LBB14_11
	.p2align	4, 0x90
.LBB14_12:                              # %vector.ph
                                        #   in Loop: Header=BB14_2 Depth=1
	cmpq	$24, -32(%rsp)                  # 8-byte Folded Reload
	jae	.LBB14_14
# %bb.13:                               #   in Loop: Header=BB14_2 Depth=1
	xorl	%r12d, %r12d
	jmp	.LBB14_16
.LBB14_14:                              # %vector.body.preheader
                                        #   in Loop: Header=BB14_2 Depth=1
	movq	96(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rax,4), %rsi
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rbx,4), %r14
	movq	80(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdx,4), %rbx
	movq	104(%rsp), %rax                 # 8-byte Reload
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB14_15:                              # %vector.body
                                        #   Parent Loop BB14_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r14,%r12,4), %ymm4
	vmulps	-96(%rsi,%r12,4), %ymm4, %ymm4
	vfmadd213ps	-96(%rbx,%r12,4), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, -96(%rbx,%r12,4)
	vmovups	-64(%r14,%r12,4), %ymm4
	vmulps	-64(%rsi,%r12,4), %ymm4, %ymm4
	vfmadd213ps	-64(%rbx,%r12,4), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, -64(%rbx,%r12,4)
	vmovups	-32(%r14,%r12,4), %ymm4
	vmulps	-32(%rsi,%r12,4), %ymm4, %ymm4
	vfmadd213ps	-32(%rbx,%r12,4), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, -32(%rbx,%r12,4)
	vmovups	(%r14,%r12,4), %ymm4
	vmulps	(%rsi,%r12,4), %ymm4, %ymm4
	vfmadd213ps	(%rbx,%r12,4), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, (%rbx,%r12,4)
	addq	$32, %r12
	addq	$4, %rax
	jne	.LBB14_15
.LBB14_16:                              # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB14_2 Depth=1
	cmpq	$0, -40(%rsp)                   # 8-byte Folded Reload
	je	.LBB14_19
# %bb.17:                               # %vector.body.epil.preheader
                                        #   in Loop: Header=BB14_2 Depth=1
	shlq	$2, %r12
	movq	-48(%rsp), %rax                 # 8-byte Reload
	.p2align	4, 0x90
.LBB14_18:                              # %vector.body.epil
                                        #   Parent Loop BB14_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%r13,%r12), %ymm4
	vmulps	(%rdi,%r12), %ymm4, %ymm4
	vfmadd213ps	(%r15,%r12), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, (%r15,%r12)
	addq	$32, %r12
	incq	%rax
	jne	.LBB14_18
.LBB14_19:                              # %middle.block
                                        #   in Loop: Header=BB14_2 Depth=1
	movq	-24(%rsp), %rdx                 # 8-byte Reload
	movq	%rdx, %rax
	cmpq	%rcx, %rdx
	movq	-88(%rsp), %rsi                 # 8-byte Reload
	movq	(%rsp), %r12                    # 8-byte Reload
	movl	-96(%rsp), %ebx                 # 4-byte Reload
	je	.LBB14_20
	.p2align	4, 0x90
.LBB14_11:                              # %"for relu1_0_d_def__.s6.n.ni"
                                        #   Parent Loop BB14_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%r13,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmulss	(%rdi,%rax,4), %xmm4, %xmm4
	vfmadd213ss	(%r15,%rax,4), %xmm2, %xmm4 # xmm4 = (xmm2 * xmm4) + mem
	vmovss	%xmm4, (%r15,%rax,4)
	incq	%rax
	cmpq	%rax, %rcx
	jne	.LBB14_11
	jmp	.LBB14_20
.LBB14_4:                               # %"end for relu1_0_d_def__.s6.w.wi"
	cmpl	%r11d, 28(%rsp)                 # 4-byte Folded Reload
	movq	-64(%rsp), %rbx                 # 8-byte Reload
	jle	.LBB14_34
# %bb.5:                                # %"for relu1_0_d_def__.s6.w.wi1.preheader"
	movq	56(%rsp), %r8                   # 8-byte Reload
	movl	%r8d, %r13d
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI14_0(%rip), %ymm0, %ymm0
	shll	$5, %r13d
	leal	-1(%rbx), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movl	%ebx, %ecx
	movl	-108(%rsp), %edx                # 4-byte Reload
	subl	%edx, %ecx
	cmpl	$9, %ecx
	movl	$8, %r14d
	cmovll	%ecx, %r14d
	xorl	%eax, %eax
	movl	%ecx, 8(%rsp)                   # 4-byte Spill
	testl	%ecx, %ecx
	cmovlel	%eax, %r14d
	addl	-72(%rsp), %edx                 # 4-byte Folded Reload
	movl	32(%rsp), %r15d                 # 4-byte Reload
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	%edx, %rax
	movl	%r14d, %edx
	andl	$2147483640, %edx               # imm = 0x7FFFFFF8
	movq	%rdx, -8(%rsp)                  # 8-byte Spill
	leaq	-8(%rdx), %rbp
	movq	%rbp, -16(%rsp)                 # 8-byte Spill
	shrq	$3, %rbp
	incq	%rbp
	movl	%ebp, %edi
	andl	$3, %edi
	movq	-120(%rsp), %rdx                # 8-byte Reload
	addq	$96, %rdx
	movq	%rdx, -40(%rsp)                 # 8-byte Spill
	movl	36(%rsp), %r9d                  # 4-byte Reload
	movq	40(%rsp), %rdx                  # 8-byte Reload
	subl	%r9d, %edx
	imull	%esi, %edx
	movq	48(%rsp), %r10                  # 8-byte Reload
	leal	(%rdx,%r10,8), %r12d
	addq	%r11, %rcx
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	imulq	%rdx, %rcx
	addq	%rax, %rcx
	movq	16(%rsp), %rax                  # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	subl	%r9d, %eax
	movq	(%rsp), %rsi                    # 8-byte Reload
	leaq	(%rsi,%rcx,4), %r9
	addq	$96, %r9
	shll	$5, %eax
	orl	$27, %eax
	imull	%r8d, %eax
	leaq	(,%rdx,4), %r8
	leal	(%rax,%r10,8), %r10d
	andq	$-4, %rbp
	negq	%rbp
	movq	%rbp, -32(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%rcx,4), %rdx
	movq	-88(%rsp), %rsi                 # 8-byte Reload
	movq	%rdi, -96(%rsp)                 # 8-byte Spill
	negq	%rdi
	movq	%rdi, -24(%rsp)                 # 8-byte Spill
	vmovss	.LCPI14_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI14_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-128(%rsp), %rax                # 8-byte Reload
	addq	$96, %rax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	jmp	.LBB14_6
	.p2align	4, 0x90
.LBB14_7:                               # %true_bb4
                                        #   in Loop: Header=BB14_6 Depth=1
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%r11d, %eax
	imull	-56(%rsp), %eax                 # 4-byte Folded Reload
	addl	-72(%rsp), %eax                 # 4-byte Folded Reload
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm3
	vextracti128	$1, %ymm3, %xmm4
	vmovd	%xmm4, %eax
	cltq
	movq	(%rsp), %rcx                    # 8-byte Reload
	vmovss	(%rcx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %eax
	cltq
	vinsertps	$16, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	vmovd	%xmm3, %eax
	cltq
	vmovss	(%rcx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %eax
	cltq
	vinsertps	$16, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm3, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm3, %eax
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	movq	72(%rsp), %rax                  # 8-byte Reload
	addq	%r11, %rax
	imulq	%rsi, %rax
	addq	64(%rsp), %rax                  # 8-byte Folded Reload
	movq	-120(%rsp), %rcx                # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	16(%rsp), %rax                  # 8-byte Reload
	addl	%r11d, %eax
	imull	%r13d, %eax
	addl	-104(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-128(%rsp), %rcx                # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rcx,%rax,4)
.LBB14_33:                              # %after_bb6
                                        #   in Loop: Header=BB14_6 Depth=1
	incq	%r11
	addl	%esi, %r12d
	addq	%r8, %r9
	addl	%r13d, %r10d
	addq	%r8, %rdx
	cmpq	%r15, %r11
	je	.LBB14_34
.LBB14_6:                               # %"for relu1_0_d_def__.s6.w.wi1"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB14_28 Depth 2
                                        #     Child Loop BB14_31 Depth 2
                                        #     Child Loop BB14_24 Depth 2
	cmpl	%ebx, -100(%rsp)                # 4-byte Folded Reload
	jle	.LBB14_7
# %bb.21:                               # %false_bb5
                                        #   in Loop: Header=BB14_6 Depth=1
	cmpl	$0, 8(%rsp)                     # 4-byte Folded Reload
	jle	.LBB14_33
# %bb.22:                               # %"for relu1_0_d_def__.s6.n.ni7.preheader"
                                        #   in Loop: Header=BB14_6 Depth=1
	movslq	%r12d, %rax
	movq	-120(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdi
	movslq	%r10d, %rsi
	movq	-128(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rsi,4), %rcx
	cmpl	$7, %r14d
	ja	.LBB14_25
# %bb.23:                               #   in Loop: Header=BB14_6 Depth=1
	xorl	%eax, %eax
	movq	-88(%rsp), %rsi                 # 8-byte Reload
	jmp	.LBB14_24
	.p2align	4, 0x90
.LBB14_25:                              # %vector.ph45
                                        #   in Loop: Header=BB14_6 Depth=1
	cmpq	$24, -16(%rsp)                  # 8-byte Folded Reload
	jae	.LBB14_27
# %bb.26:                               #   in Loop: Header=BB14_6 Depth=1
	xorl	%esi, %esi
	jmp	.LBB14_29
.LBB14_27:                              # %vector.body43.preheader
                                        #   in Loop: Header=BB14_6 Depth=1
	movq	-40(%rsp), %rbx                 # 8-byte Reload
	leaq	(%rbx,%rax,4), %rbx
	movq	-48(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rsi,4), %rax
	movq	-32(%rsp), %rbp                 # 8-byte Reload
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB14_28:                              # %vector.body43
                                        #   Parent Loop BB14_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r9,%rsi,4), %ymm3
	vmulps	-96(%rbx,%rsi,4), %ymm3, %ymm3
	vfmadd213ps	-96(%rax,%rsi,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -96(%rax,%rsi,4)
	vmovups	-64(%r9,%rsi,4), %ymm3
	vmulps	-64(%rbx,%rsi,4), %ymm3, %ymm3
	vfmadd213ps	-64(%rax,%rsi,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -64(%rax,%rsi,4)
	vmovups	-32(%r9,%rsi,4), %ymm3
	vmulps	-32(%rbx,%rsi,4), %ymm3, %ymm3
	vfmadd213ps	-32(%rax,%rsi,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -32(%rax,%rsi,4)
	vmovups	(%r9,%rsi,4), %ymm3
	vmulps	(%rbx,%rsi,4), %ymm3, %ymm3
	vfmadd213ps	(%rax,%rsi,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rax,%rsi,4)
	addq	$32, %rsi
	addq	$4, %rbp
	jne	.LBB14_28
.LBB14_29:                              # %middle.block41.unr-lcssa
                                        #   in Loop: Header=BB14_6 Depth=1
	cmpq	$0, -96(%rsp)                   # 8-byte Folded Reload
	je	.LBB14_32
# %bb.30:                               # %vector.body43.epil.preheader
                                        #   in Loop: Header=BB14_6 Depth=1
	shlq	$2, %rsi
	movq	-24(%rsp), %rax                 # 8-byte Reload
	.p2align	4, 0x90
.LBB14_31:                              # %vector.body43.epil
                                        #   Parent Loop BB14_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rdx,%rsi), %ymm3
	vmulps	(%rdi,%rsi), %ymm3, %ymm3
	vfmadd213ps	(%rcx,%rsi), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rcx,%rsi)
	addq	$32, %rsi
	incq	%rax
	jne	.LBB14_31
.LBB14_32:                              # %middle.block41
                                        #   in Loop: Header=BB14_6 Depth=1
	movq	-8(%rsp), %rsi                  # 8-byte Reload
	movq	%rsi, %rax
	cmpq	%r14, %rsi
	movq	-64(%rsp), %rbx                 # 8-byte Reload
	movq	-88(%rsp), %rsi                 # 8-byte Reload
	je	.LBB14_33
	.p2align	4, 0x90
.LBB14_24:                              # %"for relu1_0_d_def__.s6.n.ni7"
                                        #   Parent Loop BB14_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rdx,%rax,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rdi,%rax,4), %xmm3, %xmm3
	vfmadd213ss	(%rcx,%rax,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rcx,%rax,4)
	incq	%rax
	cmpq	%rax, %r14
	jne	.LBB14_24
	jmp	.LBB14_33
.LBB14_34:                              # %destructor_block
	xorl	%eax, %eax
	addq	$112, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end14:
	.size	train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n, .Lfunc_end14-train_cost_model.par_for.relu1_0_d_def__.s6.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n
.LCPI15_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI15_1:
	.long	0x3f800000                      # float 1
.LCPI15_2:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r8d
	movl	28(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %ecx
	negl	%ecx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %edi
	orl	%r10d, %ecx
	movl	%edi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r15d
	movl	%ebp, %ecx
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%ecx
	decl	%ebx
	movl	%ecx, %esi
	subl	%ebp, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	subl	%eax, %r8d
	movl	%r8d, -128(%rsp)                # 4-byte Spill
	jle	.LBB15_13
# %bb.1:                                # %"for relu1_0_d_def__.s7.w.wi.preheader"
	movl	(%r13), %r12d
	movl	16(%r13), %r8d
	movq	%r8, -120(%rsp)                 # 8-byte Spill
	movl	20(%r13), %edi
	movl	32(%r13), %edx
	movl	36(%r13), %r14d
	movl	40(%r13), %eax
	xorl	%r10d, %ebp
	addl	%ecx, %ebp
	andl	%r9d, %ebp
	movq	%r12, %r9
	addl	%ebp, %r15d
	andl	%ebx, %r15d
	leal	(,%r15,8), %ebp
	subl	%r14d, %edi
	movq	%rdi, -88(%rsp)                 # 8-byte Spill
	subl	%r14d, %eax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	-80(%rsp), %r11                 # 8-byte Reload
	movl	%r11d, %ecx
	subl	%edx, %ecx
	movq	%rcx, 24(%rsp)                  # 8-byte Spill
	movq	%r15, %r14
	movl	%r11d, %r15d
	subl	8(%r13), %r15d
	xorl	%ecx, %ecx
	cmpl	$1, -128(%rsp)                  # 4-byte Folded Reload
	sete	%cl
	movl	$2, %edi
	subq	%rcx, %rdi
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movl	%r8d, %ecx
	shll	$5, %ecx
	movl	%ecx, -128(%rsp)                # 4-byte Spill
	vmovd	%ebp, %xmm0
	movslq	%ebp, %r12
	movl	%r9d, %eax
	subl	%r12d, %eax
	cmpl	$9, %eax
	movl	$8, %r10d
	cmovll	%eax, %r10d
	xorl	%r8d, %r8d
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r8d, %r10d
	leal	-1(%r9), %eax
	vmovd	%eax, %xmm1
	shll	$6, %esi
	shll	$5, %edx
	subl	%edx, %esi
	movslq	4(%r13), %rdi
	movl	24(%r13), %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	movq	48(%r13), %rbx
	movq	64(%r13), %rbp
	movq	80(%r13), %rdx
	orl	$25, %esi
	movq	-120(%rsp), %rax                # 8-byte Reload
	imull	%eax, %esi
	leal	(%rax,%rax,4), %eax
	leal	(%rax,%rax,4), %eax
	leal	(%rax,%r14,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	movslq	%r15d, %rax
	movq	-88(%rsp), %r15                 # 8-byte Reload
	movq	%rax, %rcx
	movq	%rax, 8(%rsp)                   # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rdi, -120(%rsp)                # 8-byte Spill
	imull	%edi, %eax
	movq	%r14, %rcx
	leal	(%rax,%r14,8), %r14d
	leal	(%rsi,%rcx,8), %r13d
	leal	8(,%rcx,8), %eax
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI15_0(%rip), %ymm0, %ymm0
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm8
	movq	-72(%rsp), %rax                 # 8-byte Reload
	leal	(%r12,%rax), %eax
	cltq
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	movq	%r12, 16(%rsp)                  # 8-byte Spill
	leal	(%r12,%r15), %eax
	cltq
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movq	%r10, -112(%rsp)                # 8-byte Spill
                                        # kill: def $r10d killed $r10d killed $r10 def $r10
	andl	$2147483616, %r10d              # imm = 0x7FFFFFE0
	vmovss	.LCPI15_2(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI15_1(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI15_1(%rip), %ymm3  # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI15_2(%rip), %ymm4  # ymm4 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	%rbx, -40(%rsp)                 # 8-byte Spill
	leaq	96(%rbx), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	leaq	96(%rdx), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%rbp, -48(%rsp)                 # 8-byte Spill
	leaq	96(%rbp), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movl	%r11d, %r12d
	movq	%r9, -8(%rsp)                   # 8-byte Spill
	jmp	.LBB15_2
	.p2align	4, 0x90
.LBB15_3:                               # %true_bb
                                        #   in Loop: Header=BB15_2 Depth=1
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%r8d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %ecx
	imull	-124(%rsp), %ecx                # 4-byte Folded Reload
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addl	%ecx, %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm8, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %eax
	cltq
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm6, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm6, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm9 # xmm9 = xmm7[0,1,2],mem[0]
	vmovd	%xmm5, %eax
	cltq
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %eax
	cltq
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm10 # xmm10 = xmm7[0,1],mem[0],xmm7[3]
	addl	%r15d, %ecx
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm0
	vextracti128	$1, %ymm0, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm0, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm0, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm0, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm0 # xmm0 = xmm7[0,1,2],mem[0]
	vpextrd	$3, %xmm5, %eax
	vinsertf128	$1, %xmm6, %ymm0, %ymm0
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm10, %xmm5 # xmm5 = xmm10[0,1,2],mem[0]
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	%r8, %rax
	imulq	-120(%rsp), %rax                # 8-byte Folded Reload
	addq	16(%rsp), %rax                  # 8-byte Folded Reload
	movq	-40(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm0, %ymm0
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r8d, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	vinsertf128	$1, %xmm9, %ymm5, %ymm5
	vcmpltps	%ymm5, %ymm3, %ymm5
	vmulps	%ymm4, %ymm0, %ymm0
	vandps	%ymm0, %ymm5, %ymm0
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rax,4)
.LBB15_12:                              # %after_bb
                                        #   in Loop: Header=BB15_2 Depth=1
	incq	%r8
	addl	-120(%rsp), %r14d               # 4-byte Folded Reload
	incl	%r12d
	addl	-128(%rsp), %r13d               # 4-byte Folded Reload
	cmpq	32(%rsp), %r8                   # 8-byte Folded Reload
	je	.LBB15_13
.LBB15_2:                               # %"for relu1_0_d_def__.s7.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB15_10 Depth 2
                                        #     Child Loop BB15_8 Depth 2
	cmpl	%r9d, -92(%rsp)                 # 4-byte Folded Reload
	jle	.LBB15_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB15_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB15_12
# %bb.5:                                # %"for relu1_0_d_def__.s7.n.ni.preheader"
                                        #   in Loop: Header=BB15_2 Depth=1
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%r12d, %eax, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rdi
	movslq	%r14d, %rbx
	movslq	%r13d, %rbp
	cmpl	$32, -112(%rsp)                 # 4-byte Folded Reload
	jae	.LBB15_9
# %bb.6:                                #   in Loop: Header=BB15_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB15_7
	.p2align	4, 0x90
.LBB15_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB15_2 Depth=1
	movq	-64(%rsp), %rax                 # 8-byte Reload
	addq	%rdi, %rax
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	movq	%rdi, (%rsp)                    # 8-byte Spill
	leaq	(%rcx,%rdi), %rsi
	movq	-16(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rbx,4), %rcx
	movq	%rbp, %rdi
	movq	-24(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %r15
	leaq	(%rbp,%rsi,4), %r11
	movq	%rdi, %rbp
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %rax
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB15_10:                              # %vector.body
                                        #   Parent Loop BB15_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vcmpltps	-96(%r11,%r9,4), %ymm3, %ymm0
	vcmpltps	-64(%r11,%r9,4), %ymm3, %ymm5
	vcmpltps	-32(%r11,%r9,4), %ymm3, %ymm6
	vcmpltps	(%r11,%r9,4), %ymm3, %ymm7
	vmovups	-96(%r15,%r9,4), %ymm9
	vmovups	-64(%r15,%r9,4), %ymm10
	vmovups	-32(%r15,%r9,4), %ymm11
	vmovups	(%r15,%r9,4), %ymm12
	vmulps	-96(%rcx,%r9,4), %ymm9, %ymm9
	vmulps	-64(%rcx,%r9,4), %ymm10, %ymm10
	vmulps	-32(%rcx,%r9,4), %ymm11, %ymm11
	vmulps	(%rcx,%r9,4), %ymm12, %ymm12
	vmulps	%ymm4, %ymm9, %ymm9
	vandps	%ymm0, %ymm9, %ymm0
	vmulps	%ymm4, %ymm10, %ymm9
	vandps	%ymm5, %ymm9, %ymm5
	vmulps	%ymm4, %ymm11, %ymm9
	vandps	%ymm6, %ymm9, %ymm6
	vmulps	%ymm4, %ymm12, %ymm9
	vandps	%ymm7, %ymm9, %ymm7
	vaddps	-96(%rax,%r9,4), %ymm0, %ymm0
	vaddps	-64(%rax,%r9,4), %ymm5, %ymm5
	vaddps	-32(%rax,%r9,4), %ymm6, %ymm6
	vaddps	(%rax,%r9,4), %ymm7, %ymm7
	vmovups	%ymm0, -96(%rax,%r9,4)
	vmovups	%ymm5, -64(%rax,%r9,4)
	vmovups	%ymm6, -32(%rax,%r9,4)
	vmovups	%ymm7, (%rax,%r9,4)
	addq	$32, %r9
	cmpq	%r9, %r10
	jne	.LBB15_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB15_2 Depth=1
	movq	%r10, %rsi
	cmpq	-112(%rsp), %r10                # 8-byte Folded Reload
	movq	-8(%rsp), %r9                   # 8-byte Reload
	movq	-88(%rsp), %r15                 # 8-byte Reload
	movq	(%rsp), %rdi                    # 8-byte Reload
	je	.LBB15_12
.LBB15_7:                               # %"for relu1_0_d_def__.s7.n.ni.preheader24"
                                        #   in Loop: Header=BB15_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	addq	%rsi, %rbx
	movq	-40(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rbx,4), %rcx
	movq	-64(%rsp), %rbx                 # 8-byte Reload
	movq	%rbp, %r11
	leaq	(%rbx,%rsi), %rbp
	addq	%rdi, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	movq	-56(%rsp), %rbx                 # 8-byte Reload
	addq	%rsi, %rbx
	addq	%rdi, %rbx
	leaq	(%rdx,%rbx,4), %rdi
	addq	%rsi, %r11
	movq	-48(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r11,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB15_8:                               # %"for relu1_0_d_def__.s7.n.ni"
                                        #   Parent Loop BB15_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rbx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmulss	(%rcx,%rbx,4), %xmm0, %xmm0
	vcmpltss	(%rdi,%rbx,4), %xmm2, %xmm5
	vmulss	%xmm1, %xmm0, %xmm0
	vandps	%xmm0, %xmm5, %xmm0
	vaddss	(%rsi,%rbx,4), %xmm0, %xmm0
	vmovss	%xmm0, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rax
	jne	.LBB15_8
	jmp	.LBB15_12
.LBB15_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end15:
	.size	train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n, .Lfunc_end15-train_cost_model.par_for.relu1_0_d_def__.s7.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n
.LCPI16_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI16_1:
	.long	0xbf800000                      # float -1
.LCPI16_2:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r11d
	movl	28(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %ecx
	negl	%ecx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %edi
	orl	%r10d, %ecx
	movl	%edi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	movl	%ebp, %ecx
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%ecx
	decl	%ebx
	movl	%ecx, %esi
	subl	%ebp, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	subl	%eax, %r11d
	movl	%r11d, -128(%rsp)               # 4-byte Spill
	jle	.LBB16_13
# %bb.1:                                # %"for relu1_0_d_def__.s8.w.wi.preheader"
	movl	(%r13), %r11d
	movl	16(%r13), %r12d
	movl	20(%r13), %eax
	movl	32(%r13), %edx
	movl	36(%r13), %r14d
	movl	40(%r13), %edi
	xorl	%r10d, %ebp
	addl	%ecx, %ebp
	andl	%r9d, %ebp
	movq	%r11, %r9
	addl	%ebp, %r8d
	andl	%ebx, %r8d
	leal	(,%r8,8), %ecx
	subl	%r14d, %eax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	subl	%r14d, %edi
	movq	%rdi, -88(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rdi                # 8-byte Reload
	movl	%edi, %eax
	movl	%edi, %r14d
	subl	8(%r13), %r14d
	subl	%edx, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	leal	(%r12,%r12,4), %eax
	leal	(%rax,%rax,4), %r15d
	addl	%r12d, %r15d
	xorl	%eax, %eax
	cmpl	$1, -128(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	$2, %edi
	subq	%rax, %rdi
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movl	%r12d, %eax
	shll	$5, %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	vmovd	%ecx, %xmm0
	leal	-1(%r11), %eax
	vmovd	%eax, %xmm1
	movslq	%ecx, %r11
	movl	%r9d, %eax
	subl	%r11d, %eax
	cmpl	$9, %eax
	movl	$8, %edi
	cmovll	%eax, %edi
	xorl	%r10d, %r10d
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %edi
	shll	$6, %esi
	shll	$5, %edx
	subl	%edx, %esi
	orl	$26, %esi
	imull	%r12d, %esi
	movslq	4(%r13), %rbx
	movl	24(%r13), %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	movq	48(%r13), %rbp
	movq	64(%r13), %r12
	movq	80(%r13), %rdx
	leal	(%r15,%r8,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	movslq	%r14d, %rax
	movq	%rax, %rcx
	movq	%rax, 8(%rsp)                   # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rbx, -40(%rsp)                 # 8-byte Spill
	imull	%ebx, %eax
	leal	(%rax,%r8,8), %r14d
	leal	(%rsi,%r8,8), %r13d
	leal	8(,%r8,8), %eax
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI16_0(%rip), %ymm0, %ymm0
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm8
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%r11,%rax), %eax
	cltq
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movq	%r11, 16(%rsp)                  # 8-byte Spill
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leal	(%r11,%rax), %eax
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%rdi, -112(%rsp)                # 8-byte Spill
	movl	%edi, %r8d
	andl	$2147483616, %r8d               # imm = 0x7FFFFFE0
	vmovss	.LCPI16_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI16_2(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI16_1(%rip), %ymm3  # ymm3 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vbroadcastss	.LCPI16_2(%rip), %ymm4  # ymm4 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rdx), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	movq	%rbp, -48(%rsp)                 # 8-byte Spill
	leaq	96(%rbp), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%r12, -56(%rsp)                 # 8-byte Spill
	leaq	96(%r12), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r12d
	movq	%r9, -8(%rsp)                   # 8-byte Spill
	jmp	.LBB16_2
	.p2align	4, 0x90
.LBB16_3:                               # %true_bb
                                        #   in Loop: Header=BB16_2 Depth=1
	movq	-120(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edi
	imull	-124(%rsp), %edi                # 4-byte Folded Reload
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%edi, %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm8, %ymm6
	vextracti128	$1, %ymm6, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm6, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm9 # xmm9 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm6, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm6, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm6, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	addl	-88(%rsp), %edi                 # 4-byte Folded Reload
	vmovd	%edi, %xmm7
	vpbroadcastd	%xmm7, %ymm7
	vpaddd	%ymm7, %ymm8, %ymm7
	vextracti128	$1, %ymm7, %xmm0
	vmovd	%xmm0, %eax
	cltq
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm7, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm5, %xmm0 # xmm0 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm7, %ecx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm7, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm7, %eax
	vinsertf128	$1, %xmm9, %ymm6, %ymm6
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	%r10, %rax
	imulq	-40(%rsp), %rax                 # 8-byte Folded Reload
	addq	16(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm0, %ymm5, %ymm0
	vaddps	%ymm3, %ymm0, %ymm0
	vmulps	%ymm0, %ymm6, %ymm0
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm0, %ymm0
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r10d, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm4, %ymm0 # ymm0 = (ymm4 * ymm0) + mem
	vmovups	%ymm0, (%rcx,%rax,4)
.LBB16_12:                              # %after_bb
                                        #   in Loop: Header=BB16_2 Depth=1
	incq	%r10
	incl	%r12d
	addl	-40(%rsp), %r14d                # 4-byte Folded Reload
	addl	-128(%rsp), %r13d               # 4-byte Folded Reload
	cmpq	32(%rsp), %r10                  # 8-byte Folded Reload
	je	.LBB16_13
.LBB16_2:                               # %"for relu1_0_d_def__.s8.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB16_10 Depth 2
                                        #     Child Loop BB16_8 Depth 2
	cmpl	%r9d, -92(%rsp)                 # 4-byte Folded Reload
	jle	.LBB16_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB16_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB16_12
# %bb.5:                                # %"for relu1_0_d_def__.s8.n.ni.preheader"
                                        #   in Loop: Header=BB16_2 Depth=1
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%r12d, %eax, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rbx
	movslq	%r14d, %rdi
	movslq	%r13d, %rbp
	cmpl	$32, -112(%rsp)                 # 4-byte Folded Reload
	jae	.LBB16_9
# %bb.6:                                #   in Loop: Header=BB16_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB16_7
	.p2align	4, 0x90
.LBB16_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB16_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leaq	(%rcx,%rbx), %rsi
	movq	%rbp, %rbx
	movq	-16(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %rcx
	leaq	(%rbp,%rsi,4), %r15
	movq	%rbx, %rbp
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %r11
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB16_10:                              # %vector.body
                                        #   Parent Loop BB16_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vaddps	-96(%rcx,%r9,4), %ymm3, %ymm0
	vaddps	-64(%rcx,%r9,4), %ymm3, %ymm5
	vaddps	-32(%rcx,%r9,4), %ymm3, %ymm6
	vaddps	(%rcx,%r9,4), %ymm3, %ymm7
	vmulps	-96(%r15,%r9,4), %ymm0, %ymm0
	vmulps	-64(%r15,%r9,4), %ymm5, %ymm5
	vmulps	-32(%r15,%r9,4), %ymm6, %ymm6
	vmulps	(%r15,%r9,4), %ymm7, %ymm7
	vmulps	-96(%r11,%r9,4), %ymm0, %ymm0
	vmulps	-64(%r11,%r9,4), %ymm5, %ymm5
	vmulps	-32(%r11,%r9,4), %ymm6, %ymm6
	vmulps	(%r11,%r9,4), %ymm7, %ymm7
	vfmadd213ps	-96(%rax,%r9,4), %ymm4, %ymm0 # ymm0 = (ymm4 * ymm0) + mem
	vfmadd213ps	-64(%rax,%r9,4), %ymm4, %ymm5 # ymm5 = (ymm4 * ymm5) + mem
	vfmadd213ps	-32(%rax,%r9,4), %ymm4, %ymm6 # ymm6 = (ymm4 * ymm6) + mem
	vfmadd213ps	(%rax,%r9,4), %ymm4, %ymm7 # ymm7 = (ymm4 * ymm7) + mem
	vmovups	%ymm0, -96(%rax,%r9,4)
	vmovups	%ymm5, -64(%rax,%r9,4)
	vmovups	%ymm6, -32(%rax,%r9,4)
	vmovups	%ymm7, (%rax,%r9,4)
	addq	$32, %r9
	cmpq	%r9, %r8
	jne	.LBB16_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB16_2 Depth=1
	movq	%r8, %rsi
	cmpq	-112(%rsp), %r8                 # 8-byte Folded Reload
	movq	-8(%rsp), %r9                   # 8-byte Reload
	movq	(%rsp), %rbx                    # 8-byte Reload
	je	.LBB16_12
.LBB16_7:                               # %"for relu1_0_d_def__.s8.n.ni.preheader24"
                                        #   in Loop: Header=BB16_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rbx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rbp, %r15
	movq	-64(%rsp), %rbp                 # 8-byte Reload
	addq	%rsi, %rbp
	addq	%rbx, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	addq	%rsi, %rdi
	movq	-48(%rsp), %r11                 # 8-byte Reload
	leaq	(%r11,%rdi,4), %rdi
	addq	%rsi, %r15
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r15,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB16_8:                               # %"for relu1_0_d_def__.s8.n.ni"
                                        #   Parent Loop BB16_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vaddss	(%rcx,%rbx,4), %xmm1, %xmm0
	vmulss	(%rbp,%rbx,4), %xmm0, %xmm0
	vmulss	(%rdi,%rbx,4), %xmm0, %xmm0
	vfmadd213ss	(%rsi,%rbx,4), %xmm2, %xmm0 # xmm0 = (xmm2 * xmm0) + mem
	vmovss	%xmm0, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rax
	jne	.LBB16_8
	jmp	.LBB16_12
.LBB16_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end16:
	.size	train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n, .Lfunc_end16-train_cost_model.par_for.relu1_0_d_def__.s8.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n
.LCPI17_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI17_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$104, %rsp
	movl	%esi, %r8d
	movl	(%rdx), %ebp
	movl	8(%rdx), %r11d
	movslq	4(%rdx), %r14
	movl	12(%rdx), %r9d
	movl	16(%rdx), %r12d
	movslq	20(%rdx), %rax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movl	24(%rdx), %esi
	movl	28(%rdx), %r13d
	movl	32(%rdx), %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	movq	40(%rdx), %rax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	movq	56(%rdx), %rax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	movq	72(%rdx), %r15
	movl	%r8d, %edi
	sarl	$31, %edi
	xorl	%ecx, %ecx
	testl	%esi, %esi
	sete	%cl
	movl	%ecx, %ebx
	negl	%ebx
	subl	%edi, %r8d
	orl	%esi, %ebx
	movl	%r8d, %eax
	cltd
	idivl	%ebx
	movq	%rbp, %rbx
	movl	%esi, %eax
	sarl	$31, %eax
	movl	%eax, %ebp
	xorl	%esi, %ebp
	movl	%eax, %r10d
	notl	%r10d
	addl	%r10d, %ebp
	andl	%edi, %ebp
	addl	%edx, %ebp
	subl	%eax, %r10d
	andl	%edi, %r10d
	addl	%ecx, %esi
	leal	-1(%rcx), %edi
	andl	%edi, %ebp
	movl	%r8d, %eax
	cltd
	idivl	%esi
	addl	%eax, %r10d
	andl	%edi, %r10d
	movq	%r14, %rdi
	movl	%r10d, %eax
	sarl	$31, %eax
	andl	%r10d, %eax
	cmpl	$-2, %eax
	movl	$-1, %ecx
	cmovgl	%eax, %ecx
	addl	%ecx, %ecx
	movl	%ecx, 44(%rsp)                  # 4-byte Spill
	movl	%ecx, %r14d
	negl	%r14d
	leal	(%r10,%r10), %eax
	subl	%eax, %r9d
	cmpl	$3, %r9d
	movl	$2, %edx
	cmovll	%r9d, %edx
	cmpl	%r14d, %edx
	movl	%r14d, %ecx
	movl	%edx, 36(%rsp)                  # 4-byte Spill
	cmovgel	%edx, %ecx
	movl	%ecx, 40(%rsp)                  # 4-byte Spill
	cmpl	%r14d, %r9d
	cmovgl	%r14d, %r9d
	movl	%eax, %edx
	subl	%r11d, %edx
	movq	%r12, -64(%rsp)                 # 8-byte Spill
	leal	(%r12,%r12,2), %ecx
	addl	%ebp, %ecx
	shll	$3, %ecx
	movl	%ecx, -88(%rsp)                 # 4-byte Spill
	movq	%rax, -80(%rsp)                 # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movl	%r13d, -104(%rsp)               # 4-byte Spill
	subl	%r13d, %eax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	leal	(,%rbp,8), %ecx
	vmovd	%ecx, %xmm0
	movq	%rbp, 56(%rsp)                  # 8-byte Spill
	leal	8(,%rbp,8), %eax
	movl	%eax, -84(%rsp)                 # 4-byte Spill
	movq	%rdx, 48(%rsp)                  # 8-byte Spill
	movslq	%edx, %rax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	movl	%ecx, -92(%rsp)                 # 4-byte Spill
	movslq	%ecx, %rax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	testl	%r9d, %r9d
	movq	%rbx, -8(%rsp)                  # 8-byte Spill
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	movq	%r15, (%rsp)                    # 8-byte Spill
	jle	.LBB17_4
# %bb.1:                                # %"for relu1_0_d_def__.s9.w.wi.preheader"
	movq	-64(%rsp), %r8                  # 8-byte Reload
	movl	%r8d, %edx
	shll	$5, %edx
	vpbroadcastd	%xmm0, %ymm1
	vpor	.LCPI17_0(%rip), %ymm1, %ymm1
	leal	-1(%rbx), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	movl	%ebx, %eax
	movl	-92(%rsp), %r11d                # 4-byte Reload
	subl	%r11d, %eax
	cmpl	$9, %eax
	movl	$8, %ecx
	cmovll	%eax, %ecx
	xorl	%ebp, %ebp
	movl	%eax, -16(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%ebp, %ecx
	vpminsd	%ymm1, %ymm2, %ymm1
	movl	%r9d, %eax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movl	%ecx, %eax
	andl	$2147483640, %eax               # imm = 0x7FFFFFF8
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	addq	$-8, %rax
	movq	%rax, -40(%rsp)                 # 8-byte Spill
	movq	%rax, %rsi
	shrq	$3, %rsi
	incq	%rsi
	movl	%esi, %r13d
	andl	$3, %r13d
	movq	-112(%rsp), %rax                # 8-byte Reload
	addq	$96, %rax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movl	%edi, %eax
	imull	48(%rsp), %eax                  # 4-byte Folded Reload
	movq	56(%rsp), %r12                  # 8-byte Reload
	leal	(%rax,%r12,8), %r9d
	leaq	96(%r15), %rax
	movq	%rax, 88(%rsp)                  # 8-byte Spill
	subl	-124(%rsp), %r11d               # 4-byte Folded Reload
	movl	%r11d, -24(%rsp)                # 4-byte Spill
	shll	$6, %r10d
	movl	-104(%rsp), %eax                # 4-byte Reload
	shll	$5, %eax
	subl	%eax, %r10d
	orl	$24, %r10d
	imull	%r8d, %r10d
	leal	(%r10,%r12,8), %r8d
	andq	$-4, %rsi
	negq	%rsi
	movq	%rsi, -56(%rsp)                 # 8-byte Spill
	movq	%r13, -104(%rsp)                # 8-byte Spill
	negq	%r13
	movq	%r13, -48(%rsp)                 # 8-byte Spill
	vmovss	.LCPI17_1(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI17_1(%rip), %ymm3  # ymm3 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-120(%rsp), %rax                # 8-byte Reload
	leaq	96(%rax), %rax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	movq	-80(%rsp), %rax                 # 8-byte Reload
	movl	%eax, %r10d
	jmp	.LBB17_2
	.p2align	4, 0x90
.LBB17_3:                               # %true_bb
                                        #   in Loop: Header=BB17_2 Depth=1
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%ebp, %eax
	movl	%eax, %esi
	sarl	$31, %esi
	andnl	%eax, %esi, %eax
	imull	-72(%rsp), %eax                 # 4-byte Folded Reload
	subl	-124(%rsp), %eax                # 4-byte Folded Reload
	vmovd	%eax, %xmm4
	vpbroadcastd	%xmm4, %ymm4
	vpaddd	%ymm1, %ymm4, %ymm4
	vextracti128	$1, %ymm4, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vmovss	(%r15,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %eax
	cltq
	vinsertps	$16, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm6, %xmm5 # xmm5 = xmm6[0,1,2],mem[0]
	vmovd	%xmm4, %eax
	cltq
	vmovss	(%r15,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %eax
	cltq
	vinsertps	$16, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm6, %xmm4 # xmm4 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm4, %ymm4
	movq	72(%rsp), %rax                  # 8-byte Reload
	addq	%rbp, %rax
	imulq	%rdi, %rax
	addq	64(%rsp), %rax                  # 8-byte Folded Reload
	movq	-112(%rsp), %rsi                # 8-byte Reload
	vmulps	(%rsi,%rax,4), %ymm4, %ymm4
	movq	16(%rsp), %rax                  # 8-byte Reload
	addl	%ebp, %eax
	imull	%edx, %eax
	addl	-88(%rsp), %eax                 # 4-byte Folded Reload
	cltq
	movq	-120(%rsp), %rsi                # 8-byte Reload
	vfmadd213ps	(%rsi,%rax,4), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovaps	%ymm4, (%rsi,%rax,4)
.LBB17_20:                              # %after_bb
                                        #   in Loop: Header=BB17_2 Depth=1
	incq	%rbp
	addl	%edi, %r9d
	incl	%r10d
	addl	%edx, %r8d
	cmpq	8(%rsp), %rbp                   # 8-byte Folded Reload
	je	.LBB17_4
.LBB17_2:                               # %"for relu1_0_d_def__.s9.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB17_15 Depth 2
                                        #     Child Loop BB17_18 Depth 2
                                        #     Child Loop BB17_11 Depth 2
	cmpl	%ebx, -84(%rsp)                 # 4-byte Folded Reload
	jle	.LBB17_3
# %bb.8:                                # %false_bb
                                        #   in Loop: Header=BB17_2 Depth=1
	cmpl	$0, -16(%rsp)                   # 4-byte Folded Reload
	jle	.LBB17_20
# %bb.9:                                # %"for relu1_0_d_def__.s9.n.ni.preheader"
                                        #   in Loop: Header=BB17_2 Depth=1
	movl	%r10d, %eax
	sarl	$31, %eax
	andnl	%r10d, %eax, %esi
	imull	-72(%rsp), %esi                 # 4-byte Folded Reload
	addl	-24(%rsp), %esi                 # 4-byte Folded Reload
	movslq	%r9d, %rax
	movq	-112(%rsp), %rdi                # 8-byte Reload
	leaq	(%rdi,%rax,4), %r13
	movslq	%esi, %rdi
	leaq	(%r15,%rdi,4), %r11
	movslq	%r8d, %rbx
	movq	-120(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rbx,4), %r12
	cmpl	$7, %ecx
	ja	.LBB17_12
# %bb.10:                               #   in Loop: Header=BB17_2 Depth=1
	xorl	%eax, %eax
	movq	-8(%rsp), %rbx                  # 8-byte Reload
	movq	24(%rsp), %rdi                  # 8-byte Reload
	jmp	.LBB17_11
	.p2align	4, 0x90
.LBB17_12:                              # %vector.ph
                                        #   in Loop: Header=BB17_2 Depth=1
	cmpq	$24, -40(%rsp)                  # 8-byte Folded Reload
	jae	.LBB17_14
# %bb.13:                               #   in Loop: Header=BB17_2 Depth=1
	xorl	%edi, %edi
	jmp	.LBB17_16
.LBB17_14:                              # %vector.body.preheader
                                        #   in Loop: Header=BB17_2 Depth=1
	movq	96(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rax,4), %rsi
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdi,4), %r15
	movq	80(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rbx,4), %rbx
	movq	-56(%rsp), %rax                 # 8-byte Reload
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB17_15:                              # %vector.body
                                        #   Parent Loop BB17_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r15,%rdi,4), %ymm4
	vmulps	-96(%rsi,%rdi,4), %ymm4, %ymm4
	vfmadd213ps	-96(%rbx,%rdi,4), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, -96(%rbx,%rdi,4)
	vmovups	-64(%r15,%rdi,4), %ymm4
	vmulps	-64(%rsi,%rdi,4), %ymm4, %ymm4
	vfmadd213ps	-64(%rbx,%rdi,4), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, -64(%rbx,%rdi,4)
	vmovups	-32(%r15,%rdi,4), %ymm4
	vmulps	-32(%rsi,%rdi,4), %ymm4, %ymm4
	vfmadd213ps	-32(%rbx,%rdi,4), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, -32(%rbx,%rdi,4)
	vmovups	(%r15,%rdi,4), %ymm4
	vmulps	(%rsi,%rdi,4), %ymm4, %ymm4
	vfmadd213ps	(%rbx,%rdi,4), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, (%rbx,%rdi,4)
	addq	$32, %rdi
	addq	$4, %rax
	jne	.LBB17_15
.LBB17_16:                              # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB17_2 Depth=1
	cmpq	$0, -104(%rsp)                  # 8-byte Folded Reload
	movq	(%rsp), %r15                    # 8-byte Reload
	je	.LBB17_19
# %bb.17:                               # %vector.body.epil.preheader
                                        #   in Loop: Header=BB17_2 Depth=1
	shlq	$2, %rdi
	movq	-48(%rsp), %rax                 # 8-byte Reload
	.p2align	4, 0x90
.LBB17_18:                              # %vector.body.epil
                                        #   Parent Loop BB17_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%r11,%rdi), %ymm4
	vmulps	(%r13,%rdi), %ymm4, %ymm4
	vfmadd213ps	(%r12,%rdi), %ymm3, %ymm4 # ymm4 = (ymm3 * ymm4) + mem
	vmovups	%ymm4, (%r12,%rdi)
	addq	$32, %rdi
	incq	%rax
	jne	.LBB17_18
.LBB17_19:                              # %middle.block
                                        #   in Loop: Header=BB17_2 Depth=1
	movq	-32(%rsp), %rsi                 # 8-byte Reload
	movq	%rsi, %rax
	cmpq	%rcx, %rsi
	movq	-8(%rsp), %rbx                  # 8-byte Reload
	movq	24(%rsp), %rdi                  # 8-byte Reload
	je	.LBB17_20
	.p2align	4, 0x90
.LBB17_11:                              # %"for relu1_0_d_def__.s9.n.ni"
                                        #   Parent Loop BB17_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%r11,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmulss	(%r13,%rax,4), %xmm4, %xmm4
	vfmadd213ss	(%r12,%rax,4), %xmm2, %xmm4 # xmm4 = (xmm2 * xmm4) + mem
	vmovss	%xmm4, (%r12,%rax,4)
	incq	%rax
	cmpq	%rax, %rcx
	jne	.LBB17_11
	jmp	.LBB17_20
.LBB17_4:                               # %"end for relu1_0_d_def__.s9.w.wi"
	cmpl	%r14d, 36(%rsp)                 # 4-byte Folded Reload
	jle	.LBB17_34
# %bb.5:                                # %"for relu1_0_d_def__.s9.w.wi1.preheader"
	movq	-64(%rsp), %rax                 # 8-byte Reload
	movl	%eax, %r13d
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI17_0(%rip), %ymm0, %ymm0
	shll	$5, %r13d
	leal	-1(%rbx), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movl	%ebx, %ecx
	movl	-92(%rsp), %edx                 # 4-byte Reload
	subl	%edx, %ecx
	cmpl	$9, %ecx
	movl	$8, %r15d
	cmovll	%ecx, %r15d
	xorl	%eax, %eax
	movl	%ecx, 8(%rsp)                   # 4-byte Spill
	testl	%ecx, %ecx
	cmovlel	%eax, %r15d
	subl	-124(%rsp), %edx                # 4-byte Folded Reload
	movl	40(%rsp), %r11d                 # 4-byte Reload
	movslq	-80(%rsp), %rcx                 # 4-byte Folded Reload
	movslq	%edx, %rax
	movl	%r15d, %edx
	andl	$2147483640, %edx               # imm = 0x7FFFFFF8
	movq	%rdx, -16(%rsp)                 # 8-byte Spill
	leaq	-8(%rdx), %rbp
	movq	%rbp, -24(%rsp)                 # 8-byte Spill
	shrq	$3, %rbp
	incq	%rbp
	movl	%ebp, %esi
	andl	$3, %esi
	movq	-112(%rsp), %rdx                # 8-byte Reload
	addq	$96, %rdx
	movq	%rdx, -48(%rsp)                 # 8-byte Spill
	movl	44(%rsp), %r8d                  # 4-byte Reload
	movq	48(%rsp), %rdx                  # 8-byte Reload
	subl	%r8d, %edx
	imull	%edi, %edx
	movq	56(%rsp), %r10                  # 8-byte Reload
	leal	(%rdx,%r10,8), %r12d
	addq	%r14, %rcx
	movq	-72(%rsp), %r9                  # 8-byte Reload
	imulq	%r9, %rcx
	addq	%rax, %rcx
	movq	16(%rsp), %rax                  # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	subl	%r8d, %eax
	movq	(%rsp), %rdx                    # 8-byte Reload
	leaq	(%rdx,%rcx,4), %r8
	addq	$96, %r8
	shll	$5, %eax
	orl	$24, %eax
	imull	-64(%rsp), %eax                 # 4-byte Folded Reload
	leaq	(,%r9,4), %r9
	leal	(%rax,%r10,8), %r10d
	andq	$-4, %rbp
	negq	%rbp
	movq	%rbp, -104(%rsp)                # 8-byte Spill
	leaq	(%rdx,%rcx,4), %rdx
	movq	%rsi, -32(%rsp)                 # 8-byte Spill
	negq	%rsi
	movq	%rsi, -40(%rsp)                 # 8-byte Spill
	vmovss	.LCPI17_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI17_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	-120(%rsp), %rax                # 8-byte Reload
	addq	$96, %rax
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	jmp	.LBB17_6
	.p2align	4, 0x90
.LBB17_7:                               # %true_bb4
                                        #   in Loop: Header=BB17_6 Depth=1
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%r14d, %eax
	imull	-72(%rsp), %eax                 # 4-byte Folded Reload
	subl	-124(%rsp), %eax                # 4-byte Folded Reload
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm3
	vextracti128	$1, %ymm3, %xmm4
	vmovd	%xmm4, %eax
	cltq
	movq	(%rsp), %rcx                    # 8-byte Reload
	vmovss	(%rcx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm4, %eax
	cltq
	vinsertps	$16, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	vmovd	%xmm3, %eax
	cltq
	vmovss	(%rcx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm3, %eax
	cltq
	vinsertps	$16, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm3, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm3, %eax
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	movq	72(%rsp), %rax                  # 8-byte Reload
	addq	%r14, %rax
	imulq	%rdi, %rax
	addq	64(%rsp), %rax                  # 8-byte Folded Reload
	movq	-112(%rsp), %rcx                # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	16(%rsp), %rax                  # 8-byte Reload
	addl	%r14d, %eax
	imull	%r13d, %eax
	addl	-88(%rsp), %eax                 # 4-byte Folded Reload
	cltq
	movq	-120(%rsp), %rcx                # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovaps	%ymm3, (%rcx,%rax,4)
.LBB17_33:                              # %after_bb6
                                        #   in Loop: Header=BB17_6 Depth=1
	incq	%r14
	movq	24(%rsp), %rdi                  # 8-byte Reload
	addl	%edi, %r12d
	addq	%r9, %r8
	addl	%r13d, %r10d
	addq	%r9, %rdx
	cmpq	%r11, %r14
	je	.LBB17_34
.LBB17_6:                               # %"for relu1_0_d_def__.s9.w.wi1"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB17_28 Depth 2
                                        #     Child Loop BB17_31 Depth 2
                                        #     Child Loop BB17_24 Depth 2
	cmpl	%ebx, -84(%rsp)                 # 4-byte Folded Reload
	jle	.LBB17_7
# %bb.21:                               # %false_bb5
                                        #   in Loop: Header=BB17_6 Depth=1
	cmpl	$0, 8(%rsp)                     # 4-byte Folded Reload
	jle	.LBB17_33
# %bb.22:                               # %"for relu1_0_d_def__.s9.n.ni7.preheader"
                                        #   in Loop: Header=BB17_6 Depth=1
	movslq	%r12d, %rax
	movq	-112(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movslq	%r10d, %rsi
	movq	-120(%rsp), %rdi                # 8-byte Reload
	leaq	(%rdi,%rsi,4), %rdi
	cmpl	$7, %r15d
	ja	.LBB17_25
# %bb.23:                               #   in Loop: Header=BB17_6 Depth=1
	xorl	%eax, %eax
	jmp	.LBB17_24
	.p2align	4, 0x90
.LBB17_25:                              # %vector.ph45
                                        #   in Loop: Header=BB17_6 Depth=1
	cmpq	$24, -24(%rsp)                  # 8-byte Folded Reload
	jae	.LBB17_27
# %bb.26:                               #   in Loop: Header=BB17_6 Depth=1
	xorl	%esi, %esi
	jmp	.LBB17_29
.LBB17_27:                              # %vector.body43.preheader
                                        #   in Loop: Header=BB17_6 Depth=1
	movq	-48(%rsp), %rbx                 # 8-byte Reload
	leaq	(%rbx,%rax,4), %rbx
	movq	-56(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rsi,4), %rax
	movq	-104(%rsp), %rbp                # 8-byte Reload
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB17_28:                              # %vector.body43
                                        #   Parent Loop BB17_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r8,%rsi,4), %ymm3
	vmulps	-96(%rbx,%rsi,4), %ymm3, %ymm3
	vfmadd213ps	-96(%rax,%rsi,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -96(%rax,%rsi,4)
	vmovups	-64(%r8,%rsi,4), %ymm3
	vmulps	-64(%rbx,%rsi,4), %ymm3, %ymm3
	vfmadd213ps	-64(%rax,%rsi,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -64(%rax,%rsi,4)
	vmovups	-32(%r8,%rsi,4), %ymm3
	vmulps	-32(%rbx,%rsi,4), %ymm3, %ymm3
	vfmadd213ps	-32(%rax,%rsi,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, -32(%rax,%rsi,4)
	vmovups	(%r8,%rsi,4), %ymm3
	vmulps	(%rbx,%rsi,4), %ymm3, %ymm3
	vfmadd213ps	(%rax,%rsi,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rax,%rsi,4)
	addq	$32, %rsi
	addq	$4, %rbp
	jne	.LBB17_28
.LBB17_29:                              # %middle.block41.unr-lcssa
                                        #   in Loop: Header=BB17_6 Depth=1
	cmpq	$0, -32(%rsp)                   # 8-byte Folded Reload
	je	.LBB17_32
# %bb.30:                               # %vector.body43.epil.preheader
                                        #   in Loop: Header=BB17_6 Depth=1
	shlq	$2, %rsi
	movq	-40(%rsp), %rax                 # 8-byte Reload
	.p2align	4, 0x90
.LBB17_31:                              # %vector.body43.epil
                                        #   Parent Loop BB17_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rdx,%rsi), %ymm3
	vmulps	(%rcx,%rsi), %ymm3, %ymm3
	vfmadd213ps	(%rdi,%rsi), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rdi,%rsi)
	addq	$32, %rsi
	incq	%rax
	jne	.LBB17_31
.LBB17_32:                              # %middle.block41
                                        #   in Loop: Header=BB17_6 Depth=1
	movq	-16(%rsp), %rsi                 # 8-byte Reload
	movq	%rsi, %rax
	cmpq	%r15, %rsi
	movq	-8(%rsp), %rbx                  # 8-byte Reload
	je	.LBB17_33
	.p2align	4, 0x90
.LBB17_24:                              # %"for relu1_0_d_def__.s9.n.ni7"
                                        #   Parent Loop BB17_6 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rdx,%rax,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rcx,%rax,4), %xmm3, %xmm3
	vfmadd213ss	(%rdi,%rax,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rdi,%rax,4)
	incq	%rax
	cmpq	%rax, %r15
	jne	.LBB17_24
	jmp	.LBB17_33
.LBB17_34:                              # %destructor_block
	xorl	%eax, %eax
	addq	$104, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end17:
	.size	train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n, .Lfunc_end17-train_cost_model.par_for.relu1_0_d_def__.s9.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n
.LCPI18_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI18_1:
	.long	0x3f800000                      # float 1
.LCPI18_2:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$128, %rsp
	movq	%rdx, %rcx
	movl	%esi, %edi
	movl	12(%rdx), %r12d
	movl	24(%rdx), %r11d
	movl	%esi, %r10d
	sarl	$31, %r10d
	xorl	%ebp, %ebp
	testl	%r11d, %r11d
	sete	%bpl
	movl	%ebp, %esi
	negl	%esi
	movl	%r11d, %ebx
	sarl	$31, %ebx
	subl	%r10d, %edi
	orl	%r11d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	movl	%ebx, %r14d
	leal	(%r11,%rbp), %r9d
	movl	%edi, %eax
	cltd
	idivl	%r9d
	notl	%r14d
	decl	%ebp
	movl	%r14d, %edi
	subl	%ebx, %edi
	andl	%r10d, %edi
	addl	%eax, %edi
	andl	%ebp, %edi
	leal	(%rdi,%rdi), %eax
	movq	%rax, -104(%rsp)                # 8-byte Spill
	subl	%eax, %r12d
	jle	.LBB18_17
# %bb.1:                                # %"for relu1_0_d_def__.s10.w.wi.preheader"
	movl	(%rcx), %eax
	movq	%rax, -40(%rsp)                 # 8-byte Spill
	movl	16(%rcx), %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	movl	28(%rcx), %edx
	movl	32(%rcx), %eax
	movl	36(%rcx), %r9d
	movl	40(%rcx), %r8d
	xorl	%r11d, %ebx
	movq	%r8, %r11
	movl	44(%rcx), %esi
	addl	%r14d, %ebx
	andl	%r10d, %ebx
	addl	%ebx, %r15d
	andl	%ebp, %r15d
	leal	(,%r15,8), %ebx
	movl	%ebx, -112(%rsp)                # 4-byte Spill
	subl	%eax, %r9d
	subl	%eax, %r11d
	subl	%eax, %esi
	movq	%rsi, -32(%rsp)                 # 8-byte Spill
	movq	-104(%rsp), %r13                # 8-byte Reload
	movl	%r13d, %eax
	subl	%edx, %eax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
                                        # kill: def $r13d killed $r13d killed $r13 def $r13
	subl	8(%rcx), %r13d
	xorl	%eax, %eax
	cmpl	$1, %r12d
	movq	-40(%rsp), %r8                  # 8-byte Reload
	sete	%al
	movl	$2, %ebp
	subq	%rax, %rbp
	movq	%rbp, 120(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rsi                # 8-byte Reload
	movl	%esi, %eax
	shll	$5, %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	leal	-1(%r8), %eax
	vmovd	%eax, %xmm0
	movslq	%ebx, %rbp
	movl	%r8d, %eax
	subl	%ebp, %eax
	cmpl	$9, %eax
	movl	$8, %r12d
	cmovll	%eax, %r12d
	xorl	%r10d, %r10d
	movl	%eax, -72(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %r12d
	shll	$6, %edi
	shll	$5, %edx
	subl	%edx, %edi
	movslq	4(%rcx), %rdx
	movl	20(%rcx), %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	movq	48(%rcx), %r14
	movq	64(%rcx), %rbx
	movq	80(%rcx), %rcx
	orl	$15, %edi
	movq	%rsi, %rax
	imull	%eax, %edi
	leal	(%rsi,%rsi,4), %eax
	leal	(%rax,%rax,2), %eax
	leal	(%rax,%r15,8), %eax
	movl	%eax, -108(%rsp)                # 4-byte Spill
	movq	%r13, (%rsp)                    # 8-byte Spill
	movslq	%r13d, %rax
	movq	%rax, %rsi
	movq	%rax, 104(%rsp)                 # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rdx, -80(%rsp)                 # 8-byte Spill
	imull	%edx, %eax
	leal	(%rax,%r15,8), %eax
	movl	%eax, -120(%rsp)                # 4-byte Spill
	leal	(%rdi,%r15,8), %r13d
	leal	8(,%r15,8), %eax
	movl	%eax, -68(%rsp)                 # 4-byte Spill
	vmovd	-112(%rsp), %xmm1               # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm1, %ymm1
	vpor	.LCPI18_0(%rip), %ymm1, %ymm1
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm9
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leal	(%rbp,%rax), %eax
	movslq	%eax, %rdx
	leal	(%rbp,%r9), %eax
	movslq	%eax, %rsi
	movq	%rbp, 112(%rsp)                 # 8-byte Spill
	leal	(%r11,%rbp), %eax
	cltq
	movl	%r12d, %edi
	andl	$2147483640, %edi               # imm = 0x7FFFFFF8
	movq	%rdi, 56(%rsp)                  # 8-byte Spill
	addq	$-8, %rdi
	movq	%rdi, 48(%rsp)                  # 8-byte Spill
	movq	%rdi, %rbp
	shrq	$3, %rbp
	incq	%rbp
	movq	%rbp, %rdi
	movq	%rbp, 40(%rsp)                  # 8-byte Spill
	andq	$-2, %rbp
	negq	%rbp
	movq	%rbp, 32(%rsp)                  # 8-byte Spill
	vmovss	.LCPI18_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI18_2(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI18_1(%rip), %ymm3  # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI18_2(%rip), %ymm4  # ymm4 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	32(%rcx), %rdi
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	movq	%r14, -88(%rsp)                 # 8-byte Spill
	leaq	32(%r14), %rdi
	movq	%rdi, 16(%rsp)                  # 8-byte Spill
	movq	%rbx, -96(%rsp)                 # 8-byte Spill
	leaq	32(%rbx), %rdi
	movq	%rdi, 8(%rsp)                   # 8-byte Spill
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movq	%rsi, -56(%rsp)                 # 8-byte Spill
	leaq	(%rcx,%rsi,4), %rax
	movq	%rax, 88(%rsp)                  # 8-byte Spill
	movq	%rdx, -48(%rsp)                 # 8-byte Spill
	leaq	(%rcx,%rdx,4), %rax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	movq	-104(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r15d
	movq	%r9, 72(%rsp)                   # 8-byte Spill
	movq	%r11, 64(%rsp)                  # 8-byte Spill
	jmp	.LBB18_2
	.p2align	4, 0x90
.LBB18_3:                               # %true_bb
                                        #   in Loop: Header=BB18_2 Depth=1
	movq	-104(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %edx
	imull	-128(%rsp), %edx                # 4-byte Folded Reload
	movq	-32(%rsp), %rax                 # 8-byte Reload
	addl	%edx, %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm9, %ymm6
	vextracti128	$1, %ymm6, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vpextrd	$1, %xmm5, %esi
	movslq	%esi, %rsi
	vmovss	(%rcx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm6, %eax
	vpextrd	$2, %xmm5, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rcx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %esi
	cltq
	movslq	%esi, %rsi
	vinsertps	$48, (%rcx,%rsi,4), %xmm7, %xmm10 # xmm10 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm6, %esi
	movslq	%esi, %rsi
	vmovss	(%rcx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rcx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	cltq
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rcx,%rax,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%rcx,%rsi,4), %xmm6, %xmm11 # xmm11 = xmm6[0,1,2],mem[0]
	leal	(%rdx,%r9), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %ymm7
	vpaddd	%ymm7, %ymm9, %ymm8
	vextracti128	$1, %ymm8, %xmm7
	vmovd	%xmm7, %eax
	cltq
	vpextrd	$1, %xmm7, %esi
	movslq	%esi, %rsi
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm7, %eax
	vinsertps	$16, (%rcx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovd	%xmm8, %esi
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movslq	%esi, %rax
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rcx,%rsi,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	vpextrd	$1, %xmm8, %esi
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	movslq	%esi, %rax
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$2, %xmm8, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrd	$3, %xmm8, %eax
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	addl	%r11d, %edx
	vmovd	%edx, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm9, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %edx
	vmovss	(%rcx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%edx, %rax
	vinsertps	$16, (%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm6, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm6, %eax
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vmovd	%xmm5, %eax
	cltq
	vmovss	(%rcx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %eax
	vinsertf128	$1, %xmm10, %ymm11, %ymm8
	cltq
	vinsertps	$16, (%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertf128	$1, %xmm12, %ymm0, %ymm0
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	movq	104(%rsp), %rax                 # 8-byte Reload
	addq	%r10, %rax
	imulq	-80(%rsp), %rax                 # 8-byte Folded Reload
	addq	112(%rsp), %rax                 # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vmulps	%ymm5, %ymm0, %ymm0
	vmaxps	%ymm3, %ymm0, %ymm0
	vmulps	%ymm0, %ymm8, %ymm0
	movq	-88(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%rdx,%rax,4), %ymm0, %ymm0
	movq	-24(%rsp), %rax                 # 8-byte Reload
	addl	%r10d, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	addl	-108(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-96(%rsp), %rdx                 # 8-byte Reload
	vfmadd213ps	(%rdx,%rax,4), %ymm4, %ymm0 # ymm0 = (ymm4 * ymm0) + mem
	vmovups	%ymm0, (%rdx,%rax,4)
.LBB18_16:                              # %after_bb
                                        #   in Loop: Header=BB18_2 Depth=1
	incq	%r10
	incl	%r15d
	movl	-120(%rsp), %eax                # 4-byte Reload
	addl	-80(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -120(%rsp)                # 4-byte Spill
	addl	-124(%rsp), %r13d               # 4-byte Folded Reload
	cmpq	120(%rsp), %r10                 # 8-byte Folded Reload
	je	.LBB18_17
.LBB18_2:                               # %"for relu1_0_d_def__.s10.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB18_12 Depth 2
                                        #     Child Loop BB18_8 Depth 2
	cmpl	%r8d, -68(%rsp)                 # 4-byte Folded Reload
	jle	.LBB18_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB18_2 Depth=1
	cmpl	$0, -72(%rsp)                   # 4-byte Folded Reload
	jle	.LBB18_16
# %bb.5:                                # %"for relu1_0_d_def__.s10.n.ni.preheader"
                                        #   in Loop: Header=BB18_2 Depth=1
	movl	%r15d, %eax
	sarl	$31, %eax
	andnl	%r15d, %eax, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rdi
	movslq	-120(%rsp), %rax                # 4-byte Folded Reload
	movq	%rax, -8(%rsp)                  # 8-byte Spill
	movslq	%r13d, %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	cmpl	$7, %r12d
	ja	.LBB18_9
# %bb.6:                                #   in Loop: Header=BB18_2 Depth=1
	xorl	%edx, %edx
	jmp	.LBB18_7
	.p2align	4, 0x90
.LBB18_9:                               # %vector.ph
                                        #   in Loop: Header=BB18_2 Depth=1
	cmpq	$0, 48(%rsp)                    # 8-byte Folded Reload
	je	.LBB18_10
# %bb.11:                               # %vector.body.preheader
                                        #   in Loop: Header=BB18_2 Depth=1
	movq	-64(%rsp), %rax                 # 8-byte Reload
	addq	%rdi, %rax
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rdi), %rdx
	movq	-48(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rdi), %rbp
	movq	24(%rsp), %rbx                  # 8-byte Reload
	leaq	(%rbx,%rax,4), %rsi
	leaq	(%rbx,%rdx,4), %r8
	leaq	(%rbx,%rbp,4), %rbp
	movq	16(%rsp), %rax                  # 8-byte Reload
	movq	-8(%rsp), %rdx                  # 8-byte Reload
	leaq	(%rax,%rdx,4), %r11
	movq	8(%rsp), %rax                   # 8-byte Reload
	movq	-16(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %r14
	movq	32(%rsp), %rdx                  # 8-byte Reload
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB18_12:                              # %vector.body
                                        #   Parent Loop BB18_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-32(%r8,%r9,4), %ymm0
	vmulps	-32(%rsi,%r9,4), %ymm0, %ymm0
	vmaxps	%ymm3, %ymm0, %ymm0
	vmulps	-32(%rbp,%r9,4), %ymm0, %ymm0
	vmulps	-32(%r11,%r9,4), %ymm0, %ymm0
	vfmadd213ps	-32(%r14,%r9,4), %ymm4, %ymm0 # ymm0 = (ymm4 * ymm0) + mem
	vmovups	%ymm0, -32(%r14,%r9,4)
	vmovups	(%r8,%r9,4), %ymm0
	vmulps	(%rsi,%r9,4), %ymm0, %ymm0
	vmaxps	%ymm3, %ymm0, %ymm0
	vmulps	(%rbp,%r9,4), %ymm0, %ymm0
	vmulps	(%r11,%r9,4), %ymm0, %ymm0
	vfmadd213ps	(%r14,%r9,4), %ymm4, %ymm0 # ymm0 = (ymm4 * ymm0) + mem
	vmovups	%ymm0, (%r14,%r9,4)
	addq	$16, %r9
	addq	$2, %rdx
	jne	.LBB18_12
# %bb.13:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB18_2 Depth=1
	testb	$1, 40(%rsp)                    # 1-byte Folded Reload
	je	.LBB18_15
.LBB18_14:                              # %vector.body.epil
                                        #   in Loop: Header=BB18_2 Depth=1
	movq	-104(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movq	-24(%rsp), %rdx                 # 8-byte Reload
	addl	%r10d, %edx
	movl	%eax, %esi
	sarl	$31, %esi
	andnl	%eax, %esi, %eax
	movq	(%rsp), %rsi                    # 8-byte Reload
	addl	%r10d, %esi
	imull	-124(%rsp), %edx                # 4-byte Folded Reload
	imull	-80(%rsp), %esi                 # 4-byte Folded Reload
	addl	-108(%rsp), %edx                # 4-byte Folded Reload
	addl	-112(%rsp), %esi                # 4-byte Folded Reload
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	movslq	%esi, %rsi
	cltq
	movslq	%edx, %rdx
	addq	%r9, %rdx
	addq	%r9, %rsi
	addq	%r9, %rax
	movq	-56(%rsp), %rbp                 # 8-byte Reload
	addq	%rax, %rbp
	vmovups	(%rcx,%rbp,4), %ymm0
	movq	-48(%rsp), %rbp                 # 8-byte Reload
	addq	%rax, %rbp
	addq	-64(%rsp), %rax                 # 8-byte Folded Reload
	vmulps	(%rcx,%rax,4), %ymm0, %ymm0
	vmaxps	%ymm3, %ymm0, %ymm0
	vmulps	(%rcx,%rbp,4), %ymm0, %ymm0
	movq	-88(%rsp), %rax                 # 8-byte Reload
	vmulps	(%rax,%rsi,4), %ymm0, %ymm0
	movq	-96(%rsp), %rax                 # 8-byte Reload
	vfmadd213ps	(%rax,%rdx,4), %ymm4, %ymm0 # ymm0 = (ymm4 * ymm0) + mem
	vmovups	%ymm0, (%rax,%rdx,4)
.LBB18_15:                              # %middle.block
                                        #   in Loop: Header=BB18_2 Depth=1
	movq	56(%rsp), %rax                  # 8-byte Reload
	movq	%rax, %rdx
	cmpq	%r12, %rax
	movq	-40(%rsp), %r8                  # 8-byte Reload
	movq	72(%rsp), %r9                   # 8-byte Reload
	movq	64(%rsp), %r11                  # 8-byte Reload
	je	.LBB18_16
.LBB18_7:                               # %"for relu1_0_d_def__.s10.n.ni.preheader13"
                                        #   in Loop: Header=BB18_2 Depth=1
	movq	96(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdi,4), %rsi
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdi,4), %rbp
	movq	80(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdi,4), %rbx
	movq	-88(%rsp), %rax                 # 8-byte Reload
	movq	-8(%rsp), %rdi                  # 8-byte Reload
	leaq	(%rax,%rdi,4), %rdi
	movq	-96(%rsp), %rax                 # 8-byte Reload
	movq	-16(%rsp), %r14                 # 8-byte Reload
	leaq	(%rax,%r14,4), %rax
	.p2align	4, 0x90
.LBB18_8:                               # %"for relu1_0_d_def__.s10.n.ni"
                                        #   Parent Loop BB18_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rdx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmulss	(%rsi,%rdx,4), %xmm0, %xmm0
	vmaxss	%xmm1, %xmm0, %xmm0
	vmulss	(%rbx,%rdx,4), %xmm0, %xmm0
	vmulss	(%rdi,%rdx,4), %xmm0, %xmm0
	vfmadd213ss	(%rax,%rdx,4), %xmm2, %xmm0 # xmm0 = (xmm2 * xmm0) + mem
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %r12
	jne	.LBB18_8
	jmp	.LBB18_16
.LBB18_10:                              #   in Loop: Header=BB18_2 Depth=1
	xorl	%r9d, %r9d
	testb	$1, 40(%rsp)                    # 1-byte Folded Reload
	jne	.LBB18_14
	jmp	.LBB18_15
.LBB18_17:                              # %destructor_block
	xorl	%eax, %eax
	addq	$128, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end18:
	.size	train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n, .Lfunc_end18-train_cost_model.par_for.relu1_0_d_def__.s10.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n
.LCPI19_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI19_1:
	.long	0x3f800000                      # float 1
.LCPI19_2:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	movq	%rdx, %rcx
	movl	%esi, %edi
	movl	12(%rdx), %r12d
	movl	24(%rdx), %r11d
	movl	%esi, %r10d
	sarl	$31, %r10d
	xorl	%ebx, %ebx
	testl	%r11d, %r11d
	sete	%bl
	movl	%ebx, %esi
	negl	%esi
	movl	%r11d, %ebp
	sarl	$31, %ebp
	subl	%r10d, %edi
	orl	%r11d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %esi
	movl	%ebp, %r13d
	leal	(%r11,%rbx), %r8d
	movl	%edi, %eax
	cltd
	idivl	%r8d
	notl	%r13d
	decl	%ebx
	movl	%r13d, %edi
	subl	%ebp, %edi
	andl	%r10d, %edi
	addl	%eax, %edi
	andl	%ebx, %edi
	leal	(%rdi,%rdi), %eax
	movq	%rax, -104(%rsp)                # 8-byte Spill
	subl	%eax, %r12d
	jle	.LBB19_17
# %bb.1:                                # %"for relu1_0_d_def__.s11.w.wi.preheader"
	movl	(%rcx), %eax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movl	16(%rcx), %edx
	movl	28(%rcx), %r8d
	movl	32(%rcx), %r14d
	movl	36(%rcx), %r15d
	movl	40(%rcx), %r9d
	xorl	%r11d, %ebp
	movq	%r9, %r11
	movl	44(%rcx), %eax
	addl	%r13d, %ebp
	andl	%r10d, %ebp
	addl	%ebp, %esi
	andl	%ebx, %esi
	movq	%rsi, -80(%rsp)                 # 8-byte Spill
	leal	(,%rsi,8), %ebp
	movl	%ebp, -112(%rsp)                # 4-byte Spill
	subl	%r14d, %r15d
	subl	%r14d, %r11d
	subl	%r14d, %eax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	-104(%rsp), %r14                # 8-byte Reload
	movl	%r14d, %eax
	subl	%r8d, %eax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
                                        # kill: def $r14d killed $r14d killed $r14 def $r14
	subl	8(%rcx), %r14d
	movl	%edx, %ebx
	shll	$4, %ebx
	subl	%edx, %ebx
	subl	%edx, %ebx
	xorl	%eax, %eax
	cmpl	$1, %r12d
	movq	-32(%rsp), %r9                  # 8-byte Reload
	sete	%al
	movl	$2, %esi
	subq	%rax, %rsi
	movq	%rsi, 128(%rsp)                 # 8-byte Spill
	leal	-1(%r9), %eax
	vmovd	%eax, %xmm0
	movl	%edx, %eax
	shll	$5, %eax
	movl	%eax, -120(%rsp)                # 4-byte Spill
	movslq	%ebp, %rsi
	movl	%r9d, %eax
	subl	%esi, %eax
	cmpl	$9, %eax
	movl	$8, %r12d
	cmovll	%eax, %r12d
	xorl	%r10d, %r10d
	movl	%eax, -60(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %r12d
	shll	$6, %edi
	shll	$5, %r8d
	subl	%r8d, %edi
	movq	%r15, %r8
	orl	$14, %edi
	imull	%edx, %edi
	movslq	4(%rcx), %rdx
	movl	20(%rcx), %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	movq	48(%rcx), %r15
	movq	64(%rcx), %rbp
	movq	80(%rcx), %r13
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	leal	(%rbx,%rcx,8), %eax
	movl	%eax, -108(%rsp)                # 4-byte Spill
	movq	%r14, (%rsp)                    # 8-byte Spill
	movslq	%r14d, %rax
	movq	%rax, %rbx
	movq	%rax, 104(%rsp)                 # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rdx, -72(%rsp)                 # 8-byte Spill
	imull	%edx, %eax
	leal	(%rax,%rcx,8), %eax
	movl	%eax, -116(%rsp)                # 4-byte Spill
	leal	(%rdi,%rcx,8), %r14d
	leal	8(,%rcx,8), %eax
	movl	%eax, -80(%rsp)                 # 4-byte Spill
	vmovd	-112(%rsp), %xmm1               # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm1, %ymm1
	vpor	.LCPI19_0(%rip), %ymm1, %ymm1
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm9
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leal	(%rsi,%rax), %eax
	movslq	%eax, %rdx
	leal	(%rsi,%r8), %eax
	movslq	%eax, %rdi
	movq	%rsi, 112(%rsp)                 # 8-byte Spill
	leal	(%rsi,%r11), %eax
	cltq
	movl	%r12d, %esi
	andl	$2147483640, %esi               # imm = 0x7FFFFFF8
	movq	%rsi, 56(%rsp)                  # 8-byte Spill
	addq	$-8, %rsi
	movq	%rsi, 48(%rsp)                  # 8-byte Spill
	movq	%rsi, %rbx
	shrq	$3, %rbx
	incq	%rbx
	movq	%rbx, %rsi
	movq	%rbx, 40(%rsp)                  # 8-byte Spill
	andq	$-2, %rbx
	negq	%rbx
	movq	%rbx, 32(%rsp)                  # 8-byte Spill
	vmovss	.LCPI19_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI19_2(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI19_1(%rip), %ymm3  # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI19_2(%rip), %ymm4  # ymm4 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	32(%r13), %rsi
	movq	%rsi, 24(%rsp)                  # 8-byte Spill
	movq	%r15, -88(%rsp)                 # 8-byte Spill
	leaq	32(%r15), %rsi
	movq	%rsi, 16(%rsp)                  # 8-byte Spill
	movq	%rbp, -96(%rsp)                 # 8-byte Spill
	leaq	32(%rbp), %rsi
	movq	%rsi, 8(%rsp)                   # 8-byte Spill
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	leaq	(%r13,%rax,4), %rax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movq	%rdi, -48(%rsp)                 # 8-byte Spill
	leaq	(%r13,%rdi,4), %rax
	movq	%rax, 88(%rsp)                  # 8-byte Spill
	movq	%rdx, -40(%rsp)                 # 8-byte Spill
	leaq	(%r13,%rdx,4), %rax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	movq	-104(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r15d
	movq	%r8, 72(%rsp)                   # 8-byte Spill
	movq	%r11, 64(%rsp)                  # 8-byte Spill
	jmp	.LBB19_2
	.p2align	4, 0x90
.LBB19_3:                               # %true_bb
                                        #   in Loop: Header=BB19_2 Depth=1
	movq	-104(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %edx
	imull	-124(%rsp), %edx                # 4-byte Folded Reload
	movq	-24(%rsp), %rax                 # 8-byte Reload
	addl	%edx, %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm9, %ymm6
	vextracti128	$1, %ymm6, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vpextrd	$1, %xmm5, %esi
	movslq	%esi, %rsi
	vmovss	(%r13,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm6, %eax
	vpextrd	$2, %xmm5, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%r13,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %esi
	cltq
	movslq	%esi, %rsi
	vinsertps	$48, (%r13,%rsi,4), %xmm7, %xmm10 # xmm10 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm6, %esi
	movslq	%esi, %rsi
	vmovss	(%r13,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%r13,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	cltq
	vpextrd	$3, %xmm6, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%r13,%rax,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%r13,%rsi,4), %xmm6, %xmm11 # xmm11 = xmm6[0,1,2],mem[0]
	leal	(%rdx,%r8), %eax
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %ymm7
	vpaddd	%ymm7, %ymm9, %ymm8
	vextracti128	$1, %ymm8, %xmm7
	vmovd	%xmm7, %eax
	cltq
	vpextrd	$1, %xmm7, %esi
	movslq	%esi, %rsi
	vmovss	(%r13,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm7, %eax
	vinsertps	$16, (%r13,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovd	%xmm8, %esi
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movslq	%esi, %rax
	vpextrd	$3, %xmm7, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%r13,%rsi,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	vpextrd	$1, %xmm8, %esi
	vmovss	(%r13,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	movslq	%esi, %rax
	vinsertps	$16, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$2, %xmm8, %eax
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrd	$3, %xmm8, %eax
	cltq
	vinsertps	$48, (%r13,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	addl	%r11d, %edx
	vmovd	%edx, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm9, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %edx
	vmovss	(%r13,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%edx, %rax
	vinsertps	$16, (%r13,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm6, %eax
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm6, %eax
	cltq
	vinsertps	$48, (%r13,%rax,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vmovd	%xmm5, %eax
	cltq
	vmovss	(%r13,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm5, %eax
	vinsertf128	$1, %xmm10, %ymm11, %ymm8
	cltq
	vinsertps	$16, (%r13,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertf128	$1, %xmm12, %ymm0, %ymm0
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%r13,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	movq	104(%rsp), %rax                 # 8-byte Reload
	addq	%r10, %rax
	imulq	-72(%rsp), %rax                 # 8-byte Folded Reload
	addq	112(%rsp), %rax                 # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vmulps	%ymm5, %ymm0, %ymm0
	vmaxps	%ymm3, %ymm0, %ymm0
	vmulps	%ymm0, %ymm8, %ymm0
	movq	-88(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%rdx,%rax,4), %ymm0, %ymm0
	movq	-16(%rsp), %rax                 # 8-byte Reload
	addl	%r10d, %eax
	imull	-120(%rsp), %eax                # 4-byte Folded Reload
	addl	-108(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-96(%rsp), %rdx                 # 8-byte Reload
	vfmadd213ps	(%rdx,%rax,4), %ymm4, %ymm0 # ymm0 = (ymm4 * ymm0) + mem
	vmovups	%ymm0, (%rdx,%rax,4)
.LBB19_16:                              # %after_bb
                                        #   in Loop: Header=BB19_2 Depth=1
	incq	%r10
	incl	%r15d
	movl	-116(%rsp), %eax                # 4-byte Reload
	addl	-72(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -116(%rsp)                # 4-byte Spill
	addl	-120(%rsp), %r14d               # 4-byte Folded Reload
	cmpq	128(%rsp), %r10                 # 8-byte Folded Reload
	je	.LBB19_17
.LBB19_2:                               # %"for relu1_0_d_def__.s11.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB19_12 Depth 2
                                        #     Child Loop BB19_8 Depth 2
	cmpl	%r9d, -80(%rsp)                 # 4-byte Folded Reload
	jle	.LBB19_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB19_2 Depth=1
	cmpl	$0, -60(%rsp)                   # 4-byte Folded Reload
	jle	.LBB19_16
# %bb.5:                                # %"for relu1_0_d_def__.s11.n.ni.preheader"
                                        #   in Loop: Header=BB19_2 Depth=1
	movl	%r15d, %eax
	sarl	$31, %eax
	andnl	%r15d, %eax, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rdi
	movslq	-116(%rsp), %rax                # 4-byte Folded Reload
	movq	%rax, -8(%rsp)                  # 8-byte Spill
	movslq	%r14d, %rbx
	cmpl	$7, %r12d
	movq	%rbx, 120(%rsp)                 # 8-byte Spill
	ja	.LBB19_9
# %bb.6:                                #   in Loop: Header=BB19_2 Depth=1
	xorl	%edx, %edx
	jmp	.LBB19_7
	.p2align	4, 0x90
.LBB19_9:                               # %vector.ph
                                        #   in Loop: Header=BB19_2 Depth=1
	cmpq	$0, 48(%rsp)                    # 8-byte Folded Reload
	movl	%r14d, -64(%rsp)                # 4-byte Spill
	je	.LBB19_10
# %bb.11:                               # %vector.body.preheader
                                        #   in Loop: Header=BB19_2 Depth=1
	movq	-56(%rsp), %rax                 # 8-byte Reload
	addq	%rdi, %rax
	movq	-48(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rdi), %rdx
	movq	-40(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rdi), %rsi
	movq	24(%rsp), %rbp                  # 8-byte Reload
	leaq	(%rbp,%rax,4), %r9
	leaq	(%rbp,%rdx,4), %r8
	leaq	(%rbp,%rsi,4), %rbp
	movq	16(%rsp), %rax                  # 8-byte Reload
	movq	-8(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rax,%rcx,4), %r11
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	(%rax,%rbx,4), %r14
	movq	32(%rsp), %rdx                  # 8-byte Reload
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB19_12:                              # %vector.body
                                        #   Parent Loop BB19_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-32(%r8,%rsi,4), %ymm0
	vmulps	-32(%r9,%rsi,4), %ymm0, %ymm0
	vmaxps	%ymm3, %ymm0, %ymm0
	vmulps	-32(%rbp,%rsi,4), %ymm0, %ymm0
	vmulps	-32(%r11,%rsi,4), %ymm0, %ymm0
	vfmadd213ps	-32(%r14,%rsi,4), %ymm4, %ymm0 # ymm0 = (ymm4 * ymm0) + mem
	vmovups	%ymm0, -32(%r14,%rsi,4)
	vmovups	(%r8,%rsi,4), %ymm0
	vmulps	(%r9,%rsi,4), %ymm0, %ymm0
	vmaxps	%ymm3, %ymm0, %ymm0
	vmulps	(%rbp,%rsi,4), %ymm0, %ymm0
	vmulps	(%r11,%rsi,4), %ymm0, %ymm0
	vfmadd213ps	(%r14,%rsi,4), %ymm4, %ymm0 # ymm0 = (ymm4 * ymm0) + mem
	vmovups	%ymm0, (%r14,%rsi,4)
	addq	$16, %rsi
	addq	$2, %rdx
	jne	.LBB19_12
# %bb.13:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB19_2 Depth=1
	testb	$1, 40(%rsp)                    # 1-byte Folded Reload
	movl	-64(%rsp), %r14d                # 4-byte Reload
	je	.LBB19_15
.LBB19_14:                              # %vector.body.epil
                                        #   in Loop: Header=BB19_2 Depth=1
	movq	-104(%rsp), %rax                # 8-byte Reload
	leal	(%rax,%r10), %ebp
	movq	-16(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%r10), %edx
	movl	%ebp, %eax
	sarl	$31, %eax
	andnl	%ebp, %eax, %eax
	movq	(%rsp), %rbp                    # 8-byte Reload
	addl	%r10d, %ebp
	imull	-120(%rsp), %edx                # 4-byte Folded Reload
	imull	-72(%rsp), %ebp                 # 4-byte Folded Reload
	addl	-108(%rsp), %edx                # 4-byte Folded Reload
	addl	-112(%rsp), %ebp                # 4-byte Folded Reload
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	movslq	%ebp, %rbp
	cltq
	movslq	%edx, %rdx
	addq	%rsi, %rdx
	addq	%rsi, %rbp
	addq	%rsi, %rax
	movq	-48(%rsp), %rsi                 # 8-byte Reload
	addq	%rax, %rsi
	vmovups	(%r13,%rsi,4), %ymm0
	movq	-40(%rsp), %rsi                 # 8-byte Reload
	addq	%rax, %rsi
	addq	-56(%rsp), %rax                 # 8-byte Folded Reload
	vmulps	(%r13,%rax,4), %ymm0, %ymm0
	vmaxps	%ymm3, %ymm0, %ymm0
	vmulps	(%r13,%rsi,4), %ymm0, %ymm0
	movq	-88(%rsp), %rax                 # 8-byte Reload
	vmulps	(%rax,%rbp,4), %ymm0, %ymm0
	movq	-96(%rsp), %rax                 # 8-byte Reload
	vfmadd213ps	(%rax,%rdx,4), %ymm4, %ymm0 # ymm0 = (ymm4 * ymm0) + mem
	vmovups	%ymm0, (%rax,%rdx,4)
.LBB19_15:                              # %middle.block
                                        #   in Loop: Header=BB19_2 Depth=1
	movq	56(%rsp), %rax                  # 8-byte Reload
	movq	%rax, %rdx
	cmpq	%r12, %rax
	movq	-32(%rsp), %r9                  # 8-byte Reload
	movq	72(%rsp), %r8                   # 8-byte Reload
	movq	64(%rsp), %r11                  # 8-byte Reload
	je	.LBB19_16
.LBB19_7:                               # %"for relu1_0_d_def__.s11.n.ni.preheader13"
                                        #   in Loop: Header=BB19_2 Depth=1
	movq	96(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdi,4), %rsi
	movq	88(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdi,4), %rbp
	movq	80(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdi,4), %rbx
	movq	-88(%rsp), %rax                 # 8-byte Reload
	movq	-8(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rax,%rcx,4), %rdi
	movq	-96(%rsp), %rax                 # 8-byte Reload
	movq	120(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %rax
	.p2align	4, 0x90
.LBB19_8:                               # %"for relu1_0_d_def__.s11.n.ni"
                                        #   Parent Loop BB19_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rdx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmulss	(%rsi,%rdx,4), %xmm0, %xmm0
	vmaxss	%xmm1, %xmm0, %xmm0
	vmulss	(%rbx,%rdx,4), %xmm0, %xmm0
	vmulss	(%rdi,%rdx,4), %xmm0, %xmm0
	vfmadd213ss	(%rax,%rdx,4), %xmm2, %xmm0 # xmm0 = (xmm2 * xmm0) + mem
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %r12
	jne	.LBB19_8
	jmp	.LBB19_16
.LBB19_10:                              #   in Loop: Header=BB19_2 Depth=1
	xorl	%esi, %esi
	testb	$1, 40(%rsp)                    # 1-byte Folded Reload
	movl	-64(%rsp), %r14d                # 4-byte Reload
	jne	.LBB19_14
	jmp	.LBB19_15
.LBB19_17:                              # %destructor_block
	xorl	%eax, %eax
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end19:
	.size	train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n, .Lfunc_end19-train_cost_model.par_for.relu1_0_d_def__.s11.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n
.LCPI20_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI20_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r12d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %ecx
	negl	%ecx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %edi
	orl	%r10d, %ecx
	movl	%edi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	movl	%ebp, %ecx
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%ecx
	decl	%ebx
	movl	%ecx, %esi
	subl	%ebp, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	subl	%eax, %r12d
	jle	.LBB20_13
# %bb.1:                                # %"for relu1_0_d_def__.s12.w.wi.preheader"
	movl	(%r13), %r8d
	movl	16(%r13), %r15d
	movl	28(%r13), %edx
	movl	32(%r13), %r14d
	movl	36(%r13), %eax
	movl	40(%r13), %edi
	xorl	%r10d, %ebp
	addl	%ecx, %ebp
	andl	%r9d, %ebp
	movq	%r8, %r9
	addl	%ebp, %r11d
	andl	%ebx, %r11d
	leal	(,%r11,8), %ecx
	subl	%r14d, %eax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	subl	%r14d, %edi
	movq	%rdi, -80(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rdi                # 8-byte Reload
	movl	%edi, %eax
	subl	%edx, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movl	%edi, %r14d
	subl	8(%r13), %r14d
	xorl	%eax, %eax
	cmpl	$1, %r12d
	sete	%al
	movl	$2, %edi
	subq	%rax, %rdi
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movl	%r15d, %eax
	shll	$5, %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	vmovd	%ecx, %xmm0
	leal	-1(%r8), %eax
	vmovd	%eax, %xmm1
	movslq	%ecx, %r8
	movl	%r9d, %eax
	subl	%r8d, %eax
	cmpl	$9, %eax
	movl	$8, %edi
	cmovll	%eax, %edi
	xorl	%r10d, %r10d
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %edi
	shll	$6, %esi
	shll	$5, %edx
	subl	%edx, %esi
	leal	(%r15,%r15,2), %eax
	leal	(%r15,%rax,4), %eax
	orl	$13, %esi
	imull	%r15d, %esi
	movslq	4(%r13), %rbx
	movl	20(%r13), %edx
	movl	%edx, -128(%rsp)                # 4-byte Spill
	movq	48(%r13), %rbp
	movq	64(%r13), %r12
	movq	80(%r13), %rdx
	movq	-88(%rsp), %r13                 # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	movslq	%r14d, %rax
	movq	%rax, %rcx
	movq	%rax, 8(%rsp)                   # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rbx, -40(%rsp)                 # 8-byte Spill
	imull	%ebx, %eax
	leal	(%rax,%r11,8), %r14d
	leal	(%rsi,%r11,8), %r15d
	leal	8(,%r11,8), %eax
	movq	%r13, %r11
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI20_0(%rip), %ymm0, %ymm0
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	leal	(%r8,%r13), %eax
	cltq
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movq	%r8, 16(%rsp)                   # 8-byte Spill
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%r8,%rax), %eax
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%rdi, -112(%rsp)                # 8-byte Spill
	movl	%edi, %r8d
	andl	$2147483616, %r8d               # imm = 0x7FFFFFE0
	vmovss	.LCPI20_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI20_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rdx), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	movq	%rbp, -48(%rsp)                 # 8-byte Spill
	leaq	96(%rbp), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%r12, -56(%rsp)                 # 8-byte Spill
	leaq	96(%r12), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r12d
	movq	%r9, -8(%rsp)                   # 8-byte Spill
	jmp	.LBB20_2
	.p2align	4, 0x90
.LBB20_3:                               # %true_bb
                                        #   in Loop: Header=BB20_2 Depth=1
	movq	-120(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edi
	imull	-128(%rsp), %edi                # 4-byte Folded Reload
	leal	(%rdi,%r11), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm4
	vextracti128	$1, %ymm4, %xmm3
	vmovd	%xmm3, %eax
	cltq
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %ecx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	addl	-80(%rsp), %edi                 # 4-byte Folded Reload
	vmovd	%edi, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm0, %ymm5, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	%r10, %rax
	imulq	-40(%rsp), %rax                 # 8-byte Folded Reload
	addq	16(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm5, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r10d, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rcx,%rax,4)
.LBB20_12:                              # %after_bb
                                        #   in Loop: Header=BB20_2 Depth=1
	incq	%r10
	incl	%r12d
	addl	-40(%rsp), %r14d                # 4-byte Folded Reload
	addl	-124(%rsp), %r15d               # 4-byte Folded Reload
	cmpq	32(%rsp), %r10                  # 8-byte Folded Reload
	je	.LBB20_13
.LBB20_2:                               # %"for relu1_0_d_def__.s12.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB20_10 Depth 2
                                        #     Child Loop BB20_8 Depth 2
	cmpl	%r9d, -92(%rsp)                 # 4-byte Folded Reload
	jle	.LBB20_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB20_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB20_12
# %bb.5:                                # %"for relu1_0_d_def__.s12.n.ni.preheader"
                                        #   in Loop: Header=BB20_2 Depth=1
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%r12d, %eax, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rbx
	movslq	%r14d, %rdi
	movslq	%r15d, %rbp
	cmpl	$32, -112(%rsp)                 # 4-byte Folded Reload
	jae	.LBB20_9
# %bb.6:                                #   in Loop: Header=BB20_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB20_7
	.p2align	4, 0x90
.LBB20_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB20_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leaq	(%rcx,%rbx), %rsi
	movq	%rbp, %rbx
	movq	-16(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %rcx
	leaq	(%rbp,%rsi,4), %r13
	movq	%rbx, %rbp
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %r11
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB20_10:                              # %vector.body
                                        #   Parent Loop BB20_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r13,%r9,4), %ymm3
	vmovups	-64(%r13,%r9,4), %ymm4
	vmovups	-32(%r13,%r9,4), %ymm5
	vmovups	(%r13,%r9,4), %ymm6
	vmulps	-96(%rcx,%r9,4), %ymm3, %ymm3
	vmulps	-64(%rcx,%r9,4), %ymm4, %ymm4
	vmulps	-32(%rcx,%r9,4), %ymm5, %ymm5
	vmulps	(%rcx,%r9,4), %ymm6, %ymm6
	vmulps	-96(%r11,%r9,4), %ymm3, %ymm3
	vmulps	-64(%r11,%r9,4), %ymm4, %ymm4
	vmulps	-32(%r11,%r9,4), %ymm5, %ymm5
	vmulps	(%r11,%r9,4), %ymm6, %ymm6
	vfmadd213ps	-96(%rax,%r9,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vfmadd213ps	-64(%rax,%r9,4), %ymm2, %ymm4 # ymm4 = (ymm2 * ymm4) + mem
	vfmadd213ps	-32(%rax,%r9,4), %ymm2, %ymm5 # ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rax,%r9,4), %ymm2, %ymm6 # ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm3, -96(%rax,%r9,4)
	vmovups	%ymm4, -64(%rax,%r9,4)
	vmovups	%ymm5, -32(%rax,%r9,4)
	vmovups	%ymm6, (%rax,%r9,4)
	addq	$32, %r9
	cmpq	%r9, %r8
	jne	.LBB20_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB20_2 Depth=1
	movq	%r8, %rsi
	cmpq	-112(%rsp), %r8                 # 8-byte Folded Reload
	movq	-8(%rsp), %r9                   # 8-byte Reload
	movq	-88(%rsp), %r11                 # 8-byte Reload
	movq	(%rsp), %rbx                    # 8-byte Reload
	je	.LBB20_12
.LBB20_7:                               # %"for relu1_0_d_def__.s12.n.ni.preheader24"
                                        #   in Loop: Header=BB20_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rbx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rbp, %r13
	movq	-64(%rsp), %rbp                 # 8-byte Reload
	addq	%rsi, %rbp
	addq	%rbx, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	addq	%rsi, %rdi
	movq	%r11, %rbx
	movq	-48(%rsp), %r11                 # 8-byte Reload
	leaq	(%r11,%rdi,4), %rdi
	movq	%rbx, %r11
	addq	%rsi, %r13
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r13,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB20_8:                               # %"for relu1_0_d_def__.s12.n.ni"
                                        #   Parent Loop BB20_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rcx,%rbx,4), %xmm3, %xmm3
	vmulss	(%rdi,%rbx,4), %xmm3, %xmm3
	vfmadd213ss	(%rsi,%rbx,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rax
	jne	.LBB20_8
	jmp	.LBB20_12
.LBB20_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end20:
	.size	train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n, .Lfunc_end20-train_cost_model.par_for.relu1_0_d_def__.s12.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n
.LCPI21_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI21_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r12d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %esi
	negl	%esi
	movl	%r10d, %ecx
	sarl	$31, %ecx
	subl	%r9d, %edi
	orl	%r10d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r11d
	movl	%ecx, %ebp
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%ebp
	decl	%ebx
	movl	%ebp, %esi
	subl	%ecx, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	subl	%eax, %r12d
	jle	.LBB21_13
# %bb.1:                                # %"for relu1_0_d_def__.s13.w.wi.preheader"
	movl	(%r13), %r8d
	movl	16(%r13), %r15d
	movl	28(%r13), %edx
	movl	32(%r13), %r14d
	movl	36(%r13), %eax
	movl	40(%r13), %edi
	xorl	%r10d, %ecx
	addl	%ebp, %ecx
	andl	%r9d, %ecx
	movq	%r8, %r9
	addl	%ecx, %r11d
	andl	%ebx, %r11d
	leal	(,%r11,8), %ecx
	subl	%r14d, %eax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	subl	%r14d, %edi
	movq	%rdi, -80(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rdi                # 8-byte Reload
	movl	%edi, %eax
	subl	%edx, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movl	%edi, %r14d
	subl	8(%r13), %r14d
	movq	%r15, %rbx
	leal	(%r15,%r15,2), %r15d
	shll	$2, %r15d
	xorl	%eax, %eax
	cmpl	$1, %r12d
	sete	%al
	movl	$2, %edi
	subq	%rax, %rdi
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movl	%ebx, %eax
	shll	$5, %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	vmovd	%ecx, %xmm0
	movslq	%ecx, %r8
	movl	%r9d, %eax
	subl	%r8d, %eax
	cmpl	$9, %eax
	movl	$8, %edi
	cmovll	%eax, %edi
	xorl	%r10d, %r10d
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %edi
	leal	-1(%r9), %eax
	vmovd	%eax, %xmm1
	shll	$6, %esi
	shll	$5, %edx
	subl	%edx, %esi
	orl	$12, %esi
	imull	%ebx, %esi
	movslq	4(%r13), %rbx
	movl	20(%r13), %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	movq	48(%r13), %rbp
	movq	64(%r13), %r12
	movq	80(%r13), %rdx
	movq	-88(%rsp), %r13                 # 8-byte Reload
	leal	(%r15,%r11,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	movslq	%r14d, %rax
	movq	%rax, %rcx
	movq	%rax, 8(%rsp)                   # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rbx, -40(%rsp)                 # 8-byte Spill
	imull	%ebx, %eax
	leal	(%rax,%r11,8), %r14d
	leal	(%rsi,%r11,8), %r15d
	leal	8(,%r11,8), %eax
	movq	%r13, %r11
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI21_0(%rip), %ymm0, %ymm0
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	leal	(%r8,%r13), %eax
	cltq
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movq	%r8, 16(%rsp)                   # 8-byte Spill
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%r8,%rax), %eax
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%rdi, -112(%rsp)                # 8-byte Spill
	movl	%edi, %r8d
	andl	$2147483616, %r8d               # imm = 0x7FFFFFE0
	vmovss	.LCPI21_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI21_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rdx), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	movq	%rbp, -48(%rsp)                 # 8-byte Spill
	leaq	96(%rbp), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%r12, -56(%rsp)                 # 8-byte Spill
	leaq	96(%r12), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r12d
	movq	%r9, -8(%rsp)                   # 8-byte Spill
	jmp	.LBB21_2
	.p2align	4, 0x90
.LBB21_3:                               # %true_bb
                                        #   in Loop: Header=BB21_2 Depth=1
	movq	-120(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edi
	imull	-128(%rsp), %edi                # 4-byte Folded Reload
	leal	(%rdi,%r11), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm4
	vextracti128	$1, %ymm4, %xmm3
	vmovd	%xmm3, %eax
	cltq
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %ecx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	addl	-80(%rsp), %edi                 # 4-byte Folded Reload
	vmovd	%edi, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm0, %ymm5, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	%r10, %rax
	imulq	-40(%rsp), %rax                 # 8-byte Folded Reload
	addq	16(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm5, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r10d, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rcx,%rax,4)
.LBB21_12:                              # %after_bb
                                        #   in Loop: Header=BB21_2 Depth=1
	incq	%r10
	incl	%r12d
	addl	-40(%rsp), %r14d                # 4-byte Folded Reload
	addl	-124(%rsp), %r15d               # 4-byte Folded Reload
	cmpq	32(%rsp), %r10                  # 8-byte Folded Reload
	je	.LBB21_13
.LBB21_2:                               # %"for relu1_0_d_def__.s13.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB21_10 Depth 2
                                        #     Child Loop BB21_8 Depth 2
	cmpl	%r9d, -92(%rsp)                 # 4-byte Folded Reload
	jle	.LBB21_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB21_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB21_12
# %bb.5:                                # %"for relu1_0_d_def__.s13.n.ni.preheader"
                                        #   in Loop: Header=BB21_2 Depth=1
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%r12d, %eax, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rbx
	movslq	%r14d, %rdi
	movslq	%r15d, %rbp
	cmpl	$32, -112(%rsp)                 # 4-byte Folded Reload
	jae	.LBB21_9
# %bb.6:                                #   in Loop: Header=BB21_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB21_7
	.p2align	4, 0x90
.LBB21_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB21_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leaq	(%rcx,%rbx), %rsi
	movq	%rbp, %rbx
	movq	-16(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %rcx
	leaq	(%rbp,%rsi,4), %r13
	movq	%rbx, %rbp
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %r11
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB21_10:                              # %vector.body
                                        #   Parent Loop BB21_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r13,%r9,4), %ymm3
	vmovups	-64(%r13,%r9,4), %ymm4
	vmovups	-32(%r13,%r9,4), %ymm5
	vmovups	(%r13,%r9,4), %ymm6
	vmulps	-96(%rcx,%r9,4), %ymm3, %ymm3
	vmulps	-64(%rcx,%r9,4), %ymm4, %ymm4
	vmulps	-32(%rcx,%r9,4), %ymm5, %ymm5
	vmulps	(%rcx,%r9,4), %ymm6, %ymm6
	vmulps	-96(%r11,%r9,4), %ymm3, %ymm3
	vmulps	-64(%r11,%r9,4), %ymm4, %ymm4
	vmulps	-32(%r11,%r9,4), %ymm5, %ymm5
	vmulps	(%r11,%r9,4), %ymm6, %ymm6
	vfmadd213ps	-96(%rax,%r9,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vfmadd213ps	-64(%rax,%r9,4), %ymm2, %ymm4 # ymm4 = (ymm2 * ymm4) + mem
	vfmadd213ps	-32(%rax,%r9,4), %ymm2, %ymm5 # ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rax,%r9,4), %ymm2, %ymm6 # ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm3, -96(%rax,%r9,4)
	vmovups	%ymm4, -64(%rax,%r9,4)
	vmovups	%ymm5, -32(%rax,%r9,4)
	vmovups	%ymm6, (%rax,%r9,4)
	addq	$32, %r9
	cmpq	%r9, %r8
	jne	.LBB21_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB21_2 Depth=1
	movq	%r8, %rsi
	cmpq	-112(%rsp), %r8                 # 8-byte Folded Reload
	movq	-8(%rsp), %r9                   # 8-byte Reload
	movq	-88(%rsp), %r11                 # 8-byte Reload
	movq	(%rsp), %rbx                    # 8-byte Reload
	je	.LBB21_12
.LBB21_7:                               # %"for relu1_0_d_def__.s13.n.ni.preheader24"
                                        #   in Loop: Header=BB21_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rbx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rbp, %r13
	movq	-64(%rsp), %rbp                 # 8-byte Reload
	addq	%rsi, %rbp
	addq	%rbx, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	addq	%rsi, %rdi
	movq	%r11, %rbx
	movq	-48(%rsp), %r11                 # 8-byte Reload
	leaq	(%r11,%rdi,4), %rdi
	movq	%rbx, %r11
	addq	%rsi, %r13
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r13,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB21_8:                               # %"for relu1_0_d_def__.s13.n.ni"
                                        #   Parent Loop BB21_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rcx,%rbx,4), %xmm3, %xmm3
	vmulss	(%rdi,%rbx,4), %xmm3, %xmm3
	vfmadd213ss	(%rsi,%rbx,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rax
	jne	.LBB21_8
	jmp	.LBB21_12
.LBB21_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end21:
	.size	train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n, .Lfunc_end21-train_cost_model.par_for.relu1_0_d_def__.s13.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n
.LCPI22_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI22_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r12d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %ecx
	negl	%ecx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %edi
	orl	%r10d, %ecx
	movl	%edi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	movl	%ebp, %ecx
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%ecx
	decl	%ebx
	movl	%ecx, %esi
	subl	%ebp, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	subl	%eax, %r12d
	jle	.LBB22_13
# %bb.1:                                # %"for relu1_0_d_def__.s14.w.wi.preheader"
	movl	(%r13), %r8d
	movl	16(%r13), %r15d
	movl	28(%r13), %edx
	movl	32(%r13), %r14d
	movl	36(%r13), %eax
	movl	40(%r13), %edi
	xorl	%r10d, %ebp
	addl	%ecx, %ebp
	andl	%r9d, %ebp
	movq	%r8, %r9
	addl	%ebp, %r11d
	andl	%ebx, %r11d
	leal	(,%r11,8), %ecx
	subl	%r14d, %eax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	subl	%r14d, %edi
	movq	%rdi, -80(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rdi                # 8-byte Reload
	movl	%edi, %eax
	subl	%edx, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movl	%edi, %r14d
	subl	8(%r13), %r14d
	xorl	%eax, %eax
	cmpl	$1, %r12d
	sete	%al
	movl	$2, %edi
	subq	%rax, %rdi
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movl	%r15d, %eax
	shll	$5, %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	vmovd	%ecx, %xmm0
	leal	-1(%r8), %eax
	vmovd	%eax, %xmm1
	movslq	%ecx, %r8
	movl	%r9d, %eax
	subl	%r8d, %eax
	cmpl	$9, %eax
	movl	$8, %edi
	cmovll	%eax, %edi
	xorl	%r10d, %r10d
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %edi
	shll	$6, %esi
	shll	$5, %edx
	subl	%edx, %esi
	leal	(%r15,%r15,4), %eax
	leal	(%r15,%rax,2), %eax
	orl	$11, %esi
	imull	%r15d, %esi
	movslq	4(%r13), %rbx
	movl	20(%r13), %edx
	movl	%edx, -128(%rsp)                # 4-byte Spill
	movq	48(%r13), %rbp
	movq	64(%r13), %r12
	movq	80(%r13), %rdx
	movq	-88(%rsp), %r13                 # 8-byte Reload
	leal	(%rax,%r11,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	movslq	%r14d, %rax
	movq	%rax, %rcx
	movq	%rax, 8(%rsp)                   # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rbx, -40(%rsp)                 # 8-byte Spill
	imull	%ebx, %eax
	leal	(%rax,%r11,8), %r14d
	leal	(%rsi,%r11,8), %r15d
	leal	8(,%r11,8), %eax
	movq	%r13, %r11
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI22_0(%rip), %ymm0, %ymm0
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	leal	(%r8,%r13), %eax
	cltq
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movq	%r8, 16(%rsp)                   # 8-byte Spill
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%r8,%rax), %eax
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%rdi, -112(%rsp)                # 8-byte Spill
	movl	%edi, %r8d
	andl	$2147483616, %r8d               # imm = 0x7FFFFFE0
	vmovss	.LCPI22_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI22_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rdx), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	movq	%rbp, -48(%rsp)                 # 8-byte Spill
	leaq	96(%rbp), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%r12, -56(%rsp)                 # 8-byte Spill
	leaq	96(%r12), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r12d
	movq	%r9, -8(%rsp)                   # 8-byte Spill
	jmp	.LBB22_2
	.p2align	4, 0x90
.LBB22_3:                               # %true_bb
                                        #   in Loop: Header=BB22_2 Depth=1
	movq	-120(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edi
	imull	-128(%rsp), %edi                # 4-byte Folded Reload
	leal	(%rdi,%r11), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm4
	vextracti128	$1, %ymm4, %xmm3
	vmovd	%xmm3, %eax
	cltq
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %ecx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	addl	-80(%rsp), %edi                 # 4-byte Folded Reload
	vmovd	%edi, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm0, %ymm5, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	%r10, %rax
	imulq	-40(%rsp), %rax                 # 8-byte Folded Reload
	addq	16(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm5, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r10d, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rcx,%rax,4)
.LBB22_12:                              # %after_bb
                                        #   in Loop: Header=BB22_2 Depth=1
	incq	%r10
	incl	%r12d
	addl	-40(%rsp), %r14d                # 4-byte Folded Reload
	addl	-124(%rsp), %r15d               # 4-byte Folded Reload
	cmpq	32(%rsp), %r10                  # 8-byte Folded Reload
	je	.LBB22_13
.LBB22_2:                               # %"for relu1_0_d_def__.s14.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB22_10 Depth 2
                                        #     Child Loop BB22_8 Depth 2
	cmpl	%r9d, -92(%rsp)                 # 4-byte Folded Reload
	jle	.LBB22_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB22_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB22_12
# %bb.5:                                # %"for relu1_0_d_def__.s14.n.ni.preheader"
                                        #   in Loop: Header=BB22_2 Depth=1
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%r12d, %eax, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rbx
	movslq	%r14d, %rdi
	movslq	%r15d, %rbp
	cmpl	$32, -112(%rsp)                 # 4-byte Folded Reload
	jae	.LBB22_9
# %bb.6:                                #   in Loop: Header=BB22_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB22_7
	.p2align	4, 0x90
.LBB22_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB22_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leaq	(%rcx,%rbx), %rsi
	movq	%rbp, %rbx
	movq	-16(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %rcx
	leaq	(%rbp,%rsi,4), %r13
	movq	%rbx, %rbp
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %r11
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB22_10:                              # %vector.body
                                        #   Parent Loop BB22_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r13,%r9,4), %ymm3
	vmovups	-64(%r13,%r9,4), %ymm4
	vmovups	-32(%r13,%r9,4), %ymm5
	vmovups	(%r13,%r9,4), %ymm6
	vmulps	-96(%rcx,%r9,4), %ymm3, %ymm3
	vmulps	-64(%rcx,%r9,4), %ymm4, %ymm4
	vmulps	-32(%rcx,%r9,4), %ymm5, %ymm5
	vmulps	(%rcx,%r9,4), %ymm6, %ymm6
	vmulps	-96(%r11,%r9,4), %ymm3, %ymm3
	vmulps	-64(%r11,%r9,4), %ymm4, %ymm4
	vmulps	-32(%r11,%r9,4), %ymm5, %ymm5
	vmulps	(%r11,%r9,4), %ymm6, %ymm6
	vfmadd213ps	-96(%rax,%r9,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vfmadd213ps	-64(%rax,%r9,4), %ymm2, %ymm4 # ymm4 = (ymm2 * ymm4) + mem
	vfmadd213ps	-32(%rax,%r9,4), %ymm2, %ymm5 # ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rax,%r9,4), %ymm2, %ymm6 # ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm3, -96(%rax,%r9,4)
	vmovups	%ymm4, -64(%rax,%r9,4)
	vmovups	%ymm5, -32(%rax,%r9,4)
	vmovups	%ymm6, (%rax,%r9,4)
	addq	$32, %r9
	cmpq	%r9, %r8
	jne	.LBB22_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB22_2 Depth=1
	movq	%r8, %rsi
	cmpq	-112(%rsp), %r8                 # 8-byte Folded Reload
	movq	-8(%rsp), %r9                   # 8-byte Reload
	movq	-88(%rsp), %r11                 # 8-byte Reload
	movq	(%rsp), %rbx                    # 8-byte Reload
	je	.LBB22_12
.LBB22_7:                               # %"for relu1_0_d_def__.s14.n.ni.preheader24"
                                        #   in Loop: Header=BB22_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rbx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rbp, %r13
	movq	-64(%rsp), %rbp                 # 8-byte Reload
	addq	%rsi, %rbp
	addq	%rbx, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	addq	%rsi, %rdi
	movq	%r11, %rbx
	movq	-48(%rsp), %r11                 # 8-byte Reload
	leaq	(%r11,%rdi,4), %rdi
	movq	%rbx, %r11
	addq	%rsi, %r13
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r13,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB22_8:                               # %"for relu1_0_d_def__.s14.n.ni"
                                        #   Parent Loop BB22_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rcx,%rbx,4), %xmm3, %xmm3
	vmulss	(%rdi,%rbx,4), %xmm3, %xmm3
	vfmadd213ss	(%rsi,%rbx,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rax
	jne	.LBB22_8
	jmp	.LBB22_12
.LBB22_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end22:
	.size	train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n, .Lfunc_end22-train_cost_model.par_for.relu1_0_d_def__.s14.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n
.LCPI23_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI23_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r12d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %esi
	negl	%esi
	movl	%r10d, %ecx
	sarl	$31, %ecx
	subl	%r9d, %edi
	orl	%r10d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r11d
	movl	%ecx, %ebp
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%ebp
	decl	%ebx
	movl	%ebp, %esi
	subl	%ecx, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	subl	%eax, %r12d
	jle	.LBB23_13
# %bb.1:                                # %"for relu1_0_d_def__.s15.w.wi.preheader"
	movl	(%r13), %r8d
	movl	16(%r13), %r15d
	movl	28(%r13), %edx
	movl	32(%r13), %r14d
	movl	36(%r13), %eax
	movl	40(%r13), %edi
	xorl	%r10d, %ecx
	addl	%ebp, %ecx
	andl	%r9d, %ecx
	movq	%r8, %r9
	addl	%ecx, %r11d
	andl	%ebx, %r11d
	leal	(,%r11,8), %ecx
	subl	%r14d, %eax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	subl	%r14d, %edi
	movq	%rdi, -80(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rdi                # 8-byte Reload
	movl	%edi, %eax
	subl	%edx, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movl	%edi, %r14d
	subl	8(%r13), %r14d
	movq	%r15, %rbx
	leal	(%r15,%r15,4), %r15d
	addl	%r15d, %r15d
	xorl	%eax, %eax
	cmpl	$1, %r12d
	sete	%al
	movl	$2, %edi
	subq	%rax, %rdi
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movl	%ebx, %eax
	shll	$5, %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	vmovd	%ecx, %xmm0
	movslq	%ecx, %r8
	movl	%r9d, %eax
	subl	%r8d, %eax
	cmpl	$9, %eax
	movl	$8, %edi
	cmovll	%eax, %edi
	xorl	%r10d, %r10d
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %edi
	leal	-1(%r9), %eax
	vmovd	%eax, %xmm1
	shll	$6, %esi
	shll	$5, %edx
	subl	%edx, %esi
	orl	$10, %esi
	imull	%ebx, %esi
	movslq	4(%r13), %rbx
	movl	20(%r13), %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	movq	48(%r13), %rbp
	movq	64(%r13), %r12
	movq	80(%r13), %rdx
	movq	-88(%rsp), %r13                 # 8-byte Reload
	leal	(%r15,%r11,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	movslq	%r14d, %rax
	movq	%rax, %rcx
	movq	%rax, 8(%rsp)                   # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rbx, -40(%rsp)                 # 8-byte Spill
	imull	%ebx, %eax
	leal	(%rax,%r11,8), %r14d
	leal	(%rsi,%r11,8), %r15d
	leal	8(,%r11,8), %eax
	movq	%r13, %r11
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI23_0(%rip), %ymm0, %ymm0
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	leal	(%r8,%r13), %eax
	cltq
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movq	%r8, 16(%rsp)                   # 8-byte Spill
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%r8,%rax), %eax
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%rdi, -112(%rsp)                # 8-byte Spill
	movl	%edi, %r8d
	andl	$2147483616, %r8d               # imm = 0x7FFFFFE0
	vmovss	.LCPI23_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI23_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rdx), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	movq	%rbp, -48(%rsp)                 # 8-byte Spill
	leaq	96(%rbp), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%r12, -56(%rsp)                 # 8-byte Spill
	leaq	96(%r12), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r12d
	movq	%r9, -8(%rsp)                   # 8-byte Spill
	jmp	.LBB23_2
	.p2align	4, 0x90
.LBB23_3:                               # %true_bb
                                        #   in Loop: Header=BB23_2 Depth=1
	movq	-120(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edi
	imull	-128(%rsp), %edi                # 4-byte Folded Reload
	leal	(%rdi,%r11), %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm4
	vextracti128	$1, %ymm4, %xmm3
	vmovd	%xmm3, %eax
	cltq
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %ecx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	addl	-80(%rsp), %edi                 # 4-byte Folded Reload
	vmovd	%edi, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm0, %ymm5, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	%r10, %rax
	imulq	-40(%rsp), %rax                 # 8-byte Folded Reload
	addq	16(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm5, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r10d, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rcx,%rax,4)
.LBB23_12:                              # %after_bb
                                        #   in Loop: Header=BB23_2 Depth=1
	incq	%r10
	incl	%r12d
	addl	-40(%rsp), %r14d                # 4-byte Folded Reload
	addl	-124(%rsp), %r15d               # 4-byte Folded Reload
	cmpq	32(%rsp), %r10                  # 8-byte Folded Reload
	je	.LBB23_13
.LBB23_2:                               # %"for relu1_0_d_def__.s15.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB23_10 Depth 2
                                        #     Child Loop BB23_8 Depth 2
	cmpl	%r9d, -92(%rsp)                 # 4-byte Folded Reload
	jle	.LBB23_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB23_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB23_12
# %bb.5:                                # %"for relu1_0_d_def__.s15.n.ni.preheader"
                                        #   in Loop: Header=BB23_2 Depth=1
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%r12d, %eax, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rbx
	movslq	%r14d, %rdi
	movslq	%r15d, %rbp
	cmpl	$32, -112(%rsp)                 # 4-byte Folded Reload
	jae	.LBB23_9
# %bb.6:                                #   in Loop: Header=BB23_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB23_7
	.p2align	4, 0x90
.LBB23_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB23_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leaq	(%rcx,%rbx), %rsi
	movq	%rbp, %rbx
	movq	-16(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %rcx
	leaq	(%rbp,%rsi,4), %r13
	movq	%rbx, %rbp
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %r11
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB23_10:                              # %vector.body
                                        #   Parent Loop BB23_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r13,%r9,4), %ymm3
	vmovups	-64(%r13,%r9,4), %ymm4
	vmovups	-32(%r13,%r9,4), %ymm5
	vmovups	(%r13,%r9,4), %ymm6
	vmulps	-96(%rcx,%r9,4), %ymm3, %ymm3
	vmulps	-64(%rcx,%r9,4), %ymm4, %ymm4
	vmulps	-32(%rcx,%r9,4), %ymm5, %ymm5
	vmulps	(%rcx,%r9,4), %ymm6, %ymm6
	vmulps	-96(%r11,%r9,4), %ymm3, %ymm3
	vmulps	-64(%r11,%r9,4), %ymm4, %ymm4
	vmulps	-32(%r11,%r9,4), %ymm5, %ymm5
	vmulps	(%r11,%r9,4), %ymm6, %ymm6
	vfmadd213ps	-96(%rax,%r9,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vfmadd213ps	-64(%rax,%r9,4), %ymm2, %ymm4 # ymm4 = (ymm2 * ymm4) + mem
	vfmadd213ps	-32(%rax,%r9,4), %ymm2, %ymm5 # ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rax,%r9,4), %ymm2, %ymm6 # ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm3, -96(%rax,%r9,4)
	vmovups	%ymm4, -64(%rax,%r9,4)
	vmovups	%ymm5, -32(%rax,%r9,4)
	vmovups	%ymm6, (%rax,%r9,4)
	addq	$32, %r9
	cmpq	%r9, %r8
	jne	.LBB23_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB23_2 Depth=1
	movq	%r8, %rsi
	cmpq	-112(%rsp), %r8                 # 8-byte Folded Reload
	movq	-8(%rsp), %r9                   # 8-byte Reload
	movq	-88(%rsp), %r11                 # 8-byte Reload
	movq	(%rsp), %rbx                    # 8-byte Reload
	je	.LBB23_12
.LBB23_7:                               # %"for relu1_0_d_def__.s15.n.ni.preheader24"
                                        #   in Loop: Header=BB23_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rbx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rbp, %r13
	movq	-64(%rsp), %rbp                 # 8-byte Reload
	addq	%rsi, %rbp
	addq	%rbx, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	addq	%rsi, %rdi
	movq	%r11, %rbx
	movq	-48(%rsp), %r11                 # 8-byte Reload
	leaq	(%r11,%rdi,4), %rdi
	movq	%rbx, %r11
	addq	%rsi, %r13
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r13,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB23_8:                               # %"for relu1_0_d_def__.s15.n.ni"
                                        #   Parent Loop BB23_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rcx,%rbx,4), %xmm3, %xmm3
	vmulss	(%rdi,%rbx,4), %xmm3, %xmm3
	vfmadd213ss	(%rsi,%rbx,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rax
	jne	.LBB23_8
	jmp	.LBB23_12
.LBB23_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end23:
	.size	train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n, .Lfunc_end23-train_cost_model.par_for.relu1_0_d_def__.s15.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n
.LCPI24_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI24_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r14d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %ecx
	negl	%ecx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %edi
	orl	%r10d, %ecx
	movl	%edi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	movl	%ebp, %ecx
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%ecx
	decl	%ebx
	movl	%ecx, %esi
	subl	%ebp, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	subl	%eax, %r14d
	jle	.LBB24_13
# %bb.1:                                # %"for relu1_0_d_def__.s16.w.wi.preheader"
	movl	(%r13), %r15d
	movl	16(%r13), %r12d
	movl	28(%r13), %r11d
	movl	32(%r13), %eax
	movl	36(%r13), %edi
	movl	40(%r13), %edx
	xorl	%r10d, %ebp
	addl	%ecx, %ebp
	andl	%r9d, %ebp
	movq	%r15, %r9
	addl	%ebp, %r8d
	andl	%ebx, %r8d
	leal	(,%r8,8), %ecx
	subl	%eax, %edi
	movq	%rdi, -80(%rsp)                 # 8-byte Spill
	subl	%eax, %edx
	movq	%rdx, -88(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rdx                # 8-byte Reload
	movl	%edx, %eax
	movl	%edx, %r15d
	subl	8(%r13), %r15d
	subl	%r11d, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, %r14d
	sete	%al
	movl	$2, %edi
	subq	%rax, %rdi
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movl	%r12d, %eax
	movq	%r12, %r14
	shll	$5, %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	vmovd	%ecx, %xmm0
	leal	-1(%r9), %eax
	vmovd	%eax, %xmm1
	movslq	%ecx, %r12
	movl	%r9d, %eax
	subl	%r12d, %eax
	cmpl	$9, %eax
	movl	$8, %edi
	cmovll	%eax, %edi
	xorl	%r10d, %r10d
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %edi
	shll	$6, %esi
	shll	$5, %r11d
	subl	%r11d, %esi
	movslq	4(%r13), %rbx
	movl	20(%r13), %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	movq	48(%r13), %rbp
	movq	64(%r13), %r11
	movq	80(%r13), %rdx
	orl	$9, %esi
	movq	%r14, %rax
	imull	%eax, %esi
	leal	(%r14,%r14,8), %eax
	leal	(%rax,%r8,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	movslq	%r15d, %rax
	movq	%rax, %rcx
	movq	%rax, 8(%rsp)                   # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rbx, -40(%rsp)                 # 8-byte Spill
	imull	%ebx, %eax
	leal	(%rax,%r8,8), %r14d
	leal	(%rsi,%r8,8), %r13d
	leal	8(,%r8,8), %eax
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI24_0(%rip), %ymm0, %ymm0
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%r12,%rax), %eax
	cltq
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movq	%r12, 16(%rsp)                  # 8-byte Spill
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leal	(%r12,%rax), %eax
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%rdi, -112(%rsp)                # 8-byte Spill
	movl	%edi, %r8d
	andl	$2147483616, %r8d               # imm = 0x7FFFFFE0
	vmovss	.LCPI24_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI24_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rdx), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	movq	%rbp, -48(%rsp)                 # 8-byte Spill
	leaq	96(%rbp), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%r11, -56(%rsp)                 # 8-byte Spill
	leaq	96(%r11), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r12d
	movq	%r9, -8(%rsp)                   # 8-byte Spill
	jmp	.LBB24_2
	.p2align	4, 0x90
.LBB24_3:                               # %true_bb
                                        #   in Loop: Header=BB24_2 Depth=1
	movq	-120(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edi
	imull	-128(%rsp), %edi                # 4-byte Folded Reload
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%edi, %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm4
	vextracti128	$1, %ymm4, %xmm3
	vmovd	%xmm3, %eax
	cltq
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %ecx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	addl	-88(%rsp), %edi                 # 4-byte Folded Reload
	vmovd	%edi, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm0, %ymm5, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	%r10, %rax
	imulq	-40(%rsp), %rax                 # 8-byte Folded Reload
	addq	16(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm5, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r10d, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rcx,%rax,4)
.LBB24_12:                              # %after_bb
                                        #   in Loop: Header=BB24_2 Depth=1
	incq	%r10
	incl	%r12d
	addl	-40(%rsp), %r14d                # 4-byte Folded Reload
	addl	-124(%rsp), %r13d               # 4-byte Folded Reload
	cmpq	32(%rsp), %r10                  # 8-byte Folded Reload
	je	.LBB24_13
.LBB24_2:                               # %"for relu1_0_d_def__.s16.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB24_10 Depth 2
                                        #     Child Loop BB24_8 Depth 2
	cmpl	%r9d, -92(%rsp)                 # 4-byte Folded Reload
	jle	.LBB24_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB24_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB24_12
# %bb.5:                                # %"for relu1_0_d_def__.s16.n.ni.preheader"
                                        #   in Loop: Header=BB24_2 Depth=1
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%r12d, %eax, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rbx
	movslq	%r14d, %rdi
	movslq	%r13d, %rbp
	cmpl	$32, -112(%rsp)                 # 4-byte Folded Reload
	jae	.LBB24_9
# %bb.6:                                #   in Loop: Header=BB24_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB24_7
	.p2align	4, 0x90
.LBB24_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB24_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leaq	(%rcx,%rbx), %rsi
	movq	%rbp, %rbx
	movq	-16(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %rcx
	leaq	(%rbp,%rsi,4), %r15
	movq	%rbx, %rbp
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %r11
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB24_10:                              # %vector.body
                                        #   Parent Loop BB24_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r15,%r9,4), %ymm3
	vmovups	-64(%r15,%r9,4), %ymm4
	vmovups	-32(%r15,%r9,4), %ymm5
	vmovups	(%r15,%r9,4), %ymm6
	vmulps	-96(%rcx,%r9,4), %ymm3, %ymm3
	vmulps	-64(%rcx,%r9,4), %ymm4, %ymm4
	vmulps	-32(%rcx,%r9,4), %ymm5, %ymm5
	vmulps	(%rcx,%r9,4), %ymm6, %ymm6
	vmulps	-96(%r11,%r9,4), %ymm3, %ymm3
	vmulps	-64(%r11,%r9,4), %ymm4, %ymm4
	vmulps	-32(%r11,%r9,4), %ymm5, %ymm5
	vmulps	(%r11,%r9,4), %ymm6, %ymm6
	vfmadd213ps	-96(%rax,%r9,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vfmadd213ps	-64(%rax,%r9,4), %ymm2, %ymm4 # ymm4 = (ymm2 * ymm4) + mem
	vfmadd213ps	-32(%rax,%r9,4), %ymm2, %ymm5 # ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rax,%r9,4), %ymm2, %ymm6 # ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm3, -96(%rax,%r9,4)
	vmovups	%ymm4, -64(%rax,%r9,4)
	vmovups	%ymm5, -32(%rax,%r9,4)
	vmovups	%ymm6, (%rax,%r9,4)
	addq	$32, %r9
	cmpq	%r9, %r8
	jne	.LBB24_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB24_2 Depth=1
	movq	%r8, %rsi
	cmpq	-112(%rsp), %r8                 # 8-byte Folded Reload
	movq	-8(%rsp), %r9                   # 8-byte Reload
	movq	(%rsp), %rbx                    # 8-byte Reload
	je	.LBB24_12
.LBB24_7:                               # %"for relu1_0_d_def__.s16.n.ni.preheader24"
                                        #   in Loop: Header=BB24_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rbx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rbp, %r15
	movq	-64(%rsp), %rbp                 # 8-byte Reload
	addq	%rsi, %rbp
	addq	%rbx, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	addq	%rsi, %rdi
	movq	-48(%rsp), %r11                 # 8-byte Reload
	leaq	(%r11,%rdi,4), %rdi
	addq	%rsi, %r15
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r15,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB24_8:                               # %"for relu1_0_d_def__.s16.n.ni"
                                        #   Parent Loop BB24_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rcx,%rbx,4), %xmm3, %xmm3
	vmulss	(%rdi,%rbx,4), %xmm3, %xmm3
	vfmadd213ss	(%rsi,%rbx,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rax
	jne	.LBB24_8
	jmp	.LBB24_12
.LBB24_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end24:
	.size	train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n, .Lfunc_end24-train_cost_model.par_for.relu1_0_d_def__.s16.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n
.LCPI25_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI25_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$56, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r14d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%esi, %esi
	testl	%r10d, %r10d
	sete	%sil
	movl	%esi, %ecx
	negl	%ecx
	movl	%r10d, %ebx
	sarl	$31, %ebx
	subl	%r9d, %edi
	orl	%r10d, %ecx
	movl	%edi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %ebp
	movl	%ebx, %r11d
	leal	(%r10,%rsi), %ecx
	movl	%edi, %eax
	cltd
	idivl	%ecx
	notl	%r11d
	decl	%esi
	movl	%r11d, %ecx
	subl	%ebx, %ecx
	andl	%r9d, %ecx
	addl	%eax, %ecx
	andl	%esi, %ecx
	leal	(%rcx,%rcx), %eax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	subl	%eax, %r14d
	jle	.LBB25_13
# %bb.1:                                # %"for relu1_0_d_def__.s17.w.wi.preheader"
	movl	(%r13), %r12d
	movslq	4(%r13), %rax
	movl	28(%r13), %edx
	movl	32(%r13), %edi
	movl	36(%r13), %r15d
	movl	40(%r13), %r8d
	xorl	%r10d, %ebx
	movq	%rax, %r10
	addl	%r11d, %ebx
	andl	%r9d, %ebx
	addl	%ebx, %ebp
	andl	%esi, %ebp
	movq	%rbp, -128(%rsp)                # 8-byte Spill
	leal	(,%rbp,8), %eax
	subl	%edi, %r15d
	subl	%edi, %r8d
	movq	-112(%rsp), %rsi                # 8-byte Reload
	movl	%esi, %edi
	movl	%esi, %r11d
	subl	8(%r13), %r11d
	subl	%edx, %edi
	movq	%rdi, 8(%rsp)                   # 8-byte Spill
	xorl	%edi, %edi
	cmpl	$1, %r14d
	sete	%dil
	movl	$2, %ebp
	subq	%rdi, %rbp
	movq	%rbp, 48(%rsp)                  # 8-byte Spill
	vmovd	%eax, %xmm0
	leal	-1(%r12), %edi
	vmovd	%edi, %xmm1
	movslq	%eax, %rdi
	movl	%r12d, %eax
	subl	%edi, %eax
	cmpl	$9, %eax
	movl	$8, %ebx
	cmovll	%eax, %ebx
	xorl	%r9d, %r9d
	movl	%eax, -88(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r9d, %ebx
	shll	$3, %ecx
	shll	$2, %edx
	subl	%edx, %ecx
	movl	16(%r13), %ebp
	movl	20(%r13), %eax
	movl	%eax, -116(%rsp)                # 4-byte Spill
	movq	48(%r13), %rax
	movq	%rax, -104(%rsp)                # 8-byte Spill
	movq	64(%r13), %r14
	movq	80(%r13), %rdx
	movslq	%r11d, %rax
	movq	%rax, %rsi
	movq	%rax, -8(%rsp)                  # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	imull	%r10d, %eax
	movq	-128(%rsp), %rsi                # 8-byte Reload
	leal	(%rax,%rsi,8), %r11d
	leal	(%rsi,%rbp), %eax
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	leal	8(,%rsi,8), %eax
	movl	%eax, -84(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI25_0(%rip), %ymm0, %ymm0
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	%r15, 24(%rsp)                  # 8-byte Spill
	leal	(%rdi,%r15), %eax
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%r8, 16(%rsp)                   # 8-byte Spill
	leal	(%rdi,%r8), %eax
	cltq
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	movq	%rbx, -128(%rsp)                # 8-byte Spill
	movl	%ebx, %r8d
	andl	$2147483616, %r8d               # imm = 0x7FFFFFE0
	orl	$1, %ecx
	imull	%ebp, %ecx
	movq	%rdi, (%rsp)                    # 8-byte Spill
	leal	(%rdi,%rcx,8), %r15d
	leal	(,%rbp,4), %eax
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	shll	$5, %ebp
	movq	%rbp, 40(%rsp)                  # 8-byte Spill
	vmovss	.LCPI25_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI25_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rdx), %rax
	movq	%rax, -40(%rsp)                 # 8-byte Spill
	movq	-104(%rsp), %rax                # 8-byte Reload
	leaq	96(%rax), %rax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	movq	%r14, -64(%rsp)                 # 8-byte Spill
	leaq	96(%r14), %rax
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	movq	-112(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r13d
	movq	%r12, -24(%rsp)                 # 8-byte Spill
	movq	%r10, -32(%rsp)                 # 8-byte Spill
	jmp	.LBB25_2
	.p2align	4, 0x90
.LBB25_3:                               # %true_bb
                                        #   in Loop: Header=BB25_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	addl	%r9d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %eax
	imull	-116(%rsp), %eax                # 4-byte Folded Reload
	movq	24(%rsp), %rcx                  # 8-byte Reload
	addl	%eax, %ecx
	vmovd	%ecx, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm4
	vextracti128	$1, %ymm4, %xmm3
	vmovd	%xmm3, %ecx
	movslq	%ecx, %rcx
	vpextrd	$1, %xmm3, %esi
	movslq	%esi, %rsi
	vmovss	(%rdx,%rcx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %ecx
	vinsertps	$16, (%rdx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %esi
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%esi, %rcx
	vpextrd	$3, %xmm3, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rdx,%rsi,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %esi
	vmovss	(%rdx,%rcx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%esi, %rcx
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	addl	16(%rsp), %eax                  # 4-byte Folded Reload
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm0, %ymm5, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm4 # xmm4 = xmm7[0,1,2],mem[0]
	movq	-8(%rsp), %rax                  # 8-byte Reload
	addq	%r9, %rax
	imulq	%r10, %rax
	addq	(%rsp), %rax                    # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm4, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	movq	-104(%rsp), %rcx                # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	8(%rsp), %rax                   # 8-byte Reload
	addl	%r9d, %eax
	imull	-96(%rsp), %eax                 # 4-byte Folded Reload
	addl	-92(%rsp), %eax                 # 4-byte Folded Reload
	cltq
	shlq	$5, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	vfmadd213ps	(%rcx,%rax), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovaps	%ymm3, (%rcx,%rax)
.LBB25_12:                              # %after_bb
                                        #   in Loop: Header=BB25_2 Depth=1
	incq	%r9
	incl	%r13d
	addl	%r10d, %r11d
	addl	40(%rsp), %r15d                 # 4-byte Folded Reload
	cmpq	48(%rsp), %r9                   # 8-byte Folded Reload
	je	.LBB25_13
.LBB25_2:                               # %"for relu1_0_d_def__.s17.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB25_10 Depth 2
                                        #     Child Loop BB25_8 Depth 2
	cmpl	%r12d, -84(%rsp)                # 4-byte Folded Reload
	jle	.LBB25_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB25_2 Depth=1
	cmpl	$0, -88(%rsp)                   # 4-byte Folded Reload
	jle	.LBB25_12
# %bb.5:                                # %"for relu1_0_d_def__.s17.n.ni.preheader"
                                        #   in Loop: Header=BB25_2 Depth=1
	movl	%r13d, %eax
	sarl	$31, %eax
	andnl	%r13d, %eax, %eax
	imull	-116(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rbx
	movslq	%r11d, %rax
	movslq	%r15d, %rbp
	cmpl	$32, -128(%rsp)                 # 4-byte Folded Reload
	movq	%rbp, 32(%rsp)                  # 8-byte Spill
	jae	.LBB25_9
# %bb.6:                                #   in Loop: Header=BB25_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB25_7
	.p2align	4, 0x90
.LBB25_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB25_2 Depth=1
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rbx), %rsi
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, -16(%rsp)                 # 8-byte Spill
	leaq	(%rcx,%rbx), %rdi
	movq	-40(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %r10
	leaq	(%rcx,%rdi,4), %r14
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdi
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rbp,4), %r12
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB25_10:                              # %vector.body
                                        #   Parent Loop BB25_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r14,%rbp,4), %ymm3
	vmovups	-64(%r14,%rbp,4), %ymm4
	vmovups	-32(%r14,%rbp,4), %ymm5
	vmovups	(%r14,%rbp,4), %ymm6
	vmulps	-96(%r10,%rbp,4), %ymm3, %ymm3
	vmulps	-64(%r10,%rbp,4), %ymm4, %ymm4
	vmulps	-32(%r10,%rbp,4), %ymm5, %ymm5
	vmulps	(%r10,%rbp,4), %ymm6, %ymm6
	vmulps	-96(%rdi,%rbp,4), %ymm3, %ymm3
	vmulps	-64(%rdi,%rbp,4), %ymm4, %ymm4
	vmulps	-32(%rdi,%rbp,4), %ymm5, %ymm5
	vmulps	(%rdi,%rbp,4), %ymm6, %ymm6
	vfmadd213ps	-96(%r12,%rbp,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vfmadd213ps	-64(%r12,%rbp,4), %ymm2, %ymm4 # ymm4 = (ymm2 * ymm4) + mem
	vfmadd213ps	-32(%r12,%rbp,4), %ymm2, %ymm5 # ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%r12,%rbp,4), %ymm2, %ymm6 # ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm3, -96(%r12,%rbp,4)
	vmovups	%ymm4, -64(%r12,%rbp,4)
	vmovups	%ymm5, -32(%r12,%rbp,4)
	vmovups	%ymm6, (%r12,%rbp,4)
	addq	$32, %rbp
	cmpq	%rbp, %r8
	jne	.LBB25_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB25_2 Depth=1
	movq	%r8, %rsi
	cmpq	-128(%rsp), %r8                 # 8-byte Folded Reload
	movq	-24(%rsp), %r12                 # 8-byte Reload
	movq	-32(%rsp), %r10                 # 8-byte Reload
	movq	-16(%rsp), %rbx                 # 8-byte Reload
	je	.LBB25_12
.LBB25_7:                               # %"for relu1_0_d_def__.s17.n.ni.preheader24"
                                        #   in Loop: Header=BB25_2 Depth=1
	movq	-128(%rsp), %rdi                # 8-byte Reload
	subq	%rsi, %rdi
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi), %rbp
	addq	%rbx, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rbx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	addq	%rsi, %rax
	movq	-104(%rsp), %r14                # 8-byte Reload
	leaq	(%r14,%rax,4), %rax
	movq	32(%rsp), %rbx                  # 8-byte Reload
	addq	%rsi, %rbx
	movq	-64(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rbx,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB25_8:                               # %"for relu1_0_d_def__.s17.n.ni"
                                        #   Parent Loop BB25_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rcx,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rbp,%rbx,4), %xmm3, %xmm3
	vmulss	(%rax,%rbx,4), %xmm3, %xmm3
	vfmadd213ss	(%rsi,%rbx,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rdi
	jne	.LBB25_8
	jmp	.LBB25_12
.LBB25_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$56, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end25:
	.size	train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n, .Lfunc_end25-train_cost_model.par_for.relu1_0_d_def__.s17.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n
.LCPI26_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI26_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r11d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %ecx
	negl	%ecx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %edi
	orl	%r10d, %ecx
	movl	%edi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r8d
	movl	%ebp, %ecx
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%ecx
	decl	%ebx
	movl	%ecx, %esi
	subl	%ebp, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	subl	%eax, %r11d
	movl	%r11d, -128(%rsp)               # 4-byte Spill
	jle	.LBB26_13
# %bb.1:                                # %"for relu1_0_d_def__.s18.w.wi.preheader"
	movl	(%r13), %r11d
	movl	16(%r13), %r12d
	movl	28(%r13), %edx
	movl	32(%r13), %r14d
	movl	36(%r13), %eax
	movl	40(%r13), %edi
	xorl	%r10d, %ebp
	addl	%ecx, %ebp
	andl	%r9d, %ebp
	movq	%r11, %r9
	addl	%ebp, %r8d
	andl	%ebx, %r8d
	leal	(,%r8,8), %ecx
	subl	%r14d, %eax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	subl	%r14d, %edi
	movq	%rdi, -88(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rdi                # 8-byte Reload
	movl	%edi, %eax
	subl	%edx, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movl	%edi, %r14d
	subl	8(%r13), %r14d
	leal	(,%r12,8), %r15d
	subl	%r12d, %r15d
	xorl	%eax, %eax
	cmpl	$1, -128(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	$2, %edi
	subq	%rax, %rdi
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movl	%r12d, %eax
	shll	$5, %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	vmovd	%ecx, %xmm0
	movslq	%ecx, %r11
	movl	%r9d, %eax
	subl	%r11d, %eax
	cmpl	$9, %eax
	movl	$8, %edi
	cmovll	%eax, %edi
	xorl	%r10d, %r10d
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %edi
	leal	-1(%r9), %eax
	vmovd	%eax, %xmm1
	shll	$6, %esi
	shll	$5, %edx
	subl	%edx, %esi
	orl	$7, %esi
	imull	%r12d, %esi
	movslq	4(%r13), %rbx
	movl	20(%r13), %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	movq	48(%r13), %rbp
	movq	64(%r13), %r12
	movq	80(%r13), %rdx
	leal	(%r15,%r8,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	movslq	%r14d, %rax
	movq	%rax, %rcx
	movq	%rax, 8(%rsp)                   # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rbx, -40(%rsp)                 # 8-byte Spill
	imull	%ebx, %eax
	leal	(%rax,%r8,8), %r14d
	leal	(%rsi,%r8,8), %r15d
	leal	8(,%r8,8), %eax
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI26_0(%rip), %ymm0, %ymm0
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%r11,%rax), %eax
	cltq
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movq	%r11, 16(%rsp)                  # 8-byte Spill
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leal	(%r11,%rax), %eax
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%rdi, -112(%rsp)                # 8-byte Spill
	movl	%edi, %r8d
	andl	$2147483616, %r8d               # imm = 0x7FFFFFE0
	vmovss	.LCPI26_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI26_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rdx), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	movq	%rbp, -48(%rsp)                 # 8-byte Spill
	leaq	96(%rbp), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%r12, -56(%rsp)                 # 8-byte Spill
	leaq	96(%r12), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r12d
	movq	%r9, -8(%rsp)                   # 8-byte Spill
	jmp	.LBB26_2
	.p2align	4, 0x90
.LBB26_3:                               # %true_bb
                                        #   in Loop: Header=BB26_2 Depth=1
	movq	-120(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edi
	imull	-124(%rsp), %edi                # 4-byte Folded Reload
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%edi, %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm4
	vextracti128	$1, %ymm4, %xmm3
	vmovd	%xmm3, %eax
	cltq
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %ecx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	addl	-88(%rsp), %edi                 # 4-byte Folded Reload
	vmovd	%edi, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm0, %ymm5, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	%r10, %rax
	imulq	-40(%rsp), %rax                 # 8-byte Folded Reload
	addq	16(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm5, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r10d, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rcx,%rax,4)
.LBB26_12:                              # %after_bb
                                        #   in Loop: Header=BB26_2 Depth=1
	incq	%r10
	incl	%r12d
	addl	-40(%rsp), %r14d                # 4-byte Folded Reload
	addl	-128(%rsp), %r15d               # 4-byte Folded Reload
	cmpq	32(%rsp), %r10                  # 8-byte Folded Reload
	je	.LBB26_13
.LBB26_2:                               # %"for relu1_0_d_def__.s18.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB26_10 Depth 2
                                        #     Child Loop BB26_8 Depth 2
	cmpl	%r9d, -92(%rsp)                 # 4-byte Folded Reload
	jle	.LBB26_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB26_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB26_12
# %bb.5:                                # %"for relu1_0_d_def__.s18.n.ni.preheader"
                                        #   in Loop: Header=BB26_2 Depth=1
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%r12d, %eax, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rbx
	movslq	%r14d, %rdi
	movslq	%r15d, %rbp
	cmpl	$32, -112(%rsp)                 # 4-byte Folded Reload
	jae	.LBB26_9
# %bb.6:                                #   in Loop: Header=BB26_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB26_7
	.p2align	4, 0x90
.LBB26_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB26_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leaq	(%rcx,%rbx), %rsi
	movq	%rbp, %rbx
	movq	-16(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %rcx
	leaq	(%rbp,%rsi,4), %r13
	movq	%rbx, %rbp
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %r11
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB26_10:                              # %vector.body
                                        #   Parent Loop BB26_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r13,%r9,4), %ymm3
	vmovups	-64(%r13,%r9,4), %ymm4
	vmovups	-32(%r13,%r9,4), %ymm5
	vmovups	(%r13,%r9,4), %ymm6
	vmulps	-96(%rcx,%r9,4), %ymm3, %ymm3
	vmulps	-64(%rcx,%r9,4), %ymm4, %ymm4
	vmulps	-32(%rcx,%r9,4), %ymm5, %ymm5
	vmulps	(%rcx,%r9,4), %ymm6, %ymm6
	vmulps	-96(%r11,%r9,4), %ymm3, %ymm3
	vmulps	-64(%r11,%r9,4), %ymm4, %ymm4
	vmulps	-32(%r11,%r9,4), %ymm5, %ymm5
	vmulps	(%r11,%r9,4), %ymm6, %ymm6
	vfmadd213ps	-96(%rax,%r9,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vfmadd213ps	-64(%rax,%r9,4), %ymm2, %ymm4 # ymm4 = (ymm2 * ymm4) + mem
	vfmadd213ps	-32(%rax,%r9,4), %ymm2, %ymm5 # ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rax,%r9,4), %ymm2, %ymm6 # ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm3, -96(%rax,%r9,4)
	vmovups	%ymm4, -64(%rax,%r9,4)
	vmovups	%ymm5, -32(%rax,%r9,4)
	vmovups	%ymm6, (%rax,%r9,4)
	addq	$32, %r9
	cmpq	%r9, %r8
	jne	.LBB26_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB26_2 Depth=1
	movq	%r8, %rsi
	cmpq	-112(%rsp), %r8                 # 8-byte Folded Reload
	movq	-8(%rsp), %r9                   # 8-byte Reload
	movq	(%rsp), %rbx                    # 8-byte Reload
	je	.LBB26_12
.LBB26_7:                               # %"for relu1_0_d_def__.s18.n.ni.preheader24"
                                        #   in Loop: Header=BB26_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rbx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rbp, %r13
	movq	-64(%rsp), %rbp                 # 8-byte Reload
	addq	%rsi, %rbp
	addq	%rbx, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	addq	%rsi, %rdi
	movq	-48(%rsp), %r11                 # 8-byte Reload
	leaq	(%r11,%rdi,4), %rdi
	addq	%rsi, %r13
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r13,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB26_8:                               # %"for relu1_0_d_def__.s18.n.ni"
                                        #   Parent Loop BB26_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rcx,%rbx,4), %xmm3, %xmm3
	vmulss	(%rdi,%rbx,4), %xmm3, %xmm3
	vfmadd213ss	(%rsi,%rbx,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rax
	jne	.LBB26_8
	jmp	.LBB26_12
.LBB26_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end26:
	.size	train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n, .Lfunc_end26-train_cost_model.par_for.relu1_0_d_def__.s18.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n
.LCPI27_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI27_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r8d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %esi
	negl	%esi
	movl	%r10d, %ecx
	sarl	$31, %ecx
	subl	%r9d, %edi
	orl	%r10d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r11d
	movl	%ecx, %ebp
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%ebp
	decl	%ebx
	movl	%ebp, %esi
	subl	%ecx, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	subl	%eax, %r8d
	jle	.LBB27_13
# %bb.1:                                # %"for relu1_0_d_def__.s19.w.wi.preheader"
	movl	(%r13), %r12d
	movl	16(%r13), %r15d
	movl	28(%r13), %edx
	movslq	32(%r13), %rax
	movl	36(%r13), %edi
	xorl	%r10d, %ecx
	addl	%ebp, %ecx
	andl	%r9d, %ecx
	movq	%rax, %r9
	addl	%ecx, %r11d
	andl	%ebx, %r11d
	leal	(,%r11,8), %ecx
	subl	%r9d, %edi
	movq	%rdi, -88(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rdi                # 8-byte Reload
	movl	%edi, %eax
	movl	%edi, %r14d
	subl	8(%r13), %r14d
	subl	%edx, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	%r15, %rdi
	leal	(%r15,%r15,2), %eax
	addl	%eax, %eax
	xorl	%ebp, %ebp
	cmpl	$1, %r8d
	sete	%bpl
	movl	$2, %ebx
	subq	%rbp, %rbx
	movq	%rbx, 32(%rsp)                  # 8-byte Spill
	movl	%edi, %ebp
	shll	$5, %ebp
	movl	%ebp, -124(%rsp)                # 4-byte Spill
	vmovd	%ecx, %xmm0
	leal	-1(%r12), %ebp
	vmovd	%ebp, %xmm1
	movslq	%ecx, %r15
	movl	%r12d, %ebp
	subl	%r15d, %ebp
	cmpl	$9, %ebp
	movl	$8, %r8d
	cmovll	%ebp, %r8d
	xorl	%r10d, %r10d
	movl	%ebp, -96(%rsp)                 # 4-byte Spill
	testl	%ebp, %ebp
	cmovlel	%r10d, %r8d
	shll	$6, %esi
	shll	$5, %edx
	subl	%edx, %esi
	orl	$6, %esi
	imull	%edi, %esi
	movslq	4(%r13), %rbx
	movl	20(%r13), %edx
	movl	%edx, -128(%rsp)                # 4-byte Spill
	movq	40(%r13), %rbp
	movq	56(%r13), %rcx
	movq	72(%r13), %rdx
	leal	(%rax,%r11,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI27_0(%rip), %ymm0, %ymm0
	movslq	%r14d, %rax
	movq	%rax, %rdi
	movq	%rax, 8(%rsp)                   # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rbx, -48(%rsp)                 # 8-byte Spill
	imull	%ebx, %eax
	leal	(%rax,%r11,8), %r14d
	leal	(%rsi,%r11,8), %r13d
	leal	8(,%r11,8), %eax
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leal	(%r15,%rax), %eax
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%r8, -112(%rsp)                 # 8-byte Spill
                                        # kill: def $r8d killed $r8d killed $r8 def $r8
	andl	$2147483616, %r8d               # imm = 0x7FFFFFE0
	movq	%r15, 16(%rsp)                  # 8-byte Spill
	movq	%r15, %r11
	movq	%r9, %r15
	subq	%r9, %r11
	movq	%r11, -80(%rsp)                 # 8-byte Spill
	vmovss	.LCPI27_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI27_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rdx), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%rbp, -56(%rsp)                 # 8-byte Spill
	leaq	96(%rbp), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	%rcx, -64(%rsp)                 # 8-byte Spill
	leaq	96(%rcx), %rax
	movq	%rax, -40(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r11d
	movq	%r12, -8(%rsp)                  # 8-byte Spill
	movq	%r9, -16(%rsp)                  # 8-byte Spill
	jmp	.LBB27_2
	.p2align	4, 0x90
.LBB27_3:                               # %true_bb
                                        #   in Loop: Header=BB27_2 Depth=1
	movq	-120(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edi
	imull	-128(%rsp), %edi                # 4-byte Folded Reload
	movl	%edi, %eax
	subl	%r15d, %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm4
	vextracti128	$1, %ymm4, %xmm3
	vmovd	%xmm3, %eax
	cltq
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %ecx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	addl	-88(%rsp), %edi                 # 4-byte Folded Reload
	vmovd	%edi, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm0, %ymm5, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	%r10, %rax
	imulq	-48(%rsp), %rax                 # 8-byte Folded Reload
	addq	16(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm5, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r10d, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rcx,%rax,4)
.LBB27_12:                              # %after_bb
                                        #   in Loop: Header=BB27_2 Depth=1
	incq	%r10
	incl	%r11d
	addl	-48(%rsp), %r14d                # 4-byte Folded Reload
	addl	-124(%rsp), %r13d               # 4-byte Folded Reload
	cmpq	32(%rsp), %r10                  # 8-byte Folded Reload
	je	.LBB27_13
.LBB27_2:                               # %"for relu1_0_d_def__.s19.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB27_10 Depth 2
                                        #     Child Loop BB27_8 Depth 2
	cmpl	%r12d, -92(%rsp)                # 4-byte Folded Reload
	jle	.LBB27_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB27_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB27_12
# %bb.5:                                # %"for relu1_0_d_def__.s19.n.ni.preheader"
                                        #   in Loop: Header=BB27_2 Depth=1
	movl	%r11d, %eax
	sarl	$31, %eax
	andnl	%r11d, %eax, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rbx
	movslq	%r14d, %rdi
	movslq	%r13d, %rbp
	cmpl	$32, -112(%rsp)                 # 4-byte Folded Reload
	jae	.LBB27_9
# %bb.6:                                #   in Loop: Header=BB27_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB27_7
	.p2align	4, 0x90
.LBB27_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB27_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leaq	(%rcx,%rbx), %rsi
	movq	%rbp, %rbx
	movq	-24(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %rcx
	leaq	(%rbp,%rsi,4), %r15
	movq	%rbx, %rbp
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %r12
	movq	-40(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB27_10:                              # %vector.body
                                        #   Parent Loop BB27_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r15,%r9,4), %ymm3
	vmovups	-64(%r15,%r9,4), %ymm4
	vmovups	-32(%r15,%r9,4), %ymm5
	vmovups	(%r15,%r9,4), %ymm6
	vmulps	-96(%rcx,%r9,4), %ymm3, %ymm3
	vmulps	-64(%rcx,%r9,4), %ymm4, %ymm4
	vmulps	-32(%rcx,%r9,4), %ymm5, %ymm5
	vmulps	(%rcx,%r9,4), %ymm6, %ymm6
	vmulps	-96(%r12,%r9,4), %ymm3, %ymm3
	vmulps	-64(%r12,%r9,4), %ymm4, %ymm4
	vmulps	-32(%r12,%r9,4), %ymm5, %ymm5
	vmulps	(%r12,%r9,4), %ymm6, %ymm6
	vfmadd213ps	-96(%rax,%r9,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vfmadd213ps	-64(%rax,%r9,4), %ymm2, %ymm4 # ymm4 = (ymm2 * ymm4) + mem
	vfmadd213ps	-32(%rax,%r9,4), %ymm2, %ymm5 # ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rax,%r9,4), %ymm2, %ymm6 # ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm3, -96(%rax,%r9,4)
	vmovups	%ymm4, -64(%rax,%r9,4)
	vmovups	%ymm5, -32(%rax,%r9,4)
	vmovups	%ymm6, (%rax,%r9,4)
	addq	$32, %r9
	cmpq	%r9, %r8
	jne	.LBB27_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB27_2 Depth=1
	movq	%r8, %rsi
	cmpq	-112(%rsp), %r8                 # 8-byte Folded Reload
	movq	-8(%rsp), %r12                  # 8-byte Reload
	movq	-16(%rsp), %r15                 # 8-byte Reload
	movq	(%rsp), %rbx                    # 8-byte Reload
	je	.LBB27_12
.LBB27_7:                               # %"for relu1_0_d_def__.s19.n.ni.preheader24"
                                        #   in Loop: Header=BB27_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rbx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rbp, %r9
	movq	-80(%rsp), %rbp                 # 8-byte Reload
	addq	%rsi, %rbp
	addq	%rbx, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	addq	%rsi, %rdi
	movq	%r15, %rbx
	movq	-56(%rsp), %r15                 # 8-byte Reload
	leaq	(%r15,%rdi,4), %rdi
	movq	%rbx, %r15
	addq	%rsi, %r9
	movq	-64(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r9,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB27_8:                               # %"for relu1_0_d_def__.s19.n.ni"
                                        #   Parent Loop BB27_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rcx,%rbx,4), %xmm3, %xmm3
	vmulss	(%rdi,%rbx,4), %xmm3, %xmm3
	vfmadd213ss	(%rsi,%rbx,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rax
	jne	.LBB27_8
	jmp	.LBB27_12
.LBB27_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end27:
	.size	train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n, .Lfunc_end27-train_cost_model.par_for.relu1_0_d_def__.s19.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n
.LCPI28_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI28_1:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$40, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r8d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %ecx
	negl	%ecx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %edi
	orl	%r10d, %ecx
	movl	%edi, %eax
	cltd
	idivl	%ecx
	movl	%edx, %r11d
	movl	%ebp, %ecx
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%ecx
	decl	%ebx
	movl	%ecx, %esi
	subl	%ebp, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	subl	%eax, %r8d
	jle	.LBB28_13
# %bb.1:                                # %"for relu1_0_d_def__.s20.w.wi.preheader"
	movl	(%r13), %r12d
	movl	16(%r13), %r15d
	movl	28(%r13), %edx
	movslq	32(%r13), %rax
	movl	36(%r13), %edi
	xorl	%r10d, %ebp
	addl	%ecx, %ebp
	andl	%r9d, %ebp
	movq	%rax, %r9
	addl	%ebp, %r11d
	andl	%ebx, %r11d
	leal	(,%r11,8), %eax
	subl	%r9d, %edi
	movq	%rdi, -88(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rdi                # 8-byte Reload
	movl	%edi, %ecx
	subl	%edx, %ecx
	movq	%rcx, 24(%rsp)                  # 8-byte Spill
	movl	%edi, %r14d
	subl	8(%r13), %r14d
	xorl	%ecx, %ecx
	cmpl	$1, %r8d
	sete	%cl
	movl	$2, %ebp
	subq	%rcx, %rbp
	movq	%rbp, 32(%rsp)                  # 8-byte Spill
	movl	%r15d, %ecx
	movq	%r15, %rdi
	shll	$5, %ecx
	movl	%ecx, -124(%rsp)                # 4-byte Spill
	vmovd	%eax, %xmm0
	movslq	%eax, %r15
	movl	%r12d, %eax
	subl	%r15d, %eax
	cmpl	$9, %eax
	movl	$8, %r8d
	cmovll	%eax, %r8d
	xorl	%r10d, %r10d
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %r8d
	leal	-1(%r12), %eax
	vmovd	%eax, %xmm1
	shll	$6, %esi
	shll	$5, %edx
	subl	%edx, %esi
	movslq	4(%r13), %rbx
	movl	20(%r13), %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	movq	40(%r13), %rbp
	movq	56(%r13), %rcx
	movq	72(%r13), %rdx
	orl	$5, %esi
	movq	%rdi, %rax
	imull	%eax, %esi
	leal	(%rdi,%rdi,4), %eax
	leal	(%rax,%r11,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI28_0(%rip), %ymm0, %ymm0
	movslq	%r14d, %rax
	movq	%rax, %rdi
	movq	%rax, 8(%rsp)                   # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rbx, -48(%rsp)                 # 8-byte Spill
	imull	%ebx, %eax
	leal	(%rax,%r11,8), %r14d
	leal	(%rsi,%r11,8), %r13d
	leal	8(,%r11,8), %eax
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm0
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leal	(%r15,%rax), %eax
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%r8, -112(%rsp)                 # 8-byte Spill
                                        # kill: def $r8d killed $r8d killed $r8 def $r8
	andl	$2147483616, %r8d               # imm = 0x7FFFFFE0
	movq	%r15, 16(%rsp)                  # 8-byte Spill
	movq	%r15, %r11
	movq	%r9, %r15
	subq	%r9, %r11
	movq	%r11, -80(%rsp)                 # 8-byte Spill
	vmovss	.LCPI28_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI28_1(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	leaq	96(%rdx), %rax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%rbp, -56(%rsp)                 # 8-byte Spill
	leaq	96(%rbp), %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	%rcx, -64(%rsp)                 # 8-byte Spill
	leaq	96(%rcx), %rax
	movq	%rax, -40(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r11d
	movq	%r12, -8(%rsp)                  # 8-byte Spill
	movq	%r9, -16(%rsp)                  # 8-byte Spill
	jmp	.LBB28_2
	.p2align	4, 0x90
.LBB28_3:                               # %true_bb
                                        #   in Loop: Header=BB28_2 Depth=1
	movq	-120(%rsp), %rax                # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edi
	imull	-128(%rsp), %edi                # 4-byte Folded Reload
	movl	%edi, %eax
	subl	%r15d, %eax
	vmovd	%eax, %xmm3
	vpbroadcastd	%xmm3, %ymm3
	vpaddd	%ymm0, %ymm3, %ymm4
	vextracti128	$1, %ymm4, %xmm3
	vmovd	%xmm3, %eax
	cltq
	vpextrd	$1, %xmm3, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm3, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm4, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm3, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm4, %ecx
	vmovss	(%rdx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	addl	-88(%rsp), %edi                 # 4-byte Folded Reload
	vmovd	%edi, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm0, %ymm5, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%rdx,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%rdx,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%rdx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%rdx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%rdx,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	8(%rsp), %rax                   # 8-byte Reload
	addq	%r10, %rax
	imulq	-48(%rsp), %rax                 # 8-byte Folded Reload
	addq	16(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm5, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm3, %ymm3
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r10d, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	vfmadd213ps	(%rcx,%rax,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vmovups	%ymm3, (%rcx,%rax,4)
.LBB28_12:                              # %after_bb
                                        #   in Loop: Header=BB28_2 Depth=1
	incq	%r10
	incl	%r11d
	addl	-48(%rsp), %r14d                # 4-byte Folded Reload
	addl	-124(%rsp), %r13d               # 4-byte Folded Reload
	cmpq	32(%rsp), %r10                  # 8-byte Folded Reload
	je	.LBB28_13
.LBB28_2:                               # %"for relu1_0_d_def__.s20.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB28_10 Depth 2
                                        #     Child Loop BB28_8 Depth 2
	cmpl	%r12d, -92(%rsp)                # 4-byte Folded Reload
	jle	.LBB28_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB28_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB28_12
# %bb.5:                                # %"for relu1_0_d_def__.s20.n.ni.preheader"
                                        #   in Loop: Header=BB28_2 Depth=1
	movl	%r11d, %eax
	sarl	$31, %eax
	andnl	%r11d, %eax, %eax
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rbx
	movslq	%r14d, %rdi
	movslq	%r13d, %rbp
	cmpl	$32, -112(%rsp)                 # 4-byte Folded Reload
	jae	.LBB28_9
# %bb.6:                                #   in Loop: Header=BB28_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB28_7
	.p2align	4, 0x90
.LBB28_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB28_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leaq	(%rcx,%rbx), %rsi
	movq	%rbp, %rbx
	movq	-24(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %rcx
	leaq	(%rbp,%rsi,4), %r15
	movq	%rbx, %rbp
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %r12
	movq	-40(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	xorl	%r9d, %r9d
	.p2align	4, 0x90
.LBB28_10:                              # %vector.body
                                        #   Parent Loop BB28_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r15,%r9,4), %ymm3
	vmovups	-64(%r15,%r9,4), %ymm4
	vmovups	-32(%r15,%r9,4), %ymm5
	vmovups	(%r15,%r9,4), %ymm6
	vmulps	-96(%rcx,%r9,4), %ymm3, %ymm3
	vmulps	-64(%rcx,%r9,4), %ymm4, %ymm4
	vmulps	-32(%rcx,%r9,4), %ymm5, %ymm5
	vmulps	(%rcx,%r9,4), %ymm6, %ymm6
	vmulps	-96(%r12,%r9,4), %ymm3, %ymm3
	vmulps	-64(%r12,%r9,4), %ymm4, %ymm4
	vmulps	-32(%r12,%r9,4), %ymm5, %ymm5
	vmulps	(%r12,%r9,4), %ymm6, %ymm6
	vfmadd213ps	-96(%rax,%r9,4), %ymm2, %ymm3 # ymm3 = (ymm2 * ymm3) + mem
	vfmadd213ps	-64(%rax,%r9,4), %ymm2, %ymm4 # ymm4 = (ymm2 * ymm4) + mem
	vfmadd213ps	-32(%rax,%r9,4), %ymm2, %ymm5 # ymm5 = (ymm2 * ymm5) + mem
	vfmadd213ps	(%rax,%r9,4), %ymm2, %ymm6 # ymm6 = (ymm2 * ymm6) + mem
	vmovups	%ymm3, -96(%rax,%r9,4)
	vmovups	%ymm4, -64(%rax,%r9,4)
	vmovups	%ymm5, -32(%rax,%r9,4)
	vmovups	%ymm6, (%rax,%r9,4)
	addq	$32, %r9
	cmpq	%r9, %r8
	jne	.LBB28_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB28_2 Depth=1
	movq	%r8, %rsi
	cmpq	-112(%rsp), %r8                 # 8-byte Folded Reload
	movq	-8(%rsp), %r12                  # 8-byte Reload
	movq	-16(%rsp), %r15                 # 8-byte Reload
	movq	(%rsp), %rbx                    # 8-byte Reload
	je	.LBB28_12
.LBB28_7:                               # %"for relu1_0_d_def__.s20.n.ni.preheader24"
                                        #   in Loop: Header=BB28_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rbx, %rcx
	leaq	(%rdx,%rcx,4), %rcx
	movq	%rbp, %r9
	movq	-80(%rsp), %rbp                 # 8-byte Reload
	addq	%rsi, %rbp
	addq	%rbx, %rbp
	leaq	(%rdx,%rbp,4), %rbp
	addq	%rsi, %rdi
	movq	%r15, %rbx
	movq	-56(%rsp), %r15                 # 8-byte Reload
	leaq	(%r15,%rdi,4), %rdi
	movq	%rbx, %r15
	addq	%rsi, %r9
	movq	-64(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r9,4), %rsi
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB28_8:                               # %"for relu1_0_d_def__.s20.n.ni"
                                        #   Parent Loop BB28_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbp,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	(%rcx,%rbx,4), %xmm3, %xmm3
	vmulss	(%rdi,%rbx,4), %xmm3, %xmm3
	vfmadd213ss	(%rsi,%rbx,4), %xmm1, %xmm3 # xmm3 = (xmm1 * xmm3) + mem
	vmovss	%xmm3, (%rsi,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rax
	jne	.LBB28_8
	jmp	.LBB28_12
.LBB28_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$40, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end28:
	.size	train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n, .Lfunc_end28-train_cost_model.par_for.relu1_0_d_def__.s20.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n
.LCPI29_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI29_1:
	.long	0x3f800000                      # float 1
.LCPI29_2:
	.long	0x45800000                      # float 4096
.LCPI29_3:
	.long	0x3109705f                      # float 1.99999994E-9
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	movq	%rdx, %rcx
	movl	%esi, %edi
	movl	12(%rdx), %r10d
	movl	24(%rdx), %r15d
	movl	%esi, %r14d
	sarl	$31, %r14d
	xorl	%ebx, %ebx
	testl	%r15d, %r15d
	sete	%bl
	movl	%ebx, %esi
	negl	%esi
	movl	%r15d, %ebp
	sarl	$31, %ebp
	subl	%r14d, %edi
	orl	%r15d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r13d
	movl	%ebp, %r8d
	leal	(%r15,%rbx), %r9d
	movl	%edi, %eax
	cltd
	idivl	%r9d
	notl	%r8d
	decl	%ebx
	movl	%r8d, %edi
	subl	%ebp, %edi
	andl	%r14d, %edi
	addl	%eax, %edi
	andl	%ebx, %edi
	leal	(%rdi,%rdi), %eax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	subl	%eax, %r10d
	movl	%r10d, -108(%rsp)               # 4-byte Spill
	jle	.LBB29_13
# %bb.1:                                # %"for relu1_0_d_def__.s21.w.wi.preheader"
	movl	16(%rcx), %r12d
	movl	28(%rcx), %r10d
	movl	32(%rcx), %r11d
	movl	36(%rcx), %r9d
	movl	40(%rcx), %eax
	movl	44(%rcx), %esi
	xorl	%r15d, %ebp
	movl	48(%rcx), %r15d
	addl	%r8d, %ebp
	andl	%r14d, %ebp
	addl	%ebp, %r13d
	andl	%ebx, %r13d
	movl	%r9d, %edx
	movq	%r9, %rbx
	subl	%r11d, %edx
	movq	%rdx, 48(%rsp)                  # 8-byte Spill
	subl	%r11d, %eax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	subl	%r11d, %esi
	movq	%rsi, -80(%rsp)                 # 8-byte Spill
	subl	%r11d, %r15d
	movq	-64(%rsp), %r14                 # 8-byte Reload
	movl	%r14d, %edx
	movl	%r14d, %r9d
	subl	8(%rcx), %r9d
	subl	%r10d, %edx
	movq	%rdx, 40(%rsp)                  # 8-byte Spill
	leal	(%r12,%r12,2), %edx
	shll	$3, %edx
	subl	%r12d, %edx
	xorl	%esi, %esi
	cmpl	$1, -108(%rsp)                  # 4-byte Folded Reload
	sete	%sil
	movl	$2, %ebp
	subq	%rsi, %rbp
	movq	%rbp, 72(%rsp)                  # 8-byte Spill
	movl	(%rcx), %r8d
	leal	-1(%r8), %esi
	vmovd	%esi, %xmm0
	leal	(%rbx,%r13,8), %esi
	movq	%r12, %rbp
	movl	%ebp, %eax
	shll	$5, %eax
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	leal	(,%r13,8), %eax
	vmovd	%eax, %xmm1
	movslq	%eax, %rbx
	movq	%r8, 64(%rsp)                   # 8-byte Spill
	movl	%r8d, %eax
	subl	%ebx, %eax
	cmpl	$9, %eax
	movl	$8, %r12d
	cmovll	%eax, %r12d
	xorl	%r8d, %r8d
	movl	%eax, -88(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r8d, %r12d
	subl	%r11d, %esi
	movq	%rsi, 32(%rsp)                  # 8-byte Spill
	shll	$6, %edi
	shll	$5, %r10d
	subl	%r10d, %edi
	orl	$23, %edi
	imull	%ebp, %edi
	movslq	4(%rcx), %rsi
	movl	20(%rcx), %eax
	movl	%eax, -104(%rsp)                # 4-byte Spill
	movq	56(%rcx), %rax
	movq	%rax, -40(%rsp)                 # 8-byte Spill
	movq	72(%rcx), %rax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	movq	88(%rcx), %rcx
	leal	(%rdx,%r13,8), %eax
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	movslq	%r9d, %rax
	movq	%rax, %rdx
	movq	%rax, 16(%rsp)                  # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	imull	%esi, %eax
	leal	(%rax,%r13,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	leal	(%rdi,%r13,8), %eax
	movl	%eax, -108(%rsp)                # 4-byte Spill
	leal	8(,%r13,8), %eax
	movl	%eax, -84(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm1, %ymm1
	vpor	.LCPI29_0(%rip), %ymm1, %ymm1
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm0
	vmovdqu	%ymm0, 96(%rsp)                 # 32-byte Spill
	movq	-72(%rsp), %rax                 # 8-byte Reload
	leal	(%rbx,%rax), %eax
	movslq	%eax, %rdx
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%rbx,%rax), %eax
	movslq	%eax, %rdi
	movq	%r15, 56(%rsp)                  # 8-byte Spill
	movq	%rbx, 24(%rsp)                  # 8-byte Spill
	leal	(%rbx,%r15), %eax
	cltq
	movl	%r12d, %r15d
	andl	$2147483640, %r15d              # imm = 0x7FFFFFF8
	vmovss	.LCPI29_1(%rip), %xmm14         # xmm14 = mem[0],zero,zero,zero
	vmovss	.LCPI29_2(%rip), %xmm12         # xmm12 = mem[0],zero,zero,zero
	vmovss	.LCPI29_3(%rip), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI29_1(%rip), %ymm4  # ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI29_2(%rip), %ymm5  # ymm5 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vbroadcastss	.LCPI29_3(%rip), %ymm6  # ymm6 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	%rdi, -24(%rsp)                 # 8-byte Spill
	leaq	(%rcx,%rdi,4), %rax
	movq	%rax, (%rsp)                    # 8-byte Spill
	movq	%rdx, -16(%rsp)                 # 8-byte Spill
	leaq	(%rcx,%rdx,4), %rax
	movq	%rax, -8(%rsp)                  # 8-byte Spill
	movl	%r14d, %r9d
	movq	%rsi, -56(%rsp)                 # 8-byte Spill
	jmp	.LBB29_2
	.p2align	4, 0x90
.LBB29_3:                               # %true_bb
                                        #   in Loop: Header=BB29_2 Depth=1
	movq	-64(%rsp), %rax                 # 8-byte Reload
	addl	%r8d, %eax
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %edx
	imull	-104(%rsp), %edx                # 4-byte Folded Reload
	movq	48(%rsp), %rax                  # 8-byte Reload
	addl	%edx, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vmovdqu	96(%rsp), %ymm2                 # 32-byte Reload
	vpaddd	%ymm2, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm7
	vmovd	%xmm7, %eax
	cltq
	vpextrd	$1, %xmm7, %esi
	movslq	%esi, %rsi
	vmovss	(%rcx,%rax,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovd	%xmm0, %eax
	vpextrd	$2, %xmm7, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vpextrd	$3, %xmm7, %esi
	cltq
	movslq	%esi, %rsi
	vinsertps	$48, (%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	%xmm1, 80(%rsp)                 # 16-byte Spill
	vpextrd	$1, %xmm0, %esi
	movslq	%esi, %rsi
	vmovss	(%rcx,%rax,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	cltq
	vpextrd	$3, %xmm0, %esi
	movslq	%esi, %rsi
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rcx,%rsi,4), %xmm0, %xmm8 # xmm8 = xmm0[0,1,2],mem[0]
	movq	-72(%rsp), %rax                 # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm2, %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vmovd	%xmm1, %eax
	cltq
	vpextrd	$1, %xmm1, %esi
	movslq	%esi, %rsi
	vmovss	(%rcx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	vinsertps	$16, (%rcx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm0, %esi
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%esi, %rax
	vpextrd	$3, %xmm1, %esi
	movslq	%esi, %rsi
	vinsertps	$48, (%rcx,%rsi,4), %xmm7, %xmm13 # xmm13 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %esi
	vmovss	(%rcx,%rax,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	movslq	%esi, %rax
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpextrd	$2, %xmm0, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vpextrd	$3, %xmm0, %eax
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm10 # xmm10 = xmm1[0,1,2],mem[0]
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm2, %ymm0, %ymm11
	vextracti128	$1, %ymm11, %xmm0
	vmovd	%xmm0, %eax
	cltq
	vmovss	(%rcx,%rax,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm0, %eax
	cltq
	vinsertps	$16, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpextrd	$2, %xmm0, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vpextrd	$3, %xmm0, %eax
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm1, %xmm15 # xmm15 = xmm1[0,1,2],mem[0]
	vmovd	%xmm11, %eax
	cltq
	vmovss	(%rcx,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm11, %eax
	cltq
	vinsertps	$16, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$2, %xmm11, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	addl	56(%rsp), %edx                  # 4-byte Folded Reload
	vmovd	%edx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm2, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm7
	vmovd	%xmm7, %eax
	cltq
	vpextrd	$1, %xmm7, %edx
	movslq	%edx, %rdx
	vmovss	(%rcx,%rax,4), %xmm9            # xmm9 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm7, %eax
	vinsertps	$16, (%rcx,%rdx,4), %xmm9, %xmm2 # xmm2 = xmm9[0],mem[0],xmm9[2,3]
	vmovd	%xmm1, %edx
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movslq	%edx, %rax
	vpextrd	$3, %xmm7, %edx
	movslq	%edx, %rdx
	vinsertps	$48, (%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vpextrd	$1, %xmm1, %edx
	vmovss	(%rcx,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%edx, %rax
	vinsertps	$16, (%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertf128	$1, 80(%rsp), %ymm8, %ymm8 # 16-byte Folded Reload
	vpextrd	$2, %xmm1, %eax
	cltq
	vinsertps	$32, (%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm1, %eax
	vinsertf128	$1, %xmm13, %ymm10, %ymm1
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vpextrd	$3, %xmm11, %eax
	vinsertf128	$1, %xmm2, %ymm7, %ymm2
	cltq
	vinsertps	$48, (%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	16(%rsp), %rax                  # 8-byte Reload
	addq	%r8, %rax
	imulq	%rsi, %rax
	vmaxps	%ymm4, %ymm2, %ymm2
	vdivps	%ymm2, %ymm5, %ymm2
	addq	24(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm15, %ymm0, %ymm0
	vminps	%ymm8, %ymm2, %ymm2
	vmulps	%ymm0, %ymm8, %ymm0
	vmulps	%ymm1, %ymm0, %ymm0
	vmulps	%ymm0, %ymm2, %ymm0
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%rdx,%rax,4), %ymm0, %ymm0
	movq	40(%rsp), %rax                  # 8-byte Reload
	addl	%r8d, %eax
	imull	-96(%rsp), %eax                 # 4-byte Folded Reload
	addl	-92(%rsp), %eax                 # 4-byte Folded Reload
	cltq
	movq	-48(%rsp), %rdx                 # 8-byte Reload
	vfmadd213ps	(%rdx,%rax,4), %ymm6, %ymm0 # ymm0 = (ymm6 * ymm0) + mem
	vmovups	%ymm0, (%rdx,%rax,4)
.LBB29_12:                              # %after_bb
                                        #   in Loop: Header=BB29_2 Depth=1
	incq	%r8
	incl	%r9d
	addl	%esi, -100(%rsp)                # 4-byte Folded Spill
	movl	-108(%rsp), %eax                # 4-byte Reload
	addl	-96(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -108(%rsp)                # 4-byte Spill
	cmpq	72(%rsp), %r8                   # 8-byte Folded Reload
	je	.LBB29_13
.LBB29_2:                               # %"for relu1_0_d_def__.s21.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB29_10 Depth 2
                                        #     Child Loop BB29_8 Depth 2
	movl	-84(%rsp), %eax                 # 4-byte Reload
	cmpl	64(%rsp), %eax                  # 4-byte Folded Reload
	jle	.LBB29_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB29_2 Depth=1
	cmpl	$0, -88(%rsp)                   # 4-byte Folded Reload
	jle	.LBB29_12
# %bb.5:                                # %"for relu1_0_d_def__.s21.n.ni.preheader"
                                        #   in Loop: Header=BB29_2 Depth=1
	movl	%r9d, %eax
	sarl	$31, %eax
	andnl	%r9d, %eax, %eax
	imull	-104(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %r11
	movq	32(%rsp), %rax                  # 8-byte Reload
	addl	%r11d, %eax
	movslq	-100(%rsp), %rdx                # 4-byte Folded Reload
	movq	-40(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%rdx,4), %rdx
	cltq
	leaq	(%rcx,%rax,4), %rbx
	movslq	-108(%rsp), %rax                # 4-byte Folded Reload
	movq	-48(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%rax,4), %r10
	cmpl	$8, %r12d
	jae	.LBB29_9
# %bb.6:                                #   in Loop: Header=BB29_2 Depth=1
	xorl	%r13d, %r13d
	jmp	.LBB29_7
	.p2align	4, 0x90
.LBB29_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB29_2 Depth=1
	movq	-32(%rsp), %rax                 # 8-byte Reload
	addq	%r11, %rax
	movq	-24(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r11), %rsi
	movq	-16(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%r11), %r14
	leaq	(%rcx,%rax,4), %rbp
	leaq	(%rcx,%rsi,4), %rdi
	leaq	(%rcx,%r14,4), %rsi
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB29_10:                              # %vector.body
                                        #   Parent Loop BB29_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rbp,%rax,4), %ymm0
	vmaxps	%ymm4, %ymm0, %ymm0
	vdivps	%ymm0, %ymm5, %ymm0
	vmovups	(%rbx,%rax,4), %ymm1
	vminps	%ymm1, %ymm0, %ymm0
	vmulps	(%rdi,%rax,4), %ymm0, %ymm0
	vmulps	%ymm0, %ymm1, %ymm0
	vmulps	(%rsi,%rax,4), %ymm0, %ymm0
	vmulps	(%rdx,%rax,4), %ymm0, %ymm0
	vfmadd213ps	(%r10,%rax,4), %ymm6, %ymm0 # ymm0 = (ymm6 * ymm0) + mem
	vmovups	%ymm0, (%r10,%rax,4)
	addq	$8, %rax
	cmpq	%rax, %r15
	jne	.LBB29_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB29_2 Depth=1
	movq	%r15, %r13
	cmpq	%r12, %r15
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	je	.LBB29_12
.LBB29_7:                               # %"for relu1_0_d_def__.s21.n.ni.preheader14"
                                        #   in Loop: Header=BB29_2 Depth=1
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	(%rax,%r11,4), %r14
	movq	(%rsp), %rax                    # 8-byte Reload
	leaq	(%rax,%r11,4), %rbp
	movq	-8(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r11,4), %rdi
	.p2align	4, 0x90
.LBB29_8:                               # %"for relu1_0_d_def__.s21.n.ni"
                                        #   Parent Loop BB29_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r13,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovss	(%r14,%r13,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmaxss	%xmm14, %xmm1, %xmm1
	vdivss	%xmm1, %xmm12, %xmm1
	vminss	%xmm0, %xmm1, %xmm1
	vmulss	(%rbp,%r13,4), %xmm1, %xmm1
	vmulss	%xmm1, %xmm0, %xmm0
	vmulss	(%rdi,%r13,4), %xmm0, %xmm0
	vmulss	(%rdx,%r13,4), %xmm0, %xmm0
	vfmadd213ss	(%r10,%r13,4), %xmm3, %xmm0 # xmm0 = (xmm3 * xmm0) + mem
	vmovss	%xmm0, (%r10,%r13,4)
	incq	%r13
	cmpq	%r13, %r12
	jne	.LBB29_8
	jmp	.LBB29_12
.LBB29_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end29:
	.size	train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n, .Lfunc_end29-train_cost_model.par_for.relu1_0_d_def__.s21.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n
.LCPI30_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI30_1:
	.long	0x3f800000                      # float 1
.LCPI30_2:
	.long	0x3109705f                      # float 1.99999994E-9
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %ecx
	movl	24(%rdx), %r11d
	movl	%esi, %r10d
	sarl	$31, %r10d
	xorl	%ebp, %ebp
	testl	%r11d, %r11d
	sete	%bpl
	movl	%ebp, %esi
	negl	%esi
	movl	%r11d, %ebx
	sarl	$31, %ebx
	subl	%r10d, %edi
	orl	%r11d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	movl	%ebx, %esi
	leal	(%r11,%rbp), %r8d
	movl	%edi, %eax
	cltd
	idivl	%r8d
	notl	%esi
	decl	%ebp
	movl	%esi, %edx
	subl	%ebx, %edx
	andl	%r10d, %edx
	addl	%eax, %edx
	andl	%ebp, %edx
	leal	(%rdx,%rdx), %eax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	subl	%eax, %ecx
	movl	%ecx, -128(%rsp)                # 4-byte Spill
	jle	.LBB30_15
# %bb.1:                                # %"for relu1_0_d_def__.s22.w.wi.preheader"
	movl	(%r13), %r8d
	movl	16(%r13), %r9d
	movl	28(%r13), %r15d
	movl	32(%r13), %edi
	movl	36(%r13), %ecx
	movl	40(%r13), %eax
	xorl	%r11d, %ebx
	movl	44(%r13), %r14d
	addl	%esi, %ebx
	movl	48(%r13), %r11d
	andl	%r10d, %ebx
	addl	%ebx, %r12d
	andl	%ebp, %r12d
	subl	%edi, %ecx
	movq	%rcx, -80(%rsp)                 # 8-byte Spill
	subl	%edi, %eax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	subl	%edi, %r14d
	subl	%edi, %r11d
	movq	-112(%rsp), %rax                # 8-byte Reload
	movl	%eax, %esi
	subl	%r15d, %esi
	movq	%rsi, 40(%rsp)                  # 8-byte Spill
	movl	%eax, %edi
	subl	8(%r13), %edi
	leal	(%r9,%r9,4), %esi
	leal	(%r9,%rsi,4), %ebx
	addl	%r9d, %ebx
	xorl	%esi, %esi
	cmpl	$1, -128(%rsp)                  # 4-byte Folded Reload
	sete	%sil
	movl	$2, %ebp
	subq	%rsi, %rbp
	movq	%rbp, 64(%rsp)                  # 8-byte Spill
	leal	-1(%r8), %esi
	vmovd	%esi, %xmm0
	movl	%r9d, %esi
	shll	$5, %esi
	movl	%esi, -116(%rsp)                # 4-byte Spill
	leal	(,%r12,8), %esi
	vmovd	%esi, %xmm1
	movslq	%esi, %rbp
	movq	%r8, %rsi
	movl	%esi, %eax
	subl	%ebp, %eax
	cmpl	$9, %eax
	movl	$8, %r10d
	cmovll	%eax, %r10d
	xorl	%r8d, %r8d
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r8d, %r10d
	shll	$6, %edx
	shll	$5, %r15d
	subl	%r15d, %edx
	orl	$22, %edx
	imull	%r9d, %edx
	movslq	4(%r13), %r9
	movl	20(%r13), %eax
	movl	%eax, -120(%rsp)                # 4-byte Spill
	movq	56(%r13), %rax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movq	72(%r13), %rax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	88(%r13), %r15
	leal	(%rbx,%r12,8), %eax
	movl	%eax, -100(%rsp)                # 4-byte Spill
	vpbroadcastd	%xmm1, %ymm1
	vpor	.LCPI30_0(%rip), %ymm1, %ymm1
	movslq	%edi, %rax
	movq	%rax, %rdi
	movq	%rax, 24(%rsp)                  # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%r9, -56(%rsp)                  # 8-byte Spill
	imull	%r9d, %eax
	leal	(%rax,%r12,8), %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	leal	(%rdx,%r12,8), %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	leal	8(,%r12,8), %eax
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm8
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%rbp,%rax), %eax
	movslq	%eax, %rcx
	movq	%r11, 48(%rsp)                  # 8-byte Spill
	leal	(%rbp,%r11), %eax
	movslq	%eax, %rdx
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leal	(%rbp,%rax), %eax
	movslq	%eax, %rdi
	movq	%r14, 56(%rsp)                  # 8-byte Spill
	movq	%rbp, 32(%rsp)                  # 8-byte Spill
	leal	(%r14,%rbp), %eax
	cltq
	movl	%r10d, %r14d
	andl	$2147483640, %r14d              # imm = 0x7FFFFFF8
	vmovss	.LCPI30_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI30_2(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI30_1(%rip), %ymm3  # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI30_2(%rip), %ymm4  # ymm4 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	leaq	(%r15,%rax,4), %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movq	%rdi, -40(%rsp)                 # 8-byte Spill
	leaq	(%r15,%rdi,4), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	%rdx, -32(%rsp)                 # 8-byte Spill
	leaq	(%r15,%rdx,4), %rax
	movq	%rax, (%rsp)                    # 8-byte Spill
	movq	%rcx, -24(%rsp)                 # 8-byte Spill
	leaq	(%r15,%rcx,4), %rax
	movq	%rax, -8(%rsp)                  # 8-byte Spill
	movq	-112(%rsp), %rax                # 8-byte Reload
	movl	%eax, %r12d
	movq	%rsi, -16(%rsp)                 # 8-byte Spill
	jmp	.LBB30_2
	.p2align	4, 0x90
.LBB30_3:                               # %true_bb
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	addl	%r8d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edx
	imull	-120(%rsp), %edx                # 4-byte Folded Reload
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%edx, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm0
	vextracti128	$1, %ymm0, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %eax
	vinsertps	$16, (%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm0, %ecx
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm6, %xmm11 # xmm11 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$3, %xmm0, %ecx
	cltq
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rax,4), %xmm5, %xmm0 # xmm0 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r15,%rcx,4), %xmm0, %xmm12 # xmm12 = xmm0[0,1,2],mem[0]
	movq	24(%rsp), %rax                  # 8-byte Reload
	addq	%r8, %rax
	imulq	-56(%rsp), %rax                 # 8-byte Folded Reload
	addq	32(%rsp), %rax                  # 8-byte Folded Reload
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm4, %ymm10
	movq	48(%rsp), %rax                  # 8-byte Reload
	addl	%edx, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm5
	vextracti128	$1, %ymm5, %xmm0
	vmovd	%xmm0, %eax
	cltq
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm5, %eax
	vpextrd	$2, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm0, %ecx
	cltq
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm6, %xmm13 # xmm13 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %eax
	vinsertps	$16, (%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	cltq
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rax,4), %xmm6, %xmm5 # xmm5 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r15,%rcx,4), %xmm5, %xmm9 # xmm9 = xmm5[0,1,2],mem[0]
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm8, %ymm6
	vextracti128	$1, %ymm6, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %eax
	vinsertps	$16, (%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm6, %ecx
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm7, %xmm14 # xmm14 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm6, %ecx
	vmovss	(%r15,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm6, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm6, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	addl	56(%rsp), %edx                  # 4-byte Folded Reload
	vmovd	%edx, %xmm7
	vpbroadcastd	%xmm7, %ymm7
	vpaddd	%ymm7, %ymm8, %ymm7
	vextracti128	$1, %ymm7, %xmm0
	vmovd	%xmm0, %eax
	cltq
	vpextrd	$1, %xmm0, %ecx
	vmovss	(%r15,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertf128	$1, %xmm11, %ymm12, %ymm11
	vpextrd	$2, %xmm0, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm0, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm5, %xmm0 # xmm0 = xmm5[0,1,2],mem[0]
	vmovd	%xmm7, %eax
	cltq
	vmovss	(%r15,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %eax
	vinsertf128	$1, %xmm13, %ymm9, %ymm9
	cltq
	vinsertps	$16, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertf128	$1, %xmm14, %ymm6, %ymm6
	vpextrd	$2, %xmm7, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm7, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	40(%rsp), %rax                  # 8-byte Reload
	addl	%r8d, %eax
	imull	-116(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	vcmpltps	%ymm11, %ymm3, %ymm7
	vmaxps	%ymm3, %ymm9, %ymm9
	vinsertf128	$1, %xmm0, %ymm5, %ymm0
	vaddps	%ymm0, %ymm6, %ymm0
	vmulps	%ymm0, %ymm10, %ymm0
	vdivps	%ymm9, %ymm0, %ymm0
	vandps	%ymm0, %ymm7, %ymm0
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rax,4)
.LBB30_14:                              # %after_bb
                                        #   in Loop: Header=BB30_2 Depth=1
	incq	%r8
	movl	-128(%rsp), %eax                # 4-byte Reload
	addl	-56(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -128(%rsp)                # 4-byte Spill
	incl	%r12d
	movl	-124(%rsp), %eax                # 4-byte Reload
	addl	-116(%rsp), %eax                # 4-byte Folded Reload
	movl	%eax, -124(%rsp)                # 4-byte Spill
	cmpq	64(%rsp), %r8                   # 8-byte Folded Reload
	je	.LBB30_15
.LBB30_2:                               # %"for relu1_0_d_def__.s22.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB30_12 Depth 2
                                        #     Child Loop BB30_8 Depth 2
	cmpl	%esi, -92(%rsp)                 # 4-byte Folded Reload
	jle	.LBB30_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB30_2 Depth=1
	cmpl	$0, -96(%rsp)                   # 4-byte Folded Reload
	jle	.LBB30_14
# %bb.5:                                # %"for relu1_0_d_def__.s22.n.ni.preheader"
                                        #   in Loop: Header=BB30_2 Depth=1
	movl	%r12d, %eax
	sarl	$31, %eax
	andnl	%r12d, %eax, %eax
	imull	-120(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %r9
	movslq	-128(%rsp), %rax                # 4-byte Folded Reload
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdx
	movslq	-124(%rsp), %rax                # 4-byte Folded Reload
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rbx
	cmpl	$8, %r10d
	jae	.LBB30_11
# %bb.6:                                #   in Loop: Header=BB30_2 Depth=1
	xorl	%r13d, %r13d
	jmp	.LBB30_7
	.p2align	4, 0x90
.LBB30_11:                              # %vector.body.preheader
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	-48(%rsp), %rax                 # 8-byte Reload
	addq	%r9, %rax
	movq	-40(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r9), %rcx
	movq	-32(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r9), %rsi
	movq	-24(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%r9), %r11
	leaq	(%r15,%rax,4), %rax
	leaq	(%r15,%rcx,4), %rdi
	leaq	(%r15,%rsi,4), %rbp
	leaq	(%r15,%r11,4), %rsi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB30_12:                              # %vector.body
                                        #   Parent Loop BB30_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vcmpltps	(%rsi,%rcx,4), %ymm3, %ymm0
	vmulps	(%rdx,%rcx,4), %ymm4, %ymm5
	vmovups	(%rbp,%rcx,4), %ymm6
	vmaxps	%ymm3, %ymm6, %ymm6
	vmovups	(%rdi,%rcx,4), %ymm7
	vaddps	(%rax,%rcx,4), %ymm7, %ymm7
	vmulps	%ymm7, %ymm5, %ymm5
	vdivps	%ymm6, %ymm5, %ymm5
	vandps	%ymm5, %ymm0, %ymm0
	vaddps	(%rbx,%rcx,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rbx,%rcx,4)
	addq	$8, %rcx
	cmpq	%rcx, %r14
	jne	.LBB30_12
# %bb.13:                               # %middle.block
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	%r14, %r13
	cmpq	%r10, %r14
	movq	-16(%rsp), %rsi                 # 8-byte Reload
	je	.LBB30_14
.LBB30_7:                               # %"for relu1_0_d_def__.s22.n.ni.preheader14"
                                        #   in Loop: Header=BB30_2 Depth=1
	movq	16(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r9,4), %rax
	movq	8(%rsp), %rcx                   # 8-byte Reload
	leaq	(%rcx,%r9,4), %rbp
	movq	(%rsp), %rcx                    # 8-byte Reload
	leaq	(%rcx,%r9,4), %r11
	movq	-8(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%r9,4), %rdi
	jmp	.LBB30_8
	.p2align	4, 0x90
.LBB30_10:                              # %select.end
                                        #   in Loop: Header=BB30_8 Depth=2
	vaddss	%xmm6, %xmm0, %xmm0
	vmovss	%xmm0, (%rbx,%r13,4)
	incq	%r13
	cmpq	%r13, %r10
	je	.LBB30_14
.LBB30_8:                               # %"for relu1_0_d_def__.s22.n.ni"
                                        #   Parent Loop BB30_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r13,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vucomiss	(%rdi,%r13,4), %xmm1
	setb	%cl
	vmulss	(%rdx,%r13,4), %xmm2, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	testb	$1, %cl
	je	.LBB30_10
# %bb.9:                                # %select.true.sink
                                        #   in Loop: Header=BB30_8 Depth=2
	vmovss	(%r11,%r13,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmaxss	%xmm1, %xmm6, %xmm6
	vmovss	(%rbp,%r13,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vaddss	(%rax,%r13,4), %xmm7, %xmm7
	vmulss	%xmm7, %xmm5, %xmm5
	vdivss	%xmm6, %xmm5, %xmm6
	jmp	.LBB30_10
.LBB30_15:                              # %destructor_block
	xorl	%eax, %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end30:
	.size	train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n, .Lfunc_end30-train_cost_model.par_for.relu1_0_d_def__.s22.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n
.LCPI31_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI31_1:
	.long	0x3f800000                      # float 1
.LCPI31_2:
	.long	0x3109705f                      # float 1.99999994E-9
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	movq	%rdx, %rcx
	movl	%esi, %edi
	movl	12(%rdx), %r8d
	movl	24(%rdx), %r11d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r11d, %r11d
	sete	%bl
	movl	%ebx, %esi
	negl	%esi
	subl	%r9d, %edi
	orl	%r11d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r10d
	movl	%r11d, %ebp
	sarl	$31, %ebp
	movl	%ebp, %r14d
	notl	%r14d
	leal	(%r11,%rbx), %esi
	decl	%ebx
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%r14d, %esi
	subl	%ebp, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %edx
	movl	%r8d, %eax
	subl	%edx, %eax
	movl	$1, %edi
	movq	%rdx, -72(%rsp)                 # 8-byte Spill
	subl	%edx, %edi
	cmpl	$1, %r8d
	cmovlel	%eax, %edi
	cmpl	$3, %edi
	movl	$2, %r8d
	cmovgel	%r8d, %edi
	movl	%edi, %edx
	sarl	$31, %edx
	andnl	%edi, %edx, %edi
	cmpl	$3, %eax
	cmovgel	%r8d, %eax
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %eax
	movq	%rdi, -112(%rsp)                # 8-byte Spill
	movl	%eax, -56(%rsp)                 # 4-byte Spill
	cmpl	%eax, %edi
	jge	.LBB31_17
# %bb.1:                                # %"for relu1_0_d_def__.s23.w.wi.preheader"
	movl	(%rcx), %eax
	movl	8(%rcx), %r15d
	movl	16(%rcx), %edx
	movl	28(%rcx), %r8d
	movslq	32(%rcx), %r12
	xorl	%r11d, %ebp
	movl	36(%rcx), %r11d
	addl	%r14d, %ebp
	movl	40(%rcx), %r14d
	andl	%r9d, %ebp
	movq	%rax, %r9
	addl	%ebp, %r10d
	andl	%ebx, %r10d
	subl	%r12d, %r11d
	subl	%r12d, %r14d
	movq	-72(%rsp), %rdi                 # 8-byte Reload
	movl	%edi, %eax
	subl	%r8d, %eax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
                                        # kill: def $edi killed $edi killed $rdi def $rdi
	subl	%r15d, %edi
	movq	%rdi, -32(%rsp)                 # 8-byte Spill
	leal	-1(%r9), %ebp
	vmovd	%ebp, %xmm0
	movl	%edx, %ebx
	shll	$5, %ebx
	movl	%ebx, -124(%rsp)                # 4-byte Spill
	leal	(,%r10,8), %eax
	movl	%eax, -104(%rsp)                # 4-byte Spill
	movslq	%eax, %rbx
	movl	%r9d, %edi
	subl	%ebx, %edi
	cmpl	$9, %edi
	movl	$8, %eax
	cmovll	%edi, %eax
	xorl	%ebp, %ebp
	movl	%edi, -40(%rsp)                 # 4-byte Spill
	testl	%edi, %edi
	cmovlel	%ebp, %eax
	movq	-112(%rsp), %rdi                # 8-byte Reload
	leal	(%rdi,%rsi,2), %esi
	movl	%esi, %ebp
	subl	%r15d, %ebp
	movl	20(%rcx), %edi
	movl	%edi, -128(%rsp)                # 4-byte Spill
	imull	%esi, %edi
	movl	%edi, -116(%rsp)                # 4-byte Spill
	subl	%r8d, %esi
	movq	%r12, %r8
	leal	(%rdx,%rdx,4), %edi
	leal	(%rdx,%rdi,4), %r12d
	shll	$5, %esi
	orl	$21, %esi
	imull	%edx, %esi
	movslq	4(%rcx), %rdx
	movq	48(%rcx), %r13
	movq	64(%rcx), %rdi
	movq	%rdi, -96(%rsp)                 # 8-byte Spill
	movq	80(%rcx), %r15
	leal	(%r12,%r10,8), %ecx
	movl	%ecx, -100(%rsp)                # 4-byte Spill
	movq	%rdx, -48(%rsp)                 # 8-byte Spill
	imull	%edx, %ebp
	leal	(%rbp,%r10,8), %ecx
	movl	%ecx, -120(%rsp)                # 4-byte Spill
	leal	(%rsi,%r10,8), %r12d
	leal	8(,%r10,8), %ecx
	movl	%ecx, -36(%rsp)                 # 4-byte Spill
	vmovd	-104(%rsp), %xmm1               # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm1, %ymm1
	vpor	.LCPI31_0(%rip), %ymm1, %ymm1
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm8
	movl	%ebx, %ecx
	subl	%r8d, %ecx
	leal	(%rbx,%r11), %edx
	movslq	%edx, %rdx
	movq	%rdx, -80(%rsp)                 # 8-byte Spill
	leal	(%rbx,%r14), %edx
	movslq	%edx, %rdx
	movq	%rdx, -88(%rsp)                 # 8-byte Spill
	movslq	%ecx, %rcx
	movl	-112(%rsp), %r10d               # 4-byte Reload
	movl	-56(%rsp), %edx                 # 4-byte Reload
	movq	%rdx, -56(%rsp)                 # 8-byte Spill
	movq	%rax, -112(%rsp)                # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$2147483640, %eax               # imm = 0x7FFFFFF8
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	addq	$-8, %rax
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	movq	%rax, %rdx
	shrq	$3, %rdx
	incq	%rdx
	movq	%rdx, %rax
	movq	%rdx, 40(%rsp)                  # 8-byte Spill
	andq	$-2, %rdx
	negq	%rdx
	movq	%rdx, 32(%rsp)                  # 8-byte Spill
	movq	%rbx, 112(%rsp)                 # 8-byte Spill
	subq	%r8, %rbx
	movq	%rbx, 96(%rsp)                  # 8-byte Spill
	vmovss	.LCPI31_2(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI31_1(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI31_1(%rip), %ymm3  # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI31_2(%rip), %ymm4  # ymm4 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movslq	-32(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 104(%rsp)                 # 8-byte Spill
	movq	%r13, -64(%rsp)                 # 8-byte Spill
	leaq	32(%r13), %rax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	%rcx, -8(%rsp)                  # 8-byte Spill
	leaq	(%r15,%rcx,4), %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	leaq	32(%r15), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	-96(%rsp), %rax                 # 8-byte Reload
	leaq	32(%rax), %rax
	movq	%rax, (%rsp)                    # 8-byte Spill
	movq	%r9, 88(%rsp)                   # 8-byte Spill
	movq	%r8, 80(%rsp)                   # 8-byte Spill
	movq	%r11, 72(%rsp)                  # 8-byte Spill
	movq	%r14, 64(%rsp)                  # 8-byte Spill
	jmp	.LBB31_2
	.p2align	4, 0x90
.LBB31_3:                               # %true_bb
                                        #   in Loop: Header=BB31_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%r10), %edx
	imull	-128(%rsp), %edx                # 4-byte Folded Reload
	leal	(%rdx,%r11), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm0
	vextracti128	$1, %ymm0, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %eax
	vinsertps	$16, (%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm0, %ecx
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm6, %xmm9 # xmm9 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %ecx
	vmovss	(%r15,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm0, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm0, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm5, %xmm10 # xmm10 = xmm5[0,1,2],mem[0]
	leal	(%rdx,%r14), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm7
	vextracti128	$1, %ymm7, %xmm0
	vmovd	%xmm0, %eax
	cltq
	vmovss	(%r15,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm0, %eax
	cltq
	vinsertps	$16, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm0, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm0, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm5, %xmm11 # xmm11 = xmm5[0,1,2],mem[0]
	vmovd	%xmm7, %eax
	cltq
	vmovss	(%r15,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %eax
	cltq
	vinsertps	$16, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm7, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm5, %xmm12 # xmm12 = xmm5[0,1],mem[0],xmm5[3]
	subl	%r8d, %edx
	vmovd	%edx, %xmm6
	vpbroadcastd	%xmm6, %ymm6
	vpaddd	%ymm6, %ymm8, %ymm6
	vextracti128	$1, %ymm6, %xmm0
	vmovd	%xmm0, %eax
	cltq
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm6, %ecx
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm5, %xmm0 # xmm0 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm6, %ecx
	vmovss	(%r15,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm6, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm6, %eax
	vinsertf128	$1, %xmm9, %ymm10, %ymm6
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vpextrd	$3, %xmm7, %eax
	vinsertf128	$1, %xmm0, %ymm5, %ymm0
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm12, %xmm5 # xmm5 = xmm12[0,1,2],mem[0]
	movq	104(%rsp), %rax                 # 8-byte Reload
	addq	%r10, %rax
	imulq	-48(%rsp), %rax                 # 8-byte Folded Reload
	addq	112(%rsp), %rax                 # 8-byte Folded Reload
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm0, %ymm0
	movq	-24(%rsp), %rax                 # 8-byte Reload
	addl	%r10d, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	cltq
	vinsertf128	$1, %xmm11, %ymm5, %ymm5
	vmulps	%ymm4, %ymm5, %ymm5
	vmulps	%ymm5, %ymm0, %ymm0
	vcmpleps	%ymm3, %ymm6, %ymm5
	vandps	%ymm0, %ymm5, %ymm0
	movq	-96(%rsp), %rcx                 # 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rax,4)
.LBB31_16:                              # %after_bb
                                        #   in Loop: Header=BB31_2 Depth=1
	incq	%r10
	movl	-120(%rsp), %eax                # 4-byte Reload
	addl	-48(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -120(%rsp)                # 4-byte Spill
	movl	-116(%rsp), %eax                # 4-byte Reload
	addl	-128(%rsp), %eax                # 4-byte Folded Reload
	movl	%eax, -116(%rsp)                # 4-byte Spill
	addl	-124(%rsp), %r12d               # 4-byte Folded Reload
	cmpq	-56(%rsp), %r10                 # 8-byte Folded Reload
	je	.LBB31_17
.LBB31_2:                               # %"for relu1_0_d_def__.s23.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB31_12 Depth 2
                                        #     Child Loop BB31_8 Depth 2
	cmpl	%r9d, -36(%rsp)                 # 4-byte Folded Reload
	jle	.LBB31_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB31_2 Depth=1
	cmpl	$0, -40(%rsp)                   # 4-byte Folded Reload
	jle	.LBB31_16
# %bb.5:                                # %"for relu1_0_d_def__.s23.n.ni.preheader"
                                        #   in Loop: Header=BB31_2 Depth=1
	movslq	-116(%rsp), %rdx                # 4-byte Folded Reload
	movslq	-120(%rsp), %rdi                # 4-byte Folded Reload
	movslq	%r12d, %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	cmpl	$7, -112(%rsp)                  # 4-byte Folded Reload
	ja	.LBB31_9
# %bb.6:                                #   in Loop: Header=BB31_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB31_7
	.p2align	4, 0x90
.LBB31_9:                               # %vector.ph
                                        #   in Loop: Header=BB31_2 Depth=1
	cmpq	$0, 48(%rsp)                    # 8-byte Folded Reload
	je	.LBB31_10
# %bb.11:                               # %vector.body.preheader
                                        #   in Loop: Header=BB31_2 Depth=1
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdx), %rcx
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdx), %rsi
	movq	24(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdi,4), %rax
	movq	16(%rsp), %rbp                  # 8-byte Reload
	leaq	(%rbp,%rdx,4), %r13
	movq	8(%rsp), %rbp                   # 8-byte Reload
	leaq	(%rbp,%rcx,4), %rbx
	leaq	(%rbp,%rsi,4), %r14
	movq	(%rsp), %rcx                    # 8-byte Reload
	movq	-16(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %r9
	movq	32(%rsp), %r11                  # 8-byte Reload
	xorl	%r8d, %r8d
	.p2align	4, 0x90
.LBB31_12:                              # %vector.body
                                        #   Parent Loop BB31_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%r13,%r8,4), %ymm0
	vmulps	-32(%rax,%r8,4), %ymm0, %ymm0
	vmulps	-32(%rbx,%r8,4), %ymm0, %ymm0
	vmovups	-32(%r14,%r8,4), %ymm5
	vmulps	%ymm4, %ymm0, %ymm0
	vcmpleps	%ymm3, %ymm5, %ymm5
	vandps	%ymm0, %ymm5, %ymm0
	vaddps	-32(%r9,%r8,4), %ymm0, %ymm0
	vmovups	%ymm0, -32(%r9,%r8,4)
	vmovups	32(%r13,%r8,4), %ymm0
	vmulps	(%rax,%r8,4), %ymm0, %ymm0
	vmovups	(%r14,%r8,4), %ymm5
	vmulps	(%rbx,%r8,4), %ymm0, %ymm0
	vmulps	%ymm4, %ymm0, %ymm0
	vcmpleps	%ymm3, %ymm5, %ymm5
	vandps	%ymm0, %ymm5, %ymm0
	vaddps	(%r9,%r8,4), %ymm0, %ymm0
	vmovups	%ymm0, (%r9,%r8,4)
	addq	$16, %r8
	addq	$2, %r11
	jne	.LBB31_12
# %bb.13:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB31_2 Depth=1
	testb	$1, 40(%rsp)                    # 1-byte Folded Reload
	je	.LBB31_15
.LBB31_14:                              # %vector.body.epil
                                        #   in Loop: Header=BB31_2 Depth=1
	movq	-24(%rsp), %rax                 # 8-byte Reload
	addl	%r10d, %eax
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	addl	%r10d, %ecx
	imull	-128(%rsp), %ecx                # 4-byte Folded Reload
	movslq	%ecx, %rcx
	addq	%r8, %rcx
	movq	-80(%rsp), %rsi                 # 8-byte Reload
	addq	%rcx, %rsi
	movq	-88(%rsp), %rbp                 # 8-byte Reload
	addq	%rcx, %rbp
	addq	-8(%rsp), %rcx                  # 8-byte Folded Reload
	vmovups	(%r15,%rcx,4), %ymm0
	movq	-32(%rsp), %rcx                 # 8-byte Reload
	addl	%r10d, %ecx
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	imull	-48(%rsp), %ecx                 # 4-byte Folded Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	addl	-104(%rsp), %ecx                # 4-byte Folded Reload
	movslq	%ecx, %rcx
	cltq
	addq	%r8, %rax
	addq	%r8, %rcx
	movq	-64(%rsp), %rbx                 # 8-byte Reload
	vmulps	(%rbx,%rcx,4), %ymm0, %ymm0
	vmulps	(%r15,%rbp,4), %ymm0, %ymm0
	vmovups	(%r15,%rsi,4), %ymm5
	vmulps	%ymm4, %ymm0, %ymm0
	vcmpleps	%ymm3, %ymm5, %ymm5
	vandps	%ymm0, %ymm5, %ymm0
	movq	-96(%rsp), %rcx                 # 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rax,4)
.LBB31_15:                              # %middle.block
                                        #   in Loop: Header=BB31_2 Depth=1
	movq	56(%rsp), %rax                  # 8-byte Reload
	movq	%rax, %rsi
	cmpq	-112(%rsp), %rax                # 8-byte Folded Reload
	movq	88(%rsp), %r9                   # 8-byte Reload
	movq	80(%rsp), %r8                   # 8-byte Reload
	movq	72(%rsp), %r11                  # 8-byte Reload
	movq	64(%rsp), %r14                  # 8-byte Reload
	je	.LBB31_16
.LBB31_7:                               # %"for relu1_0_d_def__.s23.n.ni.preheader13"
                                        #   in Loop: Header=BB31_2 Depth=1
	movq	-112(%rsp), %rax                # 8-byte Reload
	subq	%rsi, %rax
	addq	%rsi, %rdi
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rdi,4), %rdi
	movq	96(%rsp), %rcx                  # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rdx, %rcx
	leaq	(%r15,%rcx,4), %rbx
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rdx, %rcx
	leaq	(%r15,%rcx,4), %rbp
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	addq	%rsi, %rcx
	addq	%rdx, %rcx
	leaq	(%r15,%rcx,4), %rdx
	movq	-16(%rsp), %r13                 # 8-byte Reload
	addq	%rsi, %r13
	movq	-96(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r13,4), %rsi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB31_8:                               # %"for relu1_0_d_def__.s23.n.ni"
                                        #   Parent Loop BB31_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmulss	(%rdi,%rcx,4), %xmm0, %xmm0
	vmulss	(%rbp,%rcx,4), %xmm0, %xmm0
	vcmpltss	(%rdx,%rcx,4), %xmm2, %xmm5
	vmulss	%xmm1, %xmm0, %xmm0
	vandnps	%xmm0, %xmm5, %xmm0
	vaddss	(%rsi,%rcx,4), %xmm0, %xmm0
	vmovss	%xmm0, (%rsi,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rax
	jne	.LBB31_8
	jmp	.LBB31_16
.LBB31_10:                              #   in Loop: Header=BB31_2 Depth=1
	xorl	%r8d, %r8d
	testb	$1, 40(%rsp)                    # 1-byte Folded Reload
	jne	.LBB31_14
	jmp	.LBB31_15
.LBB31_17:                              # %destructor_block
	xorl	%eax, %eax
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end31:
	.size	train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n, .Lfunc_end31-train_cost_model.par_for.relu1_0_d_def__.s23.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n
.LCPI32_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI32_1:
	.long	0x3f800000                      # float 1
.LCPI32_2:
	.long	0x3109705f                      # float 1.99999994E-9
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$176, %rsp
	movq	%rdx, %r11
	movl	%esi, %edi
	movl	12(%rdx), %r8d
	movl	20(%rdx), %ebp
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%esi, %esi
	testl	%ebp, %ebp
	sete	%sil
	movl	%esi, %ebx
	negl	%ebx
	movl	%ebp, %ecx
	sarl	$31, %ecx
	subl	%r9d, %edi
	orl	%ebp, %ebx
	movl	%edi, %eax
	cltd
	idivl	%ebx
                                        # kill: def $edx killed $edx def $rdx
	movq	%rdx, -120(%rsp)                # 8-byte Spill
	movl	%ecx, %r12d
	leal	(%rsi,%rbp), %r10d
	movl	%edi, %eax
	cltd
	idivl	%r10d
	notl	%r12d
	decl	%esi
	movl	%r12d, %r13d
	subl	%ecx, %r13d
	andl	%r9d, %r13d
	addl	%eax, %r13d
	andl	%esi, %r13d
	leal	(%r13,%r13), %eax
	cmpl	$2, %r8d
	movl	$1, %edi
	cmovll	%r8d, %edi
	subl	%eax, %edi
	jle	.LBB32_13
# %bb.1:                                # %"for relu1_0_d_def__.s24.w.wi.preheader"
	movl	(%r11), %edx
	movq	%rdx, -64(%rsp)                 # 8-byte Spill
	movl	16(%r11), %r15d
	movl	24(%r11), %r14d
	movslq	28(%r11), %r8
	movl	32(%r11), %ebx
	xorl	%ebp, %ecx
	movl	36(%r11), %ebp
	addl	%r12d, %ecx
	andl	%r9d, %ecx
	movq	-120(%rsp), %rdx                # 8-byte Reload
	addl	%ecx, %edx
	andl	%esi, %edx
	leal	(,%rdx,8), %r10d
	subl	%r8d, %ebx
	movl	%ebx, -112(%rsp)                # 4-byte Spill
	subl	%r8d, %ebp
	movl	%ebp, -104(%rsp)                # 4-byte Spill
	movl	%eax, %ecx
	subl	%r14d, %ecx
	movl	%r14d, %r12d
	movq	%rcx, 160(%rsp)                 # 8-byte Spill
	subl	8(%r11), %eax
	leal	(%r15,%r15,4), %ecx
	shll	$2, %ecx
	movq	%rcx, -128(%rsp)                # 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$1, %edi
	sete	%cl
	movl	$2, %esi
	subq	%rcx, %rsi
	movq	%rsi, 168(%rsp)                 # 8-byte Spill
	movq	-64(%rsp), %r14                 # 8-byte Reload
	leal	-1(%r14), %ecx
	vmovd	%ecx, %xmm0
	movl	%r8d, %ecx
	negl	%ecx
	vmovd	%r10d, %xmm1
	vmovd	%ecx, %xmm2
	movslq	%r10d, %rbp
	movl	%eax, %r9d
	subl	%r8d, %r10d
	movl	%r15d, %ecx
	movq	%r15, %rdi
	shll	$5, %ecx
	movl	%ecx, -120(%rsp)                # 4-byte Spill
	movl	%r14d, %ecx
	subl	%ebp, %ecx
	cmpl	$9, %ecx
	movl	$8, %esi
	cmovll	%ecx, %esi
	xorl	%ebx, %ebx
	movl	%ecx, -72(%rsp)                 # 4-byte Spill
	testl	%ecx, %ecx
	cmovlel	%ebx, %esi
	movq	%rbp, %r15
	subq	%r8, %r15
	shll	$6, %r13d
	movl	%r12d, %eax
	shll	$5, %eax
	subl	%eax, %r13d
	orl	$20, %r13d
	imull	%edi, %r13d
	movslq	4(%r11), %r12
	movq	40(%r11), %rax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	movq	56(%r11), %rax
	movq	%rax, -96(%rsp)                 # 8-byte Spill
	movq	72(%r11), %rcx
	movq	-128(%rsp), %rax                # 8-byte Reload
	leal	(%rax,%rdx,8), %eax
	movl	%eax, -76(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm1, %ymm1
	vpor	.LCPI32_0(%rip), %ymm1, %ymm1
	movslq	%r9d, %rax
	movq	%rax, %rdi
	movq	%rax, 144(%rsp)                 # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%r12, -128(%rsp)                # 8-byte Spill
	imull	%r12d, %eax
	leal	(%rax,%rdx,8), %r9d
	leal	(%r13,%rdx,8), %r12d
	leal	8(,%rdx,8), %edx
	movl	%edx, -68(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm0
	movl	-112(%rsp), %r8d                # 4-byte Reload
	vmovd	%r8d, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm1
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 136(%rsp)                 # 8-byte Spill
	vpextrd	$1, %xmm1, %edx
	vpextrd	$2, %xmm1, %edi
	movslq	%edx, %rdx
	movq	%rdx, 128(%rsp)                 # 8-byte Spill
	movslq	%edi, %rdx
	movq	%rdx, 120(%rsp)                 # 8-byte Spill
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 112(%rsp)                 # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 104(%rsp)                 # 8-byte Spill
	vpextrd	$1, %xmm1, %edx
	vpextrd	$2, %xmm1, %edi
	movslq	%edx, %rdx
	movq	%rdx, 96(%rsp)                  # 8-byte Spill
	movslq	%edi, %rdx
	movq	%rdx, 88(%rsp)                  # 8-byte Spill
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 80(%rsp)                  # 8-byte Spill
	movl	-104(%rsp), %edi                # 4-byte Reload
	vmovd	%edi, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm1
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 72(%rsp)                  # 8-byte Spill
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 64(%rsp)                  # 8-byte Spill
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 56(%rsp)                  # 8-byte Spill
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 48(%rsp)                  # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovd	%xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 40(%rsp)                  # 8-byte Spill
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 32(%rsp)                  # 8-byte Spill
	vpextrd	$2, %xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 24(%rsp)                  # 8-byte Spill
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	movq	%rdx, 16(%rsp)                  # 8-byte Spill
	vpbroadcastd	%xmm2, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	vmovd	%xmm0, %edx
	vpextrd	$1, %xmm0, %r11d
	movslq	%edx, %rdx
	movq	%rdx, 8(%rsp)                   # 8-byte Spill
	movslq	%r11d, %rdx
	movq	%rdx, (%rsp)                    # 8-byte Spill
	vpextrd	$2, %xmm0, %edx
	movslq	%edx, %rdx
	movq	%rdx, -8(%rsp)                  # 8-byte Spill
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	movq	%rdx, -16(%rsp)                 # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovd	%xmm0, %edx
	vpextrd	$1, %xmm0, %r11d
	movslq	%edx, %rdx
	movq	%rdx, -24(%rsp)                 # 8-byte Spill
	movslq	%r11d, %rdx
	movq	%rdx, -32(%rsp)                 # 8-byte Spill
	vpextrd	$2, %xmm0, %edx
	movslq	%edx, %rdx
	movq	%rdx, -40(%rsp)                 # 8-byte Spill
	vpextrd	$3, %xmm0, %edx
	movslq	%edx, %rdx
	movq	%rdx, -48(%rsp)                 # 8-byte Spill
	addl	%ebp, %edi
	movq	%rbp, 152(%rsp)                 # 8-byte Spill
	addl	%ebp, %r8d
	movslq	%r8d, %rdx
	movslq	%edi, %rdi
	movslq	%r10d, %rbp
	movl	%esi, %r10d
	andl	$2147483616, %r10d              # imm = 0x7FFFFFE0
	leaq	(%rcx,%r15,4), %r15
	addq	$96, %r15
	leaq	96(%rcx,%rdi,4), %r13
	leaq	96(%rcx,%rdx,4), %r8
	leaq	(%rcx,%rbp,4), %r11
	vmovss	.LCPI32_2(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	.LCPI32_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI32_1(%rip), %ymm2  # ymm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI32_2(%rip), %ymm3  # ymm3 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %rdx
	movq	%rdx, -112(%rsp)                # 8-byte Spill
	movq	-96(%rsp), %rax                 # 8-byte Reload
	leaq	96(%rax), %rdx
	movq	%rdx, -56(%rsp)                 # 8-byte Spill
	jmp	.LBB32_2
	.p2align	4, 0x90
.LBB32_3:                               # %true_bb
                                        #   in Loop: Header=BB32_2 Depth=1
	movq	-24(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%rcx,%rdx,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	movq	-32(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, (%rcx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$32, (%rcx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	-48(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	8(%rsp), %rdx                   # 8-byte Reload
	vmovss	(%rcx,%rdx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movq	(%rsp), %rdx                    # 8-byte Reload
	vinsertps	$16, (%rcx,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	-8(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$32, (%rcx,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	-16(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	movq	144(%rsp), %rdx                 # 8-byte Reload
	addq	%rbx, %rdx
	imulq	-128(%rsp), %rdx                # 8-byte Folded Reload
	addq	152(%rsp), %rdx                 # 8-byte Folded Reload
	movq	-88(%rsp), %rdi                 # 8-byte Reload
	vmulps	(%rdi,%rdx,4), %ymm4, %ymm4
	movq	160(%rsp), %rdx                 # 8-byte Reload
	addl	%ebx, %edx
	movq	104(%rsp), %rdi                 # 8-byte Reload
	vmovss	(%rcx,%rdi,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movq	96(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$16, (%rcx,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	88(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$32, (%rcx,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	imull	-120(%rsp), %edx                # 4-byte Folded Reload
	movq	80(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$48, (%rcx,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	136(%rsp), %rdi                 # 8-byte Reload
	vmovss	(%rcx,%rdi,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	movq	128(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, (%rcx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	120(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$32, (%rcx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	addl	-76(%rsp), %edx                 # 4-byte Folded Reload
	movq	112(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	movq	40(%rsp), %rdi                  # 8-byte Reload
	vmovss	(%rcx,%rdi,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movq	32(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$16, (%rcx,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	24(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$32, (%rcx,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	movq	16(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$48, (%rcx,%rdi,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	movq	72(%rsp), %rdi                  # 8-byte Reload
	vmovss	(%rcx,%rdi,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movq	64(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$16, (%rcx,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	56(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$32, (%rcx,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%edx, %rdx
	movq	48(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$48, (%rcx,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm7, %ymm6
	vmulps	%ymm3, %ymm6, %ymm6
	vmulps	%ymm6, %ymm4, %ymm4
	vcmpleps	%ymm2, %ymm5, %ymm5
	vandps	%ymm4, %ymm5, %ymm4
	movq	-96(%rsp), %rdi                 # 8-byte Reload
	vaddps	(%rdi,%rdx,4), %ymm4, %ymm4
	vmovups	%ymm4, (%rdi,%rdx,4)
.LBB32_12:                              # %after_bb
                                        #   in Loop: Header=BB32_2 Depth=1
	incq	%rbx
	addl	-128(%rsp), %r9d                # 4-byte Folded Reload
	addl	-120(%rsp), %r12d               # 4-byte Folded Reload
	cmpq	168(%rsp), %rbx                 # 8-byte Folded Reload
	je	.LBB32_13
.LBB32_2:                               # %"for relu1_0_d_def__.s24.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB32_10 Depth 2
                                        #     Child Loop BB32_8 Depth 2
	cmpl	%r14d, -68(%rsp)                # 4-byte Folded Reload
	jle	.LBB32_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB32_2 Depth=1
	cmpl	$0, -72(%rsp)                   # 4-byte Folded Reload
	jle	.LBB32_12
# %bb.5:                                # %"for relu1_0_d_def__.s24.n.ni.preheader"
                                        #   in Loop: Header=BB32_2 Depth=1
	movslq	%r9d, %rax
	movslq	%r12d, %rbp
	cmpl	$32, %esi
	movq	%rbp, -104(%rsp)                # 8-byte Spill
	jae	.LBB32_9
# %bb.6:                                #   in Loop: Header=BB32_2 Depth=1
	xorl	%edx, %edx
	jmp	.LBB32_7
	.p2align	4, 0x90
.LBB32_9:                               # %vector.body.preheader
                                        #   in Loop: Header=BB32_2 Depth=1
	movq	-112(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	movq	-56(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%rbp,4), %r14
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB32_10:                              # %vector.body
                                        #   Parent Loop BB32_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	-96(%r8,%rbp,4), %ymm4
	vmovups	-64(%r8,%rbp,4), %ymm5
	vmovups	-32(%r8,%rbp,4), %ymm6
	vmovups	(%r8,%rbp,4), %ymm7
	vmovups	-96(%r15,%rbp,4), %ymm8
	vmovups	-64(%r15,%rbp,4), %ymm9
	vmovups	-32(%r15,%rbp,4), %ymm10
	vmovups	(%r15,%rbp,4), %ymm11
	vmulps	-96(%rdx,%rbp,4), %ymm8, %ymm8
	vmulps	-64(%rdx,%rbp,4), %ymm9, %ymm9
	vmulps	-32(%rdx,%rbp,4), %ymm10, %ymm10
	vmulps	(%rdx,%rbp,4), %ymm11, %ymm11
	vmulps	-96(%r13,%rbp,4), %ymm8, %ymm8
	vmulps	-64(%r13,%rbp,4), %ymm9, %ymm9
	vmulps	-32(%r13,%rbp,4), %ymm10, %ymm10
	vmulps	(%r13,%rbp,4), %ymm11, %ymm11
	vmulps	%ymm3, %ymm8, %ymm8
	vmulps	%ymm3, %ymm9, %ymm9
	vmulps	%ymm3, %ymm10, %ymm10
	vmulps	%ymm3, %ymm11, %ymm11
	vcmpleps	%ymm2, %ymm4, %ymm4
	vandps	%ymm4, %ymm8, %ymm4
	vcmpleps	%ymm2, %ymm5, %ymm5
	vandps	%ymm5, %ymm9, %ymm5
	vcmpleps	%ymm2, %ymm6, %ymm6
	vandps	%ymm6, %ymm10, %ymm6
	vcmpleps	%ymm2, %ymm7, %ymm7
	vandps	%ymm7, %ymm11, %ymm7
	vaddps	-96(%r14,%rbp,4), %ymm4, %ymm4
	vaddps	-64(%r14,%rbp,4), %ymm5, %ymm5
	vaddps	-32(%r14,%rbp,4), %ymm6, %ymm6
	vaddps	(%r14,%rbp,4), %ymm7, %ymm7
	vmovups	%ymm4, -96(%r14,%rbp,4)
	vmovups	%ymm5, -64(%r14,%rbp,4)
	vmovups	%ymm6, -32(%r14,%rbp,4)
	vmovups	%ymm7, (%r14,%rbp,4)
	addq	$32, %rbp
	cmpq	%rbp, %r10
	jne	.LBB32_10
# %bb.11:                               # %middle.block
                                        #   in Loop: Header=BB32_2 Depth=1
	movq	%r10, %rdx
	cmpq	%rsi, %r10
	movq	-64(%rsp), %r14                 # 8-byte Reload
	je	.LBB32_12
.LBB32_7:                               # %"for relu1_0_d_def__.s24.n.ni.preheader28"
                                        #   in Loop: Header=BB32_2 Depth=1
	movq	-88(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%rax,4), %rbp
	movq	-96(%rsp), %rdi                 # 8-byte Reload
	movq	-104(%rsp), %rax                # 8-byte Reload
	leaq	(%rdi,%rax,4), %rdi
	.p2align	4, 0x90
.LBB32_8:                               # %"for relu1_0_d_def__.s24.n.ni"
                                        #   Parent Loop BB32_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%r11,%rdx,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmulss	(%rbp,%rdx,4), %xmm4, %xmm4
	vmulss	-96(%r13,%rdx,4), %xmm4, %xmm4
	vcmpltss	-96(%r8,%rdx,4), %xmm1, %xmm5
	vmulss	%xmm0, %xmm4, %xmm4
	vandnps	%xmm4, %xmm5, %xmm4
	vaddss	(%rdi,%rdx,4), %xmm4, %xmm4
	vmovss	%xmm4, (%rdi,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %rsi
	jne	.LBB32_8
	jmp	.LBB32_12
.LBB32_13:                              # %destructor_block
	xorl	%eax, %eax
	addq	$176, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end32:
	.size	train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n, .Lfunc_end32-train_cost_model.par_for.relu1_0_d_def__.s24.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n
.LCPI33_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI33_1:
	.long	0x3f800000                      # float 1
.LCPI33_2:
	.long	0x3109705f                      # float 1.99999994E-9
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$120, %rsp
	movq	%rdx, %r13
	movl	%esi, %edi
	movl	12(%rdx), %r8d
	movl	24(%rdx), %r11d
	movl	%esi, %r10d
	sarl	$31, %r10d
	xorl	%ebp, %ebp
	testl	%r11d, %r11d
	sete	%bpl
	movl	%ebp, %esi
	negl	%esi
	movl	%r11d, %ebx
	sarl	$31, %ebx
	subl	%r10d, %edi
	orl	%r11d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r15d
	movl	%ebx, %esi
	leal	(%r11,%rbp), %r9d
	movl	%edi, %eax
	cltd
	idivl	%r9d
	notl	%esi
	decl	%ebp
	movl	%esi, %edi
	subl	%ebx, %edi
	andl	%r10d, %edi
	addl	%eax, %edi
	andl	%ebp, %edi
	leal	(%rdi,%rdi), %eax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	subl	%eax, %r8d
	jle	.LBB33_17
# %bb.1:                                # %"for relu1_0_d_def__.s25.w.wi.preheader"
	movl	(%r13), %edx
	movl	16(%r13), %r9d
	movl	28(%r13), %r12d
	movslq	32(%r13), %rcx
	movl	36(%r13), %r14d
	movl	40(%r13), %eax
	xorl	%r11d, %ebx
	movq	%r14, %r11
	addl	%esi, %ebx
	andl	%r10d, %ebx
	addl	%ebx, %r15d
	andl	%ebp, %r15d
	leal	(,%r15,8), %ebx
	movl	%ebx, -112(%rsp)                # 4-byte Spill
	subl	%ecx, %r11d
	subl	%ecx, %eax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movq	-80(%rsp), %r14                 # 8-byte Reload
	movl	%r14d, %eax
	movl	%r14d, %esi
	subl	8(%r13), %esi
	movq	%rsi, -40(%rsp)                 # 8-byte Spill
	subl	%r12d, %eax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, %r8d
	movq	%rdx, %r8
	sete	%al
	movl	$2, %ebp
	subq	%rax, %rbp
	movq	%rbp, 112(%rsp)                 # 8-byte Spill
	movl	%r9d, %eax
	shll	$5, %eax
	movl	%eax, -120(%rsp)                # 4-byte Spill
	leal	-1(%rdx), %eax
	vmovd	%eax, %xmm0
	movslq	%ebx, %rbp
	movl	%r8d, %eax
	subl	%ebp, %eax
	cmpl	$9, %eax
	movl	$8, %edx
	cmovll	%eax, %edx
	xorl	%r10d, %r10d
	movl	%eax, -48(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r10d, %edx
	shll	$6, %edi
	shll	$5, %r12d
	subl	%r12d, %edi
	leal	(%r9,%r9,8), %eax
	leal	(%r9,%rax,2), %eax
	orl	$19, %edi
	imull	%r9d, %edi
	movq	%rcx, %r9
	movslq	4(%r13), %rcx
	movl	20(%r13), %esi
	movl	%esi, -124(%rsp)                # 4-byte Spill
	movq	48(%r13), %rsi
	movq	64(%r13), %rbx
	movq	%rbx, -104(%rsp)                # 8-byte Spill
	movq	80(%r13), %r12
	leal	(%rax,%r15,8), %eax
	movl	%eax, -108(%rsp)                # 4-byte Spill
	vmovd	-112(%rsp), %xmm1               # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpbroadcastd	%xmm1, %ymm1
	vpor	.LCPI33_0(%rip), %ymm1, %ymm1
	movslq	-40(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, %rbx
	movq	%rax, 88(%rsp)                  # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rcx, -56(%rsp)                 # 8-byte Spill
	imull	%ecx, %eax
	leal	(%rax,%r15,8), %eax
	movl	%eax, -116(%rsp)                # 4-byte Spill
	leal	(%rdi,%r15,8), %r13d
	leal	8(,%r15,8), %eax
	movl	%eax, -44(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm8
	movl	%ebp, %eax
	subl	%r9d, %eax
	leal	(%r11,%rbp), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, -88(%rsp)                 # 8-byte Spill
	movq	-32(%rsp), %rcx                 # 8-byte Reload
	leal	(%rbp,%rcx), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, -96(%rsp)                 # 8-byte Spill
	cltq
	movq	%rdx, -64(%rsp)                 # 8-byte Spill
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$2147483640, %edx               # imm = 0x7FFFFFF8
	movq	%rdx, 48(%rsp)                  # 8-byte Spill
	leaq	-8(%rdx), %rcx
	movq	%rcx, 40(%rsp)                  # 8-byte Spill
	movq	%rcx, %rdx
	shrq	$3, %rdx
	incq	%rdx
	movq	%rdx, %rcx
	movq	%rdx, 32(%rsp)                  # 8-byte Spill
	andq	$-2, %rdx
	negq	%rdx
	movq	%rdx, 24(%rsp)                  # 8-byte Spill
	movq	%rbp, 96(%rsp)                  # 8-byte Spill
	subq	%r9, %rbp
	movq	%rbp, 80(%rsp)                  # 8-byte Spill
	vmovss	.LCPI33_2(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI33_1(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI33_1(%rip), %ymm3  # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI33_2(%rip), %ymm4  # ymm4 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movq	%rsi, -72(%rsp)                 # 8-byte Spill
	leaq	32(%rsi), %rcx
	movq	%rcx, 16(%rsp)                  # 8-byte Spill
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	leaq	(%r12,%rax,4), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	leaq	32(%r12), %rax
	movq	%rax, (%rsp)                    # 8-byte Spill
	movq	-104(%rsp), %rax                # 8-byte Reload
	leaq	32(%rax), %rax
	movq	%rax, -8(%rsp)                  # 8-byte Spill
	movl	%r14d, %r15d
	movq	%r8, 72(%rsp)                   # 8-byte Spill
	movq	%r9, 64(%rsp)                   # 8-byte Spill
	movq	%r11, 56(%rsp)                  # 8-byte Spill
	jmp	.LBB33_2
	.p2align	4, 0x90
.LBB33_3:                               # %true_bb
                                        #   in Loop: Header=BB33_2 Depth=1
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%r10d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edx
	imull	-124(%rsp), %edx                # 4-byte Folded Reload
	leal	(%rdx,%r11), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm0
	vextracti128	$1, %ymm0, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r12,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm5, %eax
	vinsertps	$16, (%r12,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm0, %ecx
	cltq
	vinsertps	$32, (%r12,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r12,%rcx,4), %xmm6, %xmm9 # xmm9 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %ecx
	vmovss	(%r12,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r12,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm0, %eax
	cltq
	vinsertps	$32, (%r12,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm0, %eax
	cltq
	vinsertps	$48, (%r12,%rax,4), %xmm5, %xmm10 # xmm10 = xmm5[0,1,2],mem[0]
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm7
	vextracti128	$1, %ymm7, %xmm0
	vmovd	%xmm0, %eax
	cltq
	vmovss	(%r12,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm0, %eax
	cltq
	vinsertps	$16, (%r12,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm0, %eax
	cltq
	vinsertps	$32, (%r12,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm0, %eax
	cltq
	vinsertps	$48, (%r12,%rax,4), %xmm5, %xmm11 # xmm11 = xmm5[0,1,2],mem[0]
	vmovd	%xmm7, %eax
	cltq
	vmovss	(%r12,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %eax
	cltq
	vinsertps	$16, (%r12,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm7, %eax
	cltq
	vinsertps	$32, (%r12,%rax,4), %xmm5, %xmm12 # xmm12 = xmm5[0,1],mem[0],xmm5[3]
	subl	%r9d, %edx
	vmovd	%edx, %xmm6
	vpbroadcastd	%xmm6, %ymm6
	vpaddd	%ymm6, %ymm8, %ymm6
	vextracti128	$1, %ymm6, %xmm0
	vmovd	%xmm0, %eax
	cltq
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r12,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%r12,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm6, %ecx
	cltq
	vinsertps	$32, (%r12,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r12,%rcx,4), %xmm5, %xmm0 # xmm0 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm6, %ecx
	vmovss	(%r12,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r12,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrd	$2, %xmm6, %eax
	cltq
	vinsertps	$32, (%r12,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vpextrd	$3, %xmm6, %eax
	vinsertf128	$1, %xmm9, %ymm10, %ymm6
	cltq
	vinsertps	$48, (%r12,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vpextrd	$3, %xmm7, %eax
	vinsertf128	$1, %xmm0, %ymm5, %ymm0
	cltq
	vinsertps	$48, (%r12,%rax,4), %xmm12, %xmm5 # xmm5 = xmm12[0,1,2],mem[0]
	movq	88(%rsp), %rax                  # 8-byte Reload
	addq	%r10, %rax
	imulq	-56(%rsp), %rax                 # 8-byte Folded Reload
	addq	96(%rsp), %rax                  # 8-byte Folded Reload
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm0, %ymm0
	movq	-24(%rsp), %rax                 # 8-byte Reload
	addl	%r10d, %eax
	imull	-120(%rsp), %eax                # 4-byte Folded Reload
	addl	-108(%rsp), %eax                # 4-byte Folded Reload
	cltq
	vcmpltps	%ymm6, %ymm3, %ymm6
	vinsertf128	$1, %xmm11, %ymm5, %ymm5
	vmulps	%ymm4, %ymm5, %ymm5
	vmulps	%ymm5, %ymm0, %ymm0
	vandps	%ymm0, %ymm6, %ymm0
	movq	-104(%rsp), %rcx                # 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rax,4)
.LBB33_16:                              # %after_bb
                                        #   in Loop: Header=BB33_2 Depth=1
	incq	%r10
	movl	-116(%rsp), %eax                # 4-byte Reload
	addl	-56(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -116(%rsp)                # 4-byte Spill
	incl	%r15d
	addl	-120(%rsp), %r13d               # 4-byte Folded Reload
	cmpq	112(%rsp), %r10                 # 8-byte Folded Reload
	je	.LBB33_17
.LBB33_2:                               # %"for relu1_0_d_def__.s25.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB33_12 Depth 2
                                        #     Child Loop BB33_8 Depth 2
	cmpl	%r8d, -44(%rsp)                 # 4-byte Folded Reload
	jle	.LBB33_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB33_2 Depth=1
	cmpl	$0, -48(%rsp)                   # 4-byte Folded Reload
	jle	.LBB33_16
# %bb.5:                                # %"for relu1_0_d_def__.s25.n.ni.preheader"
                                        #   in Loop: Header=BB33_2 Depth=1
	movl	%r15d, %eax
	sarl	$31, %eax
	andnl	%r15d, %eax, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rdx
	movslq	-116(%rsp), %rbx                # 4-byte Folded Reload
	movslq	%r13d, %rdi
	cmpl	$7, -64(%rsp)                   # 4-byte Folded Reload
	movq	%rdi, 104(%rsp)                 # 8-byte Spill
	ja	.LBB33_9
# %bb.6:                                #   in Loop: Header=BB33_2 Depth=1
	xorl	%edi, %edi
	jmp	.LBB33_7
	.p2align	4, 0x90
.LBB33_9:                               # %vector.ph
                                        #   in Loop: Header=BB33_2 Depth=1
	cmpq	$0, 40(%rsp)                    # 8-byte Folded Reload
	je	.LBB33_10
# %bb.11:                               # %vector.body.preheader
                                        #   in Loop: Header=BB33_2 Depth=1
	movq	-96(%rsp), %rax                 # 8-byte Reload
	addq	%rdx, %rax
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rdx), %rcx
	movq	16(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rbx,4), %r9
	movq	8(%rsp), %rsi                   # 8-byte Reload
	leaq	(%rsi,%rdx,4), %r8
	movq	(%rsp), %rsi                    # 8-byte Reload
	leaq	(%rsi,%rax,4), %rax
	leaq	(%rsi,%rcx,4), %rbp
	movq	-8(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rdi,4), %r11
	movq	24(%rsp), %r14                  # 8-byte Reload
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB33_12:                              # %vector.body
                                        #   Parent Loop BB33_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vcmpltps	-32(%rbp,%rsi,4), %ymm3, %ymm0
	vmovups	(%r8,%rsi,4), %ymm5
	vmulps	-32(%r9,%rsi,4), %ymm5, %ymm5
	vmulps	-32(%rax,%rsi,4), %ymm5, %ymm5
	vmulps	%ymm4, %ymm5, %ymm5
	vandps	%ymm5, %ymm0, %ymm0
	vaddps	-32(%r11,%rsi,4), %ymm0, %ymm0
	vmovups	%ymm0, -32(%r11,%rsi,4)
	vcmpltps	(%rbp,%rsi,4), %ymm3, %ymm0
	vmovups	32(%r8,%rsi,4), %ymm5
	vmulps	(%r9,%rsi,4), %ymm5, %ymm5
	vmulps	(%rax,%rsi,4), %ymm5, %ymm5
	vmulps	%ymm4, %ymm5, %ymm5
	vandps	%ymm5, %ymm0, %ymm0
	vaddps	(%r11,%rsi,4), %ymm0, %ymm0
	vmovups	%ymm0, (%r11,%rsi,4)
	addq	$16, %rsi
	addq	$2, %r14
	jne	.LBB33_12
# %bb.13:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB33_2 Depth=1
	testb	$1, 32(%rsp)                    # 1-byte Folded Reload
	je	.LBB33_15
.LBB33_14:                              # %vector.body.epil
                                        #   in Loop: Header=BB33_2 Depth=1
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addl	%r10d, %eax
	movq	-24(%rsp), %rcx                 # 8-byte Reload
	addl	%r10d, %ecx
	movl	%eax, %edi
	sarl	$31, %edi
	andnl	%eax, %edi, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	cltq
	addq	%rsi, %rax
	movq	-88(%rsp), %rdi                 # 8-byte Reload
	addq	%rax, %rdi
	vcmpltps	(%r12,%rdi,4), %ymm3, %ymm0
	movq	-96(%rsp), %rdi                 # 8-byte Reload
	addq	%rax, %rdi
	addq	-16(%rsp), %rax                 # 8-byte Folded Reload
	vmovups	(%r12,%rax,4), %ymm5
	movq	-40(%rsp), %rax                 # 8-byte Reload
	addl	%r10d, %eax
	imull	-120(%rsp), %ecx                # 4-byte Folded Reload
	imull	-56(%rsp), %eax                 # 4-byte Folded Reload
	addl	-108(%rsp), %ecx                # 4-byte Folded Reload
	addl	-112(%rsp), %eax                # 4-byte Folded Reload
	cltq
	movslq	%ecx, %rcx
	addq	%rsi, %rcx
	addq	%rsi, %rax
	movq	-72(%rsp), %rsi                 # 8-byte Reload
	vmulps	(%rsi,%rax,4), %ymm5, %ymm5
	vmulps	(%r12,%rdi,4), %ymm5, %ymm5
	vmulps	%ymm4, %ymm5, %ymm5
	vandps	%ymm5, %ymm0, %ymm0
	movq	-104(%rsp), %rax                # 8-byte Reload
	vaddps	(%rax,%rcx,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rax,%rcx,4)
.LBB33_15:                              # %middle.block
                                        #   in Loop: Header=BB33_2 Depth=1
	movq	48(%rsp), %rax                  # 8-byte Reload
	movq	%rax, %rdi
	cmpq	-64(%rsp), %rax                 # 8-byte Folded Reload
	movq	72(%rsp), %r8                   # 8-byte Reload
	movq	64(%rsp), %r9                   # 8-byte Reload
	movq	56(%rsp), %r11                  # 8-byte Reload
	je	.LBB33_16
.LBB33_7:                               # %"for relu1_0_d_def__.s25.n.ni.preheader13"
                                        #   in Loop: Header=BB33_2 Depth=1
	movq	-64(%rsp), %rsi                 # 8-byte Reload
	subq	%rdi, %rsi
	addq	%rdi, %rbx
	movq	-72(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rbx
	movq	80(%rsp), %rax                  # 8-byte Reload
	addq	%rdi, %rax
	addq	%rdx, %rax
	leaq	(%r12,%rax,4), %rax
	movq	-96(%rsp), %rcx                 # 8-byte Reload
	addq	%rdi, %rcx
	addq	%rdx, %rcx
	leaq	(%r12,%rcx,4), %rbp
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	addq	%rdi, %rcx
	addq	%rdx, %rcx
	leaq	(%r12,%rcx,4), %rdx
	movq	104(%rsp), %r14                 # 8-byte Reload
	addq	%rdi, %r14
	movq	-104(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%r14,4), %rdi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB33_8:                               # %"for relu1_0_d_def__.s25.n.ni"
                                        #   Parent Loop BB33_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmulss	(%rbx,%rcx,4), %xmm0, %xmm0
	vmulss	(%rbp,%rcx,4), %xmm0, %xmm0
	vcmpltss	(%rdx,%rcx,4), %xmm2, %xmm5
	vmulss	%xmm1, %xmm0, %xmm0
	vandps	%xmm0, %xmm5, %xmm0
	vaddss	(%rdi,%rcx,4), %xmm0, %xmm0
	vmovss	%xmm0, (%rdi,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rsi
	jne	.LBB33_8
	jmp	.LBB33_16
.LBB33_10:                              #   in Loop: Header=BB33_2 Depth=1
	xorl	%esi, %esi
	testb	$1, 32(%rsp)                    # 1-byte Folded Reload
	jne	.LBB33_14
	jmp	.LBB33_15
.LBB33_17:                              # %destructor_block
	xorl	%eax, %eax
	addq	$120, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end33:
	.size	train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n, .Lfunc_end33-train_cost_model.par_for.relu1_0_d_def__.s25.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n
.LCPI34_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI34_1:
	.long	0x3f800000                      # float 1
.LCPI34_2:
	.long	0x3109705f                      # float 1.99999994E-9
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$48, %rsp
	movq	%rdx, %rcx
	movl	%esi, %edi
	movl	12(%rdx), %r8d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %esi
	negl	%esi
	subl	%r9d, %edi
	orl	%r10d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r13d
	movl	%r10d, %esi
	sarl	$31, %esi
	movl	%esi, %r11d
	notl	%r11d
	leal	(%r10,%rbx), %ebp
	decl	%ebx
	movl	%edi, %eax
	cltd
	idivl	%ebp
	movl	%r11d, %ebp
	subl	%esi, %ebp
	andl	%r9d, %ebp
	addl	%eax, %ebp
	andl	%ebx, %ebp
	leal	(%rbp,%rbp), %edx
	movl	%r8d, %eax
	subl	%edx, %eax
	movl	$1, %edi
	movq	%rdx, -48(%rsp)                 # 8-byte Spill
	subl	%edx, %edi
	cmpl	$1, %r8d
	cmovlel	%eax, %edi
	cmpl	$3, %edi
	movl	$2, %r8d
	cmovgel	%r8d, %edi
	movl	%edi, %edx
	sarl	$31, %edx
	andnl	%edi, %edx, %r14d
	cmpl	$3, %eax
	cmovgel	%r8d, %eax
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %eax
	movl	%eax, -120(%rsp)                # 4-byte Spill
	cmpl	%eax, %r14d
	jge	.LBB34_15
# %bb.1:                                # %"for relu1_0_d_def__.s26.w.wi.preheader"
	movl	(%rcx), %r8d
	movl	8(%rcx), %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	movl	28(%rcx), %r12d
	movslq	32(%rcx), %rdi
	movl	36(%rcx), %r15d
	xorl	%r10d, %esi
	movl	40(%rcx), %eax
	addl	%r11d, %esi
	movl	44(%rcx), %r11d
	andl	%r9d, %esi
	addl	%esi, %r13d
	andl	%ebx, %r13d
	movl	%r15d, %edx
	movq	%rdi, -56(%rsp)                 # 8-byte Spill
	subl	%edi, %edx
	movq	%rdx, 16(%rsp)                  # 8-byte Spill
	subl	%edi, %eax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	subl	%edi, %r11d
	movq	-48(%rsp), %rsi                 # 8-byte Reload
	movl	%esi, %edx
	subl	%r12d, %edx
	movl	%r12d, %r9d
	movq	%rdx, 8(%rsp)                   # 8-byte Spill
	leal	-1(%r8), %edx
	vmovd	%edx, %xmm0
	movl	%esi, %r10d
	movl	-128(%rsp), %edi                # 4-byte Reload
	subl	%edi, %r10d
	movl	16(%rcx), %esi
	leal	(%rsi,%rsi,8), %r12d
	addl	%r12d, %r12d
	movl	%esi, %ebx
	shll	$5, %ebx
	movl	%ebx, -100(%rsp)                # 4-byte Spill
	leal	(,%r13,8), %ebx
	vmovd	%ebx, %xmm1
	movslq	%ebx, %rdx
	movq	%r8, 40(%rsp)                   # 8-byte Spill
	movl	%r8d, %eax
	subl	%edx, %eax
	cmpl	$9, %eax
	movl	$8, %r8d
	cmovll	%eax, %r8d
	xorl	%ebx, %ebx
	movl	%eax, -72(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%ebx, %r8d
	leal	(%r14,%rbp,2), %ebp
	movl	%ebp, %ebx
	subl	%edi, %ebx
	movl	20(%rcx), %eax
	movl	%eax, -104(%rsp)                # 4-byte Spill
	imull	%ebp, %eax
	subl	%r9d, %ebp
	shll	$5, %ebp
	orl	$18, %ebp
	imull	%esi, %ebp
	movslq	4(%rcx), %rsi
	movq	%r14, %r9
	movq	48(%rcx), %rdi
	movq	%rdi, -88(%rsp)                 # 8-byte Spill
	movq	64(%rcx), %rdi
	movq	%rdi, -96(%rsp)                 # 8-byte Spill
	movq	80(%rcx), %r14
	leal	(%r12,%r13,8), %ecx
	movl	%ecx, -76(%rsp)                 # 4-byte Spill
	movq	%rsi, -16(%rsp)                 # 8-byte Spill
	imull	%esi, %ebx
	leal	(%rbx,%r13,8), %ecx
	movl	%ecx, -128(%rsp)                # 4-byte Spill
	movl	%eax, -108(%rsp)                # 4-byte Spill
	addl	%eax, %r15d
	leal	(%r15,%r13,8), %r15d
	leal	(%rbp,%r13,8), %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	leal	8(,%r13,8), %eax
	movl	%eax, -68(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm1, %ymm1
	vpor	.LCPI34_0(%rip), %ymm1, %ymm1
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm1, %ymm0, %ymm8
	movslq	%r10d, %rax
	movq	%rax, -8(%rsp)                  # 8-byte Spill
	movq	-64(%rsp), %rax                 # 8-byte Reload
	leal	(%rdx,%rax), %eax
	cltq
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	%r11, 24(%rsp)                  # 8-byte Spill
	leal	(%rdx,%r11), %eax
	cltq
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movl	%r9d, %r12d
	movl	-120(%rsp), %eax                # 4-byte Reload
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	%r8, -120(%rsp)                 # 8-byte Spill
                                        # kill: def $r8d killed $r8d killed $r8 def $r8
	andl	$2147483640, %r8d               # imm = 0x7FFFFFF8
	movq	%rdx, (%rsp)                    # 8-byte Spill
	movq	-56(%rsp), %rax                 # 8-byte Reload
	subq	%rax, %rdx
	movq	%rdx, -40(%rsp)                 # 8-byte Spill
	subl	%eax, %r15d
	vmovss	.LCPI34_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI34_2(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI34_1(%rip), %ymm3  # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI34_2(%rip), %ymm4  # ymm4 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	jmp	.LBB34_2
	.p2align	4, 0x90
.LBB34_3:                               # %true_bb
                                        #   in Loop: Header=BB34_2 Depth=1
	movq	-48(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%r12), %esi
	imull	-104(%rsp), %esi                # 4-byte Folded Reload
	movq	16(%rsp), %rax                  # 8-byte Reload
	addl	%esi, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm0
	vextracti128	$1, %ymm0, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r14,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r14,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm0, %eax
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r14,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %ecx
	cltq
	movslq	%ecx, %rcx
	vinsertps	$48, (%r14,%rcx,4), %xmm6, %xmm9 # xmm9 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r14,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%r14,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	cltq
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r14,%rax,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r14,%rcx,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1,2],mem[0]
	movl	%esi, %eax
	subl	-56(%rsp), %eax                 # 4-byte Folded Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm0
	vextracti128	$1, %ymm0, %xmm7
	vmovd	%xmm7, %eax
	cltq
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r14,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm7, %eax
	vinsertps	$16, (%r14,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm0, %ecx
	cltq
	vinsertps	$32, (%r14,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r14,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r14,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%r14,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$3, %xmm0, %ecx
	cltq
	movslq	%ecx, %rcx
	vinsertps	$32, (%r14,%rax,4), %xmm7, %xmm0 # xmm0 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%r14,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm0, %ymm0
	movq	-8(%rsp), %rax                  # 8-byte Reload
	addq	%r12, %rax
	imulq	-16(%rsp), %rax                 # 8-byte Folded Reload
	addq	(%rsp), %rax                    # 8-byte Folded Reload
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm0, %ymm11
	movq	-64(%rsp), %rax                 # 8-byte Reload
	addl	%esi, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm5
	vextracti128	$1, %ymm5, %xmm0
	vmovd	%xmm0, %eax
	cltq
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r14,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%r14,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%r14,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r14,%rcx,4), %xmm6, %xmm12 # xmm12 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%r14,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r14,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%r14,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%r14,%rax,4), %xmm6, %xmm5 # xmm5 = xmm6[0,1,2],mem[0]
	addl	24(%rsp), %esi                  # 4-byte Folded Reload
	vmovd	%esi, %xmm6
	vpbroadcastd	%xmm6, %ymm6
	vpaddd	%ymm6, %ymm8, %ymm6
	vextracti128	$1, %ymm6, %xmm7
	vmovd	%xmm7, %eax
	cltq
	vmovss	(%r14,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm7, %eax
	cltq
	vinsertps	$16, (%r14,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpextrd	$2, %xmm7, %eax
	cltq
	vinsertps	$32, (%r14,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrd	$3, %xmm7, %eax
	cltq
	vinsertps	$48, (%r14,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovd	%xmm6, %eax
	cltq
	vmovss	(%r14,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %eax
	cltq
	vinsertps	$16, (%r14,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$2, %xmm6, %eax
	vinsertf128	$1, %xmm9, %ymm10, %ymm9
	cltq
	vinsertps	$32, (%r14,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertf128	$1, %xmm12, %ymm5, %ymm5
	vpextrd	$3, %xmm6, %eax
	cltq
	vinsertps	$48, (%r14,%rax,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	movq	8(%rsp), %rax                   # 8-byte Reload
	addl	%r12d, %eax
	imull	-100(%rsp), %eax                # 4-byte Folded Reload
	vinsertf128	$1, %xmm0, %ymm6, %ymm0
	vmaxps	%ymm3, %ymm0, %ymm0
	vmulps	%ymm5, %ymm9, %ymm5
	vmulps	%ymm4, %ymm5, %ymm5
	vmulps	%ymm5, %ymm11, %ymm5
	vdivps	%ymm0, %ymm5, %ymm0
	addl	-76(%rsp), %eax                 # 4-byte Folded Reload
	cltq
	vcmpleps	%ymm3, %ymm9, %ymm5
	vandps	%ymm0, %ymm5, %ymm0
	movq	-96(%rsp), %rcx                 # 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rax,4)
.LBB34_14:                              # %after_bb
                                        #   in Loop: Header=BB34_2 Depth=1
	incq	%r12
	movl	-128(%rsp), %eax                # 4-byte Reload
	addl	-16(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -128(%rsp)                # 4-byte Spill
	movl	-104(%rsp), %eax                # 4-byte Reload
	addl	%eax, -108(%rsp)                # 4-byte Folded Spill
	addl	%eax, %r15d
	movl	-124(%rsp), %eax                # 4-byte Reload
	addl	-100(%rsp), %eax                # 4-byte Folded Reload
	movl	%eax, -124(%rsp)                # 4-byte Spill
	cmpq	32(%rsp), %r12                  # 8-byte Folded Reload
	je	.LBB34_15
.LBB34_2:                               # %"for relu1_0_d_def__.s26.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB34_12 Depth 2
                                        #     Child Loop BB34_8 Depth 2
	movl	-68(%rsp), %eax                 # 4-byte Reload
	cmpl	40(%rsp), %eax                  # 4-byte Folded Reload
	jle	.LBB34_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB34_2 Depth=1
	cmpl	$0, -72(%rsp)                   # 4-byte Folded Reload
	jle	.LBB34_14
# %bb.5:                                # %"for relu1_0_d_def__.s26.n.ni.preheader"
                                        #   in Loop: Header=BB34_2 Depth=1
	movslq	-108(%rsp), %r10                # 4-byte Folded Reload
	movslq	-128(%rsp), %rdx                # 4-byte Folded Reload
	movslq	%r15d, %r13
	movslq	-124(%rsp), %r9                 # 4-byte Folded Reload
	cmpl	$8, -120(%rsp)                  # 4-byte Folded Reload
	jae	.LBB34_11
# %bb.6:                                #   in Loop: Header=BB34_2 Depth=1
	xorl	%ecx, %ecx
	jmp	.LBB34_7
	.p2align	4, 0x90
.LBB34_11:                              # %vector.body.preheader
                                        #   in Loop: Header=BB34_2 Depth=1
	movq	-32(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r10), %rcx
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r10), %rsi
	movq	-40(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r10), %rdi
	movq	-88(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdx,4), %rax
	leaq	(%r14,%rcx,4), %rcx
	leaq	(%r14,%rsi,4), %rsi
	leaq	(%r14,%rdi,4), %rdi
	leaq	(%r14,%r13,4), %rbx
	movq	-96(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%r9,4), %rbp
	xorl	%r11d, %r11d
	.p2align	4, 0x90
.LBB34_12:                              # %vector.body
                                        #   Parent Loop BB34_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rbx,%r11,4), %ymm0
	vmovups	(%rdi,%r11,4), %ymm5
	vmulps	(%rax,%r11,4), %ymm5, %ymm5
	vmovups	(%rcx,%r11,4), %ymm6
	vmaxps	%ymm3, %ymm6, %ymm6
	vmulps	(%rsi,%r11,4), %ymm0, %ymm7
	vmulps	%ymm7, %ymm5, %ymm5
	vmulps	%ymm4, %ymm5, %ymm5
	vdivps	%ymm6, %ymm5, %ymm5
	vcmpleps	%ymm3, %ymm0, %ymm0
	vandps	%ymm5, %ymm0, %ymm0
	vaddps	(%rbp,%r11,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rbp,%r11,4)
	addq	$8, %r11
	cmpq	%r11, %r8
	jne	.LBB34_12
# %bb.13:                               # %middle.block
                                        #   in Loop: Header=BB34_2 Depth=1
	movq	%r8, %rcx
	cmpq	-120(%rsp), %r8                 # 8-byte Folded Reload
	je	.LBB34_14
.LBB34_7:                               # %"for relu1_0_d_def__.s26.n.ni.preheader14"
                                        #   in Loop: Header=BB34_2 Depth=1
	movq	-32(%rsp), %rax                 # 8-byte Reload
	addq	%rcx, %rax
	addq	%r10, %rax
	movq	-24(%rsp), %rsi                 # 8-byte Reload
	addq	%rcx, %rsi
	addq	%r10, %rsi
	movq	-40(%rsp), %rdi                 # 8-byte Reload
	addq	%rcx, %rdi
	addq	%r10, %rdi
	movq	-120(%rsp), %rbx                # 8-byte Reload
	subq	%rcx, %rbx
	addq	%rcx, %rdx
	movq	-88(%rsp), %rbp                 # 8-byte Reload
	leaq	(,%rdx,4), %r11
	addq	%rbp, %r11
	leaq	(%r14,%rax,4), %r10
	leaq	(%r14,%rsi,4), %rax
	leaq	(%r14,%rdi,4), %rsi
	addq	%rcx, %r13
	leaq	(%r14,%r13,4), %rdi
	addq	%rcx, %r9
	movq	-96(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r9,4), %rcx
	xorl	%ebp, %ebp
	jmp	.LBB34_8
	.p2align	4, 0x90
.LBB34_10:                              # %select.end
                                        #   in Loop: Header=BB34_8 Depth=2
	vaddss	%xmm6, %xmm0, %xmm0
	vmovss	%xmm0, (%rcx,%rbp,4)
	incq	%rbp
	cmpq	%rbp, %rbx
	je	.LBB34_14
.LBB34_8:                               # %"for relu1_0_d_def__.s26.n.ni"
                                        #   Parent Loop BB34_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rdi,%rbp,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vmovss	(%rcx,%rbp,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vucomiss	%xmm5, %xmm1
	setb	%dl
	vmovss	(%rsi,%rbp,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmulss	(%r11,%rbp,4), %xmm6, %xmm6
	vmulss	(%rax,%rbp,4), %xmm5, %xmm5
	vmulss	%xmm5, %xmm6, %xmm5
	vmulss	%xmm2, %xmm5, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	testb	$1, %dl
	jne	.LBB34_10
# %bb.9:                                # %select.false.sink
                                        #   in Loop: Header=BB34_8 Depth=2
	vmovss	(%r10,%rbp,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmaxss	%xmm1, %xmm6, %xmm6
	vdivss	%xmm6, %xmm5, %xmm6
	jmp	.LBB34_10
.LBB34_15:                              # %destructor_block
	xorl	%eax, %eax
	addq	$48, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end34:
	.size	train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n, .Lfunc_end34-train_cost_model.par_for.relu1_0_d_def__.s26.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n
.LCPI35_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI35_1:
	.long	0x3f800000                      # float 1
.LCPI35_2:
	.long	0x3109705f                      # float 1.99999994E-9
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$208, %rsp
	movq	%rdx, %r15
	movl	%esi, %edi
	movl	12(%rdx), %ecx
	movl	20(%rdx), %r9d
	movl	%esi, %r8d
	sarl	$31, %r8d
	xorl	%esi, %esi
	testl	%r9d, %r9d
	sete	%sil
	movl	%esi, %ebp
	negl	%ebp
	movl	%r9d, %ebx
	sarl	$31, %ebx
	subl	%r8d, %edi
	orl	%r9d, %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r13d
	movl	%ebx, %r10d
	leal	(%r9,%rsi), %ebp
	movl	%edi, %eax
	cltd
	idivl	%ebp
	notl	%r10d
	decl	%esi
	movl	%r10d, %ebp
	subl	%ebx, %ebp
	andl	%r8d, %ebp
	addl	%eax, %ebp
	andl	%esi, %ebp
	leal	(%rbp,%rbp), %edi
	cmpl	$2, %ecx
	movl	$1, %edx
	cmovll	%ecx, %edx
	subl	%edi, %edx
	jle	.LBB35_12
# %bb.1:                                # %"for relu1_0_d_def__.s27.w.wi.preheader"
	movl	(%r15), %eax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	movl	16(%r15), %r11d
	movl	24(%r15), %r14d
	movl	28(%r15), %ecx
	movl	32(%r15), %eax
	xorl	%r9d, %ebx
	movl	36(%r15), %r12d
	addl	%r10d, %ebx
	movl	40(%r15), %r10d
	andl	%r8d, %ebx
	movq	%r13, %r8
	addl	%ebx, %r8d
	andl	%esi, %r8d
	subl	%ecx, %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	subl	%ecx, %r12d
	subl	%ecx, %r10d
	movl	%edi, %eax
	subl	%r14d, %eax
	movq	%rax, 192(%rsp)                 # 8-byte Spill
	subl	8(%r15), %edi
	movl	%r11d, %eax
	shll	$4, %eax
	addl	%r11d, %eax
	movl	%r11d, %r9d
	xorl	%esi, %esi
	cmpl	$1, %edx
	sete	%sil
	movl	$2, %edx
	subq	%rsi, %rdx
	movq	%rdx, 200(%rsp)                 # 8-byte Spill
	movq	-88(%rsp), %rbx                 # 8-byte Reload
	leal	-1(%rbx), %edx
	vmovd	%edx, %xmm0
	leal	(,%r8,8), %edx
	movl	%edx, %esi
	subl	%ecx, %esi
	movl	%esi, -120(%rsp)                # 4-byte Spill
	negl	%ecx
	vmovd	%ecx, %xmm1
	vmovd	%edx, %xmm2
	movslq	%edx, %r11
	movl	%r9d, %ecx
	shll	$5, %ecx
	movl	%ecx, -128(%rsp)                # 4-byte Spill
	movl	%ebx, %ecx
	subl	%r11d, %ecx
	cmpl	$9, %ecx
	movl	$8, %edx
	cmovll	%ecx, %edx
	xorl	%esi, %esi
	movl	%ecx, -112(%rsp)                # 4-byte Spill
	testl	%ecx, %ecx
	cmovlel	%esi, %edx
	shll	$6, %ebp
	shll	$5, %r14d
	subl	%r14d, %ebp
	orl	$17, %ebp
	imull	%r9d, %ebp
	movslq	4(%r15), %r13
	movq	48(%r15), %rcx
	movq	%rcx, -96(%rsp)                 # 8-byte Spill
	movq	64(%r15), %rcx
	movq	%rcx, -104(%rsp)                # 8-byte Spill
	movq	80(%r15), %rcx
	leal	(%rax,%r8,8), %eax
	movl	%eax, -116(%rsp)                # 4-byte Spill
	vpbroadcastd	%xmm2, %ymm2
	vpor	.LCPI35_0(%rip), %ymm2, %ymm2
	movslq	%edi, %rax
	movq	%rax, %rdi
	movq	%rax, 176(%rsp)                 # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	imull	%r13d, %eax
	leal	(%rax,%r8,8), %r9d
	leal	(%rbp,%r8,8), %ebp
	leal	8(,%r8,8), %eax
	movl	%eax, -108(%rsp)                # 4-byte Spill
	vpbroadcastd	%xmm0, %ymm0
	vpminsd	%ymm2, %ymm0, %ymm0
	movl	-124(%rsp), %ebx                # 4-byte Reload
	vmovd	%ebx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm2, %ymm0, %ymm2
	vmovd	%xmm2, %eax
	cltq
	movq	%rax, 168(%rsp)                 # 8-byte Spill
	vpextrd	$1, %xmm2, %eax
	vpextrd	$2, %xmm2, %edi
	cltq
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, 152(%rsp)                 # 8-byte Spill
	vpextrd	$3, %xmm2, %eax
	cltq
	movq	%rax, 144(%rsp)                 # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm2
	vmovd	%xmm2, %eax
	cltq
	movq	%rax, 136(%rsp)                 # 8-byte Spill
	vpextrd	$1, %xmm2, %eax
	vpextrd	$2, %xmm2, %edi
	cltq
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	vpextrd	$3, %xmm2, %eax
	cltq
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm1
	vmovd	%xmm1, %eax
	cltq
	movq	%rax, 104(%rsp)                 # 8-byte Spill
	vpextrd	$1, %xmm1, %eax
	cltq
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	vpextrd	$2, %xmm1, %eax
	vpextrd	$3, %xmm1, %edi
	cltq
	movq	%rax, 88(%rsp)                  # 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovd	%xmm1, %eax
	cltq
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	vpextrd	$1, %xmm1, %eax
	cltq
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	vpextrd	$2, %xmm1, %eax
	vpextrd	$3, %xmm1, %edi
	cltq
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	vmovd	%r12d, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm1
	vmovd	%xmm1, %eax
	vpextrd	$1, %xmm1, %edi
	cltq
	movq	%rax, 40(%rsp)                  # 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	vpextrd	$2, %xmm1, %eax
	cltq
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	vpextrd	$3, %xmm1, %eax
	cltq
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	vextracti128	$1, %ymm1, %xmm1
	vmovd	%xmm1, %eax
	vpextrd	$1, %xmm1, %edi
	cltq
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, (%rsp)                    # 8-byte Spill
	vpextrd	$2, %xmm1, %eax
	cltq
	movq	%rax, -8(%rsp)                  # 8-byte Spill
	vpextrd	$3, %xmm1, %eax
	cltq
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	vmovd	%r10d, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm1, %ymm0, %ymm0
	vmovd	%xmm0, %eax
	cltq
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	vpextrd	$1, %xmm0, %eax
	cltq
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$3, %xmm0, %edi
	cltq
	movq	%rax, -40(%rsp)                 # 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	vextracti128	$1, %ymm0, %xmm0
	vmovd	%xmm0, %eax
	cltq
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	vpextrd	$1, %xmm0, %eax
	cltq
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	vpextrd	$2, %xmm0, %eax
	vpextrd	$3, %xmm0, %edi
	cltq
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	addl	%r11d, %r10d
	movl	%r10d, %eax
	addl	%r11d, %r12d
	movq	%r11, 184(%rsp)                 # 8-byte Spill
	movl	%ebx, %edi
	addl	%r11d, %edi
	movslq	%edi, %r10
	movslq	-120(%rsp), %rbx                # 4-byte Folded Reload
	movslq	%r12d, %rdi
	cltq
	movl	%edx, %r8d
	andl	$2147483640, %r8d               # imm = 0x7FFFFFF8
	leaq	(%rcx,%rax,4), %r12
	leaq	(%rcx,%rdi,4), %rdi
	leaq	(%rcx,%rbx,4), %r15
	leaq	(%rcx,%r10,4), %r14
	vmovss	.LCPI35_1(%rip), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	.LCPI35_2(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI35_1(%rip), %ymm7  # ymm7 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	jmp	.LBB35_2
	.p2align	4, 0x90
.LBB35_3:                               # %true_bb
                                        #   in Loop: Header=BB35_2 Depth=1
	movq	136(%rsp), %rax                 # 8-byte Reload
	vmovss	(%rcx,%rax,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	movq	128(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	120(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, (%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	112(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	168(%rsp), %rax                 # 8-byte Reload
	vmovss	(%rcx,%rax,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	movq	160(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	152(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, (%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	72(%rsp), %rax                  # 8-byte Reload
	vmovss	(%rcx,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	movq	64(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	56(%rsp), %rax                  # 8-byte Reload
	vinsertps	$32, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	48(%rsp), %rax                  # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	104(%rsp), %rax                 # 8-byte Reload
	vmovss	(%rcx,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movq	96(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	88(%rsp), %rax                  # 8-byte Reload
	vinsertps	$32, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	80(%rsp), %rax                  # 8-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	movq	176(%rsp), %rax                 # 8-byte Reload
	addq	%rsi, %rax
	imulq	%r13, %rax
	addq	184(%rsp), %rax                 # 8-byte Folded Reload
	movq	-96(%rsp), %rbx                 # 8-byte Reload
	vmulps	(%rbx,%rax,4), %ymm4, %ymm4
	movq	192(%rsp), %rax                 # 8-byte Reload
	addl	%esi, %eax
	movq	144(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	8(%rsp), %rbx                   # 8-byte Reload
	vmovss	(%rcx,%rbx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movq	(%rsp), %rbx                    # 8-byte Reload
	vinsertps	$16, (%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	-8(%rsp), %rbx                  # 8-byte Reload
	vinsertps	$32, (%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	imull	-128(%rsp), %eax                # 4-byte Folded Reload
	movq	-16(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	40(%rsp), %rbx                  # 8-byte Reload
	vmovss	(%rcx,%rbx,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	movq	32(%rsp), %rbx                  # 8-byte Reload
	vinsertps	$16, (%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	24(%rsp), %rbx                  # 8-byte Reload
	vinsertps	$32, (%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movq	16(%rsp), %rbx                  # 8-byte Reload
	vinsertps	$48, (%rcx,%rbx,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1,2],mem[0]
	movq	-56(%rsp), %rbx                 # 8-byte Reload
	vmovss	(%rcx,%rbx,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	movq	-64(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, (%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	-72(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$32, (%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertf128	$1, %xmm5, %ymm3, %ymm3
	movq	-80(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rbx,4), %xmm6, %xmm5 # xmm5 = xmm6[0,1,2],mem[0]
	movq	-24(%rsp), %rbx                 # 8-byte Reload
	vmovss	(%rcx,%rbx,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	movq	-32(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, (%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	-40(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$32, (%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	-48(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	vmaxps	%ymm7, %ymm5, %ymm5
	vmulps	%ymm3, %ymm2, %ymm3
	vbroadcastss	.LCPI35_2(%rip), %ymm6  # ymm6 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	vmulps	%ymm6, %ymm3, %ymm3
	vmulps	%ymm3, %ymm4, %ymm3
	vdivps	%ymm5, %ymm3, %ymm3
	addl	-116(%rsp), %eax                # 4-byte Folded Reload
	cltq
	vcmpleps	%ymm7, %ymm2, %ymm2
	vandps	%ymm3, %ymm2, %ymm2
	movq	-104(%rsp), %rbx                # 8-byte Reload
	vaddps	(%rbx,%rax,4), %ymm2, %ymm2
	vmovups	%ymm2, (%rbx,%rax,4)
.LBB35_11:                              # %after_bb
                                        #   in Loop: Header=BB35_2 Depth=1
	incq	%rsi
	addl	%r13d, %r9d
	addl	-128(%rsp), %ebp                # 4-byte Folded Reload
	cmpq	200(%rsp), %rsi                 # 8-byte Folded Reload
	je	.LBB35_12
.LBB35_2:                               # %"for relu1_0_d_def__.s27.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB35_9 Depth 2
                                        #     Child Loop BB35_6 Depth 2
	movl	-108(%rsp), %eax                # 4-byte Reload
	cmpl	-88(%rsp), %eax                 # 4-byte Folded Reload
	jle	.LBB35_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB35_2 Depth=1
	cmpl	$0, -112(%rsp)                  # 4-byte Folded Reload
	jle	.LBB35_11
# %bb.5:                                # %"for relu1_0_d_def__.s27.n.ni.preheader"
                                        #   in Loop: Header=BB35_2 Depth=1
	movslq	%r9d, %rax
	movq	-96(%rsp), %rbx                 # 8-byte Reload
	leaq	(%rbx,%rax,4), %r10
	movslq	%ebp, %rax
	movq	-104(%rsp), %rbx                # 8-byte Reload
	leaq	(%rbx,%rax,4), %rax
	xorl	%ebx, %ebx
	cmpl	$8, %edx
	jb	.LBB35_6
	.p2align	4, 0x90
.LBB35_9:                               # %vector.body
                                        #   Parent Loop BB35_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%r14,%rbx,4), %ymm2
	vbroadcastss	.LCPI35_1(%rip), %ymm3  # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmovups	(%r15,%rbx,4), %ymm4
	vmulps	(%r10,%rbx,4), %ymm4, %ymm4
	vmovups	(%r12,%rbx,4), %ymm5
	vmaxps	%ymm3, %ymm5, %ymm5
	vmulps	(%rdi,%rbx,4), %ymm2, %ymm6
	vmulps	%ymm6, %ymm4, %ymm4
	vbroadcastss	.LCPI35_2(%rip), %ymm6  # ymm6 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	vmulps	%ymm6, %ymm4, %ymm4
	vdivps	%ymm5, %ymm4, %ymm4
	vcmpleps	%ymm3, %ymm2, %ymm2
	vandps	%ymm4, %ymm2, %ymm2
	vaddps	(%rax,%rbx,4), %ymm2, %ymm2
	vmovups	%ymm2, (%rax,%rbx,4)
	addq	$8, %rbx
	cmpq	%rbx, %r8
	jne	.LBB35_9
# %bb.10:                               # %middle.block
                                        #   in Loop: Header=BB35_2 Depth=1
	movq	%r8, %rbx
	cmpq	%rdx, %r8
	jne	.LBB35_6
	jmp	.LBB35_11
	.p2align	4, 0x90
.LBB35_8:                               # %select.end
                                        #   in Loop: Header=BB35_6 Depth=2
	vaddss	%xmm4, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %rdx
	je	.LBB35_11
.LBB35_6:                               # %"for relu1_0_d_def__.s27.n.ni"
                                        #   Parent Loop BB35_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%r14,%rbx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmovss	(%rax,%rbx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vucomiss	%xmm3, %xmm0
	setb	%r11b
	vmovss	(%r15,%rbx,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmulss	(%r10,%rbx,4), %xmm4, %xmm4
	vmulss	(%rdi,%rbx,4), %xmm3, %xmm3
	vmulss	%xmm3, %xmm4, %xmm3
	vmulss	%xmm1, %xmm3, %xmm3
	vxorps	%xmm4, %xmm4, %xmm4
	testb	$1, %r11b
	jne	.LBB35_8
# %bb.7:                                # %select.false.sink
                                        #   in Loop: Header=BB35_6 Depth=2
	vmovss	(%r12,%rbx,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmaxss	%xmm0, %xmm4, %xmm4
	vdivss	%xmm4, %xmm3, %xmm4
	jmp	.LBB35_8
.LBB35_12:                              # %destructor_block
	xorl	%eax, %eax
	addq	$208, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end35:
	.size	train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n, .Lfunc_end35-train_cost_model.par_for.relu1_0_d_def__.s27.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n
.LCPI36_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI36_1:
	.long	0x3f800000                      # float 1
.LCPI36_2:
	.long	0x3109705f                      # float 1.99999994E-9
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$72, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r8d
	movl	24(%rdx), %r11d
	movl	%esi, %r10d
	sarl	$31, %r10d
	xorl	%ebx, %ebx
	testl	%r11d, %r11d
	sete	%bl
	movl	%ebx, %ebp
	negl	%ebp
	movl	%r11d, %ecx
	sarl	$31, %ecx
	subl	%r10d, %esi
	orl	%r11d, %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	movl	%edx, %r15d
	movl	%ecx, %r14d
	leal	(%r11,%rbx), %ebp
	movl	%esi, %eax
	cltd
	idivl	%ebp
	notl	%r14d
	decl	%ebx
	movl	%r14d, %ebp
	subl	%ecx, %ebp
	andl	%r10d, %ebp
	addl	%eax, %ebp
	andl	%ebx, %ebp
	leal	(%rbp,%rbp), %eax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	subl	%eax, %r8d
	movl	%r8d, -128(%rsp)                # 4-byte Spill
	jle	.LBB36_15
# %bb.1:                                # %"for relu1_0_d_def__.s28.w.wi.preheader"
	movl	(%rdi), %r12d
	movl	28(%rdi), %edx
	movslq	32(%rdi), %r9
	movl	36(%rdi), %r13d
	movl	40(%rdi), %r8d
	movl	44(%rdi), %esi
	xorl	%r11d, %ecx
	addl	%r14d, %ecx
	andl	%r10d, %ecx
	addl	%ecx, %r15d
	andl	%ebx, %r15d
	leal	(,%r15,8), %eax
	movl	%r13d, %ecx
	subl	%r9d, %ecx
	movq	%rcx, 24(%rsp)                  # 8-byte Spill
	subl	%r9d, %r8d
	subl	%r9d, %esi
	movq	%rsi, -80(%rsp)                 # 8-byte Spill
	movq	-72(%rsp), %r11                 # 8-byte Reload
	movl	%r11d, %ecx
	movl	%r11d, %r10d
	subl	8(%rdi), %r10d
	subl	%edx, %ecx
	movq	%rcx, 16(%rsp)                  # 8-byte Spill
	xorl	%ecx, %ecx
	cmpl	$1, -128(%rsp)                  # 4-byte Folded Reload
	sete	%cl
	movl	$2, %ebx
	subq	%rcx, %rbx
	movq	%rbx, 56(%rsp)                  # 8-byte Spill
	vmovd	%eax, %xmm0
	leal	-1(%r12), %ecx
	vmovd	%ecx, %xmm1
	movslq	%eax, %rcx
	movq	%r12, 64(%rsp)                  # 8-byte Spill
	movl	%r12d, %eax
	subl	%ecx, %eax
	cmpl	$9, %eax
	movl	$8, %esi
	cmovll	%eax, %esi
	xorl	%r12d, %r12d
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r12d, %esi
	shll	$2, %ebp
	addl	%edx, %edx
	subl	%edx, %ebp
	leal	(%r13,%r15,8), %edx
	movslq	4(%rdi), %r14
	movl	16(%rdi), %ebx
	movl	20(%rdi), %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	movq	48(%rdi), %rax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	movq	64(%rdi), %rax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	movq	80(%rdi), %r13
	vpbroadcastd	%xmm0, %ymm0
	vpor	.LCPI36_0(%rip), %ymm0, %ymm0
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm0, %ymm1, %ymm8
	movslq	%r10d, %rdi
	movq	%r8, 32(%rsp)                   # 8-byte Spill
	leal	(%rcx,%r8), %eax
	cltq
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%rcx,%rax), %eax
	cltq
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	movq	%rsi, -104(%rsp)                # 8-byte Spill
	movl	%esi, %r10d
	andl	$2147483640, %r10d              # imm = 0x7FFFFFF8
	movq	%rdi, %rax
	movq	%rdi, -8(%rsp)                  # 8-byte Spill
	imull	%r14d, %eax
	leal	(%rax,%r15,8), %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	movq	%rcx, 8(%rsp)                   # 8-byte Spill
	subq	%r9, %rcx
	movq	%rcx, -64(%rsp)                 # 8-byte Spill
	movl	%edx, %eax
	subl	%r9d, %eax
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	orl	$1, %ebp
	imull	%ebx, %ebp
	shll	$4, %ebp
	leal	(%rbp,%r15,8), %r8d
	movl	%ebx, %eax
	shll	$5, %eax
	movl	%eax, -84(%rsp)                 # 4-byte Spill
	movq	%r9, 40(%rsp)                   # 8-byte Spill
	subl	%r9d, %edx
	movq	%rdx, -16(%rsp)                 # 8-byte Spill
	vmovss	.LCPI36_1(%rip), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	.LCPI36_2(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI36_1(%rip), %ymm3  # ymm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI36_2(%rip), %ymm4  # ymm4 = [1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9,1.99999994E-9]
	movq	%r15, 48(%rsp)                  # 8-byte Spill
	leal	8(,%r15,8), %eax
	movl	%eax, -88(%rsp)                 # 4-byte Spill
	movq	%rbx, (%rsp)                    # 8-byte Spill
	leal	(%rbx,%rbx), %eax
	movl	%eax, -96(%rsp)                 # 4-byte Spill
                                        # kill: def $r11d killed $r11d killed $r11
	movq	%r14, -40(%rsp)                 # 8-byte Spill
	jmp	.LBB36_2
	.p2align	4, 0x90
.LBB36_3:                               # %true_bb
                                        #   in Loop: Header=BB36_2 Depth=1
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addl	%r12d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edx
	imull	-124(%rsp), %edx                # 4-byte Folded Reload
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%edx, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm0
	vextracti128	$1, %ymm0, %xmm5
	vmovd	%xmm5, %eax
	cltq
	vpextrd	$1, %xmm5, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r13,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm0, %eax
	vpextrd	$2, %xmm5, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r13,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %ecx
	cltq
	movslq	%ecx, %rcx
	vinsertps	$48, (%r13,%rcx,4), %xmm6, %xmm10 # xmm10 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r13,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%r13,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	cltq
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r13,%rax,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r13,%rcx,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1,2],mem[0]
	movl	%edx, %eax
	subl	40(%rsp), %eax                  # 4-byte Folded Reload
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm0
	vextracti128	$1, %ymm0, %xmm7
	vmovd	%xmm7, %eax
	cltq
	vpextrd	$1, %xmm7, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r13,%rax,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm7, %eax
	vinsertps	$16, (%r13,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovd	%xmm0, %ecx
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm7, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r13,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r13,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%r13,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrd	$3, %xmm0, %ecx
	cltq
	movslq	%ecx, %rcx
	vinsertps	$32, (%r13,%rax,4), %xmm7, %xmm0 # xmm0 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, (%r13,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm0, %ymm0
	movq	-8(%rsp), %rax                  # 8-byte Reload
	addq	%r12, %rax
	imulq	%r14, %rax
	addq	8(%rsp), %rax                   # 8-byte Folded Reload
	movq	-112(%rsp), %rcx                # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm0, %ymm12
	movq	32(%rsp), %rax                  # 8-byte Reload
	addl	%edx, %eax
	vmovd	%eax, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	%ymm0, %ymm8, %ymm5
	vextracti128	$1, %ymm5, %xmm0
	vmovd	%xmm0, %eax
	cltq
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r13,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm0, %eax
	vinsertps	$16, (%r13,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm0, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r13,%rcx,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%r13,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r13,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%r13,%rax,4), %xmm6, %xmm9 # xmm9 = xmm6[0,1,2],mem[0]
	addl	-80(%rsp), %edx                 # 4-byte Folded Reload
	vmovd	%edx, %xmm5
	vpbroadcastd	%xmm5, %ymm5
	vpaddd	%ymm5, %ymm8, %ymm5
	vextracti128	$1, %ymm5, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vpextrd	$1, %xmm6, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r13,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm6, %eax
	vinsertps	$16, (%r13,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovd	%xmm5, %ecx
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm6, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r13,%rcx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vpextrd	$1, %xmm5, %ecx
	vmovss	(%r13,%rax,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r13,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertf128	$1, %xmm10, %ymm11, %ymm10
	vpextrd	$2, %xmm5, %eax
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm5, %eax
	cltq
	vinsertps	$48, (%r13,%rax,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1,2],mem[0]
	movq	16(%rsp), %rax                  # 8-byte Reload
	addl	%r12d, %eax
	imull	-96(%rsp), %eax                 # 4-byte Folded Reload
	addl	(%rsp), %eax                    # 4-byte Folded Reload
	movq	48(%rsp), %rcx                  # 8-byte Reload
	leal	(%rcx,%rax,2), %eax
	cltq
	shlq	$5, %rax
	vinsertf128	$1, %xmm0, %ymm9, %ymm0
	vcmpltps	%ymm10, %ymm3, %ymm7
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	vmaxps	%ymm3, %ymm5, %ymm5
	vmulps	%ymm0, %ymm10, %ymm0
	vmulps	%ymm4, %ymm0, %ymm0
	vmulps	%ymm0, %ymm12, %ymm0
	vdivps	%ymm5, %ymm0, %ymm0
	vandps	%ymm0, %ymm7, %ymm0
	movq	-120(%rsp), %rcx                # 8-byte Reload
	vaddps	(%rcx,%rax), %ymm0, %ymm0
	vmovaps	%ymm0, (%rcx,%rax)
.LBB36_14:                              # %after_bb
                                        #   in Loop: Header=BB36_2 Depth=1
	incq	%r12
	addl	%r14d, -128(%rsp)               # 4-byte Folded Spill
	incl	%r11d
	addl	-84(%rsp), %r8d                 # 4-byte Folded Reload
	cmpq	56(%rsp), %r12                  # 8-byte Folded Reload
	je	.LBB36_15
.LBB36_2:                               # %"for relu1_0_d_def__.s28.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB36_12 Depth 2
                                        #     Child Loop BB36_8 Depth 2
	movl	-88(%rsp), %eax                 # 4-byte Reload
	cmpl	64(%rsp), %eax                  # 4-byte Folded Reload
	jle	.LBB36_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB36_2 Depth=1
	cmpl	$0, -92(%rsp)                   # 4-byte Folded Reload
	jle	.LBB36_14
# %bb.5:                                # %"for relu1_0_d_def__.s28.n.ni.preheader"
                                        #   in Loop: Header=BB36_2 Depth=1
	movl	%r11d, %eax
	sarl	$31, %eax
	andnl	%r11d, %eax, %eax
	imull	-124(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %r9
	movslq	-128(%rsp), %r15                # 4-byte Folded Reload
	movslq	%r8d, %rax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	cmpl	$8, -104(%rsp)                  # 4-byte Folded Reload
	jae	.LBB36_11
# %bb.6:                                #   in Loop: Header=BB36_2 Depth=1
	xorl	%edi, %edi
	jmp	.LBB36_7
	.p2align	4, 0x90
.LBB36_11:                              # %vector.body.preheader
                                        #   in Loop: Header=BB36_2 Depth=1
	movq	-56(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r9), %rcx
	movq	-48(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r9), %rdx
	movq	-64(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r9), %rsi
	movq	-24(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%r9), %edi
	movq	-112(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%r15,4), %rax
	leaq	(%r13,%rcx,4), %rcx
	leaq	(%r13,%rdx,4), %rdx
	leaq	(%r13,%rsi,4), %rsi
	movslq	%edi, %rdi
	leaq	(%r13,%rdi,4), %rdi
	movq	-120(%rsp), %rbp                # 8-byte Reload
	movq	-32(%rsp), %rbx                 # 8-byte Reload
	leaq	(%rbp,%rbx,4), %rbp
	xorl	%r14d, %r14d
	.p2align	4, 0x90
.LBB36_12:                              # %vector.body
                                        #   Parent Loop BB36_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rdi,%r14,4), %ymm0
	vmovups	(%rsi,%r14,4), %ymm5
	vmulps	(%rax,%r14,4), %ymm5, %ymm5
	vmovups	(%rcx,%r14,4), %ymm6
	vmaxps	%ymm3, %ymm6, %ymm6
	vmulps	(%rdx,%r14,4), %ymm0, %ymm7
	vmulps	%ymm7, %ymm5, %ymm5
	vmulps	%ymm4, %ymm5, %ymm5
	vdivps	%ymm6, %ymm5, %ymm5
	vcmpltps	%ymm0, %ymm3, %ymm0
	vandps	%ymm5, %ymm0, %ymm0
	vaddps	(%rbp,%r14,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rbp,%r14,4)
	addq	$8, %r14
	cmpq	%r14, %r10
	jne	.LBB36_12
# %bb.13:                               # %middle.block
                                        #   in Loop: Header=BB36_2 Depth=1
	movq	%r10, %rdi
	cmpq	-104(%rsp), %r10                # 8-byte Folded Reload
	movq	-40(%rsp), %r14                 # 8-byte Reload
	je	.LBB36_14
.LBB36_7:                               # %"for relu1_0_d_def__.s28.n.ni.preheader14"
                                        #   in Loop: Header=BB36_2 Depth=1
	movq	-16(%rsp), %rax                 # 8-byte Reload
	addl	%r9d, %eax
	movslq	%eax, %r14
	movq	-56(%rsp), %rax                 # 8-byte Reload
	addq	%rdi, %rax
	addq	%r9, %rax
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rdi), %rsi
	addq	%r9, %rsi
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	addq	%rdi, %rcx
	addq	%r9, %rcx
	movq	-104(%rsp), %rdx                # 8-byte Reload
	subq	%rdi, %rdx
	addq	%rdi, %r15
	movq	-112(%rsp), %rbp                # 8-byte Reload
	movl	%r8d, %ebx
	movq	-32(%rsp), %r8                  # 8-byte Reload
	leaq	(,%r15,4), %r15
	addq	%rbp, %r15
	leaq	(,%rax,4), %r9
	addq	%r13, %r9
	leaq	(,%rsi,4), %rbp
	addq	%r13, %rbp
	leaq	(,%rcx,4), %rax
	addq	%r13, %rax
	addq	%rdi, %r14
	leaq	(,%r14,4), %rsi
	addq	%r13, %rsi
	movq	-40(%rsp), %r14                 # 8-byte Reload
	addq	%rdi, %r8
	movq	-120(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%r8,4), %rdi
	movl	%ebx, %r8d
	xorl	%ecx, %ecx
	jmp	.LBB36_8
	.p2align	4, 0x90
.LBB36_10:                              # %select.end
                                        #   in Loop: Header=BB36_8 Depth=2
	vaddss	%xmm6, %xmm0, %xmm0
	vmovss	%xmm0, (%rdi,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rdx
	je	.LBB36_14
.LBB36_8:                               # %"for relu1_0_d_def__.s28.n.ni"
                                        #   Parent Loop BB36_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rsi,%rcx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vmovss	(%rdi,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vucomiss	%xmm5, %xmm1
	setb	%bl
	vmovss	(%rax,%rcx,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmulss	(%r15,%rcx,4), %xmm6, %xmm6
	vmulss	(%rbp,%rcx,4), %xmm5, %xmm5
	vmulss	%xmm5, %xmm6, %xmm5
	vmulss	%xmm2, %xmm5, %xmm5
	vxorps	%xmm6, %xmm6, %xmm6
	testb	$1, %bl
	je	.LBB36_10
# %bb.9:                                # %select.true.sink
                                        #   in Loop: Header=BB36_8 Depth=2
	vmovss	(%r9,%rcx,4), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vmaxss	%xmm1, %xmm6, %xmm6
	vdivss	%xmm6, %xmm5, %xmm6
	jmp	.LBB36_10
.LBB36_15:                              # %destructor_block
	xorl	%eax, %eax
	addq	$72, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end36:
	.size	train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n, .Lfunc_end36-train_cost_model.par_for.relu1_0_d_def__.s28.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n
.LCPI37_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI37_1:
	.long	0x3f800000                      # float 1
.LCPI37_2:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	movq	%rdx, %rcx
	movl	%esi, %edi
	movl	12(%rdx), %r8d
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %esi
	negl	%esi
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %edi
	orl	%r10d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r12d
	movl	%ebp, %r11d
	leal	(%r10,%rbx), %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	notl	%r11d
	decl	%ebx
	movl	%r11d, %esi
	subl	%ebp, %esi
	andl	%r9d, %esi
	addl	%eax, %esi
	andl	%ebx, %esi
	leal	(%rsi,%rsi), %eax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	subl	%eax, %r8d
	movl	%r8d, -108(%rsp)                # 4-byte Spill
	jle	.LBB37_15
# %bb.1:                                # %"for relu1_0_d_def__.s29.w.wi.preheader"
	movl	(%rcx), %r15d
	movl	16(%rcx), %eax
	movq	%rax, -104(%rsp)                # 8-byte Spill
	movl	28(%rcx), %edx
	movl	32(%rcx), %edi
	movl	40(%rcx), %r13d
	movl	44(%rcx), %eax
	xorl	%r10d, %ebp
	movl	48(%rcx), %r14d
	addl	%r11d, %ebp
	movl	52(%rcx), %r11d
	andl	%r9d, %ebp
	addl	%ebp, %r12d
	andl	%ebx, %r12d
	leal	(,%r12,8), %r8d
	subl	%edi, %r13d
	subl	%edi, %eax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	subl	%edi, %r14d
	subl	%edi, %r11d
	movq	-88(%rsp), %rbp                 # 8-byte Reload
	movl	%ebp, %edi
	subl	%edx, %edi
	movq	%rdi, 56(%rsp)                  # 8-byte Spill
	movl	%ebp, %ebx
	subl	8(%rcx), %ebx
	xorl	%edi, %edi
	cmpl	$1, -108(%rsp)                  # 4-byte Folded Reload
	sete	%dil
	movl	$2, %ebp
	subq	%rdi, %rbp
	movq	%rbp, 88(%rsp)                  # 8-byte Spill
	leal	-1(%r15), %edi
	vmovd	%edi, %xmm1
	movq	%r15, %rdi
	movq	-104(%rsp), %r9                 # 8-byte Reload
	movl	%r9d, %ebp
	shll	$5, %ebp
	movl	%ebp, -92(%rsp)                 # 4-byte Spill
	vmovd	%r8d, %xmm2
	movslq	%r8d, %rbp
	movl	%edi, %eax
	subl	%ebp, %eax
	cmpl	$9, %eax
	movl	$8, %r10d
	cmovll	%eax, %r10d
	xorl	%r8d, %r8d
	movl	%eax, -72(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	cmovlel	%r8d, %r10d
	shll	$6, %esi
	shll	$5, %edx
	subl	%edx, %esi
	vmovss	36(%rcx), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	movslq	4(%rcx), %rdx
	movl	20(%rcx), %eax
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	movq	56(%rcx), %rax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	movq	72(%rcx), %rax
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	movq	88(%rcx), %r15
	orl	$3, %esi
	imull	%r9d, %esi
	leal	(%r9,%r9,2), %eax
	leal	(%rax,%r12,8), %eax
	movl	%eax, -76(%rsp)                 # 4-byte Spill
	movslq	%ebx, %rax
	movq	%rax, %rcx
	movq	%rax, 40(%rsp)                  # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rdx, -40(%rsp)                 # 8-byte Spill
	imull	%edx, %eax
	leal	(%rax,%r12,8), %eax
	movl	%eax, -108(%rsp)                # 4-byte Spill
	leal	(%rsi,%r12,8), %eax
	movl	%eax, -104(%rsp)                # 4-byte Spill
	leal	8(,%r12,8), %eax
	movl	%eax, -68(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm2, %ymm2
	vpor	.LCPI37_0(%rip), %ymm2, %ymm2
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 96(%rsp)                 # 32-byte Spill
	vbroadcastss	%xmm0, %ymm8
	movq	%r13, 80(%rsp)                  # 8-byte Spill
	leal	(%rbp,%r13), %eax
	movslq	%eax, %rcx
	movq	-64(%rsp), %rax                 # 8-byte Reload
	leal	(%rbp,%rax), %eax
	movslq	%eax, %rdx
	movq	%r14, 72(%rsp)                  # 8-byte Spill
	leal	(%rbp,%r14), %eax
	movslq	%eax, %rsi
	movq	%r11, 64(%rsp)                  # 8-byte Spill
	movq	%rbp, 48(%rsp)                  # 8-byte Spill
	leal	(%r11,%rbp), %eax
	cltq
	movl	%r10d, %r14d
	andl	$2147483640, %r14d              # imm = 0x7FFFFFF8
	vmovss	.LCPI37_1(%rip), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmovss	.LCPI37_2(%rip), %xmm15         # xmm15 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI37_1(%rip), %ymm5  # ymm5 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vxorps	%xmm14, %xmm14, %xmm14
	vbroadcastss	.LCPI37_2(%rip), %ymm7  # ymm7 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	leaq	(%r15,%rax,4), %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	%rsi, -24(%rsp)                 # 8-byte Spill
	leaq	(%r15,%rsi,4), %rax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	%rdx, -16(%rsp)                 # 8-byte Spill
	leaq	(%r15,%rdx,4), %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movq	%rcx, -8(%rsp)                  # 8-byte Spill
	leaq	(%r15,%rcx,4), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	-88(%rsp), %rax                 # 8-byte Reload
	movl	%eax, %r13d
	movq	%rdi, (%rsp)                    # 8-byte Spill
	jmp	.LBB37_2
	.p2align	4, 0x90
.LBB37_3:                               # %true_bb
                                        #   in Loop: Header=BB37_2 Depth=1
	movq	-88(%rsp), %rax                 # 8-byte Reload
	addl	%r8d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edx
	imull	-96(%rsp), %edx                 # 4-byte Folded Reload
	movq	80(%rsp), %rax                  # 8-byte Reload
	addl	%edx, %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vmovdqu	96(%rsp), %ymm6                 # 32-byte Reload
	vpaddd	%ymm6, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, %eax
	cltq
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$16, (%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovd	%xmm1, %eax
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm4, %xmm10 # xmm10 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm1, %ecx
	cltq
	vmovss	(%r15,%rax,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	movslq	%ecx, %rcx
	cltq
	vinsertps	$16, (%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$3, %xmm1, %ecx
	vinsertps	$32, (%r15,%rax,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1],mem[0],xmm2[3]
	movslq	%ecx, %rax
	vinsertps	$48, (%r15,%rax,4), %xmm1, %xmm9 # xmm9 = xmm1[0,1,2],mem[0]
	movq	-64(%rsp), %rax                 # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm6, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, %eax
	cltq
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm1, %eax
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm2, %ecx
	cltq
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm4, %xmm12 # xmm12 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	vinsertps	$16, (%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	cltq
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r15,%rax,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r15,%rcx,4), %xmm1, %xmm11 # xmm11 = xmm1[0,1,2],mem[0]
	movq	72(%rsp), %rax                  # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm6, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm1
	vmovd	%xmm1, %eax
	cltq
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r15,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	vinsertps	$16, (%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm2, %ecx
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r15,%rcx,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm2, %ecx
	vmovss	(%r15,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm2, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm4, %xmm13 # xmm13 = xmm4[0,1,2],mem[0]
	addl	64(%rsp), %edx                  # 4-byte Folded Reload
	vmovd	%edx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm6, %ymm2, %ymm2
	vextracti128	$1, %ymm2, %xmm4
	vmovd	%xmm4, %eax
	cltq
	vpextrd	$1, %xmm4, %ecx
	vmovss	(%r15,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertf128	$1, %xmm10, %ymm9, %ymm9
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm6, %xmm4 # xmm4 = xmm6[0,1,2],mem[0]
	vmovd	%xmm2, %eax
	cltq
	vmovss	(%r15,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %eax
	vinsertf128	$1, %xmm12, %ymm11, %ymm10
	cltq
	vinsertps	$16, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmulps	%ymm10, %ymm9, %ymm9
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm2, %eax
	vmaxps	%ymm5, %ymm9, %ymm2
	vdivps	%ymm8, %ymm2, %ymm2
	cltq
	vinsertps	$48, (%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm6, %ymm4
	movq	40(%rsp), %rax                  # 8-byte Reload
	addq	%r8, %rax
	imulq	-40(%rsp), %rax                 # 8-byte Folded Reload
	addq	48(%rsp), %rax                  # 8-byte Folded Reload
	vroundps	$2, %ymm2, %ymm6
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm6, %ymm6
	vmulps	%ymm7, %ymm4, %ymm4
	vmulps	%ymm4, %ymm6, %ymm4
	movq	56(%rsp), %rax                  # 8-byte Reload
	addl	%r8d, %eax
	imull	-92(%rsp), %eax                 # 4-byte Folded Reload
	addl	-76(%rsp), %eax                 # 4-byte Folded Reload
	vmaxps	%ymm5, %ymm2, %ymm2
	vdivps	%ymm2, %ymm4, %ymm2
	cltq
	vinsertf128	$1, %xmm1, %ymm13, %ymm1
	vcmpneqps	%ymm1, %ymm14, %ymm1
	vandps	%ymm2, %ymm1, %ymm1
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rcx,%rax,4)
.LBB37_14:                              # %after_bb
                                        #   in Loop: Header=BB37_2 Depth=1
	incq	%r8
	movl	-108(%rsp), %eax                # 4-byte Reload
	addl	-40(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -108(%rsp)                # 4-byte Spill
	incl	%r13d
	movl	-104(%rsp), %eax                # 4-byte Reload
	addl	-92(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -104(%rsp)                # 4-byte Spill
	cmpq	88(%rsp), %r8                   # 8-byte Folded Reload
	je	.LBB37_15
.LBB37_2:                               # %"for relu1_0_d_def__.s29.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB37_12 Depth 2
                                        #     Child Loop BB37_8 Depth 2
	cmpl	%edi, -68(%rsp)                 # 4-byte Folded Reload
	jle	.LBB37_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB37_2 Depth=1
	cmpl	$0, -72(%rsp)                   # 4-byte Folded Reload
	jle	.LBB37_14
# %bb.5:                                # %"for relu1_0_d_def__.s29.n.ni.preheader"
                                        #   in Loop: Header=BB37_2 Depth=1
	movl	%r13d, %eax
	sarl	$31, %eax
	andnl	%r13d, %eax, %eax
	imull	-96(%rsp), %eax                 # 4-byte Folded Reload
	movslq	%eax, %r9
	movslq	-108(%rsp), %rax                # 4-byte Folded Reload
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdx
	movslq	-104(%rsp), %rax                # 4-byte Folded Reload
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rbx
	cmpl	$8, %r10d
	jae	.LBB37_11
# %bb.6:                                #   in Loop: Header=BB37_2 Depth=1
	xorl	%r12d, %r12d
	jmp	.LBB37_7
	.p2align	4, 0x90
.LBB37_11:                              # %vector.body.preheader
                                        #   in Loop: Header=BB37_2 Depth=1
	movq	-32(%rsp), %rax                 # 8-byte Reload
	addq	%r9, %rax
	movq	-24(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r9), %rcx
	movq	-16(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r9), %rdi
	movq	-8(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%r9), %r11
	leaq	(%r15,%rax,4), %rax
	leaq	(%r15,%rcx,4), %rsi
	leaq	(%r15,%rdi,4), %rbp
	leaq	(%r15,%r11,4), %rdi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB37_12:                              # %vector.body
                                        #   Parent Loop BB37_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rdi,%rcx,4), %ymm1
	vmulps	(%rbp,%rcx,4), %ymm1, %ymm1
	vmaxps	%ymm5, %ymm1, %ymm1
	vdivps	%ymm8, %ymm1, %ymm1
	vroundps	$10, %ymm1, %ymm2
	vmaxps	%ymm5, %ymm1, %ymm1
	vmulps	(%rdx,%rcx,4), %ymm2, %ymm2
	vmulps	(%rax,%rcx,4), %ymm2, %ymm2
	vmulps	%ymm7, %ymm2, %ymm2
	vdivps	%ymm1, %ymm2, %ymm1
	vcmpneqps	(%rsi,%rcx,4), %ymm14, %ymm2
	vandps	%ymm1, %ymm2, %ymm1
	vaddps	(%rbx,%rcx,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rbx,%rcx,4)
	addq	$8, %rcx
	cmpq	%rcx, %r14
	jne	.LBB37_12
# %bb.13:                               # %middle.block
                                        #   in Loop: Header=BB37_2 Depth=1
	movq	%r14, %r12
	cmpq	%r10, %r14
	movq	(%rsp), %rdi                    # 8-byte Reload
	je	.LBB37_14
.LBB37_7:                               # %"for relu1_0_d_def__.s29.n.ni.preheader14"
                                        #   in Loop: Header=BB37_2 Depth=1
	movq	32(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r9,4), %rsi
	movq	24(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r9,4), %rbp
	movq	16(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r9,4), %r11
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	(%rax,%r9,4), %rax
	jmp	.LBB37_8
	.p2align	4, 0x90
.LBB37_10:                              # %select.end
                                        #   in Loop: Header=BB37_8 Depth=2
	vaddss	%xmm6, %xmm1, %xmm1
	vmovss	%xmm1, (%rbx,%r12,4)
	incq	%r12
	cmpq	%r12, %r10
	je	.LBB37_14
.LBB37_8:                               # %"for relu1_0_d_def__.s29.n.ni"
                                        #   Parent Loop BB37_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%r12,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	(%r11,%r12,4), %xmm1, %xmm1
	vmaxss	%xmm3, %xmm1, %xmm1
	vdivss	%xmm0, %xmm1, %xmm4
	vmovss	(%rbx,%r12,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vxorps	%xmm6, %xmm6, %xmm6
	vucomiss	(%rbp,%r12,4), %xmm6
	sete	%cl
	vroundss	$10, %xmm4, %xmm4, %xmm2
	vmulss	(%rdx,%r12,4), %xmm2, %xmm2
	vmulss	(%rsi,%r12,4), %xmm2, %xmm2
	vmulss	%xmm2, %xmm15, %xmm2
	testb	$1, %cl
	jne	.LBB37_10
# %bb.9:                                # %select.false.sink
                                        #   in Loop: Header=BB37_8 Depth=2
	vmaxss	%xmm3, %xmm4, %xmm4
	vdivss	%xmm4, %xmm2, %xmm6
	jmp	.LBB37_10
.LBB37_15:                              # %destructor_block
	xorl	%eax, %eax
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end37:
	.size	train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n, .Lfunc_end37-train_cost_model.par_for.relu1_0_d_def__.s29.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n
.LCPI38_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI38_1:
	.long	0x3f800000                      # float 1
.LCPI38_2:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$232, %rsp
	movq	%rdx, %r12
	movl	%esi, %edi
	movl	12(%rdx), %ecx
	movl	24(%rdx), %r10d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %esi
	negl	%esi
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r9d, %edi
	orl	%r10d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
                                        # kill: def $edx killed $edx def $rdx
	movq	%rdx, -120(%rsp)                # 8-byte Spill
	movl	%ebp, %r8d
	leal	(%r10,%rbx), %r11d
	movl	%edi, %eax
	cltd
	idivl	%r11d
	notl	%r8d
	decl	%ebx
	movl	%r8d, %edx
	subl	%ebp, %edx
	andl	%r9d, %edx
	addl	%eax, %edx
	andl	%ebx, %edx
	leal	(%rdx,%rdx), %eax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	subl	%eax, %ecx
	movl	%ecx, -124(%rsp)                # 4-byte Spill
	jle	.LBB38_15
# %bb.1:                                # %"for relu1_0_d_def__.s30.w.wi.preheader"
	movl	(%r12), %eax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	movl	16(%r12), %r15d
	movl	28(%r12), %eax
	movl	32(%r12), %edi
	movl	40(%r12), %esi
	movl	44(%r12), %ecx
	xorl	%r10d, %ebp
	movl	48(%r12), %r11d
	addl	%r8d, %ebp
	movl	52(%r12), %r13d
	andl	%r9d, %ebp
	movl	56(%r12), %r14d
	movq	-120(%rsp), %r10                # 8-byte Reload
	addl	%ebp, %r10d
	andl	%ebx, %r10d
	subl	%edi, %esi
	movq	%rsi, -72(%rsp)                 # 8-byte Spill
	subl	%edi, %ecx
	movq	%rcx, -80(%rsp)                 # 8-byte Spill
	subl	%edi, %r11d
	subl	%edi, %r13d
	subl	%edi, %r14d
	movq	-64(%rsp), %rbp                 # 8-byte Reload
	movl	%ebp, %esi
	subl	%eax, %esi
	movq	%rsi, 64(%rsp)                  # 8-byte Spill
	movl	%ebp, %edi
	subl	8(%r12), %edi
	xorl	%esi, %esi
	cmpl	$1, -124(%rsp)                  # 4-byte Folded Reload
	sete	%sil
	movl	$2, %ebx
	subq	%rsi, %rbx
	movq	%rbx, 96(%rsp)                  # 8-byte Spill
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	leal	-1(%rcx), %esi
	vmovd	%esi, %xmm1
	movl	%r15d, %esi
	shll	$4, %esi
	movl	%esi, -96(%rsp)                 # 4-byte Spill
	leal	(,%r10,8), %esi
	vmovd	%esi, %xmm2
	movslq	%esi, %rsi
	movl	%ecx, %ebx
	subl	%esi, %ebx
	cmpl	$9, %ebx
	movl	$8, %r8d
	cmovll	%ebx, %r8d
	xorl	%r9d, %r9d
	movl	%ebx, -100(%rsp)                # 4-byte Spill
	testl	%ebx, %ebx
	cmovlel	%r9d, %r8d
	shll	$5, %edx
	shll	$4, %eax
	subl	%eax, %edx
	vmovss	36(%r12), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	movslq	4(%r12), %rbx
	movl	20(%r12), %eax
	movl	%eax, -112(%rsp)                # 4-byte Spill
	movq	64(%r12), %rax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	movq	80(%r12), %rax
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	movq	96(%r12), %r9
	xorl	%ecx, %ecx
	movslq	%edi, %rax
	movq	%rax, %rdi
	movq	%rax, 48(%rsp)                  # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rbx, -40(%rsp)                 # 8-byte Spill
	imull	%ebx, %eax
	leal	(%rax,%r10,8), %eax
	movl	%eax, -120(%rsp)                # 4-byte Spill
	leal	8(,%r10,8), %eax
	movl	%eax, -92(%rsp)                 # 4-byte Spill
	leal	(%r15,%r10,4), %eax
	movl	%eax, -104(%rsp)                # 4-byte Spill
	vpbroadcastd	%xmm2, %ymm2
	vpor	.LCPI38_0(%rip), %ymm2, %ymm2
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 128(%rsp)                # 32-byte Spill
	vbroadcastss	%xmm0, %ymm10
	movq	-72(%rsp), %rax                 # 8-byte Reload
	leal	(%rsi,%rax), %eax
	movslq	%eax, %rdi
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%rsi,%rax), %eax
	movslq	%eax, %r12
	movq	%r11, 88(%rsp)                  # 8-byte Spill
	leal	(%rsi,%r11), %eax
	movslq	%eax, %r11
	movq	%r13, 80(%rsp)                  # 8-byte Spill
	leal	(%rsi,%r13), %eax
	movslq	%eax, %rbx
	movq	%r14, 72(%rsp)                  # 8-byte Spill
	leal	(%rsi,%r14), %eax
	cltq
	movl	%r8d, %r14d
	andl	$2147483640, %r14d              # imm = 0x7FFFFFF8
	orl	$1, %edx
	imull	%r15d, %edx
	movq	%rsi, 56(%rsp)                  # 8-byte Spill
	leal	(%rsi,%rdx,2), %edx
	movl	%edx, -124(%rsp)                # 4-byte Spill
	shll	$5, %r15d
	movq	%r15, 104(%rsp)                 # 8-byte Spill
	vmovss	.LCPI38_1(%rip), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmovss	.LCPI38_2(%rip), %xmm9          # xmm9 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI38_1(%rip), %ymm5  # ymm5 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vxorps	%xmm11, %xmm11, %xmm11
	vbroadcastss	.LCPI38_2(%rip), %ymm14 # ymm14 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	leaq	(%r9,%rax,4), %rax
	movq	%rax, 40(%rsp)                  # 8-byte Spill
	movq	%rbx, -24(%rsp)                 # 8-byte Spill
	leaq	(%r9,%rbx,4), %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	%r11, -16(%rsp)                 # 8-byte Spill
	leaq	(%r9,%r11,4), %rax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	%r12, -8(%rsp)                  # 8-byte Spill
	leaq	(%r9,%r12,4), %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movq	%rdi, (%rsp)                    # 8-byte Spill
	leaq	(%r9,%rdi,4), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movl	%ebp, %eax
	movl	%ebp, -108(%rsp)                # 4-byte Spill
	movq	-88(%rsp), %rbp                 # 8-byte Reload
	jmp	.LBB38_2
	.p2align	4, 0x90
.LBB38_3:                               # %true_bb
                                        #   in Loop: Header=BB38_2 Depth=1
	movq	-64(%rsp), %rax                 # 8-byte Reload
	addl	%ecx, %eax
	movq	%rcx, %rsi
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edx
	imull	-112(%rsp), %edx                # 4-byte Folded Reload
	movq	-72(%rsp), %rax                 # 8-byte Reload
	addl	%edx, %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vmovdqu	128(%rsp), %ymm7                # 32-byte Reload
	vpaddd	%ymm7, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, %eax
	cltq
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r9,%rax,4), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovd	%xmm1, %eax
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r9,%rcx,4), %xmm4, %xmm12 # xmm12 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm1, %ecx
	cltq
	vmovss	(%r9,%rax,4), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	movslq	%ecx, %rcx
	cltq
	vinsertps	$16, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$3, %xmm1, %ecx
	vinsertps	$32, (%r9,%rax,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1],mem[0],xmm2[3]
	movslq	%ecx, %rax
	vinsertps	$48, (%r9,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovups	%ymm1, 192(%rsp)                # 32-byte Spill
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm7, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, %eax
	cltq
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r9,%rax,4), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm1, %eax
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm2, %ecx
	cltq
	movslq	%ecx, %rcx
	vinsertps	$48, (%r9,%rcx,4), %xmm6, %xmm2 # xmm2 = xmm6[0,1,2],mem[0]
	vmovaps	%xmm2, 112(%rsp)                # 16-byte Spill
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r9,%rax,4), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	vinsertps	$16, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	cltq
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r9,%rax,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovups	%ymm1, 160(%rsp)                # 32-byte Spill
	movq	88(%rsp), %rax                  # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm7, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, %eax
	cltq
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r9,%rax,4), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %eax
	vinsertps	$16, (%r9,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm1, %ecx
	cltq
	vinsertps	$32, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r9,%rcx,4), %xmm6, %xmm4 # xmm4 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm1, %ecx
	vmovss	(%r9,%rax,4), %xmm2             # xmm2 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	movq	%rsi, %rcx
	vinsertps	$16, (%r9,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$2, %xmm1, %eax
	cltq
	vinsertps	$32, (%r9,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vpextrd	$3, %xmm1, %eax
	cltq
	vinsertps	$48, (%r9,%rax,4), %xmm2, %xmm13 # xmm13 = xmm2[0,1,2],mem[0]
	movq	80(%rsp), %rax                  # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm7, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm1
	vmovd	%xmm1, %eax
	cltq
	vmovss	(%r9,%rax,4), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm1, %eax
	cltq
	vinsertps	$16, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm1, %eax
	cltq
	vinsertps	$32, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm1, %eax
	cltq
	vinsertps	$48, (%r9,%rax,4), %xmm6, %xmm1 # xmm1 = xmm6[0,1,2],mem[0]
	vmovd	%xmm2, %eax
	cltq
	vmovss	(%r9,%rax,4), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %eax
	cltq
	vinsertps	$16, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm2, %eax
	cltq
	vinsertps	$48, (%r9,%rax,4), %xmm6, %xmm15 # xmm15 = xmm6[0,1,2],mem[0]
	addl	72(%rsp), %edx                  # 4-byte Folded Reload
	vmovd	%edx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm7, %ymm2, %ymm2
	vextracti128	$1, %ymm2, %xmm6
	vmovd	%xmm6, %eax
	cltq
	vmovss	(%r9,%rax,4), %xmm8             # xmm8 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm6, %eax
	vmovups	192(%rsp), %ymm7                # 32-byte Reload
	vinsertf128	$1, %xmm12, %ymm7, %ymm12
	cltq
	vinsertps	$16, (%r9,%rax,4), %xmm8, %xmm7 # xmm7 = xmm8[0],mem[0],xmm8[2,3]
	vmovups	160(%rsp), %ymm8                # 32-byte Reload
	vinsertf128	$1, 112(%rsp), %ymm8, %ymm8 # 16-byte Folded Reload
	vpextrd	$2, %xmm6, %eax
	cltq
	vinsertps	$32, (%r9,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm6, %eax
	cltq
	vinsertps	$48, (%r9,%rax,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1,2],mem[0]
	vmovd	%xmm2, %eax
	cltq
	vmovss	(%r9,%rax,4), %xmm7             # xmm7 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %eax
	vmulps	%ymm8, %ymm12, %ymm8
	cltq
	vinsertps	$16, (%r9,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertf128	$1, %xmm1, %ymm15, %ymm1
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%r9,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vpextrd	$3, %xmm2, %eax
	vmaxps	%ymm5, %ymm8, %ymm2
	vdivps	%ymm10, %ymm2, %ymm2
	cltq
	vinsertps	$48, (%r9,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	movq	48(%rsp), %rax                  # 8-byte Reload
	addq	%rsi, %rax
	imulq	-40(%rsp), %rax                 # 8-byte Folded Reload
	addq	56(%rsp), %rax                  # 8-byte Folded Reload
	vinsertf128	$1, %xmm6, %ymm7, %ymm6
	vroundps	$2, %ymm2, %ymm7
	movq	-48(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%rdx,%rax,4), %ymm7, %ymm7
	movq	64(%rsp), %rax                  # 8-byte Reload
	addl	%esi, %eax
	imull	-96(%rsp), %eax                 # 4-byte Folded Reload
	addl	-104(%rsp), %eax                # 4-byte Folded Reload
	vmaxps	%ymm5, %ymm2, %ymm2
	vmulps	%ymm6, %ymm1, %ymm1
	vmulps	%ymm1, %ymm14, %ymm1
	vmulps	%ymm1, %ymm7, %ymm1
	vdivps	%ymm2, %ymm1, %ymm1
	cltq
	vinsertf128	$1, %xmm4, %ymm13, %ymm2
	vcmpneqps	%ymm2, %ymm11, %ymm2
	vandps	%ymm1, %ymm2, %ymm1
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	vaddps	(%rdx,%rax,8), %ymm1, %ymm1
	vmovups	%ymm1, (%rdx,%rax,8)
.LBB38_14:                              # %after_bb
                                        #   in Loop: Header=BB38_2 Depth=1
	incq	%rcx
	movl	-120(%rsp), %eax                # 4-byte Reload
	addl	-40(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -120(%rsp)                # 4-byte Spill
	incl	-108(%rsp)                      # 4-byte Folded Spill
	movl	-124(%rsp), %eax                # 4-byte Reload
	addl	104(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -124(%rsp)                # 4-byte Spill
	cmpq	96(%rsp), %rcx                  # 8-byte Folded Reload
	je	.LBB38_15
.LBB38_2:                               # %"for relu1_0_d_def__.s30.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB38_12 Depth 2
                                        #     Child Loop BB38_8 Depth 2
	cmpl	%ebp, -92(%rsp)                 # 4-byte Folded Reload
	jle	.LBB38_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB38_2 Depth=1
	cmpl	$0, -100(%rsp)                  # 4-byte Folded Reload
	jle	.LBB38_14
# %bb.5:                                # %"for relu1_0_d_def__.s30.n.ni.preheader"
                                        #   in Loop: Header=BB38_2 Depth=1
	movl	-108(%rsp), %edx                # 4-byte Reload
	movl	%edx, %eax
	sarl	$31, %eax
	andnl	%edx, %eax, %eax
	imull	-112(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %r13
	movslq	-120(%rsp), %rax                # 4-byte Folded Reload
	movq	-48(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	movslq	-124(%rsp), %rax                # 4-byte Folded Reload
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rax,4), %rdi
	cmpl	$8, %r8d
	jae	.LBB38_11
# %bb.6:                                #   in Loop: Header=BB38_2 Depth=1
	xorl	%esi, %esi
	jmp	.LBB38_7
	.p2align	4, 0x90
.LBB38_11:                              # %vector.body.preheader
                                        #   in Loop: Header=BB38_2 Depth=1
	movq	%rcx, %r12
	movq	%rbp, %r15
	movq	-32(%rsp), %rax                 # 8-byte Reload
	addq	%r13, %rax
	movq	-24(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r13), %rcx
	movq	-16(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r13), %rsi
	movq	-8(%rsp), %rbp                  # 8-byte Reload
	leaq	(%rbp,%r13), %rbx
	movq	(%rsp), %rbp                    # 8-byte Reload
	leaq	(%rbp,%r13), %r11
	leaq	(%r9,%rax,4), %r10
	leaq	(%r9,%rcx,4), %rbp
	leaq	(%r9,%rsi,4), %rax
	leaq	(%r9,%rbx,4), %rbx
	leaq	(%r9,%r11,4), %rcx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB38_12:                              # %vector.body
                                        #   Parent Loop BB38_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rcx,%rsi,4), %ymm1
	vmulps	(%rbx,%rsi,4), %ymm1, %ymm1
	vmaxps	%ymm5, %ymm1, %ymm1
	vdivps	%ymm10, %ymm1, %ymm1
	vroundps	$10, %ymm1, %ymm2
	vmaxps	%ymm5, %ymm1, %ymm1
	vmulps	(%rdx,%rsi,4), %ymm2, %ymm2
	vmovups	(%rbp,%rsi,4), %ymm4
	vmulps	(%r10,%rsi,4), %ymm4, %ymm4
	vmulps	%ymm4, %ymm14, %ymm4
	vmulps	%ymm4, %ymm2, %ymm2
	vdivps	%ymm1, %ymm2, %ymm1
	vcmpneqps	(%rax,%rsi,4), %ymm11, %ymm2
	vandps	%ymm1, %ymm2, %ymm1
	vaddps	(%rdi,%rsi,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rdi,%rsi,4)
	addq	$8, %rsi
	cmpq	%rsi, %r14
	jne	.LBB38_12
# %bb.13:                               # %middle.block
                                        #   in Loop: Header=BB38_2 Depth=1
	movq	%r14, %rsi
	cmpq	%r8, %r14
	movq	%r15, %rbp
	movq	%r12, %rcx
	je	.LBB38_14
.LBB38_7:                               # %"for relu1_0_d_def__.s30.n.ni.preheader15"
                                        #   in Loop: Header=BB38_2 Depth=1
	movq	40(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r13,4), %r11
	movq	32(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r13,4), %r12
	movq	24(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r13,4), %r15
	movq	16(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r13,4), %r10
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	(%rax,%r13,4), %rbx
	jmp	.LBB38_8
	.p2align	4, 0x90
.LBB38_10:                              # %select.end
                                        #   in Loop: Header=BB38_8 Depth=2
	vaddss	%xmm1, %xmm6, %xmm1
	vmovss	%xmm1, (%rdi,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %r8
	je	.LBB38_14
.LBB38_8:                               # %"for relu1_0_d_def__.s30.n.ni"
                                        #   Parent Loop BB38_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rsi,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	(%r10,%rsi,4), %xmm1, %xmm1
	vmaxss	%xmm3, %xmm1, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	(%r15,%rsi,4), %xmm1
	vdivss	%xmm0, %xmm2, %xmm4
	vroundss	$10, %xmm4, %xmm4, %xmm2
	vmulss	(%rdx,%rsi,4), %xmm2, %xmm2
	vmovss	(%rdi,%rsi,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmovss	(%r12,%rsi,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vmulss	(%r11,%rsi,4), %xmm7, %xmm7
	sete	%al
	vmulss	%xmm7, %xmm2, %xmm2
	vmulss	%xmm2, %xmm9, %xmm2
	testb	$1, %al
	jne	.LBB38_10
# %bb.9:                                # %select.false.sink
                                        #   in Loop: Header=BB38_8 Depth=2
	vmaxss	%xmm3, %xmm4, %xmm1
	vdivss	%xmm1, %xmm2, %xmm1
	jmp	.LBB38_10
.LBB38_15:                              # %destructor_block
	xorl	%eax, %eax
	addq	$232, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end38:
	.size	train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n, .Lfunc_end38-train_cost_model.par_for.relu1_0_d_def__.s30.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n
.LCPI39_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI39_1:
	.long	0x3f800000                      # float 1
.LCPI39_2:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
	movq	%rdx, %rcx
	movl	%esi, %edi
	movl	12(%rdx), %r15d
	movl	24(%rdx), %r11d
	movl	%esi, %r9d
	sarl	$31, %r9d
	xorl	%ebp, %ebp
	testl	%r11d, %r11d
	sete	%bpl
	movl	%ebp, %esi
	negl	%esi
	movl	%r11d, %ebx
	sarl	$31, %ebx
	subl	%r9d, %edi
	orl	%r11d, %esi
	movl	%edi, %eax
	cltd
	idivl	%esi
	movl	%edx, %r14d
	movl	%ebx, %r10d
	leal	(%r11,%rbp), %r8d
	movl	%edi, %eax
	cltd
	idivl	%r8d
	notl	%r10d
	decl	%ebp
	movl	%r10d, %edx
	subl	%ebx, %edx
	andl	%r9d, %edx
	addl	%eax, %edx
	andl	%ebp, %edx
	leal	(%rdx,%rdx), %edi
	subl	%edi, %r15d
	jle	.LBB39_15
# %bb.1:                                # %"for relu1_0_d_def__.s31.w.wi.preheader"
	movl	%r15d, %eax
	movl	(%rcx), %r8d
	movl	16(%rcx), %esi
	movq	%rsi, -104(%rsp)                # 8-byte Spill
	movl	28(%rcx), %r15d
	movq	%rdi, -64(%rsp)                 # 8-byte Spill
	movl	32(%rcx), %edi
	movl	40(%rcx), %esi
	movl	44(%rcx), %r12d
	xorl	%r11d, %ebx
	movl	48(%rcx), %r13d
	addl	%r10d, %ebx
	movl	52(%rcx), %r10d
	andl	%r9d, %ebx
	addl	%ebx, %r14d
	andl	%ebp, %r14d
	movq	%r14, -88(%rsp)                 # 8-byte Spill
	leal	(,%r14,8), %r9d
	subl	%edi, %esi
	movq	%rsi, -56(%rsp)                 # 8-byte Spill
	subl	%edi, %r12d
	subl	%edi, %r13d
	subl	%edi, %r10d
	movq	-64(%rsp), %r14                 # 8-byte Reload
	movl	%r14d, %esi
	subl	%r15d, %esi
	movq	%rsi, 56(%rsp)                  # 8-byte Spill
	movl	%r14d, %ebx
	subl	8(%rcx), %ebx
	xorl	%esi, %esi
	cmpl	$1, %eax
	sete	%sil
	movl	$2, %ebp
	subq	%rsi, %rbp
	movq	%rbp, 88(%rsp)                  # 8-byte Spill
	leal	-1(%r8), %esi
	vmovd	%esi, %xmm1
	movq	%r8, %rsi
	movq	-104(%rsp), %rax                # 8-byte Reload
	movl	%eax, %ebp
	shll	$5, %ebp
	movl	%ebp, -92(%rsp)                 # 4-byte Spill
	movslq	%r9d, %rbp
	movl	%esi, %edi
	subl	%ebp, %edi
	cmpl	$9, %edi
	movl	$8, %r11d
	cmovll	%edi, %r11d
	xorl	%r8d, %r8d
	movl	%edi, -72(%rsp)                 # 4-byte Spill
	testl	%edi, %edi
	cmovlel	%r8d, %r11d
	shll	$6, %edx
	shll	$5, %r15d
	subl	%r15d, %edx
	vmovd	%r9d, %xmm2
	movq	%rax, %rdi
	addl	%r9d, %eax
	movl	%eax, -76(%rsp)                 # 4-byte Spill
	orl	$1, %edx
	imull	%edi, %edx
	vmovss	36(%rcx), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	movslq	4(%rcx), %rdi
	movl	20(%rcx), %eax
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	movq	56(%rcx), %rax
	movq	%rax, -40(%rsp)                 # 8-byte Spill
	movq	72(%rcx), %rax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	movq	88(%rcx), %r9
	movslq	%ebx, %rax
	movq	%rax, %rcx
	movq	%rax, 40(%rsp)                  # 8-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	movq	%rdi, -32(%rsp)                 # 8-byte Spill
	imull	%edi, %eax
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	leal	(%rax,%rcx,8), %eax
	movl	%eax, -104(%rsp)                # 4-byte Spill
	leal	(%rdx,%rcx,8), %eax
	movl	%eax, -108(%rsp)                # 4-byte Spill
	leal	8(,%rcx,8), %eax
	movl	%eax, -68(%rsp)                 # 4-byte Spill
	vpbroadcastd	%xmm2, %ymm2
	vpor	.LCPI39_0(%rip), %ymm2, %ymm2
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 96(%rsp)                 # 32-byte Spill
	vbroadcastss	%xmm0, %ymm8
	movq	-56(%rsp), %rax                 # 8-byte Reload
	leal	(%rbp,%rax), %eax
	movslq	%eax, %rcx
	movq	%r12, 80(%rsp)                  # 8-byte Spill
	leal	(%rbp,%r12), %eax
	movslq	%eax, %rdx
	movq	%r13, 72(%rsp)                  # 8-byte Spill
	leal	(%rbp,%r13), %eax
	movslq	%eax, %rdi
	movq	%r10, 64(%rsp)                  # 8-byte Spill
	movq	%rbp, 48(%rsp)                  # 8-byte Spill
	leal	(%r10,%rbp), %eax
	cltq
	movl	%r11d, %r12d
	andl	$2147483640, %r12d              # imm = 0x7FFFFFF8
	vmovss	.LCPI39_1(%rip), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmovss	.LCPI39_2(%rip), %xmm15         # xmm15 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI39_1(%rip), %ymm5  # ymm5 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vxorps	%xmm14, %xmm14, %xmm14
	vbroadcastss	.LCPI39_2(%rip), %ymm7  # ymm7 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	leaq	(%r9,%rax,4), %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	%rdi, -16(%rsp)                 # 8-byte Spill
	leaq	(%r9,%rdi,4), %rax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	%rdx, -8(%rsp)                  # 8-byte Spill
	leaq	(%r9,%rdx,4), %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movq	%rcx, (%rsp)                    # 8-byte Spill
	leaq	(%r9,%rcx,4), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	%r14, %rdi
	movl	%r14d, %eax
	movl	%r14d, -88(%rsp)                # 4-byte Spill
	jmp	.LBB39_2
	.p2align	4, 0x90
.LBB39_3:                               # %true_bb
                                        #   in Loop: Header=BB39_2 Depth=1
	leal	(%rdi,%r8), %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %edx
	imull	-96(%rsp), %edx                 # 4-byte Folded Reload
	movq	-56(%rsp), %rax                 # 8-byte Reload
	addl	%edx, %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vmovdqu	96(%rsp), %ymm6                 # 32-byte Reload
	vpaddd	%ymm6, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, %eax
	cltq
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r9,%rax,4), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r9,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovd	%xmm1, %eax
	vpextrd	$3, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r9,%rcx,4), %xmm4, %xmm10 # xmm10 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm1, %ecx
	cltq
	vmovss	(%r9,%rax,4), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	movslq	%ecx, %rcx
	cltq
	vinsertps	$16, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$3, %xmm1, %ecx
	vinsertps	$32, (%r9,%rax,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1],mem[0],xmm2[3]
	movslq	%ecx, %rax
	vinsertps	$48, (%r9,%rax,4), %xmm1, %xmm9 # xmm9 = xmm1[0,1,2],mem[0]
	movq	80(%rsp), %rax                  # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm6, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, %eax
	cltq
	vpextrd	$1, %xmm2, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r9,%rax,4), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm1, %eax
	vpextrd	$2, %xmm2, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm2, %ecx
	cltq
	movslq	%ecx, %rcx
	vinsertps	$48, (%r9,%rcx,4), %xmm4, %xmm12 # xmm12 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r9,%rax,4), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	vinsertps	$16, (%r9,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	cltq
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$32, (%r9,%rax,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r9,%rcx,4), %xmm1, %xmm11 # xmm11 = xmm1[0,1,2],mem[0]
	movq	72(%rsp), %rax                  # 8-byte Reload
	leal	(%rdx,%rax), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm6, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm1
	vmovd	%xmm1, %eax
	cltq
	vpextrd	$1, %xmm1, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r9,%rax,4), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	vinsertps	$16, (%r9,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm2, %ecx
	cltq
	vinsertps	$32, (%r9,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm1, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r9,%rcx,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm2, %ecx
	vmovss	(%r9,%rax,4), %xmm4             # xmm4 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r9,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%r9,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm2, %eax
	cltq
	vinsertps	$48, (%r9,%rax,4), %xmm4, %xmm13 # xmm13 = xmm4[0,1,2],mem[0]
	addl	64(%rsp), %edx                  # 4-byte Folded Reload
	vmovd	%edx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm6, %ymm2, %ymm2
	vextracti128	$1, %ymm2, %xmm4
	vmovd	%xmm4, %eax
	cltq
	vpextrd	$1, %xmm4, %ecx
	vmovss	(%r9,%rax,4), %xmm6             # xmm6 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertf128	$1, %xmm10, %ymm9, %ymm9
	vpextrd	$2, %xmm4, %eax
	cltq
	vinsertps	$32, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm4, %eax
	cltq
	vinsertps	$48, (%r9,%rax,4), %xmm6, %xmm4 # xmm4 = xmm6[0,1,2],mem[0]
	vmovd	%xmm2, %eax
	cltq
	vmovss	(%r9,%rax,4), %xmm6             # xmm6 = mem[0],zero,zero,zero
	vpextrd	$1, %xmm2, %eax
	vinsertf128	$1, %xmm12, %ymm11, %ymm10
	cltq
	vinsertps	$16, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmulps	%ymm10, %ymm9, %ymm9
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm2, %eax
	vmaxps	%ymm5, %ymm9, %ymm2
	vdivps	%ymm8, %ymm2, %ymm2
	cltq
	vinsertps	$48, (%r9,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm13, %ymm1
	vinsertf128	$1, %xmm4, %ymm6, %ymm4
	movq	40(%rsp), %rax                  # 8-byte Reload
	addq	%r8, %rax
	imulq	-32(%rsp), %rax                 # 8-byte Folded Reload
	addq	48(%rsp), %rax                  # 8-byte Folded Reload
	vroundps	$2, %ymm2, %ymm6
	movq	-40(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rcx,%rax,4), %ymm6, %ymm6
	movq	56(%rsp), %rax                  # 8-byte Reload
	addl	%r8d, %eax
	imull	-92(%rsp), %eax                 # 4-byte Folded Reload
	addl	-76(%rsp), %eax                 # 4-byte Folded Reload
	cltq
	vcmpeqps	%ymm1, %ymm14, %ymm1
	vmaxps	%ymm5, %ymm2, %ymm2
	vmulps	%ymm7, %ymm4, %ymm4
	vmulps	%ymm4, %ymm6, %ymm4
	vdivps	%ymm2, %ymm4, %ymm2
	vandps	%ymm2, %ymm1, %ymm1
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	vaddps	(%rcx,%rax,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rcx,%rax,4)
.LBB39_14:                              # %after_bb
                                        #   in Loop: Header=BB39_2 Depth=1
	incq	%r8
	movl	-104(%rsp), %eax                # 4-byte Reload
	addl	-32(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -104(%rsp)                # 4-byte Spill
	incl	-88(%rsp)                       # 4-byte Folded Spill
	movl	-108(%rsp), %eax                # 4-byte Reload
	addl	-92(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -108(%rsp)                # 4-byte Spill
	cmpq	88(%rsp), %r8                   # 8-byte Folded Reload
	je	.LBB39_15
.LBB39_2:                               # %"for relu1_0_d_def__.s31.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB39_12 Depth 2
                                        #     Child Loop BB39_8 Depth 2
	cmpl	%esi, -68(%rsp)                 # 4-byte Folded Reload
	jle	.LBB39_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB39_2 Depth=1
	cmpl	$0, -72(%rsp)                   # 4-byte Folded Reload
	jle	.LBB39_14
# %bb.5:                                # %"for relu1_0_d_def__.s31.n.ni.preheader"
                                        #   in Loop: Header=BB39_2 Depth=1
	movl	-88(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %eax
	imull	-96(%rsp), %eax                 # 4-byte Folded Reload
	movslq	%eax, %r10
	movslq	-104(%rsp), %rax                # 4-byte Folded Reload
	movq	-40(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdx
	movslq	-108(%rsp), %rax                # 4-byte Folded Reload
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rbx
	cmpl	$8, %r11d
	jae	.LBB39_11
# %bb.6:                                #   in Loop: Header=BB39_2 Depth=1
	xorl	%r13d, %r13d
	jmp	.LBB39_7
	.p2align	4, 0x90
.LBB39_11:                              # %vector.body.preheader
                                        #   in Loop: Header=BB39_2 Depth=1
	movq	%rsi, %r15
	movq	-24(%rsp), %rax                 # 8-byte Reload
	addq	%r10, %rax
	movq	-16(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r10), %rcx
	movq	-8(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%r10), %rsi
	movq	(%rsp), %rdi                    # 8-byte Reload
	leaq	(%rdi,%r10), %r14
	leaq	(%r9,%rax,4), %rax
	leaq	(%r9,%rcx,4), %rbp
	leaq	(%r9,%rsi,4), %rdi
	leaq	(%r9,%r14,4), %rsi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB39_12:                              # %vector.body
                                        #   Parent Loop BB39_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rsi,%rcx,4), %ymm1
	vmulps	(%rdi,%rcx,4), %ymm1, %ymm1
	vmaxps	%ymm5, %ymm1, %ymm1
	vdivps	%ymm8, %ymm1, %ymm1
	vcmpeqps	(%rbp,%rcx,4), %ymm14, %ymm2
	vroundps	$10, %ymm1, %ymm4
	vmaxps	%ymm5, %ymm1, %ymm1
	vmulps	(%rdx,%rcx,4), %ymm4, %ymm4
	vmulps	(%rax,%rcx,4), %ymm4, %ymm4
	vmulps	%ymm7, %ymm4, %ymm4
	vdivps	%ymm1, %ymm4, %ymm1
	vandps	%ymm1, %ymm2, %ymm1
	vaddps	(%rbx,%rcx,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rbx,%rcx,4)
	addq	$8, %rcx
	cmpq	%rcx, %r12
	jne	.LBB39_12
# %bb.13:                               # %middle.block
                                        #   in Loop: Header=BB39_2 Depth=1
	movq	%r12, %r13
	cmpq	%r11, %r12
	movq	-64(%rsp), %rdi                 # 8-byte Reload
	movq	%r15, %rsi
	je	.LBB39_14
.LBB39_7:                               # %"for relu1_0_d_def__.s31.n.ni.preheader14"
                                        #   in Loop: Header=BB39_2 Depth=1
	movq	32(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r10,4), %r15
	movq	24(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r10,4), %rbp
	movq	16(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r10,4), %r14
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	(%rax,%r10,4), %rax
	jmp	.LBB39_8
	.p2align	4, 0x90
.LBB39_10:                              # %select.end
                                        #   in Loop: Header=BB39_8 Depth=2
	vaddss	%xmm6, %xmm1, %xmm1
	vmovss	%xmm1, (%rbx,%r13,4)
	incq	%r13
	cmpq	%r13, %r11
	je	.LBB39_14
.LBB39_8:                               # %"for relu1_0_d_def__.s31.n.ni"
                                        #   Parent Loop BB39_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%r13,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	(%r14,%r13,4), %xmm1, %xmm1
	vmaxss	%xmm3, %xmm1, %xmm1
	vdivss	%xmm0, %xmm1, %xmm4
	vmovss	(%rbx,%r13,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vxorps	%xmm6, %xmm6, %xmm6
	vucomiss	(%rbp,%r13,4), %xmm6
	sete	%cl
	vroundss	$10, %xmm4, %xmm4, %xmm2
	vmulss	(%rdx,%r13,4), %xmm2, %xmm2
	vmulss	(%r15,%r13,4), %xmm2, %xmm2
	vmulss	%xmm2, %xmm15, %xmm2
	testb	$1, %cl
	je	.LBB39_10
# %bb.9:                                # %select.true.sink
                                        #   in Loop: Header=BB39_8 Depth=2
	vmaxss	%xmm3, %xmm4, %xmm4
	vdivss	%xmm4, %xmm2, %xmm6
	jmp	.LBB39_10
.LBB39_15:                              # %destructor_block
	xorl	%eax, %eax
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end39:
	.size	train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n, .Lfunc_end39-train_cost_model.par_for.relu1_0_d_def__.s31.n.n.n
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n
.LCPI40_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI40_1:
	.long	0x3f800000                      # float 1
.LCPI40_2:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n,@function
train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n: # @train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$168, %rsp
	movq	%rdx, %rdi
	movl	12(%rdx), %r15d
	movl	24(%rdx), %r10d
	movl	%esi, %r13d
	sarl	$31, %r13d
	xorl	%ebx, %ebx
	testl	%r10d, %r10d
	sete	%bl
	movl	%ebx, %ecx
	negl	%ecx
	movl	%r10d, %ebp
	sarl	$31, %ebp
	subl	%r13d, %esi
	orl	%r10d, %ecx
	movl	%esi, %eax
	cltd
	idivl	%ecx
                                        # kill: def $edx killed $edx def $rdx
	movq	%rdx, -96(%rsp)                 # 8-byte Spill
	movl	%ebp, %ecx
	leal	(%r10,%rbx), %r11d
	movl	%esi, %eax
	cltd
	idivl	%r11d
	notl	%ecx
	decl	%ebx
	movl	%ecx, %edx
	subl	%ebp, %edx
	andl	%r13d, %edx
	addl	%eax, %edx
	andl	%ebx, %edx
	addl	%edx, %edx
	movq	%rdx, -104(%rsp)                # 8-byte Spill
	subl	%edx, %r15d
	jle	.LBB40_15
# %bb.1:                                # %"for relu1_0_d_def__.s32.w.wi.preheader"
	movl	(%rdi), %r14d
	movslq	4(%rdi), %rax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	movl	16(%rdi), %eax
	movl	20(%rdi), %edx
	movl	%edx, -112(%rsp)                # 4-byte Spill
	movl	32(%rdi), %edx
	vmovss	36(%rdi), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	movl	40(%rdi), %r11d
	movl	44(%rdi), %r9d
	movl	48(%rdi), %r12d
	xorl	%r10d, %ebp
	movl	52(%rdi), %r10d
	addl	%ecx, %ebp
	movl	56(%rdi), %r8d
	andl	%r13d, %ebp
	movq	64(%rdi), %rcx
	movq	%rcx, -56(%rsp)                 # 8-byte Spill
	movq	-96(%rsp), %rsi                 # 8-byte Reload
	addl	%ebp, %esi
	andl	%ebx, %esi
	subl	%edx, %r11d
	subl	%edx, %r9d
	subl	%edx, %r12d
	subl	%edx, %r10d
	movq	-104(%rsp), %rcx                # 8-byte Reload
	movl	%ecx, %ebp
	subl	28(%rdi), %ebp
	subl	%edx, %r8d
                                        # kill: def $ecx killed $ecx killed $rcx
	subl	8(%rdi), %ecx
	xorl	%edx, %edx
	cmpl	$1, %r15d
	sete	%dl
	movl	$2, %ebx
	subq	%rdx, %rbx
	movq	%rbx, 104(%rsp)                 # 8-byte Spill
	leal	-1(%r14), %edx
	vmovd	%edx, %xmm1
	leal	(,%rsi,8), %edx
	vmovd	%edx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpor	.LCPI40_0(%rip), %ymm2, %ymm2
	movq	80(%rdi), %rbx
	movq	%rbx, -64(%rsp)                 # 8-byte Spill
	movq	96(%rdi), %r13
	movq	%r14, %rdi
	vpbroadcastd	%xmm1, %ymm1
	vpminsd	%ymm2, %ymm1, %ymm1
	vmovdqu	%ymm1, 128(%rsp)                # 32-byte Spill
	vbroadcastss	%xmm0, %ymm9
	movslq	%ecx, %rbx
	movslq	%edx, %rdx
	movl	%edi, %ecx
	subl	%edx, %ecx
	cmpl	$9, %ecx
	movl	$8, %r14d
	cmovll	%ecx, %r14d
	xorl	%r15d, %r15d
	movl	%ecx, -76(%rsp)                 # 4-byte Spill
	testl	%ecx, %ecx
	cmovlel	%r15d, %r14d
	movl	%eax, %ecx
	shll	$5, %ecx
	movl	%ecx, -68(%rsp)                 # 4-byte Spill
	movq	%r11, 96(%rsp)                  # 8-byte Spill
	leal	(%rdx,%r11), %ecx
	movslq	%ecx, %r11
	movq	%r9, 88(%rsp)                   # 8-byte Spill
	leal	(%rdx,%r9), %ecx
	movslq	%ecx, %r9
	movq	%r12, 80(%rsp)                  # 8-byte Spill
	leal	(%rdx,%r12), %ecx
	movslq	%ecx, %r12
	movq	%r10, 72(%rsp)                  # 8-byte Spill
	leal	(%rdx,%r10), %ecx
	movslq	%ecx, %r10
	movq	%r8, 64(%rsp)                   # 8-byte Spill
	movq	%rdx, 40(%rsp)                  # 8-byte Spill
	leal	(%rdx,%r8), %ecx
	movslq	%ecx, %rdx
	movl	%r14d, %r8d
	andl	$2147483640, %r8d               # imm = 0x7FFFFFF8
	movq	%rbx, %rcx
	movq	%rbx, 48(%rsp)                  # 8-byte Spill
	imull	-88(%rsp), %ecx                 # 4-byte Folded Reload
	leal	(%rcx,%rsi,8), %ecx
	movl	%ecx, -116(%rsp)                # 4-byte Spill
	leal	(,%rax,4), %ecx
	movl	%ecx, -80(%rsp)                 # 4-byte Spill
	movq	%rbp, 56(%rsp)                  # 8-byte Spill
	imull	%ebp, %eax
	shll	$5, %eax
	leal	(%rax,%rsi,8), %eax
	movl	%eax, -120(%rsp)                # 4-byte Spill
	vmovss	.LCPI40_1(%rip), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmovss	.LCPI40_2(%rip), %xmm15         # xmm15 = mem[0],zero,zero,zero
	vbroadcastss	.LCPI40_1(%rip), %ymm5  # ymm5 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vxorps	%xmm14, %xmm14, %xmm14
	vbroadcastss	.LCPI40_2(%rip), %ymm12 # ymm12 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	movq	%rsi, -96(%rsp)                 # 8-byte Spill
	leal	8(,%rsi,8), %eax
	movl	%eax, -72(%rsp)                 # 4-byte Spill
	movq	%rdx, -48(%rsp)                 # 8-byte Spill
	leaq	(%r13,%rdx,4), %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	%r10, -40(%rsp)                 # 8-byte Spill
	leaq	(%r13,%r10,4), %rax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	%r12, -32(%rsp)                 # 8-byte Spill
	leaq	(%r13,%r12,4), %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movq	%r9, -24(%rsp)                  # 8-byte Spill
	leaq	(%r13,%r9,4), %rax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	%r11, -16(%rsp)                 # 8-byte Spill
	leaq	(%r13,%r11,4), %rax
	movq	%rax, (%rsp)                    # 8-byte Spill
	movq	-104(%rsp), %rax                # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax
	movl	%eax, -108(%rsp)                # 4-byte Spill
	movq	%rdi, -8(%rsp)                  # 8-byte Spill
	jmp	.LBB40_2
	.p2align	4, 0x90
.LBB40_3:                               # %true_bb
                                        #   in Loop: Header=BB40_2 Depth=1
	movq	-104(%rsp), %rax                # 8-byte Reload
	addl	%r15d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %ecx
	imull	-112(%rsp), %ecx                # 4-byte Folded Reload
	movq	96(%rsp), %rax                  # 8-byte Reload
	addl	%ecx, %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vmovdqu	128(%rsp), %ymm7                # 32-byte Reload
	vpaddd	%ymm7, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, %eax
	cltq
	vpextrd	$1, %xmm2, %edx
	movslq	%edx, %rdx
	vmovss	(%r13,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%r13,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm1, %eax
	vpextrd	$2, %xmm2, %edx
	movslq	%edx, %rdx
	vinsertps	$32, (%r13,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm2, %edx
	cltq
	movslq	%edx, %rdx
	vinsertps	$48, (%r13,%rdx,4), %xmm4, %xmm2 # xmm2 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vmovss	(%r13,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	vinsertps	$16, (%r13,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	cltq
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vinsertps	$32, (%r13,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r13,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	movq	88(%rsp), %rax                  # 8-byte Reload
	leal	(%rcx,%rax), %eax
	vmovd	%eax, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm7, %ymm2, %ymm2
	vextracti128	$1, %ymm2, %xmm4
	vmovd	%xmm4, %eax
	cltq
	vpextrd	$1, %xmm4, %edx
	movslq	%edx, %rdx
	vmovss	(%r13,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm4, %eax
	vinsertps	$16, (%r13,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm2, %edx
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movslq	%edx, %rax
	vpextrd	$3, %xmm4, %edx
	movslq	%edx, %rdx
	vinsertps	$48, (%r13,%rdx,4), %xmm6, %xmm4 # xmm4 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm2, %edx
	movslq	%edx, %rdx
	vmovss	(%r13,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %eax
	vinsertps	$16, (%r13,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$3, %xmm2, %edx
	cltq
	movslq	%edx, %rdx
	vinsertps	$32, (%r13,%rax,4), %xmm6, %xmm2 # xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, (%r13,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm2, %ymm2
	vmulps	%ymm2, %ymm1, %ymm1
	vmaxps	%ymm5, %ymm1, %ymm1
	vdivps	%ymm9, %ymm1, %ymm8
	movq	80(%rsp), %rax                  # 8-byte Reload
	leal	(%rcx,%rax), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm7, %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vmovd	%xmm2, %eax
	cltq
	vpextrd	$1, %xmm2, %edx
	movslq	%edx, %rdx
	vmovss	(%r13,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm2, %eax
	vinsertps	$16, (%r13,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm1, %edx
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%edx, %rax
	vpextrd	$3, %xmm2, %edx
	movslq	%edx, %rdx
	vinsertps	$48, (%r13,%rdx,4), %xmm4, %xmm2 # xmm2 = xmm4[0,1,2],mem[0]
	vmovaps	%xmm2, 112(%rsp)                # 16-byte Spill
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vmovss	(%r13,%rax,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	vinsertps	$16, (%r13,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrd	$3, %xmm1, %edx
	cltq
	movslq	%edx, %rdx
	vinsertps	$32, (%r13,%rax,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r13,%rdx,4), %xmm1, %xmm10 # xmm10 = xmm1[0,1,2],mem[0]
	movq	48(%rsp), %rax                  # 8-byte Reload
	addq	%r15, %rax
	imulq	-88(%rsp), %rax                 # 8-byte Folded Reload
	addq	40(%rsp), %rax                  # 8-byte Folded Reload
	vroundps	$2, %ymm8, %ymm1
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%rdx,%rax,4), %ymm1, %ymm11
	movq	72(%rsp), %rax                  # 8-byte Reload
	leal	(%rcx,%rax), %eax
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpaddd	%ymm7, %ymm1, %ymm2
	vextracti128	$1, %ymm2, %xmm1
	vmovd	%xmm1, %eax
	cltq
	vpextrd	$1, %xmm1, %edx
	movslq	%edx, %rdx
	vmovss	(%r13,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm1, %eax
	vinsertps	$16, (%r13,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovd	%xmm2, %edx
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%edx, %rax
	vpextrd	$3, %xmm1, %edx
	movslq	%edx, %rdx
	vinsertps	$48, (%r13,%rdx,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	vpextrd	$1, %xmm2, %edx
	vmovss	(%r13,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	movslq	%edx, %rax
	vinsertps	$16, (%r13,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrd	$3, %xmm2, %eax
	cltq
	vinsertps	$48, (%r13,%rax,4), %xmm4, %xmm13 # xmm13 = xmm4[0,1,2],mem[0]
	addl	64(%rsp), %ecx                  # 4-byte Folded Reload
	vmovd	%ecx, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vpaddd	%ymm7, %ymm2, %ymm2
	vextracti128	$1, %ymm2, %xmm4
	vmovd	%xmm4, %eax
	cltq
	vpextrd	$1, %xmm4, %ecx
	movslq	%ecx, %rcx
	vmovss	(%r13,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vpextrd	$2, %xmm4, %eax
	vinsertps	$16, (%r13,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovd	%xmm2, %ecx
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movslq	%ecx, %rax
	vpextrd	$3, %xmm4, %ecx
	movslq	%ecx, %rcx
	vinsertps	$48, (%r13,%rcx,4), %xmm6, %xmm4 # xmm4 = xmm6[0,1,2],mem[0]
	vpextrd	$1, %xmm2, %ecx
	vmovss	(%r13,%rax,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	movslq	%ecx, %rax
	vinsertps	$16, (%r13,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrd	$2, %xmm2, %eax
	cltq
	vinsertps	$32, (%r13,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrd	$3, %xmm2, %eax
	vinsertf128	$1, 112(%rsp), %ymm10, %ymm2 # 16-byte Folded Reload
	cltq
	vinsertps	$48, (%r13,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm13, %ymm1
	movq	56(%rsp), %rax                  # 8-byte Reload
	addl	%r15d, %eax
	imull	-80(%rsp), %eax                 # 4-byte Folded Reload
	addl	-96(%rsp), %eax                 # 4-byte Folded Reload
	cltq
	shlq	$5, %rax
	vcmpeqps	%ymm2, %ymm14, %ymm2
	vmaxps	%ymm5, %ymm8, %ymm8
	vinsertf128	$1, %xmm4, %ymm6, %ymm4
	vmulps	%ymm4, %ymm1, %ymm1
	vmulps	%ymm1, %ymm12, %ymm1
	vmulps	%ymm1, %ymm11, %ymm1
	vdivps	%ymm8, %ymm1, %ymm1
	vandps	%ymm1, %ymm2, %ymm1
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	vaddps	(%rcx,%rax), %ymm1, %ymm1
	vmovaps	%ymm1, (%rcx,%rax)
.LBB40_14:                              # %after_bb
                                        #   in Loop: Header=BB40_2 Depth=1
	incq	%r15
	movl	-116(%rsp), %eax                # 4-byte Reload
	addl	-88(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -116(%rsp)                # 4-byte Spill
	incl	-108(%rsp)                      # 4-byte Folded Spill
	movl	-120(%rsp), %eax                # 4-byte Reload
	addl	-68(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -120(%rsp)                # 4-byte Spill
	cmpq	104(%rsp), %r15                 # 8-byte Folded Reload
	je	.LBB40_15
.LBB40_2:                               # %"for relu1_0_d_def__.s32.w.wi"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB40_12 Depth 2
                                        #     Child Loop BB40_8 Depth 2
	cmpl	%edi, -72(%rsp)                 # 4-byte Folded Reload
	jle	.LBB40_3
# %bb.4:                                # %false_bb
                                        #   in Loop: Header=BB40_2 Depth=1
	cmpl	$0, -76(%rsp)                   # 4-byte Folded Reload
	jle	.LBB40_14
# %bb.5:                                # %"for relu1_0_d_def__.s32.n.ni.preheader"
                                        #   in Loop: Header=BB40_2 Depth=1
	movl	-108(%rsp), %ecx                # 4-byte Reload
	movl	%ecx, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %eax
	imull	-112(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %r11
	movslq	-116(%rsp), %rax                # 4-byte Folded Reload
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movslq	-120(%rsp), %rax                # 4-byte Folded Reload
	movq	-64(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %rbx
	cmpl	$8, %r14d
	jae	.LBB40_11
# %bb.6:                                #   in Loop: Header=BB40_2 Depth=1
	xorl	%r12d, %r12d
	jmp	.LBB40_7
	.p2align	4, 0x90
.LBB40_11:                              # %vector.body.preheader
                                        #   in Loop: Header=BB40_2 Depth=1
	movq	-48(%rsp), %rax                 # 8-byte Reload
	addq	%r11, %rax
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%r11), %rdx
	movq	-32(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%r11), %rsi
	movq	-24(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%r11), %rdi
	movq	-16(%rsp), %rbp                 # 8-byte Reload
	leaq	(%rbp,%r11), %r10
	leaq	(%r13,%rax,4), %r9
	leaq	(%r13,%rdx,4), %rbp
	leaq	(,%rsi,4), %rdx
	addq	%r13, %rdx
	leaq	(,%rdi,4), %rax
	addq	%r13, %rax
	leaq	(,%r10,4), %rdi
	addq	%r13, %rdi
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB40_12:                              # %vector.body
                                        #   Parent Loop BB40_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rdi,%rsi,4), %ymm1
	vmulps	(%rax,%rsi,4), %ymm1, %ymm1
	vmaxps	%ymm5, %ymm1, %ymm1
	vdivps	%ymm9, %ymm1, %ymm1
	vcmpeqps	(%rdx,%rsi,4), %ymm14, %ymm2
	vroundps	$10, %ymm1, %ymm4
	vmaxps	%ymm5, %ymm1, %ymm1
	vmulps	(%rcx,%rsi,4), %ymm4, %ymm4
	vmovups	(%rbp,%rsi,4), %ymm6
	vmulps	(%r9,%rsi,4), %ymm6, %ymm6
	vmulps	%ymm6, %ymm12, %ymm6
	vmulps	%ymm6, %ymm4, %ymm4
	vdivps	%ymm1, %ymm4, %ymm1
	vandps	%ymm1, %ymm2, %ymm1
	vaddps	(%rbx,%rsi,4), %ymm1, %ymm1
	vmovups	%ymm1, (%rbx,%rsi,4)
	addq	$8, %rsi
	cmpq	%rsi, %r8
	jne	.LBB40_12
# %bb.13:                               # %middle.block
                                        #   in Loop: Header=BB40_2 Depth=1
	movq	%r8, %r12
	cmpq	%r14, %r8
	movq	-8(%rsp), %rdi                  # 8-byte Reload
	je	.LBB40_14
.LBB40_7:                               # %"for relu1_0_d_def__.s32.n.ni.preheader15"
                                        #   in Loop: Header=BB40_2 Depth=1
	movq	32(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r11,4), %r9
	movq	24(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r11,4), %r10
	movq	16(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%r11,4), %rsi
	movq	8(%rsp), %rax                   # 8-byte Reload
	leaq	(%rax,%r11,4), %rbp
	movq	(%rsp), %rax                    # 8-byte Reload
	leaq	(%rax,%r11,4), %rax
	jmp	.LBB40_8
	.p2align	4, 0x90
.LBB40_10:                              # %select.end
                                        #   in Loop: Header=BB40_8 Depth=2
	vaddss	%xmm1, %xmm6, %xmm1
	vmovss	%xmm1, (%rbx,%r12,4)
	incq	%r12
	cmpq	%r12, %r14
	je	.LBB40_14
.LBB40_8:                               # %"for relu1_0_d_def__.s32.n.ni"
                                        #   Parent Loop BB40_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rax,%r12,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	(%rbp,%r12,4), %xmm1, %xmm1
	vmaxss	%xmm3, %xmm1, %xmm2
	vxorps	%xmm1, %xmm1, %xmm1
	vucomiss	(%rsi,%r12,4), %xmm1
	vdivss	%xmm0, %xmm2, %xmm4
	vroundss	$10, %xmm4, %xmm4, %xmm2
	vmulss	(%rcx,%r12,4), %xmm2, %xmm2
	vmovss	(%rbx,%r12,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmovss	(%r10,%r12,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vmulss	(%r9,%r12,4), %xmm7, %xmm7
	sete	%dl
	vmulss	%xmm7, %xmm2, %xmm2
	vmulss	%xmm2, %xmm15, %xmm2
	testb	$1, %dl
	je	.LBB40_10
# %bb.9:                                # %select.true.sink
                                        #   in Loop: Header=BB40_8 Depth=2
	vmaxss	%xmm3, %xmm4, %xmm1
	vdivss	%xmm1, %xmm2, %xmm1
	jmp	.LBB40_10
.LBB40_15:                              # %destructor_block
	xorl	%eax, %eax
	addq	$168, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end40:
	.size	train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n, .Lfunc_end40-train_cost_model.par_for.relu1_0_d_def__.s32.n.n.n
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.sum_1_d_def__.s0.n.n
.LCPI41_0:
	.long	0x3727c5ac                      # float 9.99999974E-6
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5
.LCPI41_1:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.text.train_cost_model.par_for.sum_1_d_def__.s0.n.n,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.sum_1_d_def__.s0.n.n,@function
train_cost_model.par_for.sum_1_d_def__.s0.n.n: # @train_cost_model.par_for.sum_1_d_def__.s0.n.n
# %bb.0:                                # %entry
                                        # kill: def $esi killed $esi def $rsi
	movl	(%rdx), %ecx
	movq	8(%rdx), %rax
	movq	24(%rdx), %r9
	leal	(,%rsi,8), %edi
	cmpl	%esi, 4(%rdx)
	jle	.LBB41_2
# %bb.1:                                # %true_bb
	orl	$1, %edi
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpaddd	.LCPI41_1(%rip), %ymm0, %ymm0
	vmovd	%ecx, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpcmpgtd	%ymm1, %ymm0, %ymm0
	vmovss	(%rax), %xmm1                   # xmm1 = mem[0],zero,zero,zero
	vmulss	.LCPI41_0(%rip), %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vpandn	%ymm1, %ymm0, %ymm0
	movslq	%esi, %rax
	shlq	$5, %rax
	vmovdqa	%ymm0, (%r9,%rax)
	xorl	%eax, %eax
	vzeroupper
	retq
.LBB41_2:                               # %false_bb
	subl	%edi, %ecx
	jle	.LBB41_15
# %bb.3:                                # %"for sum_1_d_def__.s0.n.ni.preheader"
	cmpl	$9, %ecx
	movl	$8, %r10d
	cmovll	%ecx, %r10d
	vmovss	(%rax), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	vmulss	.LCPI41_0(%rip), %xmm0, %xmm0
	movslq	%edi, %r8
	cmpl	$32, %r10d
	jae	.LBB41_5
# %bb.4:
	xorl	%edx, %edx
	jmp	.LBB41_13
.LBB41_5:                               # %vector.ph
	movl	%r10d, %edx
	andl	$-32, %edx
	vbroadcastss	%xmm0, %ymm1
	leaq	-32(%rdx), %rax
	movq	%rax, %rdi
	shrq	$5, %rdi
	incq	%rdi
	movl	%edi, %esi
	andl	$7, %esi
	cmpq	$224, %rax
	jae	.LBB41_7
# %bb.6:
	xorl	%eax, %eax
	jmp	.LBB41_9
.LBB41_7:                               # %vector.ph.new
	leaq	(%r9,%r8,4), %rcx
	addq	$992, %rcx                      # imm = 0x3E0
	andq	$-8, %rdi
	negq	%rdi
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB41_8:                               # %vector.body
                                        # =>This Inner Loop Header: Depth=1
	vmovups	%ymm1, -992(%rcx,%rax,4)
	vmovups	%ymm1, -960(%rcx,%rax,4)
	vmovups	%ymm1, -928(%rcx,%rax,4)
	vmovups	%ymm1, -896(%rcx,%rax,4)
	vmovups	%ymm1, -864(%rcx,%rax,4)
	vmovups	%ymm1, -832(%rcx,%rax,4)
	vmovups	%ymm1, -800(%rcx,%rax,4)
	vmovups	%ymm1, -768(%rcx,%rax,4)
	vmovups	%ymm1, -736(%rcx,%rax,4)
	vmovups	%ymm1, -704(%rcx,%rax,4)
	vmovups	%ymm1, -672(%rcx,%rax,4)
	vmovups	%ymm1, -640(%rcx,%rax,4)
	vmovups	%ymm1, -608(%rcx,%rax,4)
	vmovups	%ymm1, -576(%rcx,%rax,4)
	vmovups	%ymm1, -544(%rcx,%rax,4)
	vmovups	%ymm1, -512(%rcx,%rax,4)
	vmovups	%ymm1, -480(%rcx,%rax,4)
	vmovups	%ymm1, -448(%rcx,%rax,4)
	vmovups	%ymm1, -416(%rcx,%rax,4)
	vmovups	%ymm1, -384(%rcx,%rax,4)
	vmovups	%ymm1, -352(%rcx,%rax,4)
	vmovups	%ymm1, -320(%rcx,%rax,4)
	vmovups	%ymm1, -288(%rcx,%rax,4)
	vmovups	%ymm1, -256(%rcx,%rax,4)
	vmovups	%ymm1, -224(%rcx,%rax,4)
	vmovups	%ymm1, -192(%rcx,%rax,4)
	vmovups	%ymm1, -160(%rcx,%rax,4)
	vmovups	%ymm1, -128(%rcx,%rax,4)
	vmovups	%ymm1, -96(%rcx,%rax,4)
	vmovups	%ymm1, -64(%rcx,%rax,4)
	vmovups	%ymm1, -32(%rcx,%rax,4)
	vmovups	%ymm1, (%rcx,%rax,4)
	addq	$256, %rax                      # imm = 0x100
	addq	$8, %rdi
	jne	.LBB41_8
.LBB41_9:                               # %middle.block.unr-lcssa
	testq	%rsi, %rsi
	je	.LBB41_12
# %bb.10:                               # %vector.body.epil.preheader
	addq	%r8, %rax
	leaq	(%r9,%rax,4), %rax
	addq	$96, %rax
	negq	%rsi
	.p2align	4, 0x90
.LBB41_11:                              # %vector.body.epil
                                        # =>This Inner Loop Header: Depth=1
	vmovups	%ymm1, -96(%rax)
	vmovups	%ymm1, -64(%rax)
	vmovups	%ymm1, -32(%rax)
	vmovups	%ymm1, (%rax)
	subq	$-128, %rax
	incq	%rsi
	jne	.LBB41_11
.LBB41_12:                              # %middle.block
	cmpq	%r10, %rdx
	je	.LBB41_15
.LBB41_13:                              # %"for sum_1_d_def__.s0.n.ni.preheader7"
	leaq	(%r9,%r8,4), %rax
	.p2align	4, 0x90
.LBB41_14:                              # %"for sum_1_d_def__.s0.n.ni"
                                        # =>This Inner Loop Header: Depth=1
	vmovss	%xmm0, (%rax,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %r10
	jne	.LBB41_14
.LBB41_15:                              # %destructor_block
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end41:
	.size	train_cost_model.par_for.sum_1_d_def__.s0.n.n, .Lfunc_end41-train_cost_model.par_for.sum_1_d_def__.s0.n.n
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c
.LCPI42_0:
	.long	0x80000000                      # float -0
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5
.LCPI42_1:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.section	.text.train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c,@function
train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c: # @train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$328, %rsp                      # imm = 0x148
	movl	%esi, -124(%rsp)                # 4-byte Spill
	movl	24(%rdx), %eax
	movl	%eax, -88(%rsp)                 # 4-byte Spill
	testl	%eax, %eax
	jle	.LBB42_33
# %bb.1:                                # %"for conv1_stage2_1_d_def__.s0.w.preheader"
	movl	(%rdx), %esi
	movl	4(%rdx), %r13d
	movslq	8(%rdx), %r9
	movslq	12(%rdx), %r10
	movslq	16(%rdx), %rbx
	movslq	20(%rdx), %r8
	movslq	28(%rdx), %r14
	movl	36(%rdx), %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movl	40(%rdx), %r11d
	movq	48(%rdx), %rbp
	movq	%rbp, -96(%rsp)                 # 8-byte Spill
	movq	64(%rdx), %rax
	movq	%rax, -104(%rsp)                # 8-byte Spill
	movq	80(%rdx), %rax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	leal	-1(%rsi), %eax
	movq	%rsi, %rdi
	movq	%rsi, 136(%rsp)                 # 8-byte Spill
	vmovd	%eax, %xmm0
	leaq	(,%r14,4), %rax
	subq	%rax, %rbp
	movq	%rbp, 152(%rsp)                 # 8-byte Spill
	movq	96(%rdx), %rax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	movl	-124(%rsp), %eax                # 4-byte Reload
	addl	%eax, %eax
	movl	%eax, -124(%rsp)                # 4-byte Spill
	addl	%r8d, -88(%rsp)                 # 4-byte Folded Spill
	movl	32(%rdx), %r12d
	movl	%r13d, %esi
	shll	$5, %esi
	movq	%rbx, %rdx
	shll	$5, %ebx
	movl	%r9d, %ebp
	shll	$5, %ebp
	vpbroadcastd	%xmm0, %ymm11
	movslq	%r11d, %r15
	leal	(,%r15,8), %ecx
	movl	%edi, %eax
	subl	%ecx, %eax
	movl	%eax, -68(%rsp)                 # 4-byte Spill
	movl	%r11d, %eax
	andl	$-2, %eax
	movq	%rax, 8(%rsp)                   # 8-byte Spill
	movq	%r9, 72(%rsp)                   # 8-byte Spill
                                        # kill: def $r9d killed $r9d killed $r9 def $r9
	imull	%r8d, %r9d
	shll	$5, %r9d
	movl	%r13d, -72(%rsp)                # 4-byte Spill
	movl	%r13d, %eax
	movl	%ebp, %r13d
	imull	%r8d, %eax
	movl	%r8d, %edi
	movl	%r12d, -8(%rsp)                 # 4-byte Spill
	subl	%r12d, %edi
	movq	%rdx, 64(%rsp)                  # 8-byte Spill
	imull	%edx, %edi
	shll	$5, %edi
	shll	$5, %eax
	movl	%ecx, (%rsp)                    # 4-byte Spill
	movq	%r14, 80(%rsp)                  # 8-byte Spill
	subl	%r14d, %ecx
	movl	%ecx, -4(%rsp)                  # 4-byte Spill
	vxorps	%xmm1, %xmm1, %xmm1
	vpbroadcastd	.LCPI42_0(%rip), %ymm2  # ymm2 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vbroadcastss	.LCPI42_0(%rip), %xmm3  # xmm3 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vmovdqa	.LCPI42_1(%rip), %ymm10         # ymm10 = [0,1,2,3,4,5,6,7]
	leal	-1(%r10), %ecx
	movl	%ecx, -20(%rsp)                 # 4-byte Spill
	movq	24(%rsp), %rcx                  # 8-byte Reload
	leal	(%r11,%rcx), %ecx
	movl	%ecx, 4(%rsp)                   # 4-byte Spill
	movq	%rsi, %rbp
	movq	-104(%rsp), %rcx                # 8-byte Reload
	leaq	32(%rcx), %rcx
	movq	%rcx, 104(%rsp)                 # 8-byte Spill
	movq	-120(%rsp), %rcx                # 8-byte Reload
	movq	-112(%rsp), %rdx                # 8-byte Reload
	leaq	32(%rdx), %rdx
	movq	%rdx, 96(%rsp)                  # 8-byte Spill
	leal	(%r9,%r15,8), %edx
	movl	%edx, -80(%rsp)                 # 4-byte Spill
	movq	%r15, 56(%rsp)                  # 8-byte Spill
	leal	(%rdi,%r15,8), %edx
	movl	%edx, -84(%rsp)                 # 4-byte Spill
	leaq	32(%rcx), %rdx
	movq	%rdx, 280(%rsp)                 # 8-byte Spill
	movq	-96(%rsp), %rdx                 # 8-byte Reload
	leaq	32(%rdx), %rdx
	movq	%rdx, 272(%rsp)                 # 8-byte Spill
	movq	%r10, 112(%rsp)                 # 8-byte Spill
	movq	%r11, 16(%rsp)                  # 8-byte Spill
	movq	%rsi, 144(%rsp)                 # 8-byte Spill
	movl	%ebx, -12(%rsp)                 # 4-byte Spill
	movl	%r13d, -16(%rsp)                # 4-byte Spill
	jmp	.LBB42_2
	.p2align	4, 0x90
.LBB42_32:                              # %"end for conv1_stage2_1_d_def__.s0.c.ci"
                                        #   in Loop: Header=BB42_2 Depth=1
	incq	%r8
	movl	-16(%rsp), %r13d                # 4-byte Reload
	movq	168(%rsp), %r9                  # 8-byte Reload
	addl	%r13d, %r9d
	movl	-12(%rsp), %ebx                 # 4-byte Reload
	movq	160(%rsp), %rdi                 # 8-byte Reload
	addl	%ebx, %edi
	movq	144(%rsp), %rbp                 # 8-byte Reload
	movq	32(%rsp), %rax                  # 8-byte Reload
	addl	%ebp, %eax
	addl	%r13d, -80(%rsp)                # 4-byte Folded Spill
	addl	%ebx, -84(%rsp)                 # 4-byte Folded Spill
	cmpl	%r8d, -88(%rsp)                 # 4-byte Folded Reload
	movq	16(%rsp), %r11                  # 8-byte Reload
	movq	-120(%rsp), %rcx                # 8-byte Reload
	je	.LBB42_33
.LBB42_2:                               # %"for conv1_stage2_1_d_def__.s0.w"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB42_7 Depth 2
                                        #     Child Loop BB42_10 Depth 2
                                        #       Child Loop BB42_12 Depth 3
                                        #         Child Loop BB42_25 Depth 4
                                        #         Child Loop BB42_20 Depth 4
	movslq	%eax, %r15
	movq	%r9, 168(%rsp)                  # 8-byte Spill
	movslq	%r9d, %rax
	movq	%rax, 216(%rsp)                 # 8-byte Spill
	movq	%rdi, 160(%rsp)                 # 8-byte Spill
	movslq	%edi, %rax
	movq	%rax, 208(%rsp)                 # 8-byte Spill
	movl	%ebp, %esi
	imull	%r8d, %esi
	cmpq	%r10, %r8
	movl	-20(%rsp), %eax                 # 4-byte Reload
	cmovll	%r8d, %eax
	setl	%r9b
	movl	%eax, %edx
	sarl	$31, %edx
	andnl	%eax, %edx, %edi
	imull	%ebp, %edi
	imull	%r8d, %r13d
	movq	%r8, -40(%rsp)                  # 8-byte Spill
	movl	%r8d, %edx
	subl	-8(%rsp), %edx                  # 4-byte Folded Reload
	imull	%ebx, %edx
	movslq	%edi, %r8
	movq	80(%rsp), %rbp                  # 8-byte Reload
	subl	%ebp, %edi
	movslq	%esi, %rbx
	movq	%rbx, 240(%rsp)                 # 8-byte Spill
	subl	%ebp, %esi
	movq	%rdi, %rbp
	movq	%rdi, 224(%rsp)                 # 8-byte Spill
	movslq	%edi, %rdi
	movq	%rdi, 200(%rsp)                 # 8-byte Spill
	vmovd	%r9d, %xmm0
	vpbroadcastb	%xmm0, %xmm5
	movq	%rsi, %rdi
	movq	%rsi, 232(%rsp)                 # 8-byte Spill
	movslq	%esi, %rsi
	movq	%rsi, 192(%rsp)                 # 8-byte Spill
	movslq	%edx, %rdx
	movq	%rdx, 48(%rsp)                  # 8-byte Spill
	movslq	%r13d, %rax
	movq	%rax, 40(%rsp)                  # 8-byte Spill
	movq	152(%rsp), %rax                 # 8-byte Reload
	movq	%r15, 32(%rsp)                  # 8-byte Spill
	leaq	(%rax,%r15,4), %rdx
	movq	%rdx, 184(%rsp)                 # 8-byte Spill
	movq	%r8, 88(%rsp)                   # 8-byte Spill
	leaq	(%rax,%r8,4), %rax
	movq	%rax, 176(%rsp)                 # 8-byte Spill
	movl	$0, -76(%rsp)                   # 4-byte Folded Spill
	movl	-124(%rsp), %eax                # 4-byte Reload
	movq	8(%rsp), %r12                   # 8-byte Reload
	movslq	%eax, %r15
	testl	%r11d, %r11d
	jle	.LBB42_10
	.p2align	4, 0x90
.LBB42_4:                               # %"for conv1_stage2_1_d_def__.s0.n.n.preheader"
                                        #   in Loop: Header=BB42_2 Depth=1
	movl	%r15d, %eax
	imull	-72(%rsp), %eax                 # 4-byte Folded Reload
	movq	%r15, %r10
	imulq	64(%rsp), %r10                  # 8-byte Folded Reload
	movq	%r15, %r9
	imulq	72(%rsp), %r9                   # 8-byte Folded Reload
	movslq	%eax, %r8
	cmpl	$1, 56(%rsp)                    # 4-byte Folded Reload
	jne	.LBB42_6
# %bb.5:                                #   in Loop: Header=BB42_2 Depth=1
	xorl	%ebp, %ebp
	jmp	.LBB42_8
	.p2align	4, 0x90
.LBB42_6:                               # %"for conv1_stage2_1_d_def__.s0.n.n.preheader1"
                                        #   in Loop: Header=BB42_2 Depth=1
	movq	216(%rsp), %rax                 # 8-byte Reload
	addq	%r9, %rax
	movq	104(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	movq	208(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%r10), %rdx
	movq	96(%rsp), %rsi                  # 8-byte Reload
	leaq	(%rsi,%rdx,4), %rdi
	movq	184(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%r8,4), %rbx
	movq	176(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%r8,4), %rdx
	xorl	%esi, %esi
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB42_7:                               # %"for conv1_stage2_1_d_def__.s0.n.n"
                                        #   Parent Loop BB42_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rdx,%rsi), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm5, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0            # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vpsrad	$31, %ymm0, %ymm0
	vpxor	(%rcx,%rsi), %ymm2, %ymm4
	vcmpleps	(%rbx,%rsi), %ymm1, %ymm6
	vpand	%ymm4, %ymm0, %ymm0
	vandps	-32(%rdi,%rsi), %ymm6, %ymm4
	vaddps	%ymm4, %ymm0, %ymm0
	vmovups	%ymm0, -32(%rax,%rsi)
	vmovups	32(%rdx,%rsi), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm5, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0            # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vpsrad	$31, %ymm0, %ymm0
	vpxor	32(%rcx,%rsi), %ymm2, %ymm4
	vcmpleps	32(%rbx,%rsi), %ymm1, %ymm6
	vandps	(%rdi,%rsi), %ymm6, %ymm6
	vpand	%ymm4, %ymm0, %ymm0
	vaddps	%ymm6, %ymm0, %ymm0
	vmovups	%ymm0, (%rax,%rsi)
	addq	$2, %rbp
	addq	$64, %rsi
	cmpq	%rbp, %r12
	jne	.LBB42_7
.LBB42_8:                               # %"end for conv1_stage2_1_d_def__.s0.n.n.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB42_2 Depth=1
	testb	$1, %r11b
	je	.LBB42_10
# %bb.9:                                # %"for conv1_stage2_1_d_def__.s0.n.n.epil"
                                        #   in Loop: Header=BB42_2 Depth=1
	addq	48(%rsp), %r10                  # 8-byte Folded Reload
	addq	40(%rsp), %r9                   # 8-byte Folded Reload
	leaq	(%r8,%rbp,8), %rax
	movq	200(%rsp), %rdx                 # 8-byte Reload
	addq	%rax, %rdx
	movq	-96(%rsp), %rsi                 # 8-byte Reload
	vmovups	(%rsi,%rdx,4), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm5, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0            # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	movq	%rbp, %rdx
	shlq	$5, %rdx
	vpxor	(%rcx,%rdx), %ymm2, %ymm4
	vpsrad	$31, %ymm0, %ymm0
	vpand	%ymm4, %ymm0, %ymm0
	addq	192(%rsp), %rax                 # 8-byte Folded Reload
	leaq	(%r10,%rbp,8), %rdx
	vcmpleps	(%rsi,%rax,4), %ymm1, %ymm4
	movq	-112(%rsp), %rax                # 8-byte Reload
	vandps	(%rax,%rdx,4), %ymm4, %ymm4
	vaddps	%ymm4, %ymm0, %ymm0
	leaq	(%r9,%rbp,8), %rax
	movq	-104(%rsp), %rdx                # 8-byte Reload
	vmovups	%ymm0, (%rdx,%rax,4)
.LBB42_10:                              # %"end for conv1_stage2_1_d_def__.s0.n.n"
                                        #   Parent Loop BB42_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB42_12 Depth 3
                                        #         Child Loop BB42_25 Depth 4
                                        #         Child Loop BB42_20 Depth 4
	cmpl	$0, 24(%rsp)                    # 4-byte Folded Reload
	movq	112(%rsp), %r10                 # 8-byte Reload
	movq	-40(%rsp), %r8                  # 8-byte Reload
	jle	.LBB42_14
# %bb.11:                               # %"for conv1_stage2_1_d_def__.s0.n.n1.preheader"
                                        #   in Loop: Header=BB42_10 Depth=2
	movl	%r15d, %eax
	imull	-72(%rsp), %eax                 # 4-byte Folded Reload
	movq	224(%rsp), %rcx                 # 8-byte Reload
	leal	(%rax,%rcx), %edx
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %ymm12
	movq	232(%rsp), %rcx                 # 8-byte Reload
	leal	(%rax,%rcx), %edx
	vmovd	%edx, %xmm0
	vpbroadcastd	%xmm0, %ymm13
	movq	%r15, %rdx
	movq	64(%rsp), %rsi                  # 8-byte Reload
	imulq	%rsi, %rdx
	movq	48(%rsp), %rcx                  # 8-byte Reload
	addq	%rcx, %rdx
	movq	%rdx, 320(%rsp)                 # 8-byte Spill
	movq	%r15, %rbp
	movq	72(%rsp), %rdx                  # 8-byte Reload
	imulq	%rdx, %rbp
	movq	40(%rsp), %rdi                  # 8-byte Reload
	addq	%rdi, %rbp
	movq	%rbp, 312(%rsp)                 # 8-byte Spill
                                        # kill: def $esi killed $esi killed $rsi def $rsi
	imull	%r15d, %esi
	addl	%esi, %ecx
	movq	%rcx, 264(%rsp)                 # 8-byte Spill
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	imull	%r15d, %edx
	leal	(%rdi,%rdx), %ecx
	movq	%rcx, 256(%rsp)                 # 8-byte Spill
	movslq	%eax, %rcx
	addl	-80(%rsp), %edx                 # 4-byte Folded Reload
	movq	%rdx, -32(%rsp)                 # 8-byte Spill
	addl	-84(%rsp), %esi                 # 4-byte Folded Reload
	movq	32(%rsp), %rax                  # 8-byte Reload
	addq	%rcx, %rax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movq	88(%rsp), %rax                  # 8-byte Reload
	movq	%rcx, 248(%rsp)                 # 8-byte Spill
	addq	%rcx, %rax
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	xorl	%r13d, %r13d
	movl	-68(%rsp), %r15d                # 4-byte Reload
	movl	-4(%rsp), %eax                  # 4-byte Reload
	movl	%eax, -60(%rsp)                 # 4-byte Spill
	movl	(%rsp), %eax                    # 4-byte Reload
	movl	%eax, -64(%rsp)                 # 4-byte Spill
	movq	56(%rsp), %r9                   # 8-byte Reload
	jmp	.LBB42_12
	.p2align	4, 0x90
.LBB42_13:                              # %true_bb
                                        #   in Loop: Header=BB42_12 Depth=3
	vmovd	%ecx, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	%ymm0, %ymm10, %ymm0
	vpminsd	%ymm0, %ymm11, %ymm8
	vpaddd	%ymm12, %ymm8, %ymm0
	vmovd	%xmm0, %eax
	movslq	%eax, %r8
	vpextrd	$1, %xmm0, %ecx
	movslq	%ecx, %r11
	vpextrd	$2, %xmm0, %edx
	vpextrd	$3, %xmm0, %edi
	movslq	%edx, %rdx
	vextracti128	$1, %ymm0, %xmm0
	vmovd	%xmm0, %ebp
	movslq	%ebp, %rbp
	vpextrd	$1, %xmm0, %ebx
	movq	%rsi, %r14
	vpextrd	$2, %xmm0, %esi
	movslq	%ebx, %rbx
	movslq	%esi, %rcx
	vpextrd	$3, %xmm0, %eax
	cltq
	movq	-96(%rsp), %rsi                 # 8-byte Reload
	vmovss	(%rsi,%rbp,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movslq	%edi, %rax
	vmovss	(%rsi,%r8,4), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rsi,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm5, %xmm0, %xmm0
	vpmovzxwd	%xmm0, %ymm0            # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vpsrad	$31, %ymm0, %ymm0
	movq	%r9, %rax
	shlq	$5, %rax
	movq	-120(%rsp), %rcx                # 8-byte Reload
	vpxor	(%rcx,%rax), %ymm2, %ymm4
	vpand	%ymm4, %ymm0, %ymm9
	vpaddd	%ymm13, %ymm8, %ymm0
	vmovd	%xmm0, %eax
	movslq	%eax, %r8
	vpextrd	$1, %xmm0, %ecx
	vpextrd	$2, %xmm0, %edx
	movslq	%ecx, %rcx
	movslq	%edx, %r11
	vpextrd	$3, %xmm0, %edx
	vextracti128	$1, %ymm0, %xmm0
	vmovd	%xmm0, %edi
	vpextrd	$1, %xmm0, %ebp
	movslq	%edi, %rdi
	movslq	%ebp, %rbp
	vpextrd	$2, %xmm0, %ebx
	movslq	%ebx, %rbx
	vpextrd	$3, %xmm0, %eax
	vmovss	(%rsi,%rdi,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rsi,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	cltq
	vinsertps	$48, (%rsi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%rsi,%r8,4), %xmm4             # xmm4 = mem[0],zero,zero,zero
	movq	-40(%rsp), %r8                  # 8-byte Reload
	vinsertps	$16, (%rsi,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rsi,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movslq	%edx, %rax
	vinsertps	$48, (%rsi,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	%r14, %rsi
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	movq	320(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r9,8), %rax
	vcmpleps	%ymm0, %ymm1, %ymm0
	movq	-112(%rsp), %rcx                # 8-byte Reload
	vandps	(%rcx,%rax,4), %ymm0, %ymm0
	vaddps	%ymm0, %ymm9, %ymm0
	movq	312(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r9,8), %rax
	movq	-104(%rsp), %rcx                # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rax,4)
.LBB42_29:                              # %after_bb
                                        #   in Loop: Header=BB42_12 Depth=3
	incq	%r9
	incl	%r13d
	movq	-32(%rsp), %rax                 # 8-byte Reload
	addl	$8, %eax
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	addl	$8, %esi
	addl	$8, -64(%rsp)                   # 4-byte Folded Spill
	addl	$8, -60(%rsp)                   # 4-byte Folded Spill
	addl	$-8, %r15d
	cmpl	%r9d, 4(%rsp)                   # 4-byte Folded Reload
	je	.LBB42_14
.LBB42_12:                              # %"for conv1_stage2_1_d_def__.s0.n.n1"
                                        #   Parent Loop BB42_2 Depth=1
                                        #     Parent Loop BB42_10 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB42_25 Depth 4
                                        #         Child Loop BB42_20 Depth 4
	cmpl	$9, %r15d
	movl	$8, %r11d
	cmovll	%r15d, %r11d
	leal	(,%r13,8), %eax
	movl	-68(%rsp), %ecx                 # 4-byte Reload
	movl	%ecx, %edx
	subl	%eax, %edx
	cmpl	$9, %edx
	movl	$8, %eax
	cmovgel	%eax, %edx
	leal	(,%r9,8), %ecx
	leal	8(,%r9,8), %eax
	cmpl	136(%rsp), %eax                 # 4-byte Folded Reload
	jle	.LBB42_13
# %bb.16:                               # %false_bb
                                        #   in Loop: Header=BB42_12 Depth=3
	cmpl	%ecx, 136(%rsp)                 # 4-byte Folded Reload
	jle	.LBB42_29
# %bb.17:                               # %"for conv1_stage2_1_d_def__.s0.n.ni.preheader"
                                        #   in Loop: Header=BB42_12 Depth=3
	movslq	-32(%rsp), %rbx                 # 4-byte Folded Reload
	movslq	%esi, %rbp
	movslq	-64(%rsp), %rdi                 # 4-byte Folded Reload
	movslq	-60(%rsp), %r14                 # 4-byte Folded Reload
	cmpl	$7, %edx
	ja	.LBB42_22
# %bb.18:                               #   in Loop: Header=BB42_12 Depth=3
	xorl	%r12d, %r12d
	jmp	.LBB42_19
	.p2align	4, 0x90
.LBB42_22:                              # %vector.ph
                                        #   in Loop: Header=BB42_12 Depth=3
	movq	%rcx, 288(%rsp)                 # 8-byte Spill
	movl	%edx, %eax
	andl	$-8, %eax
	addq	$-8, %rax
	movq	%rax, %rcx
	shrq	$3, %rcx
	incq	%rcx
	movl	%edx, %r12d
	andl	$-8, %r12d
	testq	%rax, %rax
	movq	%r14, 304(%rsp)                 # 8-byte Spill
	movq	%rdi, -48(%rsp)                 # 8-byte Spill
	movq	%rbp, -56(%rsp)                 # 8-byte Spill
	movq	%rbx, 296(%rsp)                 # 8-byte Spill
	je	.LBB42_23
# %bb.24:                               # %vector.ph.new
                                        #   in Loop: Header=BB42_12 Depth=3
	movq	104(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbx,4), %rbx
	movq	96(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rbp,4), %r8
	movq	280(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi,4), %rdi
	movq	128(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r14), %rax
	movq	%r14, %rbp
	movq	%rsi, %r14
	movq	272(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rax,4), %r10
	movq	120(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbp), %rax
	leaq	(%rsi,%rax,4), %rax
	movq	%r14, %rsi
	movl	%r11d, %ebp
	andl	$-8, %ebp
	addq	$-8, %rbp
	shrq	$3, %rbp
	incq	%rbp
	andq	$-2, %rbp
	negq	%rbp
	xorl	%r14d, %r14d
	.p2align	4, 0x90
.LBB42_25:                              # %vector.body
                                        #   Parent Loop BB42_2 Depth=1
                                        #     Parent Loop BB42_10 Depth=2
                                        #       Parent Loop BB42_12 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovups	-32(%rax,%r14,4), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm0, %xmm5, %xmm0
	vpmovzxwd	%xmm0, %ymm0            # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vpsrad	$31, %ymm0, %ymm0
	vpxor	-32(%rdi,%r14,4), %ymm2, %ymm4
	vcmpleps	-32(%r10,%r14,4), %ymm1, %ymm8
	vandps	-32(%r8,%r14,4), %ymm8, %ymm8
	vpand	%ymm4, %ymm0, %ymm0
	vaddps	%ymm0, %ymm8, %ymm0
	vmovups	%ymm0, -32(%rbx,%r14,4)
	vmovups	(%rax,%r14,4), %ymm0
	vcmpleps	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm4
	vpackssdw	%xmm4, %xmm0, %xmm0
	vpand	%xmm0, %xmm5, %xmm0
	vpmovzxwd	%xmm0, %ymm0            # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpslld	$31, %ymm0, %ymm0
	vpsrad	$31, %ymm0, %ymm0
	vpxor	(%rdi,%r14,4), %ymm2, %ymm4
	vcmpleps	(%r10,%r14,4), %ymm1, %ymm8
	vpand	%ymm4, %ymm0, %ymm0
	vandps	(%r8,%r14,4), %ymm8, %ymm4
	vaddps	%ymm4, %ymm0, %ymm0
	vmovups	%ymm0, (%rbx,%r14,4)
	addq	$16, %r14
	addq	$2, %rbp
	jne	.LBB42_25
# %bb.26:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB42_12 Depth=3
	testb	$1, %cl
	je	.LBB42_28
.LBB42_27:                              # %vector.body.epil
                                        #   in Loop: Header=BB42_12 Depth=3
	movq	288(%rsp), %rbx                 # 8-byte Reload
	movl	%ebx, %eax
	subl	80(%rsp), %eax                  # 4-byte Folded Reload
	movq	%rsi, %rbp
	movslq	%ebx, %rsi
	cltq
	addq	%r14, %rax
	addq	248(%rsp), %rax                 # 8-byte Folded Reload
	addq	%r14, %rsi
	movq	-120(%rsp), %rdi                # 8-byte Reload
	vpxor	(%rdi,%rsi,4), %ymm2, %ymm0
	movq	88(%rsp), %rsi                  # 8-byte Reload
	addq	%rax, %rsi
	addq	240(%rsp), %rax                 # 8-byte Folded Reload
	movq	-96(%rsp), %rdi                 # 8-byte Reload
	vcmpleps	(%rdi,%rax,4), %ymm1, %ymm4
	vmovups	(%rdi,%rsi,4), %ymm8
	movq	264(%rsp), %rax                 # 8-byte Reload
	addl	%ebx, %eax
	cltq
	addq	%r14, %rax
	movq	-112(%rsp), %rsi                # 8-byte Reload
	vandps	(%rsi,%rax,4), %ymm4, %ymm4
	movq	256(%rsp), %rax                 # 8-byte Reload
	addl	%ebx, %eax
	cltq
	addq	%r14, %rax
	vcmpleps	%ymm1, %ymm8, %ymm8
	vextractf128	$1, %ymm8, %xmm6
	vpackssdw	%xmm6, %xmm8, %xmm6
	vpand	%xmm6, %xmm5, %xmm6
	vpmovzxwd	%xmm6, %ymm6            # ymm6 = xmm6[0],zero,xmm6[1],zero,xmm6[2],zero,xmm6[3],zero,xmm6[4],zero,xmm6[5],zero,xmm6[6],zero,xmm6[7],zero
	vpslld	$31, %ymm6, %ymm6
	vpsrad	$31, %ymm6, %ymm6
	vpand	%ymm0, %ymm6, %ymm0
	vaddps	%ymm4, %ymm0, %ymm0
	movq	-104(%rsp), %rsi                # 8-byte Reload
	vmovups	%ymm0, (%rsi,%rax,4)
	movq	%rbp, %rsi
.LBB42_28:                              # %middle.block
                                        #   in Loop: Header=BB42_12 Depth=3
	cmpq	%rdx, %r12
	movq	112(%rsp), %r10                 # 8-byte Reload
	movq	-40(%rsp), %r8                  # 8-byte Reload
	movq	304(%rsp), %r14                 # 8-byte Reload
	movq	-48(%rsp), %rdi                 # 8-byte Reload
	movq	-56(%rsp), %rbp                 # 8-byte Reload
	movq	296(%rsp), %rbx                 # 8-byte Reload
	je	.LBB42_29
.LBB42_19:                              # %"for conv1_stage2_1_d_def__.s0.n.ni.preheader23"
                                        #   in Loop: Header=BB42_12 Depth=3
	subq	%r12, %r11
	addq	%r12, %rbx
	movq	-104(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rbx,4), %rax
	addq	%r12, %rbp
	movq	-112(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%rbp,4), %rdx
	addq	%r12, %rdi
	movl	%r15d, -48(%rsp)                # 4-byte Spill
	movq	%r13, -56(%rsp)                 # 8-byte Spill
	movq	%rsi, %r15
	movq	-120(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdi,4), %rdi
	movq	128(%rsp), %rsi                 # 8-byte Reload
	addq	%r12, %rsi
	addq	%r14, %rsi
	movq	-96(%rsp), %rbx                 # 8-byte Reload
	leaq	(%rbx,%rsi,4), %rbp
	movq	%r15, %rsi
	movq	-56(%rsp), %r13                 # 8-byte Reload
	movl	-48(%rsp), %r15d                # 4-byte Reload
	addq	120(%rsp), %r12                 # 8-byte Folded Reload
	addq	%r14, %r12
	leaq	(%rbx,%r12,4), %rcx
	xorl	%ebx, %ebx
	jmp	.LBB42_20
	.p2align	4, 0x90
.LBB42_30:                              # %"for conv1_stage2_1_d_def__.s0.n.ni"
                                        #   in Loop: Header=BB42_20 Depth=4
	vxorps	%xmm0, %xmm0, %xmm0
.LBB42_31:                              # %"for conv1_stage2_1_d_def__.s0.n.ni"
                                        #   in Loop: Header=BB42_20 Depth=4
	vmovss	(%rbp,%rbx,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmovss	(%rdx,%rbx,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vcmpltss	%xmm4, %xmm6, %xmm4
	vandnps	%xmm7, %xmm4, %xmm4
	vaddss	%xmm4, %xmm0, %xmm0
	vmovss	%xmm0, (%rax,%rbx,4)
	incq	%rbx
	cmpq	%rbx, %r11
	je	.LBB42_29
.LBB42_20:                              # %"for conv1_stage2_1_d_def__.s0.n.ni"
                                        #   Parent Loop BB42_2 Depth=1
                                        #     Parent Loop BB42_10 Depth=2
                                        #       Parent Loop BB42_12 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vxorps	%xmm4, %xmm4, %xmm4
	cmpq	%r10, %r8
	jge	.LBB42_30
# %bb.21:                               #   in Loop: Header=BB42_20 Depth=4
	vmovss	(%rcx,%rbx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovss	(%rdi,%rbx,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vxorps	%xmm3, %xmm6, %xmm6
	vcmpless	%xmm4, %xmm0, %xmm0
	vandps	%xmm6, %xmm0, %xmm0
	jmp	.LBB42_31
.LBB42_23:                              #   in Loop: Header=BB42_12 Depth=3
	xorl	%r14d, %r14d
	testb	$1, %cl
	jne	.LBB42_27
	jmp	.LBB42_28
	.p2align	4, 0x90
.LBB42_14:                              # %"end for conv1_stage2_1_d_def__.s0.n.n2"
                                        #   in Loop: Header=BB42_10 Depth=2
	testb	$1, -76(%rsp)                   # 1-byte Folded Reload
	jne	.LBB42_32
# %bb.15:                               # %"end for conv1_stage2_1_d_def__.s0.n.n2.for conv1_stage2_1_d_def__.s0.c.ci_crit_edge"
                                        #   in Loop: Header=BB42_10 Depth=2
	movl	-124(%rsp), %eax                # 4-byte Reload
	orl	$1, %eax
	movb	$1, %cl
	movl	%ecx, -76(%rsp)                 # 4-byte Spill
	movq	16(%rsp), %r11                  # 8-byte Reload
	movq	-120(%rsp), %rcx                # 8-byte Reload
	movq	8(%rsp), %r12                   # 8-byte Reload
	movslq	%eax, %r15
	testl	%r11d, %r11d
	jg	.LBB42_4
	jmp	.LBB42_10
.LBB42_33:                              # %destructor_block
	xorl	%eax, %eax
	addq	$328, %rsp                      # imm = 0x148
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end42:
	.size	train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c, .Lfunc_end42-train_cost_model.par_for.conv1_stage2_1_d_def__.s0.c.c
                                        # -- End function
	.section	.text.train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c
	.type	train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c,@function
train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c: # @train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$24, %rsp
	movl	%esi, %ebx
	movl	(%rdx), %ebp
	movl	4(%rdx), %r15d
	movl	8(%rdx), %r13d
	movq	16(%rdx), %r14
	imull	%ebp, %ebx
	testl	%r13d, %r13d
	jle	.LBB43_2
# %bb.1:                                # %"for conv1_stage1_1_d_def__.s0.w.w.preheader"
	movslq	%ebx, %rax
	leaq	(%r14,%rax,4), %rdi
	movq	%r13, %rdx
	shlq	$5, %rdx
	xorl	%esi, %esi
	callq	memset@PLT
.LBB43_2:                               # %"end for conv1_stage1_1_d_def__.s0.w.w"
	testl	%r15d, %r15d
	jle	.LBB43_14
# %bb.3:                                # %"for conv1_stage1_1_d_def__.s0.w.w1.preheader"
	movl	%r15d, %ecx
	leal	(,%r13,8), %r15d
	movl	%ebp, %eax
	subl	%r15d, %eax
	movl	%eax, 4(%rsp)                   # 4-byte Spill
	xorl	%r12d, %r12d
	movl	%r13d, %eax
	cmpl	$1, %ecx
	je	.LBB43_11
# %bb.4:                                # %"for conv1_stage1_1_d_def__.s0.w.w1.preheader.new"
	movq	%r13, 16(%rsp)                  # 8-byte Spill
	movl	%ecx, 8(%rsp)                   # 4-byte Spill
	andl	$-2, %ecx
	negl	%ecx
	movl	%ecx, 12(%rsp)                  # 4-byte Spill
	xorl	%r12d, %r12d
	movl	4(%rsp), %eax                   # 4-byte Reload
	movl	%eax, %r13d
	.p2align	4, 0x90
.LBB43_5:                               # %"for conv1_stage1_1_d_def__.s0.w.w1"
                                        # =>This Inner Loop Header: Depth=1
	cmpl	%r15d, %ebp
	jle	.LBB43_7
# %bb.6:                                # %"for conv1_stage1_1_d_def__.s0.w.wi.preheader"
                                        #   in Loop: Header=BB43_5 Depth=1
	cmpl	$9, %r13d
	movl	$8, %eax
	cmovll	%r13d, %eax
	decl	%eax
	leaq	4(,%rax,4), %rdx
	leal	(%rbx,%r15), %eax
	cltq
	leaq	(%r14,%rax,4), %rdi
	xorl	%esi, %esi
	callq	memset@PLT
.LBB43_7:                               # %"end for conv1_stage1_1_d_def__.s0.w.wi"
                                        #   in Loop: Header=BB43_5 Depth=1
	leal	8(%r15), %eax
	cmpl	%eax, %ebp
	jle	.LBB43_9
# %bb.8:                                # %"for conv1_stage1_1_d_def__.s0.w.wi.preheader.1"
                                        #   in Loop: Header=BB43_5 Depth=1
	leal	-8(%r13), %eax
	cmpl	$9, %eax
	movl	$8, %ecx
	cmovgel	%ecx, %eax
	decl	%eax
	leaq	4(,%rax,4), %rdx
	leal	(%rbx,%r15), %eax
	addl	$8, %eax
	cltq
	leaq	(%r14,%rax,4), %rdi
	xorl	%esi, %esi
	callq	memset@PLT
.LBB43_9:                               # %"end for conv1_stage1_1_d_def__.s0.w.wi.1"
                                        #   in Loop: Header=BB43_5 Depth=1
	addl	$-2, %r12d
	addl	$16, %r15d
	addl	$-16, %r13d
	cmpl	%r12d, 12(%rsp)                 # 4-byte Folded Reload
	jne	.LBB43_5
# %bb.10:                               # %destructor_block.loopexit.unr-lcssa.loopexit
	movq	16(%rsp), %r13                  # 8-byte Reload
	movl	%r13d, %eax
	subl	%r12d, %eax
	negl	%r12d
	movl	8(%rsp), %ecx                   # 4-byte Reload
.LBB43_11:                              # %destructor_block.loopexit.unr-lcssa
	testb	$1, %cl
	je	.LBB43_14
# %bb.12:                               # %"for conv1_stage1_1_d_def__.s0.w.w1.epil"
	shll	$3, %eax
	cmpl	%eax, %ebp
	jle	.LBB43_14
# %bb.13:                               # %"for conv1_stage1_1_d_def__.s0.w.wi.preheader.epil"
	leal	(%rbx,%r13,8), %eax
	leal	(,%r12,8), %ecx
	movl	4(%rsp), %edx                   # 4-byte Reload
	subl	%ecx, %edx
	cmpl	$9, %edx
	movl	$8, %ecx
	cmovll	%edx, %ecx
	decl	%ecx
	leaq	4(,%rcx,4), %rdx
	leal	(%rax,%r12,8), %eax
	cltq
	leaq	(%r14,%rax,4), %rdi
	xorl	%esi, %esi
	callq	memset@PLT
.LBB43_14:                              # %destructor_block
	xorl	%eax, %eax
	addq	$24, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end43:
	.size	train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c, .Lfunc_end43-train_cost_model.par_for.conv1_stage1_1_d_def__.s0.c
                                        # -- End function
	.section	.text.train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c
	.type	train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c,@function
train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c: # @train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$344, %rsp                      # imm = 0x158
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%xmm0, 80(%rsp)
	movq	%rdi, 8(%rsp)                   # 8-byte Spill
	movl	(%rdx), %eax
	movl	%eax, 116(%rsp)                 # 4-byte Spill
	movl	8(%rdx), %eax
	movslq	4(%rdx), %r10
	movl	16(%rdx), %ecx
	movl	%ecx, 40(%rsp)                  # 4-byte Spill
	movl	20(%rdx), %ecx
	movq	%rcx, 280(%rsp)                 # 8-byte Spill
	movl	28(%rdx), %r9d
	movq	32(%rdx), %rcx
	movq	%rcx, 24(%rsp)                  # 8-byte Spill
	movq	48(%rdx), %rcx
	movl	%eax, 4(%rsp)                   # 4-byte Spill
                                        # kill: def $eax killed $eax def $rax
	imull	%esi, %eax
	movq	%rax, 176(%rsp)                 # 8-byte Spill
	testl	%r9d, %r9d
	movq	%rcx, 120(%rsp)                 # 8-byte Spill
	movl	%esi, 44(%rsp)                  # 4-byte Spill
	jle	.LBB44_84
# %bb.1:                                # %"for conv1_stage1_1_d_def__.s1.w.w.preheader"
	movl	12(%rdx), %eax
	movl	24(%rdx), %edx
	movq	%r10, %rdi
	shlq	$5, %rdi
	movq	%rdi, 216(%rsp)                 # 8-byte Spill
	movl	%r10d, %edi
	shll	$8, %edi
	movl	%edi, 20(%rsp)                  # 4-byte Spill
	movl	%r10d, %edi
	imull	%esi, %edi
	imulq	$896, %r10, %rsi                # imm = 0x380
	addq	%rcx, %rsi
	movq	%rsi, 168(%rsp)                 # 8-byte Spill
	movq	%rax, 192(%rsp)                 # 8-byte Spill
	addl	%edx, %eax
	movl	%eax, 160(%rsp)                 # 4-byte Spill
	movq	%rdx, 104(%rsp)                 # 8-byte Spill
	leal	(%rdi,%rdx,2), %eax
	movl	%eax, 128(%rsp)                 # 4-byte Spill
	leaq	(%r10,%r10,2), %rax
	movq	%rax, %rdx
	shlq	$8, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 248(%rsp)                 # 8-byte Spill
	leaq	(%r10,%r10,4), %rdx
	shlq	$7, %rdx
	addq	%rcx, %rdx
	movq	%rdx, 144(%rsp)                 # 8-byte Spill
	movq	%r10, %rdx
	shlq	$7, %rdx
	movq	%r10, %rsi
	shlq	$9, %rsi
	addq	%rcx, %rsi
	movq	%rsi, 224(%rsp)                 # 8-byte Spill
	shlq	$7, %rax
	addq	%rcx, %rax
	movq	%rax, 152(%rsp)                 # 8-byte Spill
	movq	%r10, %rax
	shlq	$8, %rax
	addq	%rcx, %rax
	movq	%rax, 136(%rsp)                 # 8-byte Spill
	addq	%rcx, %rdx
	movq	%rdx, 232(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	xorl	%ecx, %ecx
	movq	%rcx, 32(%rsp)                  # 8-byte Spill
	xorl	%r11d, %r11d
	xorl	%edx, %edx
	xorl	%r13d, %r13d
	movq	%r10, 240(%rsp)                 # 8-byte Spill
	movq	%r9, 184(%rsp)                  # 8-byte Spill
	.p2align	4, 0x90
.LBB44_2:                               # %"for conv1_stage1_1_d_def__.s1.w.w"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB44_7 Depth 2
                                        #     Child Loop BB44_17 Depth 2
                                        #       Child Loop BB44_22 Depth 3
	leal	(,%rdx,8), %r12d
	movl	4(%rsp), %esi                   # 4-byte Reload
	subl	%r12d, %esi
	movq	176(%rsp), %rcx                 # 8-byte Reload
	movq	%rdx, 200(%rsp)                 # 8-byte Spill
	leal	(%rcx,%rdx,8), %ecx
	movslq	%ecx, %rcx
	movq	%rcx, 72(%rsp)                  # 8-byte Spill
	cmpl	$0, 104(%rsp)                   # 4-byte Folded Reload
	movq	%rdi, 208(%rsp)                 # 8-byte Spill
	movl	%esi, 56(%rsp)                  # 4-byte Spill
	jle	.LBB44_14
# %bb.3:                                # %"for conv1_stage1_1_d_def__.s1.r997$x.r997$x.preheader"
                                        #   in Loop: Header=BB44_2 Depth=1
	cmpl	$9, %esi
	movl	$8, %ecx
	cmovll	%esi, %ecx
	movl	%ecx, %esi
	sarl	$31, %esi
	andnl	%ecx, %esi, %ecx
	leaq	(,%rcx,4), %r14
	cmpl	$536870911, %ecx                # imm = 0x1FFFFFFF
	ja	.LBB44_80
# %bb.4:                                # %"assert succeeded.lr.ph"
                                        #   in Loop: Header=BB44_2 Depth=1
	addq	$4, %r14
	movl	%edi, %r13d
	movq	104(%rsp), %r15                 # 8-byte Reload
	movq	%r14, 64(%rsp)                  # 8-byte Spill
	jmp	.LBB44_7
	.p2align	4, 0x90
.LBB44_5:                               # %"produce conv1_stage2_0_d_def__$1.1"
                                        #   in Loop: Header=BB44_7 Depth=2
	movslq	%r13d, %rcx
	movq	120(%rsp), %rbp                 # 8-byte Reload
	leaq	(,%rcx,4), %r9
	addq	%rbp, %r9
	movq	216(%rsp), %r8                  # 8-byte Reload
	leaq	(%r9,%r8,4), %rdi
	leaq	(%rdi,%r8,4), %r10
	leaq	(%r10,%r8,4), %rdx
	leaq	(%rdx,%r8,4), %rsi
	leaq	(%rsi,%r8,4), %rbx
	movq	%r11, %r14
	leaq	(%rbx,%r8,4), %r11
	vmovss	(%rdx,%r8,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rbx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r11,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	%r14, %r11
	movq	64(%rsp), %r14                  # 8-byte Reload
	vmovss	(%rbp,%rcx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmovss	4(%rbp,%rcx,4), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdi,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r10,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	leaq	4(%rdi,%r8,4), %rcx
	leaq	(%rcx,%r8,4), %rdx
	leaq	(%rdx,%r8,4), %rsi
	leaq	(%rsi,%r8,4), %rbx
	leaq	(%rbx,%r8,4), %r10
	vmovss	(%rdx,%r8,4), %xmm4             # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%rbx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r10,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$16, 4(%r9,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 4(%rdi,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, (%rcx,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vinsertf128	$1, %xmm4, %ymm3, %ymm2
	vmovaps	%ymm2, (%rax)
.LBB44_6:                               # %"consume conv1_stage2_0_d_def__$1.1"
                                        #   in Loop: Header=BB44_7 Depth=2
	vaddps	%ymm1, %ymm0, %ymm0
	vaddps	%ymm2, %ymm0, %ymm0
	movq	24(%rsp), %rcx                  # 8-byte Reload
	movq	72(%rsp), %rdx                  # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rdx,4)
	addl	$2, %r13d
	decq	%r15
	je	.LBB44_13
.LBB44_7:                               # %"assert succeeded"
                                        #   Parent Loop BB44_2 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	%r14, %r11
	jb	.LBB44_10
.LBB44_8:                               # %pseudostack_alloc.exit
                                        #   in Loop: Header=BB44_7 Depth=2
	movq	24(%rsp), %rcx                  # 8-byte Reload
	movq	72(%rsp), %rdx                  # 8-byte Reload
	vmovups	(%rcx,%rdx,4), %ymm0
	cmpl	%r12d, 4(%rsp)                  # 4-byte Folded Reload
	jg	.LBB44_5
# %bb.9:                                # %"consume conv1_stage2_0_d_def__$1"
                                        #   in Loop: Header=BB44_7 Depth=2
	vmovaps	(%rax), %ymm1
	vmovaps	%ymm1, %ymm2
	jmp	.LBB44_6
.LBB44_10:                              #   in Loop: Header=BB44_7 Depth=2
	testq	%rax, %rax
	movq	8(%rsp), %rbx                   # 8-byte Reload
	je	.LBB44_12
# %bb.11:                               #   in Loop: Header=BB44_7 Depth=2
	movq	%rbx, %rdi
	movq	%rax, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB44_12:                              #   in Loop: Header=BB44_7 Depth=2
	movq	%rbx, %rdi
	movq	%r14, %rsi
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, 80(%rsp)
	movq	%r14, 88(%rsp)
	movq	%r14, %rcx
	movq	%r14, 32(%rsp)                  # 8-byte Spill
	movq	%r14, %r11
	jmp	.LBB44_8
	.p2align	4, 0x90
.LBB44_13:                              #   in Loop: Header=BB44_2 Depth=1
	leaq	80(%rsp), %r13
.LBB44_14:                              # %"end for conv1_stage1_1_d_def__.s1.r997$x.r997$x"
                                        #   in Loop: Header=BB44_2 Depth=1
	cmpl	$0, 192(%rsp)                   # 4-byte Folded Reload
	movl	56(%rsp), %edx                  # 4-byte Reload
	jle	.LBB44_30
# %bb.15:                               # %"for conv1_stage1_1_d_def__.s1.r997$x.r997$x1.preheader"
                                        #   in Loop: Header=BB44_2 Depth=1
	cmpl	$9, %edx
	movl	$8, %ecx
	cmovgel	%ecx, %edx
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ecx
	leaq	(,%rcx,4), %rbp
	cmpl	$536870911, %ecx                # imm = 0x1FFFFFFF
	ja	.LBB44_81
# %bb.16:                               # %"assert succeeded5.lr.ph"
                                        #   in Loop: Header=BB44_2 Depth=1
	addq	$4, %rbp
	movl	128(%rsp), %ebx                 # 4-byte Reload
	movq	104(%rsp), %r14                 # 8-byte Reload
	movq	%rbp, 56(%rsp)                  # 8-byte Spill
	.p2align	4, 0x90
.LBB44_17:                              # %"assert succeeded5"
                                        #   Parent Loop BB44_2 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB44_22 Depth 3
	cmpq	%rbp, 32(%rsp)                  # 8-byte Folded Reload
	jb	.LBB44_26
# %bb.18:                               # %pseudostack_alloc.exit49
                                        #   in Loop: Header=BB44_17 Depth=2
	leal	(%r14,%r14), %ecx
	movl	116(%rsp), %edi                 # 4-byte Reload
	subl	%ecx, %edi
	jle	.LBB44_25
.LBB44_19:                              # %"for conv1_stage1_1_d_def__.s1.r997$x.r1029$xi7.preheader"
                                        #   in Loop: Header=BB44_17 Depth=2
	movl	%ebx, 64(%rsp)                  # 4-byte Spill
	movslq	%ebx, %rsi
	movq	168(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %r9
	movq	248(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %r10
	movq	144(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %r11
	movq	224(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %r13
	movq	152(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %r15
	movq	136(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %rbx
	movq	232(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %rdx
	movq	120(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %rsi
	xorl	%r8d, %r8d
	cmpl	$1, %edi
	sete	%r8b
	movl	$2, %edi
	subq	%r8, %rdi
	movq	24(%rsp), %rcx                  # 8-byte Reload
	movq	72(%rsp), %rbp                  # 8-byte Reload
	vmovups	(%rcx,%rbp,4), %ymm0
	xorl	%r8d, %r8d
	jmp	.LBB44_22
	.p2align	4, 0x90
.LBB44_20:                              # %"produce conv1_stage2_0_d_def__$113"
                                        #   in Loop: Header=BB44_22 Depth=3
	vmovss	(%r13,%r8,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r11,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r10,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%r9,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovss	(%rsi,%r8,4), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rbx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovaps	%ymm1, (%rax)
	vaddps	%ymm1, %ymm0, %ymm0
	incq	%r8
	cmpq	%r8, %rdi
	je	.LBB44_24
.LBB44_22:                              # %"for conv1_stage1_1_d_def__.s1.r997$x.r1029$xi7"
                                        #   Parent Loop BB44_2 Depth=1
                                        #     Parent Loop BB44_17 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	cmpl	%r12d, 4(%rsp)                  # 4-byte Folded Reload
	jg	.LBB44_20
# %bb.23:                               # %"for conv1_stage1_1_d_def__.s1.r997$x.r1029$xi7.consume conv1_stage2_0_d_def__$114_crit_edge"
                                        #   in Loop: Header=BB44_22 Depth=3
	vmovaps	(%rax), %ymm1
	vaddps	%ymm1, %ymm0, %ymm0
	incq	%r8
	cmpq	%r8, %rdi
	jne	.LBB44_22
.LBB44_24:                              # %"end for conv1_stage1_1_d_def__.s1.r997$x.r1029$xi8.loopexit"
                                        #   in Loop: Header=BB44_17 Depth=2
	movq	24(%rsp), %rcx                  # 8-byte Reload
	movq	72(%rsp), %rdx                  # 8-byte Reload
	vmovups	%ymm0, (%rcx,%rdx,4)
	movq	56(%rsp), %rbp                  # 8-byte Reload
	movl	64(%rsp), %ebx                  # 4-byte Reload
.LBB44_25:                              # %"end for conv1_stage1_1_d_def__.s1.r997$x.r1029$xi8"
                                        #   in Loop: Header=BB44_17 Depth=2
	incq	%r14
	addl	$2, %ebx
	cmpl	%r14d, 160(%rsp)                # 4-byte Folded Reload
	jne	.LBB44_17
	jmp	.LBB44_29
.LBB44_26:                              #   in Loop: Header=BB44_17 Depth=2
	testq	%rax, %rax
	movq	8(%rsp), %r15                   # 8-byte Reload
	je	.LBB44_28
# %bb.27:                               #   in Loop: Header=BB44_17 Depth=2
	movq	%r15, %rdi
	movq	%rax, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB44_28:                              #   in Loop: Header=BB44_17 Depth=2
	movq	%r15, %rdi
	movq	%rbp, %rsi
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, 80(%rsp)
	movq	%rbp, 88(%rsp)
	movq	%rbp, %rcx
	movq	%rbp, 32(%rsp)                  # 8-byte Spill
	leal	(%r14,%r14), %ecx
	movl	116(%rsp), %edi                 # 4-byte Reload
	subl	%ecx, %edi
	jg	.LBB44_19
	jmp	.LBB44_25
	.p2align	4, 0x90
.LBB44_29:                              #   in Loop: Header=BB44_2 Depth=1
	movq	32(%rsp), %r11                  # 8-byte Reload
	leaq	80(%rsp), %r13
.LBB44_30:                              # %"end for conv1_stage1_1_d_def__.s1.r997$x.r997$x2"
                                        #   in Loop: Header=BB44_2 Depth=1
	movq	200(%rsp), %rdx                 # 8-byte Reload
	incq	%rdx
	movl	20(%rsp), %ecx                  # 4-byte Reload
	movq	208(%rsp), %rdi                 # 8-byte Reload
	addl	%ecx, %edi
	addl	%ecx, 128(%rsp)                 # 4-byte Folded Spill
	movq	184(%rsp), %r9                  # 8-byte Reload
	cmpq	%r9, %rdx
	movq	240(%rsp), %r10                 # 8-byte Reload
	jne	.LBB44_2
# %bb.31:                               # %"end for conv1_stage1_1_d_def__.s1.w.w"
	movl	40(%rsp), %eax                  # 4-byte Reload
	testl	%eax, %eax
	jle	.LBB44_85
.LBB44_32:                              # %"for conv1_stage1_1_d_def__.s1.w.w15.preheader"
	addl	%r9d, %eax
	movl	%eax, 40(%rsp)                  # 4-byte Spill
	leal	(,%r9,8), %esi
	movl	4(%rsp), %ebp                   # 4-byte Reload
	movl	%ebp, %ecx
	subl	%esi, %ecx
	movl	44(%rsp), %eax                  # 4-byte Reload
	imull	%r10d, %eax
	movl	%eax, 44(%rsp)                  # 4-byte Spill
	movq	%r10, %rax
	shlq	$5, %rax
	movq	%rax, 136(%rsp)                 # 8-byte Spill
	movq	%r10, %r14
	shlq	$9, %r14
	movl	%esi, %edx
	subl	%ebp, %edx
	shlq	$7, %r10
	movq	24(%rsp), %rax                  # 8-byte Reload
	addq	$224, %rax
	movq	%rax, 296(%rsp)                 # 8-byte Spill
	movq	176(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%r9,8), %r11d
	movl	%ecx, 260(%rsp)                 # 4-byte Spill
	movl	%ecx, %edi
	xorl	%ebx, %ebx
	movq	%r10, 240(%rsp)                 # 8-byte Spill
	.p2align	4, 0x90
.LBB44_33:                              # %"for conv1_stage1_1_d_def__.s1.w.w15"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB44_37 Depth 2
                                        #       Child Loop BB44_41 Depth 3
                                        #         Child Loop BB44_53 Depth 4
                                        #         Child Loop BB44_47 Depth 4
                                        #         Child Loop BB44_63 Depth 4
                                        #         Child Loop BB44_67 Depth 4
	cmpl	$9, %edi
	movl	$8, %r15d
	cmovll	%edi, %r15d
	cmpl	$-9, %edx
	movl	$-8, %ebp
	cmovgl	%edx, %ebp
	leal	(,%rbx,8), %eax
	movl	260(%rsp), %ecx                 # 4-byte Reload
                                        # kill: def $ecx killed $ecx def $rcx
	subl	%eax, %ecx
	cmpl	$9, %ecx
	movl	$8, %eax
	cmovgel	%eax, %ecx
	movq	%rcx, 144(%rsp)                 # 8-byte Spill
	cmpl	$0, 280(%rsp)                   # 4-byte Folded Reload
	movq	%r9, 184(%rsp)                  # 8-byte Spill
	movl	%esi, 276(%rsp)                 # 4-byte Spill
	movl	%edx, 272(%rsp)                 # 4-byte Spill
	movl	%r11d, 268(%rsp)                # 4-byte Spill
	movl	%edi, 264(%rsp)                 # 4-byte Spill
	movq	%rbx, 304(%rsp)                 # 8-byte Spill
	jle	.LBB44_73
# %bb.34:                               # %"for conv1_stage1_1_d_def__.s1.r997$x.r997$x18.preheader"
                                        #   in Loop: Header=BB44_33 Depth=1
	leal	(,%r9,8), %eax
	movl	4(%rsp), %ecx                   # 4-byte Reload
	subl	%eax, %ecx
	cmpl	$9, %ecx
	movl	$8, %eax
	cmovll	%ecx, %eax
	movl	%ecx, 232(%rsp)                 # 4-byte Spill
	testl	%ecx, %ecx
	movl	$0, %ecx
	cmovlel	%ecx, %eax
	leaq	(,%rax,4), %rcx
	movq	%rcx, 48(%rsp)                  # 8-byte Spill
	cmpl	$536870911, %eax                # imm = 0x1FFFFFFF
	ja	.LBB44_82
# %bb.35:                               # %"assert succeeded22.lr.ph"
                                        #   in Loop: Header=BB44_33 Depth=1
	movq	%r9, %r8
	movl	%r15d, %eax
	andl	$-32, %eax
	addq	$-32, %rax
	shrq	$5, %rax
	incq	%rax
	andq	$-2, %rax
	negq	%rax
	movq	%rax, 200(%rsp)                 # 8-byte Spill
	movslq	%esi, %rdi
	movl	%r15d, %r12d
	andl	$3, %r12d
	leaq	3(%rdi), %rax
	movq	136(%rsp), %rsi                 # 8-byte Reload
	imulq	%rsi, %rax
	leaq	2(%rdi), %rcx
	imulq	%rsi, %rcx
	leaq	1(%rdi), %rdx
	imulq	%rsi, %rdx
	movq	%rdi, 216(%rsp)                 # 8-byte Spill
	imulq	%rdi, %rsi
	movl	%r15d, %edi
	andl	$-4, %edi
	movq	%rdi, 72(%rsp)                  # 8-byte Spill
	movq	144(%rsp), %rbx                 # 8-byte Reload
	movl	%ebx, %edi
	andl	$-32, %edi
	addq	$-32, %rdi
	movq	%rdi, 104(%rsp)                 # 8-byte Spill
	shrq	$5, %rdi
	incq	%rdi
	movq	%rdi, 128(%rsp)                 # 8-byte Spill
	leaq	-1(%rbx), %rdi
	movq	%rdi, 224(%rsp)                 # 8-byte Spill
	movslq	%r11d, %rdi
	movq	%rbx, %r9
	movq	24(%rsp), %rbx                  # 8-byte Reload
	leaq	(%rbx,%rdi,4), %r13
	movq	296(%rsp), %rbx                 # 8-byte Reload
	leaq	(%rbx,%rdi,4), %rbx
	movq	120(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%rax,4), %rax
	movq	%rax, 336(%rsp)                 # 8-byte Spill
	leaq	(%rdi,%rcx,4), %rax
	movq	%rax, 328(%rsp)                 # 8-byte Spill
	leaq	(%rdi,%rdx,4), %rax
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	leaq	(%rdi,%rsi,4), %rax
	movq	%rax, 312(%rsp)                 # 8-byte Spill
	movq	176(%rsp), %rax                 # 8-byte Reload
	leal	(%rax,%r8,8), %eax
	movq	48(%rsp), %rsi                  # 8-byte Reload
	addq	$4, %rsi
	cltq
	movq	%rax, 192(%rsp)                 # 8-byte Spill
                                        # kill: def $r9d killed $r9d killed $r9 def $r9
	andl	$-32, %r9d
	movq	%r9, 208(%rsp)                  # 8-byte Spill
	movl	44(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 20(%rsp)                  # 4-byte Spill
	xorl	%ecx, %ecx
	movq	%rsi, 48(%rsp)                  # 8-byte Spill
	jmp	.LBB44_37
	.p2align	4, 0x90
.LBB44_36:                              # %"end for conv1_stage1_1_d_def__.s1.r997$x.r1029$xi25"
                                        #   in Loop: Header=BB44_37 Depth=2
	movq	288(%rsp), %rcx                 # 8-byte Reload
	incq	%rcx
	addl	$2, 20(%rsp)                    # 4-byte Folded Spill
	cmpq	280(%rsp), %rcx                 # 8-byte Folded Reload
	movq	48(%rsp), %rsi                  # 8-byte Reload
	je	.LBB44_72
.LBB44_37:                              # %"assert succeeded22"
                                        #   Parent Loop BB44_33 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB44_41 Depth 3
                                        #         Child Loop BB44_53 Depth 4
                                        #         Child Loop BB44_47 Depth 4
                                        #         Child Loop BB44_63 Depth 4
                                        #         Child Loop BB44_67 Depth 4
	movq	80(%rsp), %rax
	cmpq	%rsi, 88(%rsp)
	movq	%rcx, 288(%rsp)                 # 8-byte Spill
	jb	.LBB44_69
# %bb.38:                               # %pseudostack_alloc.exit50
                                        #   in Loop: Header=BB44_37 Depth=2
	addl	%ecx, %ecx
	movl	116(%rsp), %edx                 # 4-byte Reload
	subl	%ecx, %edx
	jle	.LBB44_36
.LBB44_39:                              # %"for conv1_stage1_1_d_def__.s1.r997$x.r1029$xi24.preheader"
                                        #   in Loop: Header=BB44_37 Depth=2
	movslq	20(%rsp), %r9                   # 4-byte Folded Reload
	movq	120(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r9,4), %rcx
	movq	%rcx, 152(%rsp)                 # 8-byte Spill
	xorl	%esi, %esi
	cmpl	$1, %edx
	sete	%sil
	movl	$2, %ecx
	subq	%rsi, %rcx
	movq	%rcx, 248(%rsp)                 # 8-byte Spill
	movq	312(%rsp), %rcx                 # 8-byte Reload
	movq	320(%rsp), %r11                 # 8-byte Reload
	movq	328(%rsp), %rsi                 # 8-byte Reload
	movq	336(%rsp), %rdi                 # 8-byte Reload
	xorl	%r8d, %r8d
	jmp	.LBB44_41
	.p2align	4, 0x90
.LBB44_40:                              # %"end for conv1_stage1_1_d_def__.s1.w.wi"
                                        #   in Loop: Header=BB44_41 Depth=3
	incq	%r8
	addq	$4, %rdi
	addq	$4, %rsi
	addq	$4, %r11
	addq	$4, %rcx
	addq	$4, 152(%rsp)                   # 8-byte Folded Spill
	cmpq	248(%rsp), %r8                  # 8-byte Folded Reload
	je	.LBB44_36
.LBB44_41:                              # %"for conv1_stage1_1_d_def__.s1.r997$x.r1029$xi24"
                                        #   Parent Loop BB44_33 Depth=1
                                        #     Parent Loop BB44_37 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB44_53 Depth 4
                                        #         Child Loop BB44_47 Depth 4
                                        #         Child Loop BB44_63 Depth 4
                                        #         Child Loop BB44_67 Depth 4
	cmpl	$0, 232(%rsp)                   # 4-byte Folded Reload
	jle	.LBB44_40
# %bb.42:                               # %"for conv1_stage2_0_d_def__$1.s0.w.wi.preheader"
                                        #   in Loop: Header=BB44_41 Depth=3
	movq	%r8, 160(%rsp)                  # 8-byte Spill
	cmpq	$3, 224(%rsp)                   # 8-byte Folded Reload
	movq	%rcx, 64(%rsp)                  # 8-byte Spill
	movq	%r11, 32(%rsp)                  # 8-byte Spill
	movq	%rsi, 56(%rsp)                  # 8-byte Spill
	movq	%rdi, 168(%rsp)                 # 8-byte Spill
	jae	.LBB44_51
# %bb.43:                               #   in Loop: Header=BB44_41 Depth=3
	xorl	%r8d, %r8d
.LBB44_44:                              # %"for conv1_stage1_1_d_def__.s1.w.wi.preheader.unr-lcssa"
                                        #   in Loop: Header=BB44_41 Depth=3
	movq	144(%rsp), %r11                 # 8-byte Reload
	testb	$3, %r11b
	movq	240(%rsp), %r10                 # 8-byte Reload
	je	.LBB44_49
# %bb.45:                               # %"for conv1_stage2_0_d_def__$1.s0.w.wi.epil.preheader"
                                        #   in Loop: Header=BB44_41 Depth=3
	leaq	(%rax,%r8,4), %rdx
	movq	216(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%r8), %rsi
	imulq	136(%rsp), %rsi                 # 8-byte Folded Reload
	movq	152(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rsi,4), %rdi
	addl	%ebp, %r8d
	xorl	%esi, %esi
	jmp	.LBB44_47
	.p2align	4, 0x90
.LBB44_46:                              # %after_bb30.epil
                                        #   in Loop: Header=BB44_47 Depth=4
	incq	%rsi
	addq	%r10, %rdi
	cmpq	%rsi, %r12
	je	.LBB44_49
.LBB44_47:                              # %"for conv1_stage2_0_d_def__$1.s0.w.wi.epil"
                                        #   Parent Loop BB44_33 Depth=1
                                        #     Parent Loop BB44_37 Depth=2
                                        #       Parent Loop BB44_41 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%r8d, %ecx
	addl	%esi, %ecx
	jns	.LBB44_46
# %bb.48:                               # %true_bb28.epil
                                        #   in Loop: Header=BB44_47 Depth=4
	movl	(%rdi), %ecx
	movl	%ecx, (%rdx,%rsi,4)
	jmp	.LBB44_46
	.p2align	4, 0x90
.LBB44_49:                              # %"for conv1_stage1_1_d_def__.s1.w.wi.preheader"
                                        #   in Loop: Header=BB44_41 Depth=3
	cmpl	$32, %r11d
	jae	.LBB44_61
# %bb.50:                               #   in Loop: Header=BB44_41 Depth=3
	xorl	%edx, %edx
	movq	64(%rsp), %rcx                  # 8-byte Reload
	movq	32(%rsp), %r11                  # 8-byte Reload
	movq	56(%rsp), %rsi                  # 8-byte Reload
	movq	168(%rsp), %rdi                 # 8-byte Reload
	movq	160(%rsp), %r8                  # 8-byte Reload
	jmp	.LBB44_67
	.p2align	4, 0x90
.LBB44_51:                              # %"for conv1_stage2_0_d_def__$1.s0.w.wi.preheader1"
                                        #   in Loop: Header=BB44_41 Depth=3
	movq	%rcx, %r10
	movq	%rsi, %rdx
	xorl	%r8d, %r8d
	jmp	.LBB44_53
	.p2align	4, 0x90
.LBB44_59:                              # %true_bb28.2
                                        #   in Loop: Header=BB44_53 Depth=4
	movl	(%rdx,%r9,4), %ecx
	movl	%ecx, 8(%rax,%r8,4)
	leal	(%r8,%rbp), %ecx
	addl	$3, %ecx
	js	.LBB44_60
.LBB44_52:                              # %after_bb30.3
                                        #   in Loop: Header=BB44_53 Depth=4
	addq	$4, %r8
	addq	%r14, %rdi
	addq	%r14, %rdx
	addq	%r14, %r11
	addq	%r14, %r10
	cmpq	%r8, 72(%rsp)                   # 8-byte Folded Reload
	je	.LBB44_44
.LBB44_53:                              # %"for conv1_stage2_0_d_def__$1.s0.w.wi"
                                        #   Parent Loop BB44_33 Depth=1
                                        #     Parent Loop BB44_37 Depth=2
                                        #       Parent Loop BB44_41 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	movl	%ebp, %esi
	addl	%r8d, %esi
	js	.LBB44_57
# %bb.54:                               # %after_bb30
                                        #   in Loop: Header=BB44_53 Depth=4
	leal	(%r8,%rbp), %esi
	incl	%esi
	js	.LBB44_58
.LBB44_55:                              # %after_bb30.1
                                        #   in Loop: Header=BB44_53 Depth=4
	leal	(%r8,%rbp), %ecx
	addl	$2, %ecx
	jns	.LBB44_56
	jmp	.LBB44_59
	.p2align	4, 0x90
.LBB44_57:                              # %true_bb28
                                        #   in Loop: Header=BB44_53 Depth=4
	movl	(%r10,%r9,4), %esi
	movl	%esi, (%rax,%r8,4)
	leal	(%r8,%rbp), %esi
	incl	%esi
	jns	.LBB44_55
.LBB44_58:                              # %true_bb28.1
                                        #   in Loop: Header=BB44_53 Depth=4
	movl	(%r11,%r9,4), %ecx
	movl	%ecx, 4(%rax,%r8,4)
	leal	(%r8,%rbp), %ecx
	addl	$2, %ecx
	js	.LBB44_59
.LBB44_56:                              # %after_bb30.2
                                        #   in Loop: Header=BB44_53 Depth=4
	leal	(%r8,%rbp), %ecx
	addl	$3, %ecx
	jns	.LBB44_52
.LBB44_60:                              # %true_bb28.3
                                        #   in Loop: Header=BB44_53 Depth=4
	movl	(%rdi,%r9,4), %ecx
	movl	%ecx, 12(%rax,%r8,4)
	jmp	.LBB44_52
	.p2align	4, 0x90
.LBB44_61:                              # %vector.ph
                                        #   in Loop: Header=BB44_41 Depth=3
	cmpq	$0, 104(%rsp)                   # 8-byte Folded Reload
	movq	160(%rsp), %r8                  # 8-byte Reload
	je	.LBB44_68
# %bb.62:                               # %vector.body.preheader
                                        #   in Loop: Header=BB44_41 Depth=3
	movq	200(%rsp), %rdi                 # 8-byte Reload
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB44_63:                              # %vector.body
                                        #   Parent Loop BB44_33 Depth=1
                                        #     Parent Loop BB44_37 Depth=2
                                        #       Parent Loop BB44_41 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovups	-224(%rbx,%rdx,4), %ymm0
	vmovups	-192(%rbx,%rdx,4), %ymm1
	vmovups	-160(%rbx,%rdx,4), %ymm2
	vmovups	-128(%rbx,%rdx,4), %ymm3
	vaddps	(%rax,%rdx,4), %ymm0, %ymm0
	vaddps	32(%rax,%rdx,4), %ymm1, %ymm1
	vaddps	64(%rax,%rdx,4), %ymm2, %ymm2
	vaddps	96(%rax,%rdx,4), %ymm3, %ymm3
	vmovups	%ymm0, -224(%rbx,%rdx,4)
	vmovups	%ymm1, -192(%rbx,%rdx,4)
	vmovups	%ymm2, -160(%rbx,%rdx,4)
	vmovups	%ymm3, -128(%rbx,%rdx,4)
	vmovups	-96(%rbx,%rdx,4), %ymm0
	vmovups	-64(%rbx,%rdx,4), %ymm1
	vmovups	-32(%rbx,%rdx,4), %ymm2
	vmovups	(%rbx,%rdx,4), %ymm3
	vaddps	128(%rax,%rdx,4), %ymm0, %ymm0
	vaddps	160(%rax,%rdx,4), %ymm1, %ymm1
	vaddps	192(%rax,%rdx,4), %ymm2, %ymm2
	vaddps	224(%rax,%rdx,4), %ymm3, %ymm3
	vmovups	%ymm0, -96(%rbx,%rdx,4)
	vmovups	%ymm1, -64(%rbx,%rdx,4)
	vmovups	%ymm2, -32(%rbx,%rdx,4)
	vmovups	%ymm3, (%rbx,%rdx,4)
	addq	$64, %rdx
	addq	$2, %rdi
	jne	.LBB44_63
# %bb.64:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB44_41 Depth=3
	testb	$1, 128(%rsp)                   # 1-byte Folded Reload
	je	.LBB44_66
.LBB44_65:                              # %vector.body.epil
                                        #   in Loop: Header=BB44_41 Depth=3
	movq	192(%rsp), %rcx                 # 8-byte Reload
	addq	%rdx, %rcx
	movq	24(%rsp), %rsi                  # 8-byte Reload
	vmovups	(%rsi,%rcx,4), %ymm0
	vmovups	32(%rsi,%rcx,4), %ymm1
	vmovups	64(%rsi,%rcx,4), %ymm2
	vmovups	96(%rsi,%rcx,4), %ymm3
	vaddps	(%rax,%rdx,4), %ymm0, %ymm0
	vaddps	32(%rax,%rdx,4), %ymm1, %ymm1
	vaddps	64(%rax,%rdx,4), %ymm2, %ymm2
	vaddps	96(%rax,%rdx,4), %ymm3, %ymm3
	vmovups	%ymm0, (%rsi,%rcx,4)
	vmovups	%ymm1, 32(%rsi,%rcx,4)
	vmovups	%ymm2, 64(%rsi,%rcx,4)
	vmovups	%ymm3, 96(%rsi,%rcx,4)
.LBB44_66:                              # %middle.block
                                        #   in Loop: Header=BB44_41 Depth=3
	movq	208(%rsp), %rcx                 # 8-byte Reload
	movq	%rcx, %rdx
	cmpq	%r11, %rcx
	movq	64(%rsp), %rcx                  # 8-byte Reload
	movq	32(%rsp), %r11                  # 8-byte Reload
	movq	56(%rsp), %rsi                  # 8-byte Reload
	movq	168(%rsp), %rdi                 # 8-byte Reload
	je	.LBB44_40
	.p2align	4, 0x90
.LBB44_67:                              # %"for conv1_stage1_1_d_def__.s1.w.wi"
                                        #   Parent Loop BB44_33 Depth=1
                                        #     Parent Loop BB44_37 Depth=2
                                        #       Parent Loop BB44_41 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovss	(%r13,%rdx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vaddss	(%rax,%rdx,4), %xmm0, %xmm0
	vmovss	%xmm0, (%r13,%rdx,4)
	incq	%rdx
	cmpq	%rdx, %r15
	jne	.LBB44_67
	jmp	.LBB44_40
.LBB44_68:                              #   in Loop: Header=BB44_41 Depth=3
	xorl	%edx, %edx
	testb	$1, 128(%rsp)                   # 1-byte Folded Reload
	jne	.LBB44_65
	jmp	.LBB44_66
.LBB44_69:                              #   in Loop: Header=BB44_37 Depth=2
	testq	%rax, %rax
	movq	8(%rsp), %rdi                   # 8-byte Reload
	je	.LBB44_71
# %bb.70:                               #   in Loop: Header=BB44_37 Depth=2
	movq	8(%rsp), %rdi                   # 8-byte Reload
	movq	%rax, %rsi
	vzeroupper
	callq	halide_free@PLT
	movq	8(%rsp), %rdi                   # 8-byte Reload
	movq	48(%rsp), %rsi                  # 8-byte Reload
.LBB44_71:                              #   in Loop: Header=BB44_37 Depth=2
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, 80(%rsp)
	movq	48(%rsp), %rcx                  # 8-byte Reload
	movq	%rcx, 88(%rsp)
	movq	288(%rsp), %rcx                 # 8-byte Reload
	addl	%ecx, %ecx
	movl	116(%rsp), %edx                 # 4-byte Reload
	subl	%ecx, %edx
	jg	.LBB44_39
	jmp	.LBB44_36
	.p2align	4, 0x90
.LBB44_72:                              #   in Loop: Header=BB44_33 Depth=1
	leaq	80(%rsp), %r13
.LBB44_73:                              # %"end for conv1_stage1_1_d_def__.s1.r997$x.r997$x19"
                                        #   in Loop: Header=BB44_33 Depth=1
	movq	184(%rsp), %r9                  # 8-byte Reload
	incq	%r9
	movq	304(%rsp), %rbx                 # 8-byte Reload
	incl	%ebx
	movl	276(%rsp), %esi                 # 4-byte Reload
	addl	$8, %esi
	movl	272(%rsp), %edx                 # 4-byte Reload
	addl	$8, %edx
	movl	264(%rsp), %edi                 # 4-byte Reload
	addl	$-8, %edi
	movl	268(%rsp), %r11d                # 4-byte Reload
	addl	$8, %r11d
	cmpl	%r9d, 40(%rsp)                  # 4-byte Folded Reload
	movq	8(%rsp), %rbp                   # 8-byte Reload
	jne	.LBB44_33
# %bb.74:
	xorl	%ebx, %ebx
	testq	%r13, %r13
	je	.LBB44_79
.LBB44_76:
	movq	$0, 8(%r13)
	movq	(%r13), %rsi
	testq	%rsi, %rsi
	je	.LBB44_78
# %bb.77:
	movq	%rbp, %rdi
	vzeroupper
	callq	halide_free@PLT
.LBB44_78:                              # %pseudostack_free.exit
	movq	$0, (%r13)
.LBB44_79:                              # %call_destructor.exit
	movl	%ebx, %eax
	addq	$344, %rsp                      # imm = 0x158
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.LBB44_80:                              # %"assert failed"
	leaq	.Lstr.135(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	movq	8(%rsp), %rbp                   # 8-byte Reload
	movq	%rbp, %rdi
	movq	%r14, %rdx
	jmp	.LBB44_83
.LBB44_81:                              # %"assert failed4"
	leaq	.Lstr.135(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	movq	%rbp, %rdx
	movq	8(%rsp), %rbp                   # 8-byte Reload
	movq	%rbp, %rdi
	jmp	.LBB44_83
.LBB44_82:                              # %"assert failed21"
	leaq	.Lstr.135(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	movq	8(%rsp), %rbp                   # 8-byte Reload
	movq	%rbp, %rdi
	movq	48(%rsp), %rdx                  # 8-byte Reload
.LBB44_83:                              # %destructor_block
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	movl	%eax, %ebx
	testq	%r13, %r13
	jne	.LBB44_76
	jmp	.LBB44_79
.LBB44_84:
	xorl	%r13d, %r13d
	movl	40(%rsp), %eax                  # 4-byte Reload
	testl	%eax, %eax
	jg	.LBB44_32
.LBB44_85:
	movq	8(%rsp), %rbp                   # 8-byte Reload
	xorl	%ebx, %ebx
	testq	%r13, %r13
	jne	.LBB44_76
	jmp	.LBB44_79
.Lfunc_end44:
	.size	train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c, .Lfunc_end44-train_cost_model.par_for.conv1_stage1_1_d_def__.s1.c
                                        # -- End function
	.section	.text.train_cost_model.par_for.head1_conv_1_d_def__.s0.w,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.head1_conv_1_d_def__.s0.w
	.type	train_cost_model.par_for.head1_conv_1_d_def__.s0.w,@function
train_cost_model.par_for.head1_conv_1_d_def__.s0.w: # @train_cost_model.par_for.head1_conv_1_d_def__.s0.w
# %bb.0:                                # %entry
	movq	(%rdx), %rax
	movslq	%esi, %rcx
	shlq	$5, %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%ymm0, (%rax,%rcx)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end45:
	.size	train_cost_model.par_for.head1_conv_1_d_def__.s0.w, .Lfunc_end45-train_cost_model.par_for.head1_conv_1_d_def__.s0.w
                                        # -- End function
	.section	.text.train_cost_model.par_for.head1_conv_1_d_def__.s1.w,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.head1_conv_1_d_def__.s1.w
	.type	train_cost_model.par_for.head1_conv_1_d_def__.s1.w,@function
train_cost_model.par_for.head1_conv_1_d_def__.s1.w: # @train_cost_model.par_for.head1_conv_1_d_def__.s1.w
# %bb.0:                                # %entry
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	movslq	(%rdx), %r14
	movslq	4(%rdx), %r10
	movq	24(%rdx), %r11
	movq	40(%rdx), %r8
	movslq	%esi, %rsi
	leaq	(,%rsi,8), %r9
	movq	%rsi, %rcx
	shlq	$5, %rcx
	shlq	$2, %rsi
	addq	8(%rdx), %rsi
	vmovaps	(%r8,%rcx), %ymm0
	shlq	$2, %r10
	leaq	(,%r14,4), %r15
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB46_1:                               # %"for head1_conv_1_d_def__.s1.r1152$x"
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r11,%rcx,4), %rdi
	leaq	(%rdi,%r15), %rbx
	addq	%r15, %rbx
	leaq	(%rbx,%r15), %rax
	addq	%r15, %rax
	leaq	(%rax,%r15), %rdx
	vmovss	(%rbx,%r14,8), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rax,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdx,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	addq	%r15, %rdx
	vinsertps	$48, (%rdx,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovss	(%r11,%rcx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%rdi,%r14,8), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rbx,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vbroadcastss	(%rsi), %ymm2
	vfmadd231ps	%ymm2, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm2) + ymm0
	incq	%rcx
	addq	%r10, %rsi
	cmpq	$32, %rcx
	jne	.LBB46_1
# %bb.2:                                # %destructor_block
	vmovaps	%ymm0, (%r8,%r9,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	popq	%r15
	vzeroupper
	retq
.Lfunc_end46:
	.size	train_cost_model.par_for.head1_conv_1_d_def__.s1.w, .Lfunc_end46-train_cost_model.par_for.head1_conv_1_d_def__.s1.w
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_head1_filter.s1.v241.v241.v241
.LCPI47_0:
	.long	0xbfb8aa3b                      # float -1.44269502
.LCPI47_1:
	.long	0x3f317200                      # float 0.693145751
.LCPI47_2:
	.long	0x35bfbe8e                      # float 1.42860677E-6
.LCPI47_3:
	.long	4294967169                      # 0xffffff81
.LCPI47_4:
	.long	128                             # 0x80
.LCPI47_5:
	.long	0xb9a797f3                      # float -3.19659332E-4
.LCPI47_6:
	.long	0xbc0b192a                      # float -0.00848988629
.LCPI47_7:
	.long	0xbe2aae1f                      # float -0.166679844
.LCPI47_8:
	.long	0xbf800000                      # float -1
.LCPI47_9:
	.long	0x3f800000                      # float 1
.LCPI47_10:
	.long	0x3a9c2e66                      # float 0.00119156833
.LCPI47_11:
	.long	0x3d2a66bc                      # float 0.0416018814
.LCPI47_12:
	.long	0x3effffde                      # float 0.499998987
.LCPI47_13:
	.long	0x7f800000                      # float +Inf
	.section	.text.train_cost_model.par_for.updated_head1_filter.s1.v241.v241.v241,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_head1_filter.s1.v241.v241.v241,@function
train_cost_model.par_for.updated_head1_filter.s1.v241.v241.v241: # @train_cost_model.par_for.updated_head1_filter.s1.v241.v241.v241
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$200, %rsp
	movslq	(%rdx), %rax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	movslq	4(%rdx), %rbp
	movslq	16(%rdx), %r10
	movslq	44(%rdx), %rax
	movq	%rax, -96(%rsp)                 # 8-byte Spill
	movq	56(%rdx), %r11
	movq	72(%rdx), %rax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	movq	88(%rdx), %r12
	movq	104(%rdx), %rax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movl	%esi, %ecx
	sarl	$31, %ecx
	movl	%ecx, %eax
	xorl	%esi, %eax
	imulq	$1717986919, %rax, %rax         # imm = 0x66666667
	shrq	$35, %rax
	xorl	%ecx, %eax
	leal	(,%rax,4), %ecx
	leal	(%rcx,%rcx,4), %ecx
	movl	%esi, %edi
	subl	%ecx, %edi
	movslq	%edi, %r13
	addq	%r13, %r13
	movslq	%r13d, %rcx
	movq	%rcx, -72(%rsp)                 # 8-byte Spill
	cmpl	$59, %esi
	jg	.LBB47_13
# %bb.1:                                # %true_bb
	movl	12(%rdx), %r14d
	movslq	20(%rdx), %rcx
	movq	%rcx, -104(%rsp)                # 8-byte Spill
	movl	24(%rdx), %r8d
	movl	32(%rdx), %esi
	movl	48(%rdx), %edi
	movslq	%eax, %rbx
	addq	%rbx, %rbx
	movl	28(%rdx), %eax
	subl	%eax, %r13d
	movl	%ebx, %eax
	movq	%rbp, -8(%rsp)                  # 8-byte Spill
	imull	%ebp, %eax
	movl	%ebx, %ecx
	movl	%edi, -40(%rsp)                 # 4-byte Spill
	imull	%edi, %ecx
	movl	%esi, -44(%rsp)                 # 4-byte Spill
	addl	%esi, %ecx
	movq	%rbx, -16(%rsp)                 # 8-byte Spill
	movl	%ebx, %r9d
	movl	%r14d, -36(%rsp)                # 4-byte Spill
	imull	%r14d, %r9d
	addl	%r13d, %r9d
	cltq
	movq	%rax, -32(%rsp)                 # 8-byte Spill
	movslq	%ecx, %rax
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	vxorps	%xmm15, %xmm15, %xmm15
	vxorps	%xmm3, %xmm3, %xmm3
	testl	%r8d, %r8d
	jle	.LBB47_10
# %bb.2:                                # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.preheader"
	movslq	%r9d, %rdx
	leal	-1(%r8), %ebp
	movl	%r8d, %eax
	andl	$3, %eax
	je	.LBB47_3
# %bb.4:                                # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.prol.preheader"
	movq	-104(%rsp), %rsi                # 8-byte Reload
	movq	%rsi, %rcx
	imulq	%r10, %rcx
	addq	%rdx, %rcx
	leaq	(%r12,%rcx,4), %rcx
	leaq	(,%r10,4), %rdi
	shlq	$5, %rsi
	addq	%r11, %rsi
	negl	%eax
	vxorps	%xmm3, %xmm3, %xmm3
	xorl	%ebx, %ebx
	.p2align	4, 0x90
.LBB47_5:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rcx), %ymm0
	vfmadd231ps	(%rsi), %ymm0, %ymm3    # ymm3 = (ymm0 * mem) + ymm3
	decq	%rbx
	addq	%rdi, %rcx
	addq	$32, %rsi
	cmpl	%ebx, %eax
	jne	.LBB47_5
# %bb.6:                                # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.prol.loopexit.loopexit"
	movq	-104(%rsp), %r14                # 8-byte Reload
	subq	%rbx, %r14
	cmpl	$3, %ebp
	jae	.LBB47_8
	jmp	.LBB47_10
.LBB47_13:                              # %false_bb
	movl	8(%rdx), %r15d
	movslq	40(%rdx), %rax
	movq	%rax, -104(%rsp)                # 8-byte Spill
	movslq	36(%rdx), %r14
	addq	%rbp, %rbp
	leaq	(,%rbp,2), %r9
	addq	%rbp, %r9
	addq	%r13, %r14
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm4, %xmm4, %xmm4
	testl	%r15d, %r15d
	jle	.LBB47_21
# %bb.14:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x5.preheader"
	leaq	-1(%r15), %rax
	movl	%r15d, %ebp
	andl	$3, %ebp
	cmpq	$3, %rax
	jae	.LBB47_16
# %bb.15:
	vxorps	%xmm4, %xmm4, %xmm4
	xorl	%edx, %edx
	jmp	.LBB47_18
.LBB47_16:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x5.preheader.new"
	movl	%r15d, %ebx
	andl	$-4, %ebx
	leaq	(%r10,%r10,2), %rax
	leaq	(%r12,%rax,4), %r13
	leaq	(,%r14,4), %rcx
	movq	%r10, %rax
	shlq	$4, %rax
	leaq	96(%r11), %rdi
	leaq	(%r12,%r10,8), %rsi
	leaq	(%r12,%r10,4), %r8
	vxorps	%xmm4, %xmm4, %xmm4
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB47_17:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x5"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%r12,%rcx), %ymm1
	vfmadd132ps	-96(%rdi), %ymm4, %ymm1 # ymm1 = (ymm1 * mem) + ymm4
	vbroadcastss	(%r8,%rcx), %ymm2
	vfmadd132ps	-64(%rdi), %ymm1, %ymm2 # ymm2 = (ymm2 * mem) + ymm1
	vbroadcastss	(%rsi,%rcx), %ymm1
	vfmadd132ps	-32(%rdi), %ymm2, %ymm1 # ymm1 = (ymm1 * mem) + ymm2
	vbroadcastss	(%r13,%rcx), %ymm4
	vfmadd132ps	(%rdi), %ymm1, %ymm4    # ymm4 = (ymm4 * mem) + ymm1
	addq	$4, %rdx
	addq	%rax, %rcx
	subq	$-128, %rdi
	cmpq	%rdx, %rbx
	jne	.LBB47_17
.LBB47_18:                              # %"consume squashed_head1_filter_0_d_def__8.loopexit.unr-lcssa"
	testq	%rbp, %rbp
	je	.LBB47_21
# %bb.19:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x5.epil.preheader"
	movq	%rdx, %rax
	imulq	%r10, %rax
	addq	%r14, %rax
	leaq	(%r12,%rax,4), %rax
	leaq	(,%r10,4), %rcx
	shlq	$5, %rdx
	addq	%r11, %rdx
	.p2align	4, 0x90
.LBB47_20:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x5.epil"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax), %ymm1
	vfmadd231ps	(%rdx), %ymm1, %ymm4    # ymm4 = (ymm1 * mem) + ymm4
	addq	%rcx, %rax
	addq	$32, %rdx
	decq	%rbp
	jne	.LBB47_20
.LBB47_21:                              # %"consume squashed_head1_filter_0_d_def__8"
	movq	-72(%rsp), %r13                 # 8-byte Reload
	movq	%r13, %rax
	imulq	-80(%rsp), %rax                 # 8-byte Folded Reload
	addq	%r9, %rax
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm12
	vbroadcastss	.LCPI47_0(%rip), %ymm1  # ymm1 = [-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0]
	vmovups	%ymm1, 64(%rsp)                 # 32-byte Spill
	vmulps	%ymm1, %ymm12, %ymm1
	vroundps	$1, %ymm1, %ymm13
	vbroadcastss	.LCPI47_1(%rip), %ymm3  # ymm3 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vfmadd231ps	%ymm3, %ymm13, %ymm12   # ymm12 = (ymm13 * ymm3) + ymm12
	vbroadcastss	.LCPI47_2(%rip), %ymm1  # ymm1 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vfmadd231ps	%ymm13, %ymm1, %ymm12   # ymm12 = (ymm1 * ymm13) + ymm12
	vmulps	%ymm12, %ymm12, %ymm14
	vbroadcastss	.LCPI47_5(%rip), %ymm2  # ymm2 = [-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4]
	vbroadcastss	.LCPI47_6(%rip), %ymm5  # ymm5 = [-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3]
	vmovaps	%ymm2, %ymm15
	vmovups	%ymm5, 32(%rsp)                 # 32-byte Spill
	vfmadd213ps	%ymm5, %ymm14, %ymm15   # ymm15 = (ymm14 * ymm15) + ymm5
	vbroadcastss	.LCPI47_7(%rip), %ymm5  # ymm5 = [-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1]
	vmovups	%ymm5, (%rsp)                   # 32-byte Spill
	vfmadd213ps	%ymm5, %ymm14, %ymm15   # ymm15 = (ymm14 * ymm15) + ymm5
	vbroadcastss	.LCPI47_8(%rip), %ymm9  # ymm9 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vfmadd213ps	%ymm9, %ymm14, %ymm15   # ymm15 = (ymm14 * ymm15) + ymm9
	vbroadcastss	.LCPI47_10(%rip), %ymm7 # ymm7 = [1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3]
	vbroadcastss	.LCPI47_11(%rip), %ymm10 # ymm10 = [4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2]
	vmovaps	%ymm7, %ymm5
	vfmadd213ps	%ymm10, %ymm14, %ymm5   # ymm5 = (ymm14 * ymm5) + ymm10
	vbroadcastss	.LCPI47_12(%rip), %ymm11 # ymm11 = [4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1]
	vfmadd213ps	%ymm11, %ymm14, %ymm5   # ymm5 = (ymm14 * ymm5) + ymm11
	vbroadcastss	.LCPI47_9(%rip), %ymm6  # ymm6 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vfmadd213ps	%ymm6, %ymm14, %ymm5    # ymm5 = (ymm14 * ymm5) + ymm6
	vcvttps2dq	%ymm13, %ymm8
	vfmadd231ps	%ymm15, %ymm12, %ymm5   # ymm5 = (ymm12 * ymm15) + ymm5
	vpslld	$23, %ymm8, %ymm13
	vpbroadcastd	.LCPI47_9(%rip), %ymm12 # ymm12 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vpaddd	%ymm12, %ymm13, %ymm13
	vmulps	%ymm5, %ymm13, %ymm5
	vpbroadcastd	.LCPI47_4(%rip), %ymm13 # ymm13 = [128,128,128,128,128,128,128,128]
	vpcmpgtd	%ymm8, %ymm13, %ymm15
	vbroadcastss	.LCPI47_13(%rip), %ymm14 # ymm14 = [+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf]
	vblendvps	%ymm15, %ymm5, %ymm14, %ymm5
	vpbroadcastd	.LCPI47_3(%rip), %ymm15 # ymm15 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vpcmpgtd	%ymm15, %ymm8, %ymm8
	vpand	%ymm5, %ymm8, %ymm5
	vmulps	%ymm5, %ymm4, %ymm4
	vaddps	%ymm6, %ymm5, %ymm5
	vmulps	%ymm5, %ymm5, %ymm5
	vdivps	%ymm5, %ymm4, %ymm4
	movq	%r13, %rax
	imulq	-96(%rsp), %rax                 # 8-byte Folded Reload
	addq	-104(%rsp), %rax                # 8-byte Folded Reload
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	vmovups	%ymm4, (%rcx,%rax,4)
	testl	%r15d, %r15d
	jle	.LBB47_29
# %bb.22:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x5.preheader.1"
	incl	%r14d
	movslq	%r14d, %r14
	leaq	-1(%r15), %rax
	movl	%r15d, %esi
	andl	$3, %esi
	cmpq	$3, %rax
	jae	.LBB47_24
# %bb.23:
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%ebp, %ebp
	jmp	.LBB47_26
.LBB47_24:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x5.preheader.1.new"
	andl	$-4, %r15d
	leaq	(%r10,%r10,2), %rax
	leaq	(%r12,%rax,4), %r8
	leaq	(,%r14,4), %rcx
	movq	%r10, %rax
	shlq	$4, %rax
	leaq	96(%r11), %rdi
	leaq	(%r12,%r10,8), %rdx
	leaq	(%r12,%r10,4), %rbx
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB47_25:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x5.1"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%r12,%rcx), %ymm4
	vfmadd132ps	-96(%rdi), %ymm0, %ymm4 # ymm4 = (ymm4 * mem) + ymm0
	vbroadcastss	(%rbx,%rcx), %ymm0
	vfmadd132ps	-64(%rdi), %ymm4, %ymm0 # ymm0 = (ymm0 * mem) + ymm4
	vbroadcastss	(%rdx,%rcx), %ymm4
	vfmadd132ps	-32(%rdi), %ymm0, %ymm4 # ymm4 = (ymm4 * mem) + ymm0
	vbroadcastss	(%r8,%rcx), %ymm0
	vfmadd132ps	(%rdi), %ymm4, %ymm0    # ymm0 = (ymm0 * mem) + ymm4
	addq	$4, %rbp
	addq	%rax, %rcx
	subq	$-128, %rdi
	cmpq	%rbp, %r15
	jne	.LBB47_25
.LBB47_26:                              # %"consume squashed_head1_filter_0_d_def__8.1.loopexit.unr-lcssa"
	testq	%rsi, %rsi
	je	.LBB47_29
# %bb.27:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x5.1.epil.preheader"
	movq	%rbp, %rax
	imulq	%r10, %rax
	addq	%r14, %rax
	leaq	(%r12,%rax,4), %rax
	shlq	$2, %r10
	shlq	$5, %rbp
	addq	%rbp, %r11
	.p2align	4, 0x90
.LBB47_28:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x5.1.epil"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rax), %ymm4
	vfmadd231ps	(%r11), %ymm4, %ymm0    # ymm0 = (ymm4 * mem) + ymm0
	addq	%r10, %rax
	addq	$32, %r11
	decq	%rsi
	jne	.LBB47_28
.LBB47_29:                              # %"consume squashed_head1_filter_0_d_def__8.1"
	orq	$1, %r13
	movq	-80(%rsp), %rax                 # 8-byte Reload
	imulq	%r13, %rax
	addq	%r9, %rax
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm4
	vmulps	64(%rsp), %ymm4, %ymm5          # 32-byte Folded Reload
	vroundps	$1, %ymm5, %ymm5
	vfmadd213ps	%ymm4, %ymm5, %ymm3     # ymm3 = (ymm5 * ymm3) + ymm4
	vfmadd213ps	%ymm3, %ymm5, %ymm1     # ymm1 = (ymm5 * ymm1) + ymm3
	vmulps	%ymm1, %ymm1, %ymm3
	vfmadd213ps	32(%rsp), %ymm3, %ymm2  # 32-byte Folded Reload
                                        # ymm2 = (ymm3 * ymm2) + mem
	vfmadd213ps	(%rsp), %ymm3, %ymm2    # 32-byte Folded Reload
                                        # ymm2 = (ymm3 * ymm2) + mem
	vfmadd213ps	%ymm9, %ymm3, %ymm2     # ymm2 = (ymm3 * ymm2) + ymm9
	vfmadd213ps	%ymm10, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm10
	vfmadd213ps	%ymm11, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm11
	vfmadd213ps	%ymm6, %ymm3, %ymm7     # ymm7 = (ymm3 * ymm7) + ymm6
	vfmadd231ps	%ymm2, %ymm1, %ymm7     # ymm7 = (ymm1 * ymm2) + ymm7
	vcvttps2dq	%ymm5, %ymm1
	vpslld	$23, %ymm1, %ymm2
	vpaddd	%ymm2, %ymm12, %ymm2
	vmulps	%ymm2, %ymm7, %ymm2
	vpcmpgtd	%ymm1, %ymm13, %ymm3
	vblendvps	%ymm3, %ymm2, %ymm14, %ymm2
	vpcmpgtd	%ymm15, %ymm1, %ymm1
	vpand	%ymm2, %ymm1, %ymm1
	vaddps	%ymm6, %ymm1, %ymm2
	vmulps	%ymm1, %ymm0, %ymm0
	vmulps	%ymm2, %ymm2, %ymm1
	vdivps	%ymm1, %ymm0, %ymm0
	imulq	-96(%rsp), %r13                 # 8-byte Folded Reload
	addq	-104(%rsp), %r13                # 8-byte Folded Reload
	jmp	.LBB47_30
.LBB47_3:
	vxorps	%xmm3, %xmm3, %xmm3
	movq	-104(%rsp), %r14                # 8-byte Reload
	cmpl	$3, %ebp
	jb	.LBB47_10
.LBB47_8:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.preheader1"
	movq	-104(%rsp), %rax                # 8-byte Reload
	leal	(%r8,%rax), %ebx
	subl	%r14d, %ebx
	leaq	3(%r14), %rax
	imulq	%r10, %rax
	leaq	(%r12,%rax,4), %r15
	shlq	$2, %rdx
	movq	%r10, %rax
	shlq	$4, %rax
	movq	%r14, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$96, %rcx
	leaq	2(%r14), %rsi
	imulq	%r10, %rsi
	leaq	(%r12,%rsi,4), %rdi
	leaq	1(%r14), %rsi
	imulq	%r10, %rsi
	leaq	(%r12,%rsi,4), %rsi
	imulq	%r10, %r14
	leaq	(%r12,%r14,4), %rbp
	.p2align	4, 0x90
.LBB47_9:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rbp,%rdx), %ymm0
	vfmadd132ps	-96(%rcx), %ymm3, %ymm0 # ymm0 = (ymm0 * mem) + ymm3
	vbroadcastss	(%rsi,%rdx), %ymm1
	vfmadd132ps	-64(%rcx), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rdi,%rdx), %ymm0
	vfmadd132ps	-32(%rcx), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	(%r15,%rdx), %ymm3
	vfmadd132ps	(%rcx), %ymm0, %ymm3    # ymm3 = (ymm3 * mem) + ymm0
	addq	%rax, %rdx
	subq	$-128, %rcx
	addl	$-4, %ebx
	jne	.LBB47_9
.LBB47_10:                              # %"consume squashed_head1_filter_0_d_def__"
	movq	-72(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %rax
	imulq	-80(%rsp), %rax                 # 8-byte Folded Reload
	movq	%rax, -24(%rsp)                 # 8-byte Spill
	movq	-32(%rsp), %rcx                 # 8-byte Reload
	addq	%rcx, %rax
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm11
	vbroadcastss	.LCPI47_0(%rip), %ymm9  # ymm9 = [-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0]
	vmulps	%ymm9, %ymm11, %ymm0
	vroundps	$1, %ymm0, %ymm12
	vbroadcastss	.LCPI47_1(%rip), %ymm2  # ymm2 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vfmadd231ps	%ymm2, %ymm12, %ymm11   # ymm11 = (ymm12 * ymm2) + ymm11
	vbroadcastss	.LCPI47_2(%rip), %ymm0  # ymm0 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vfmadd231ps	%ymm12, %ymm0, %ymm11   # ymm11 = (ymm0 * ymm12) + ymm11
	vmulps	%ymm11, %ymm11, %ymm13
	vbroadcastss	.LCPI47_5(%rip), %ymm8  # ymm8 = [-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4]
	vbroadcastss	.LCPI47_6(%rip), %ymm1  # ymm1 = [-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3]
	vmovaps	%ymm8, %ymm14
	vmovups	%ymm1, 64(%rsp)                 # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm13, %ymm14   # ymm14 = (ymm13 * ymm14) + ymm1
	vbroadcastss	.LCPI47_7(%rip), %ymm1  # ymm1 = [-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1]
	vmovups	%ymm1, 32(%rsp)                 # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm13, %ymm14   # ymm14 = (ymm13 * ymm14) + ymm1
	vbroadcastss	.LCPI47_8(%rip), %ymm1  # ymm1 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vmovups	%ymm1, (%rsp)                   # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm13, %ymm14   # ymm14 = (ymm13 * ymm14) + ymm1
	vbroadcastss	.LCPI47_10(%rip), %ymm10 # ymm10 = [1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3]
	vbroadcastss	.LCPI47_11(%rip), %ymm1 # ymm1 = [4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2]
	vmovaps	%ymm10, %ymm4
	vmovups	%ymm1, 160(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm13, %ymm4    # ymm4 = (ymm13 * ymm4) + ymm1
	vbroadcastss	.LCPI47_12(%rip), %ymm1 # ymm1 = [4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1]
	vmovups	%ymm1, 128(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm13, %ymm4    # ymm4 = (ymm13 * ymm4) + ymm1
	vbroadcastss	.LCPI47_9(%rip), %ymm5  # ymm5 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vfmadd213ps	%ymm5, %ymm13, %ymm4    # ymm4 = (ymm13 * ymm4) + ymm5
	vcvttps2dq	%ymm12, %ymm7
	vfmadd231ps	%ymm14, %ymm11, %ymm4   # ymm4 = (ymm11 * ymm14) + ymm4
	vpslld	$23, %ymm7, %ymm12
	vpbroadcastd	.LCPI47_9(%rip), %ymm1  # ymm1 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmovdqu	%ymm1, 96(%rsp)                 # 32-byte Spill
	vpaddd	%ymm1, %ymm12, %ymm12
	vmulps	%ymm4, %ymm12, %ymm4
	vpbroadcastd	.LCPI47_4(%rip), %ymm12 # ymm12 = [128,128,128,128,128,128,128,128]
	vpcmpgtd	%ymm7, %ymm12, %ymm14
	vbroadcastss	.LCPI47_13(%rip), %ymm13 # ymm13 = [+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf]
	vblendvps	%ymm14, %ymm4, %ymm13, %ymm4
	vpbroadcastd	.LCPI47_3(%rip), %ymm14 # ymm14 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vpcmpgtd	%ymm14, %ymm7, %ymm7
	vpand	%ymm4, %ymm7, %ymm4
	vmulps	%ymm4, %ymm3, %ymm3
	vaddps	%ymm5, %ymm4, %ymm4
	vmulps	%ymm4, %ymm4, %ymm4
	vdivps	%ymm4, %ymm3, %ymm3
	imulq	-96(%rsp), %r14                 # 8-byte Folded Reload
	movq	-56(%rsp), %rax                 # 8-byte Reload
	addq	%r14, %rax
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	vmovups	%ymm3, (%rcx,%rax,4)
	testl	%r8d, %r8d
	jle	.LBB47_37
# %bb.11:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.preheader.1"
	incl	%r9d
	movslq	%r9d, %rbp
	leal	-1(%r8), %ebx
	movl	%r8d, %eax
	andl	$3, %eax
	je	.LBB47_12
# %bb.31:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.1.prol.preheader"
	movq	-104(%rsp), %rsi                # 8-byte Reload
	movq	%rsi, %rcx
	imulq	%r10, %rcx
	addq	%rbp, %rcx
	leaq	(%r12,%rcx,4), %rcx
	leaq	(,%r10,4), %rdx
	shlq	$5, %rsi
	addq	%r11, %rsi
	negl	%eax
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB47_32:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.1.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rcx), %ymm3
	vfmadd231ps	(%rsi), %ymm3, %ymm15   # ymm15 = (ymm3 * mem) + ymm15
	decq	%rdi
	addq	%rdx, %rcx
	addq	$32, %rsi
	cmpl	%edi, %eax
	jne	.LBB47_32
# %bb.33:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.1.prol.loopexit.loopexit"
	movq	-104(%rsp), %r9                 # 8-byte Reload
	subq	%rdi, %r9
	cmpl	$3, %ebx
	jae	.LBB47_35
	jmp	.LBB47_37
.LBB47_12:
	vxorps	%xmm15, %xmm15, %xmm15
	movq	-104(%rsp), %r9                 # 8-byte Reload
	cmpl	$3, %ebx
	jb	.LBB47_37
.LBB47_35:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.1.preheader"
	movq	-104(%rsp), %rax                # 8-byte Reload
	leal	(%r8,%rax), %ebx
	subl	%r9d, %ebx
	leaq	3(%r9), %rax
	imulq	%r10, %rax
	leaq	(%r12,%rax,4), %r15
	shlq	$2, %rbp
	movq	%r10, %rax
	shlq	$4, %rax
	movq	%r9, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$96, %rcx
	leaq	2(%r9), %rdx
	imulq	%r10, %rdx
	leaq	(%r12,%rdx,4), %rdi
	leaq	1(%r9), %rdx
	imulq	%r10, %rdx
	leaq	(%r12,%rdx,4), %rsi
	imulq	%r10, %r9
	leaq	(%r12,%r9,4), %rdx
	.p2align	4, 0x90
.LBB47_36:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.1"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rbp), %ymm3
	vfmadd132ps	-96(%rcx), %ymm15, %ymm3 # ymm3 = (ymm3 * mem) + ymm15
	vbroadcastss	(%rsi,%rbp), %ymm4
	vfmadd132ps	-64(%rcx), %ymm3, %ymm4 # ymm4 = (ymm4 * mem) + ymm3
	vbroadcastss	(%rdi,%rbp), %ymm3
	vfmadd132ps	-32(%rcx), %ymm4, %ymm3 # ymm3 = (ymm3 * mem) + ymm4
	vbroadcastss	(%r15,%rbp), %ymm15
	vfmadd132ps	(%rcx), %ymm3, %ymm15   # ymm15 = (ymm15 * mem) + ymm3
	addq	%rax, %rbp
	subq	$-128, %rcx
	addl	$-4, %ebx
	jne	.LBB47_36
.LBB47_37:                              # %"consume squashed_head1_filter_0_d_def__.1"
	movq	-72(%rsp), %rax                 # 8-byte Reload
	orq	$1, %rax
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	imulq	%rax, %rcx
	movq	%rcx, -80(%rsp)                 # 8-byte Spill
	movq	-32(%rsp), %rdx                 # 8-byte Reload
	addq	%rcx, %rdx
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	vmovups	(%rcx,%rdx,4), %ymm3
	vmulps	%ymm3, %ymm9, %ymm4
	vroundps	$1, %ymm4, %ymm4
	vfmadd231ps	%ymm2, %ymm4, %ymm3     # ymm3 = (ymm4 * ymm2) + ymm3
	vfmadd231ps	%ymm4, %ymm0, %ymm3     # ymm3 = (ymm0 * ymm4) + ymm3
	vmulps	%ymm3, %ymm3, %ymm7
	vmovaps	%ymm8, %ymm1
	vfmadd213ps	64(%rsp), %ymm7, %ymm1  # 32-byte Folded Reload
                                        # ymm1 = (ymm7 * ymm1) + mem
	vfmadd213ps	32(%rsp), %ymm7, %ymm1  # 32-byte Folded Reload
                                        # ymm1 = (ymm7 * ymm1) + mem
	vfmadd213ps	(%rsp), %ymm7, %ymm1    # 32-byte Folded Reload
                                        # ymm1 = (ymm7 * ymm1) + mem
	vmovaps	%ymm10, %ymm6
	vfmadd213ps	160(%rsp), %ymm7, %ymm6 # 32-byte Folded Reload
                                        # ymm6 = (ymm7 * ymm6) + mem
	vfmadd213ps	128(%rsp), %ymm7, %ymm6 # 32-byte Folded Reload
                                        # ymm6 = (ymm7 * ymm6) + mem
	vfmadd213ps	%ymm5, %ymm7, %ymm6     # ymm6 = (ymm7 * ymm6) + ymm5
	vfmadd231ps	%ymm1, %ymm3, %ymm6     # ymm6 = (ymm3 * ymm1) + ymm6
	vcvttps2dq	%ymm4, %ymm1
	vpslld	$23, %ymm1, %ymm3
	vpaddd	96(%rsp), %ymm3, %ymm3          # 32-byte Folded Reload
	vmulps	%ymm3, %ymm6, %ymm3
	vpcmpgtd	%ymm1, %ymm12, %ymm4
	vblendvps	%ymm4, %ymm3, %ymm13, %ymm3
	vpcmpgtd	%ymm14, %ymm1, %ymm1
	vpand	%ymm3, %ymm1, %ymm1
	imulq	-96(%rsp), %rax                 # 8-byte Folded Reload
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	addq	%rax, %rsi
	movq	-16(%rsp), %rdx                 # 8-byte Reload
	orq	$1, %rdx
	movq	%rdx, %rax
	imulq	-8(%rsp), %rax                  # 8-byte Folded Reload
	movq	%rax, -96(%rsp)                 # 8-byte Spill
	movl	-40(%rsp), %ecx                 # 4-byte Reload
	imull	%edx, %ecx
	addl	-44(%rsp), %ecx                 # 4-byte Folded Reload
	vmulps	%ymm1, %ymm15, %ymm3
	vaddps	%ymm5, %ymm1, %ymm1
	vmulps	%ymm1, %ymm1, %ymm1
	vdivps	%ymm1, %ymm3, %ymm1
	movl	-36(%rsp), %eax                 # 4-byte Reload
	imull	%edx, %eax
	addl	%eax, %r13d
	movq	-64(%rsp), %rax                 # 8-byte Reload
	vmovups	%ymm1, (%rax,%rsi,4)
	vxorps	%xmm15, %xmm15, %xmm15
	movslq	%ecx, %rax
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	vxorps	%xmm3, %xmm3, %xmm3
	testl	%r8d, %r8d
	jle	.LBB47_46
# %bb.38:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.preheader.125"
	movslq	%r13d, %rbp
	leal	-1(%r8), %ebx
	movl	%r8d, %eax
	andl	$3, %eax
	je	.LBB47_39
# %bb.40:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.130.prol.preheader"
	movq	-104(%rsp), %rsi                # 8-byte Reload
	movq	%rsi, %rcx
	imulq	%r10, %rcx
	addq	%rbp, %rcx
	leaq	(%r12,%rcx,4), %rcx
	leaq	(,%r10,4), %rdx
	shlq	$5, %rsi
	addq	%r11, %rsi
	negl	%eax
	vxorps	%xmm3, %xmm3, %xmm3
	xorl	%edi, %edi
	.p2align	4, 0x90
.LBB47_41:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.130.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rcx), %ymm1
	vfmadd231ps	(%rsi), %ymm1, %ymm3    # ymm3 = (ymm1 * mem) + ymm3
	decq	%rdi
	addq	%rdx, %rcx
	addq	$32, %rsi
	cmpl	%edi, %eax
	jne	.LBB47_41
# %bb.42:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.130.prol.loopexit.loopexit"
	movq	-104(%rsp), %r15                # 8-byte Reload
	subq	%rdi, %r15
	cmpl	$3, %ebx
	jae	.LBB47_44
	jmp	.LBB47_46
.LBB47_39:
	vxorps	%xmm3, %xmm3, %xmm3
	movq	-104(%rsp), %r15                # 8-byte Reload
	cmpl	$3, %ebx
	jb	.LBB47_46
.LBB47_44:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.130.preheader"
	movq	-104(%rsp), %rax                # 8-byte Reload
	leal	(%r8,%rax), %ebx
	subl	%r15d, %ebx
	leaq	3(%r15), %rax
	imulq	%r10, %rax
	leaq	(%r12,%rax,4), %r9
	shlq	$2, %rbp
	movq	%r10, %rax
	shlq	$4, %rax
	movq	%r15, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$96, %rcx
	leaq	2(%r15), %rdx
	imulq	%r10, %rdx
	leaq	(%r12,%rdx,4), %rdi
	leaq	1(%r15), %rdx
	imulq	%r10, %rdx
	leaq	(%r12,%rdx,4), %rsi
	imulq	%r10, %r15
	leaq	(%r12,%r15,4), %rdx
	.p2align	4, 0x90
.LBB47_45:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.130"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rdx,%rbp), %ymm1
	vfmadd132ps	-96(%rcx), %ymm3, %ymm1 # ymm1 = (ymm1 * mem) + ymm3
	vbroadcastss	(%rsi,%rbp), %ymm3
	vfmadd132ps	-64(%rcx), %ymm1, %ymm3 # ymm3 = (ymm3 * mem) + ymm1
	vbroadcastss	(%rdi,%rbp), %ymm1
	vfmadd132ps	-32(%rcx), %ymm3, %ymm1 # ymm1 = (ymm1 * mem) + ymm3
	vbroadcastss	(%r9,%rbp), %ymm3
	vfmadd132ps	(%rcx), %ymm1, %ymm3    # ymm3 = (ymm3 * mem) + ymm1
	addq	%rax, %rbp
	subq	$-128, %rcx
	addl	$-4, %ebx
	jne	.LBB47_45
.LBB47_46:                              # %"consume squashed_head1_filter_0_d_def__.138"
	movq	-24(%rsp), %rcx                 # 8-byte Reload
	addq	-96(%rsp), %rcx                 # 8-byte Folded Reload
	movq	-88(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rax,%rcx,4), %ymm1
	vmovaps	%ymm9, %ymm11
	vmulps	%ymm1, %ymm9, %ymm4
	vroundps	$1, %ymm4, %ymm4
	vfmadd231ps	%ymm2, %ymm4, %ymm1     # ymm1 = (ymm4 * ymm2) + ymm1
	vfmadd231ps	%ymm4, %ymm0, %ymm1     # ymm1 = (ymm0 * ymm4) + ymm1
	vmulps	%ymm1, %ymm1, %ymm6
	vmovaps	%ymm8, %ymm7
	vfmadd213ps	64(%rsp), %ymm6, %ymm7  # 32-byte Folded Reload
                                        # ymm7 = (ymm6 * ymm7) + mem
	vfmadd213ps	32(%rsp), %ymm6, %ymm7  # 32-byte Folded Reload
                                        # ymm7 = (ymm6 * ymm7) + mem
	vfmadd213ps	(%rsp), %ymm6, %ymm7    # 32-byte Folded Reload
                                        # ymm7 = (ymm6 * ymm7) + mem
	vmovaps	%ymm10, %ymm9
	vfmadd213ps	160(%rsp), %ymm6, %ymm9 # 32-byte Folded Reload
                                        # ymm9 = (ymm6 * ymm9) + mem
	vfmadd213ps	128(%rsp), %ymm6, %ymm9 # 32-byte Folded Reload
                                        # ymm9 = (ymm6 * ymm9) + mem
	vfmadd213ps	%ymm5, %ymm6, %ymm9     # ymm9 = (ymm6 * ymm9) + ymm5
	vfmadd231ps	%ymm7, %ymm1, %ymm9     # ymm9 = (ymm1 * ymm7) + ymm9
	vcvttps2dq	%ymm4, %ymm1
	vpslld	$23, %ymm1, %ymm4
	vpaddd	96(%rsp), %ymm4, %ymm4          # 32-byte Folded Reload
	vmulps	%ymm4, %ymm9, %ymm4
	vpcmpgtd	%ymm1, %ymm12, %ymm6
	vblendvps	%ymm6, %ymm4, %ymm13, %ymm4
	vpcmpgtd	%ymm14, %ymm1, %ymm1
	vpand	%ymm4, %ymm1, %ymm1
	vmulps	%ymm1, %ymm3, %ymm3
	vaddps	%ymm5, %ymm1, %ymm1
	vmulps	%ymm1, %ymm1, %ymm1
	vdivps	%ymm1, %ymm3, %ymm1
	movq	-56(%rsp), %r15                 # 8-byte Reload
	addq	%r15, %r14
	movq	-64(%rsp), %rax                 # 8-byte Reload
	vmovups	%ymm1, (%rax,%r14,4)
	testl	%r8d, %r8d
	jle	.LBB47_55
# %bb.47:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.preheader.1.1"
	incl	%r13d
	movslq	%r13d, %rdx
	leal	-1(%r8), %r9d
	movl	%r8d, %eax
	andl	$3, %eax
	je	.LBB47_48
# %bb.49:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.1.1.prol.preheader"
	movq	-104(%rsp), %rdi                # 8-byte Reload
	movq	%rdi, %rcx
	imulq	%r10, %rcx
	addq	%rdx, %rcx
	leaq	(%r12,%rcx,4), %rcx
	leaq	(,%r10,4), %rsi
	shlq	$5, %rdi
	addq	%r11, %rdi
	negl	%eax
	vxorps	%xmm15, %xmm15, %xmm15
	xorl	%ebp, %ebp
	.p2align	4, 0x90
.LBB47_50:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.1.1.prol"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rcx), %ymm1
	vfmadd231ps	(%rdi), %ymm1, %ymm15   # ymm15 = (ymm1 * mem) + ymm15
	decq	%rbp
	addq	%rsi, %rcx
	addq	$32, %rdi
	cmpl	%ebp, %eax
	jne	.LBB47_50
# %bb.51:                               # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.1.1.prol.loopexit.loopexit"
	movq	-104(%rsp), %rbx                # 8-byte Reload
	subq	%rbp, %rbx
	cmpl	$3, %r9d
	jae	.LBB47_53
	jmp	.LBB47_55
.LBB47_48:
	vxorps	%xmm15, %xmm15, %xmm15
	movq	-104(%rsp), %rbx                # 8-byte Reload
	cmpl	$3, %r9d
	jb	.LBB47_55
.LBB47_53:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.1.1.preheader"
	addl	-104(%rsp), %r8d                # 4-byte Folded Reload
	subl	%ebx, %r8d
	leaq	3(%rbx), %rax
	imulq	%r10, %rax
	leaq	(%r12,%rax,4), %rbp
	shlq	$2, %rdx
	movq	%r10, %rax
	shlq	$4, %rax
	movq	%rbx, %rcx
	shlq	$5, %rcx
	addq	%r11, %rcx
	addq	$96, %rcx
	leaq	2(%rbx), %rsi
	imulq	%r10, %rsi
	leaq	(%r12,%rsi,4), %rdi
	leaq	1(%rbx), %rsi
	imulq	%r10, %rsi
	leaq	(%r12,%rsi,4), %rsi
	imulq	%r10, %rbx
	leaq	(%r12,%rbx,4), %rbx
	.p2align	4, 0x90
.LBB47_54:                              # %"for squashed_head1_filter_0_d_def__.s1.r1275$x.1.1"
                                        # =>This Inner Loop Header: Depth=1
	vbroadcastss	(%rbx,%rdx), %ymm1
	vfmadd132ps	-96(%rcx), %ymm15, %ymm1 # ymm1 = (ymm1 * mem) + ymm15
	vbroadcastss	(%rsi,%rdx), %ymm3
	vfmadd132ps	-64(%rcx), %ymm1, %ymm3 # ymm3 = (ymm3 * mem) + ymm1
	vbroadcastss	(%rdi,%rdx), %ymm1
	vfmadd132ps	-32(%rcx), %ymm3, %ymm1 # ymm1 = (ymm1 * mem) + ymm3
	vbroadcastss	(%rbp,%rdx), %ymm15
	vfmadd132ps	(%rcx), %ymm1, %ymm15   # ymm15 = (ymm15 * mem) + ymm1
	addq	%rax, %rdx
	subq	$-128, %rcx
	addl	$-4, %r8d
	jne	.LBB47_54
.LBB47_55:                              # %"consume squashed_head1_filter_0_d_def__.1.1"
	movq	-80(%rsp), %rax                 # 8-byte Reload
	addq	-96(%rsp), %rax                 # 8-byte Folded Reload
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	vmovups	(%rcx,%rax,4), %ymm1
	vmulps	%ymm1, %ymm11, %ymm3
	vroundps	$1, %ymm3, %ymm3
	vfmadd213ps	%ymm1, %ymm3, %ymm2     # ymm2 = (ymm3 * ymm2) + ymm1
	vfmadd213ps	%ymm2, %ymm3, %ymm0     # ymm0 = (ymm3 * ymm0) + ymm2
	vmulps	%ymm0, %ymm0, %ymm1
	vfmadd213ps	64(%rsp), %ymm1, %ymm8  # 32-byte Folded Reload
                                        # ymm8 = (ymm1 * ymm8) + mem
	vfmadd213ps	32(%rsp), %ymm1, %ymm8  # 32-byte Folded Reload
                                        # ymm8 = (ymm1 * ymm8) + mem
	vfmadd213ps	(%rsp), %ymm1, %ymm8    # 32-byte Folded Reload
                                        # ymm8 = (ymm1 * ymm8) + mem
	vfmadd213ps	160(%rsp), %ymm1, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm1 * ymm10) + mem
	vfmadd213ps	128(%rsp), %ymm1, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm1 * ymm10) + mem
	vfmadd213ps	%ymm5, %ymm1, %ymm10    # ymm10 = (ymm1 * ymm10) + ymm5
	vfmadd231ps	%ymm8, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm8) + ymm10
	vcvttps2dq	%ymm3, %ymm0
	vpslld	$23, %ymm0, %ymm1
	vpaddd	96(%rsp), %ymm1, %ymm1          # 32-byte Folded Reload
	vmulps	%ymm1, %ymm10, %ymm1
	vpcmpgtd	%ymm0, %ymm12, %ymm2
	vblendvps	%ymm2, %ymm1, %ymm13, %ymm1
	vpcmpgtd	%ymm14, %ymm0, %ymm0
	vpand	%ymm1, %ymm0, %ymm0
	vaddps	%ymm5, %ymm0, %ymm1
	vmulps	%ymm0, %ymm15, %ymm0
	vmulps	%ymm1, %ymm1, %ymm1
	vdivps	%ymm1, %ymm0, %ymm0
	movq	-72(%rsp), %r13                 # 8-byte Reload
	addq	%r15, %r13
.LBB47_30:                              # %destructor_block
	movq	-64(%rsp), %rax                 # 8-byte Reload
	vmovups	%ymm0, (%rax,%r13,4)
	xorl	%eax, %eax
	addq	$200, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end47:
	.size	train_cost_model.par_for.updated_head1_filter.s1.v241.v241.v241, .Lfunc_end47-train_cost_model.par_for.updated_head1_filter.s1.v241.v241.v241
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_head1_filter.s2.v241.v241.v241
.LCPI48_0:
	.long	0x3f666666                      # float 0.899999976
.LCPI48_1:
	.long	0x3dcccccd                      # float 0.100000001
	.section	.text.train_cost_model.par_for.updated_head1_filter.s2.v241.v241.v241,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_head1_filter.s2.v241.v241.v241,@function
train_cost_model.par_for.updated_head1_filter.s2.v241.v241.v241: # @train_cost_model.par_for.updated_head1_filter.s2.v241.v241.v241
# %bb.0:                                # %entry
	pushq	%r14
	pushq	%rbx
	movslq	(%rdx), %r8
	movslq	4(%rdx), %r9
	movslq	8(%rdx), %r11
	movq	16(%rdx), %rax
	movl	%esi, %edx
	sarl	$31, %edx
	movl	%edx, %edi
	xorl	%esi, %edi
	imulq	$1717986919, %rdi, %rdi         # imm = 0x66666667
	shrq	$35, %rdi
	xorl	%edx, %edi
	cmpl	$59, %esi
	jg	.LBB48_2
# %bb.1:                                # %true_bb
	leal	(%rdi,%rdi), %ecx
	leal	(,%rdi,4), %edx
	leal	(%rdx,%rdx,4), %edx
	subl	%edx, %esi
	addl	%esi, %esi
	leaq	(%r11,%r11,2), %r10
	movslq	%esi, %rdx
	imull	%r9d, %ecx
	movslq	%ecx, %rcx
	movq	%rdx, %rsi
	imulq	%r8, %rsi
	leaq	(%rsi,%rcx), %rbx
	vbroadcastss	.LCPI48_0(%rip), %ymm0  # ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%r10,%rbx), %r14
	addq	%r11, %rbx
	vbroadcastss	.LCPI48_1(%rip), %ymm1  # ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rax,%r14,4), %ymm1, %ymm2
	vfmadd231ps	(%rax,%rbx,4), %ymm0, %ymm2 # ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rax,%rbx,4)
	orq	$1, %rdx
	imulq	%r8, %rdx
	addq	%rdx, %rcx
	leaq	(%r10,%rcx), %rbx
	addq	%r11, %rcx
	vmulps	(%rax,%rbx,4), %ymm1, %ymm2
	vfmadd231ps	(%rax,%rcx,4), %ymm0, %ymm2 # ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rax,%rcx,4)
	leal	(%rdi,%rdi), %ecx
	incl	%ecx
	imull	%r9d, %ecx
	movslq	%ecx, %rcx
	addq	%rcx, %rsi
	leaq	(%r10,%rsi), %rdi
	addq	%r11, %rsi
	vmulps	(%rax,%rdi,4), %ymm1, %ymm1
	vfmadd231ps	(%rax,%rsi,4), %ymm0, %ymm1 # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rsi,4)
	addq	%rcx, %rdx
	addq	%rdx, %r11
	leaq	(%rax,%r11,4), %rcx
	addq	%r10, %rdx
	jmp	.LBB48_3
.LBB48_2:                               # %false_bb
	shll	$2, %edi
	leal	(%rdi,%rdi,4), %ecx
	subl	%ecx, %esi
	addl	%esi, %esi
	addq	%r9, %r9
	leaq	(%r9,%r9,2), %rcx
	leaq	(%r11,%r11,2), %rdi
	movslq	%esi, %rdx
	movq	%rdx, %rsi
	imulq	%r8, %rsi
	addq	%rcx, %rsi
	vbroadcastss	.LCPI48_0(%rip), %ymm0  # ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%rdi,%rsi), %rbx
	addq	%r11, %rsi
	vbroadcastss	.LCPI48_1(%rip), %ymm1  # ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rax,%rbx,4), %ymm1, %ymm1
	vfmadd231ps	(%rax,%rsi,4), %ymm0, %ymm1 # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rsi,4)
	orq	$1, %rdx
	imulq	%r8, %rdx
	addq	%rcx, %rdx
	addq	%rdx, %r11
	leaq	(%rax,%r11,4), %rcx
	addq	%rdi, %rdx
.LBB48_3:                               # %destructor_block
	vbroadcastss	.LCPI48_0(%rip), %ymm0  # ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	vbroadcastss	.LCPI48_1(%rip), %ymm1  # ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rax,%rdx,4), %ymm1, %ymm1
	vfmadd231ps	(%rcx), %ymm0, %ymm1    # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rcx)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	vzeroupper
	retq
.Lfunc_end48:
	.size	train_cost_model.par_for.updated_head1_filter.s2.v241.v241.v241, .Lfunc_end48-train_cost_model.par_for.updated_head1_filter.s2.v241.v241.v241
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_head1_filter.s3.v241.v241.v241
.LCPI49_0:
	.long	0x3f7fbe77                      # float 0.999000012
.LCPI49_1:
	.long	0x3a83126f                      # float 0.00100000005
	.section	.text.train_cost_model.par_for.updated_head1_filter.s3.v241.v241.v241,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_head1_filter.s3.v241.v241.v241,@function
train_cost_model.par_for.updated_head1_filter.s3.v241.v241.v241: # @train_cost_model.par_for.updated_head1_filter.s3.v241.v241.v241
# %bb.0:                                # %entry
	pushq	%r14
	pushq	%rbx
	movslq	(%rdx), %r11
	movslq	4(%rdx), %r8
	movslq	8(%rdx), %r9
	movslq	12(%rdx), %r10
	movq	16(%rdx), %r14
	movl	%esi, %eax
	sarl	$31, %eax
	movl	%eax, %edx
	xorl	%esi, %edx
	imulq	$1717986919, %rdx, %rdx         # imm = 0x66666667
	shrq	$35, %rdx
	xorl	%eax, %edx
	cmpl	$59, %esi
	jg	.LBB49_2
# %bb.1:                                # %true_bb
	leal	(%rdx,%rdx), %eax
	leal	(,%rdx,4), %edi
	leal	(%rdi,%rdi,4), %edi
	subl	%edi, %esi
	addl	%esi, %esi
	movslq	%esi, %rsi
	imull	%r10d, %eax
	movslq	%eax, %rdi
	movq	%rsi, %rax
	imulq	%r9, %rax
	leaq	(%rax,%rdi), %rbx
	leaq	(%rbx,%r11), %rcx
	vmovups	(%r14,%rcx,4), %ymm1
	addq	%r8, %rbx
	vbroadcastss	.LCPI49_0(%rip), %ymm0  # ymm0 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm1, %ymm1, %ymm1
	vbroadcastss	.LCPI49_1(%rip), %ymm2  # ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm1, %ymm1
	vfmadd231ps	(%r14,%rbx,4), %ymm0, %ymm1 # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%r14,%rbx,4)
	orq	$1, %rsi
	imulq	%r9, %rsi
	addq	%rsi, %rdi
	leaq	(%rdi,%r11), %rcx
	vmovups	(%r14,%rcx,4), %ymm1
	addq	%r8, %rdi
	vmulps	%ymm1, %ymm1, %ymm1
	vmulps	%ymm2, %ymm1, %ymm1
	vfmadd231ps	(%r14,%rdi,4), %ymm0, %ymm1 # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%r14,%rdi,4)
	leal	(%rdx,%rdx), %ecx
	incl	%ecx
	imull	%r10d, %ecx
	movslq	%ecx, %rcx
	addq	%rcx, %rax
	leaq	(%rax,%r11), %rdx
	vmovups	(%r14,%rdx,4), %ymm1
	addq	%r8, %rax
	vmulps	%ymm1, %ymm1, %ymm1
	vmulps	%ymm2, %ymm1, %ymm1
	vfmadd231ps	(%r14,%rax,4), %ymm0, %ymm1 # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%r14,%rax,4)
	addq	%rcx, %rsi
	addq	%rsi, %r11
	vmovups	(%r14,%r11,4), %ymm0
	addq	%r8, %rsi
	leaq	(%r14,%rsi,4), %rax
	jmp	.LBB49_3
.LBB49_2:                               # %false_bb
	shll	$2, %edx
	leal	(%rdx,%rdx,4), %eax
	subl	%eax, %esi
	addl	%esi, %esi
	addq	%r10, %r10
	leaq	(%r10,%r10,2), %rax
	movslq	%esi, %rcx
	movq	%rcx, %rdx
	imulq	%r9, %rdx
	addq	%rax, %rdx
	leaq	(%rdx,%r11), %rsi
	vmovups	(%r14,%rsi,4), %ymm0
	addq	%r8, %rdx
	vbroadcastss	.LCPI49_0(%rip), %ymm1  # ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	.LCPI49_1(%rip), %ymm2  # ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%r14,%rdx,4), %ymm1, %ymm0 # ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%r14,%rdx,4)
	orq	$1, %rcx
	imulq	%r9, %rcx
	addq	%rax, %rcx
	addq	%rcx, %r11
	vmovups	(%r14,%r11,4), %ymm0
	addq	%r8, %rcx
	leaq	(%r14,%rcx,4), %rax
.LBB49_3:                               # %destructor_block
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	.LCPI49_0(%rip), %ymm1  # ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vbroadcastss	.LCPI49_1(%rip), %ymm2  # ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rax), %ymm1, %ymm0    # ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rax)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	vzeroupper
	retq
.Lfunc_end49:
	.size	train_cost_model.par_for.updated_head1_filter.s3.v241.v241.v241, .Lfunc_end49-train_cost_model.par_for.updated_head1_filter.s3.v241.v241.v241
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_head1_filter.s4.v241.v241.v241
.LCPI50_0:
	.long	0xb727c5ac                      # float -9.99999974E-6
.LCPI50_1:
	.long	0x3727c5ac                      # float 9.99999974E-6
	.section	.text.train_cost_model.par_for.updated_head1_filter.s4.v241.v241.v241,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_head1_filter.s4.v241.v241.v241,@function
train_cost_model.par_for.updated_head1_filter.s4.v241.v241.v241: # @train_cost_model.par_for.updated_head1_filter.s4.v241.v241.v241
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
                                        # kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r9
	movslq	4(%rdx), %r10
	vmovss	8(%rdx), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	vmovss	12(%rdx), %xmm2                 # xmm2 = mem[0],zero,zero,zero
	vmovss	16(%rdx), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	movl	20(%rdx), %r11d
	movl	24(%rdx), %r15d
	movslq	28(%rdx), %r14
	movq	32(%rdx), %r8
	movq	48(%rdx), %rcx
	movl	%esi, %eax
	sarl	$31, %eax
	movl	%eax, %edx
	xorl	%esi, %edx
	imulq	$1717986919, %rdx, %rbx         # imm = 0x66666667
	shrq	$35, %rbx
	xorl	%eax, %ebx
	cmpl	$59, %esi
	jg	.LBB50_2
# %bb.1:                                # %true_bb
	leal	(%rbx,%rbx), %eax
	leal	(,%rbx,4), %edx
	leal	(%rdx,%rdx,4), %edx
	subl	%edx, %esi
	leal	(%rsi,%rsi), %edx
	movslq	%edx, %r13
	vmulss	%xmm2, %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vbroadcastss	%xmm0, %ymm0
	movl	%eax, %edi
	imull	%r15d, %edi
	imull	%r10d, %eax
	movslq	%eax, %r12
	movq	%r9, %rax
	movq	%r9, -8(%rsp)                   # 8-byte Spill
	movl	%r13d, %r9d
	imull	%r11d, %r9d
	leal	(%r9,%rdi), %ebp
	movq	%r13, %rdx
	imulq	%rax, %rdx
	movslq	%ebp, %rbp
	movq	%r8, %rax
	leaq	(%r14,%rbp), %r8
	vmulps	(%rcx,%r8,4), %ymm1, %ymm3
	movq	%rax, %r8
	leaq	(%rbp,%r14,2), %rax
	vmulps	(%rcx,%rax,4), %ymm0, %ymm2
	leaq	(%rdx,%r12), %rax
	vsqrtps	%ymm2, %ymm4
	vbroadcastss	.LCPI50_1(%rip), %ymm2  # ymm2 = [9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6]
	vaddps	%ymm2, %ymm4, %ymm4
	vdivps	%ymm4, %ymm3, %ymm3
	vmovups	(%r8,%rax,4), %ymm4
	vsubps	%ymm3, %ymm4, %ymm3
	vmovups	%ymm3, (%rcx,%rbp,4)
	addl	%esi, %esi
	incl	%esi
	imull	%r11d, %esi
	addl	%esi, %edi
	movslq	%edi, %rax
	leaq	(%rax,%r14), %rdi
	vmulps	(%rcx,%rdi,4), %ymm1, %ymm3
	orq	$1, %r13
	imulq	-8(%rsp), %r13                  # 8-byte Folded Reload
	leaq	(%rax,%r14,2), %rdi
	vmulps	(%rcx,%rdi,4), %ymm0, %ymm4
	addq	%r13, %r12
	vsqrtps	%ymm4, %ymm4
	vaddps	%ymm2, %ymm4, %ymm4
	vdivps	%ymm4, %ymm3, %ymm3
	vmovups	(%r8,%r12,4), %ymm4
	vsubps	%ymm3, %ymm4, %ymm3
	vmovups	%ymm3, (%rcx,%rax,4)
	leal	(%rbx,%rbx), %eax
	incl	%eax
	imull	%eax, %r15d
	imull	%r10d, %eax
	addl	%r15d, %r9d
	movslq	%r9d, %rdi
	leaq	(%rdi,%r14), %rbp
	vmulps	(%rcx,%rbp,4), %ymm1, %ymm3
	leaq	(%rdi,%r14,2), %rbp
	vmulps	(%rcx,%rbp,4), %ymm0, %ymm4
	cltq
	addq	%rax, %rdx
	vsqrtps	%ymm4, %ymm4
	vaddps	%ymm2, %ymm4, %ymm2
	vdivps	%ymm2, %ymm3, %ymm2
	vmovups	(%r8,%rdx,4), %ymm3
	vsubps	%ymm2, %ymm3, %ymm2
	vmovups	%ymm2, (%rcx,%rdi,4)
	addl	%esi, %r15d
	addq	%rax, %r13
	movslq	%r15d, %rax
	leaq	(%rax,%r14), %rsi
	vmulps	(%rcx,%rsi,4), %ymm1, %ymm1
	jmp	.LBB50_3
.LBB50_2:                               # %false_bb
	shll	$2, %ebx
	leal	(%rbx,%rbx,4), %eax
	subl	%eax, %esi
	leal	(%rsi,%rsi), %edx
	addl	%r15d, %r15d
	leal	(%r15,%r15,2), %edi
	addq	%r10, %r10
	leaq	(%r10,%r10,2), %rax
	movslq	%edx, %r13
	vmulss	%xmm2, %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vbroadcastss	%xmm0, %ymm0
	movl	%r13d, %ebp
	imull	%r11d, %ebp
	addl	%edi, %ebp
	movq	%r13, %rdx
	imulq	%r9, %rdx
	movslq	%ebp, %rbp
	leaq	(%r14,%rbp), %rbx
	vmulps	(%rcx,%rbx,4), %ymm1, %ymm2
	addq	%rax, %rdx
	leaq	(,%r14,2), %rbx
	addq	%rbp, %rbx
	vmulps	(%rcx,%rbx,4), %ymm0, %ymm3
	vsqrtps	%ymm3, %ymm3
	vbroadcastss	.LCPI50_0(%rip), %ymm4  # ymm4 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm3, %ymm4, %ymm3
	vdivps	%ymm3, %ymm2, %ymm2
	vaddps	(%r8,%rdx,4), %ymm2, %ymm2
	vmovups	%ymm2, (%rcx,%rbp,4)
	leal	(%rsi,%rsi), %edx
	incl	%edx
	imull	%r11d, %edx
	addl	%edi, %edx
	orq	$1, %r13
	imulq	%r9, %r13
	addq	%rax, %r13
	movslq	%edx, %rax
	leaq	(%rax,%r14), %rdx
	vmulps	(%rcx,%rdx,4), %ymm1, %ymm1
.LBB50_3:                               # %destructor_block
	leaq	(%rax,%r14,2), %rsi
	vmulps	(%rcx,%rsi,4), %ymm0, %ymm0
	vsqrtps	%ymm0, %ymm0
	vbroadcastss	.LCPI50_0(%rip), %ymm2  # ymm2 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm0, %ymm2, %ymm0
	vdivps	%ymm0, %ymm1, %ymm0
	vaddps	(%r8,%r13,4), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rax,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end50:
	.size	train_cost_model.par_for.updated_head1_filter.s4.v241.v241.v241, .Lfunc_end50-train_cost_model.par_for.updated_head1_filter.s4.v241.v241.v241
                                        # -- End function
	.section	.text.train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n
	.type	train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n,@function
train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n: # @train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$264, %rsp                      # imm = 0x108
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%xmm0, 32(%rsp)
                                        # kill: def $esi killed $esi def $rsi
	movq	%rdi, 56(%rsp)                  # 8-byte Spill
	movl	(%rdx), %edi
	movl	4(%rdx), %eax
	movl	%eax, 16(%rsp)                  # 4-byte Spill
	movslq	8(%rdx), %r14
	movslq	12(%rdx), %rax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movslq	16(%rdx), %r8
	movq	40(%rdx), %rax
	movq	%rax, 136(%rsp)                 # 8-byte Spill
	movq	56(%rdx), %rax
	movq	%rax, 248(%rsp)                 # 8-byte Spill
	movq	72(%rdx), %rax
	movq	%rax, 240(%rsp)                 # 8-byte Spill
	leal	(%rsi,%rsi,4), %eax
	movl	%eax, %ebp
	subl	32(%rdx), %ebp
	movq	88(%rdx), %rbx
	movq	%rbx, 184(%rsp)                 # 8-byte Spill
	subl	%eax, %edi
	movl	%edi, 8(%rsp)                   # 4-byte Spill
	cmpl	%esi, 20(%rdx)
	jle	.LBB51_27
# %bb.1:                                # %true_bb
	movl	28(%rdx), %esi
	movl	%esi, 64(%rsp)                  # 4-byte Spill
	testl	%esi, %esi
	jle	.LBB51_26
# %bb.2:                                # %"for head2_conv_1_d_def__.s0.w.preheader"
	movl	8(%rsp), %esi                   # 4-byte Reload
	cmpl	$6, %esi
	movl	$5, %r13d
	cmovll	%esi, %r13d
	movl	%r13d, %esi
	sarl	$31, %esi
	andnl	%r13d, %esi, %esi
	movq	%rsi, %rdi
	shlq	$5, %rdi
	cmpl	$67108863, %esi                 # imm = 0x3FFFFFF
	ja	.LBB51_53
# %bb.3:                                # %"assert succeeded.lr.ph.lr.ph"
	movl	%ebp, 20(%rsp)                  # 4-byte Spill
	movslq	24(%rdx), %rsi
	addl	%esi, 64(%rsp)                  # 4-byte Folded Spill
	orq	$4, %rdi
	movslq	%eax, %r15
	movq	128(%rsp), %rdx                 # 8-byte Reload
	leal	(,%rdx,8), %edx
	movl	%edx, 192(%rsp)                 # 4-byte Spill
	leaq	(%r8,%r8,2), %rcx
	leal	-1(%r13), %edx
	movq	%rdx, 24(%rsp)                  # 8-byte Spill
	shlq	$5, %rdx
	addq	$32, %rdx
	movq	%rdx, 80(%rsp)                  # 8-byte Spill
	leaq	1(%r15), %rdx
	imulq	%rcx, %rdx
	movq	%rdx, 112(%rsp)                 # 8-byte Spill
	leaq	2(%r15), %rdx
	imulq	%rcx, %rdx
	movq	%rdx, 176(%rsp)                 # 8-byte Spill
	leaq	3(%r15), %rdx
	imulq	%rcx, %rdx
	movq	%rdx, 72(%rsp)                  # 8-byte Spill
	leaq	4(%r15), %rdx
	imulq	%rcx, %rdx
	movq	%rdx, 104(%rsp)                 # 8-byte Spill
	imulq	%r15, %rcx
	movq	%rcx, 168(%rsp)                 # 8-byte Spill
	movl	%r13d, %edx
	andl	$3, %edx
	movl	%esi, %ebp
	movl	16(%rsp), %ecx                  # 4-byte Reload
	imull	%ecx, %ebp
	shll	$5, %ebp
	movl	%ebp, 12(%rsp)                  # 4-byte Spill
	shll	$5, %ecx
	movl	%ecx, 144(%rsp)                 # 4-byte Spill
	addl	%eax, %r13d
	movq	%r15, %rax
	shlq	$5, %rax
	movl	$96, %ecx
	subq	%rax, %rcx
	movq	%rcx, 160(%rsp)                 # 8-byte Spill
	movl	%edx, 224(%rsp)                 # 4-byte Spill
	movl	%edx, %r12d
	negl	%r12d
	movq	136(%rsp), %rax                 # 8-byte Reload
	addq	$12, %rax
	movq	%rax, 216(%rsp)                 # 8-byte Spill
	movq	%rdi, 120(%rsp)                 # 8-byte Spill
	jmp	.LBB51_4
	.p2align	4, 0x90
.LBB51_22:                              # %"end for head2_conv_1_d_def__.s0.c.c"
                                        #   in Loop: Header=BB51_4 Depth=1
	movq	88(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 40(%rsp)
	movq	%rbx, 32(%rsp)
	movq	152(%rsp), %rsi                 # 8-byte Reload
	incq	%rsi
	movl	144(%rsp), %eax                 # 4-byte Reload
	addl	%eax, 12(%rsp)                  # 4-byte Folded Spill
	cmpl	%esi, 64(%rsp)                  # 4-byte Folded Reload
	je	.LBB51_23
.LBB51_4:                               # %"assert succeeded.lr.ph"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB51_5 Depth 2
                                        #       Child Loop BB51_12 Depth 3
                                        #         Child Loop BB51_15 Depth 4
                                        #         Child Loop BB51_19 Depth 4
	leal	(%rsi,%rsi,2), %eax
	movq	%rax, 208(%rsp)                 # 8-byte Spill
	movq	%rsi, 152(%rsp)                 # 8-byte Spill
	leaq	(%rsi,%rsi,2), %rax
	movq	%rax, 200(%rsp)                 # 8-byte Spill
	movq	32(%rsp), %rbx
	movq	40(%rsp), %rax
	movq	%rax, 88(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	jmp	.LBB51_5
	.p2align	4, 0x90
.LBB51_21:                              # %"consume head2_relu_0_d_def__"
                                        #   in Loop: Header=BB51_5 Depth=2
	movq	208(%rsp), %rax                 # 8-byte Reload
	movq	96(%rsp), %r11                  # 8-byte Reload
	addl	%r11d, %eax
	imull	192(%rsp), %eax                 # 4-byte Folded Reload
	addl	20(%rsp), %eax                  # 4-byte Folded Reload
	movslq	%eax, %r8
	movq	240(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%r8,4), %r9
	movq	128(%rsp), %rcx                 # 8-byte Reload
	leaq	(%r9,%rcx,4), %rsi
	leaq	(%rsi,%rcx,4), %rdi
	leaq	(%rdi,%rcx,4), %rax
	leaq	(%rax,%rcx,4), %rbp
	vmovss	(%rax,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	200(%rsp), %rax                 # 8-byte Reload
	leaq	(%r11,%rax), %rax
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$32, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$48, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%rdx,%r8,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vxorps	%xmm2, %xmm2, %xmm2
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	(%rbx), %ymm0, %ymm0
	movq	168(%rsp), %rdi                 # 8-byte Reload
	addq	%rax, %rdi
	shlq	$5, %rdi
	movq	184(%rsp), %r10                 # 8-byte Reload
	vmovaps	%ymm0, (%r10,%rdi)
	leaq	(%rsi,%rcx,4), %rdi
	addq	$4, %rdi
	leaq	(%rdi,%rcx,4), %rbp
	vmovss	(%rbp,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$16, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$32, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$48, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	4(%rdx,%r8,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 4(%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	32(%rbx), %ymm0, %ymm0
	movq	112(%rsp), %rdi                 # 8-byte Reload
	addq	%rax, %rdi
	shlq	$5, %rdi
	vmovaps	%ymm0, (%r10,%rdi)
	leaq	(%rsi,%rcx,4), %rdi
	addq	$8, %rdi
	leaq	(%rdi,%rcx,4), %rbp
	vmovss	(%rbp,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$16, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$32, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$48, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	8(%rdx,%r8,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 8(%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	64(%rbx), %ymm0, %ymm0
	movq	176(%rsp), %rdi                 # 8-byte Reload
	addq	%rax, %rdi
	shlq	$5, %rdi
	vmovaps	%ymm0, (%r10,%rdi)
	leaq	(%rsi,%rcx,4), %rdi
	addq	$12, %rdi
	leaq	(%rdi,%rcx,4), %rbp
	vmovss	(%rbp,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$16, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$32, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$48, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	12(%rdx,%r8,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 12(%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	96(%rbx), %ymm0, %ymm0
	movq	72(%rsp), %rdi                  # 8-byte Reload
	addq	%rax, %rdi
	shlq	$5, %rdi
	vmovaps	%ymm0, (%r10,%rdi)
	leaq	(%rsi,%rcx,4), %rdi
	addq	$16, %rdi
	leaq	(%rdi,%rcx,4), %rbp
	vmovss	(%rbp,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$16, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$32, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rbp,%rcx,4), %rbp
	vinsertps	$48, (%rbp,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	16(%rdx,%r8,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 16(%r9,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 16(%rsi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rdi,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	128(%rbx), %ymm0, %ymm0
	addq	104(%rsp), %rax                 # 8-byte Folded Reload
	shlq	$5, %rax
	vmovaps	%ymm0, (%r10,%rax)
	incq	%r11
	movq	%r11, 96(%rsp)                  # 8-byte Spill
	cmpq	$3, %r11
	movq	120(%rsp), %rdi                 # 8-byte Reload
	je	.LBB51_22
.LBB51_5:                               # %"assert succeeded"
                                        #   Parent Loop BB51_4 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB51_12 Depth 3
                                        #         Child Loop BB51_15 Depth 4
                                        #         Child Loop BB51_19 Depth 4
	cmpq	%rdi, 88(%rsp)                  # 8-byte Folded Reload
	jb	.LBB51_6
# %bb.9:                                # %pseudostack_alloc.exit
                                        #   in Loop: Header=BB51_5 Depth=2
	cmpl	$0, 8(%rsp)                     # 4-byte Folded Reload
	jle	.LBB51_11
.LBB51_10:                              # %"for head2_relu_0_d_def__.s0.n.preheader"
                                        #   in Loop: Header=BB51_5 Depth=2
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	80(%rsp), %rdx                  # 8-byte Reload
	vzeroupper
	callq	memset@PLT
.LBB51_11:                              # %"end for head2_relu_0_d_def__.s0.n"
                                        #   in Loop: Header=BB51_5 Depth=2
	movq	96(%rsp), %rax                  # 8-byte Reload
	leaq	8(,%rax,8), %r9
	imulq	%r14, %r9
	movq	160(%rsp), %rax                 # 8-byte Reload
	leaq	(%rbx,%rax), %r8
	movl	12(%rsp), %r10d                 # 4-byte Reload
	xorl	%r11d, %r11d
	jmp	.LBB51_12
	.p2align	4, 0x90
.LBB51_20:                              # %"end for head2_relu_0_d_def__.s1.n"
                                        #   in Loop: Header=BB51_12 Depth=3
	incq	%r11
	addl	16(%rsp), %r10d                 # 4-byte Folded Reload
	cmpq	$32, %r11
	je	.LBB51_21
.LBB51_12:                              # %"for head2_relu_0_d_def__.s1.r955$x"
                                        #   Parent Loop BB51_4 Depth=1
                                        #     Parent Loop BB51_5 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB51_15 Depth 4
                                        #         Child Loop BB51_19 Depth 4
	cmpl	$0, 8(%rsp)                     # 4-byte Folded Reload
	jle	.LBB51_20
# %bb.13:                               # %"for head2_relu_0_d_def__.s1.n.preheader"
                                        #   in Loop: Header=BB51_12 Depth=3
	leaq	(%r11,%r9), %rax
	movq	248(%rsp), %rsi                 # 8-byte Reload
	leaq	(%rsi,%rax,4), %rcx
	leaq	(%rcx,%r14,4), %rdi
	leaq	(%rdi,%r14,4), %rbp
	leaq	(%rbp,%r14,4), %rdx
	vmovss	(%rdx,%r14,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	leaq	(%rdx,%r14,4), %rdx
	vinsertps	$16, (%rdx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	leaq	(%rdx,%r14,4), %rdx
	vinsertps	$32, (%rdx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%rdx,%r14,4), %rdx
	vinsertps	$48, (%rdx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%rsi,%rax,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movslq	%r10d, %rcx
	vinsertps	$48, (%rbp,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	%r15, %rbp
	cmpl	$0, 224(%rsp)                   # 4-byte Folded Reload
	je	.LBB51_17
# %bb.14:                               # %"for head2_relu_0_d_def__.s1.n.prol.preheader"
                                        #   in Loop: Header=BB51_12 Depth=3
	leaq	(%r15,%rcx), %rax
	movq	136(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %rdi
	xorl	%ebp, %ebp
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB51_15:                              # %"for head2_relu_0_d_def__.s1.n.prol"
                                        #   Parent Loop BB51_4 Depth=1
                                        #     Parent Loop BB51_5 Depth=2
                                        #       Parent Loop BB51_12 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vbroadcastss	(%rdi,%rbp), %ymm1
	vfmadd213ps	(%rbx,%rbp,8), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rbp,8)
	decq	%rax
	addq	$4, %rbp
	cmpl	%eax, %r12d
	jne	.LBB51_15
# %bb.16:                               # %"for head2_relu_0_d_def__.s1.n.prol.loopexit.loopexit"
                                        #   in Loop: Header=BB51_12 Depth=3
	movq	%r15, %rbp
	subq	%rax, %rbp
.LBB51_17:                              # %"for head2_relu_0_d_def__.s1.n.prol.loopexit"
                                        #   in Loop: Header=BB51_12 Depth=3
	cmpl	$3, 24(%rsp)                    # 4-byte Folded Reload
	jb	.LBB51_20
# %bb.18:                               # %"for head2_relu_0_d_def__.s1.n.preheader1"
                                        #   in Loop: Header=BB51_12 Depth=3
	movq	216(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx,4), %rcx
	movq	%rbp, %rdi
	shlq	$5, %rdi
	addq	%r8, %rdi
	.p2align	4, 0x90
.LBB51_19:                              # %"for head2_relu_0_d_def__.s1.n"
                                        #   Parent Loop BB51_4 Depth=1
                                        #     Parent Loop BB51_5 Depth=2
                                        #       Parent Loop BB51_12 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vbroadcastss	-12(%rcx,%rbp,4), %ymm1
	vfmadd213ps	-96(%rdi), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdi)
	vbroadcastss	-8(%rcx,%rbp,4), %ymm1
	vfmadd213ps	-64(%rdi), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdi)
	vbroadcastss	-4(%rcx,%rbp,4), %ymm1
	vfmadd213ps	-32(%rdi), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdi)
	vbroadcastss	(%rcx,%rbp,4), %ymm1
	vfmadd213ps	(%rdi), %ymm0, %ymm1    # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdi)
	addq	$4, %rbp
	subq	$-128, %rdi
	cmpl	%ebp, %r13d
	jne	.LBB51_19
	jmp	.LBB51_20
.LBB51_6:                               #   in Loop: Header=BB51_5 Depth=2
	testq	%rbx, %rbx
	movq	56(%rsp), %rbp                  # 8-byte Reload
	je	.LBB51_8
# %bb.7:                                #   in Loop: Header=BB51_5 Depth=2
	movq	%rbp, %rdi
	movq	%rbx, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB51_8:                               #   in Loop: Header=BB51_5 Depth=2
	movq	%rbp, %rdi
	movq	120(%rsp), %rbp                 # 8-byte Reload
	movq	%rbp, %rsi
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, %rbx
	movq	%rbp, 88(%rsp)                  # 8-byte Spill
	cmpl	$0, 8(%rsp)                     # 4-byte Folded Reload
	jg	.LBB51_10
	jmp	.LBB51_11
.LBB51_27:                              # %false_bb
	testl	%r8d, %r8d
	jle	.LBB51_26
# %bb.28:                               # %"for head2_conv_1_d_def__.s0.w1.preheader"
	movl	8(%rsp), %edx                   # 4-byte Reload
	cmpl	$6, %edx
	movl	$5, %r13d
	cmovll	%edx, %r13d
	xorl	%esi, %esi
	testl	%edx, %edx
	movl	$0, %edx
	cmovgl	%r13d, %edx
	movq	%rdx, %rdi
	shlq	$5, %rdi
	movq	%rdx, 216(%rsp)                 # 8-byte Spill
	cmpl	$67108863, %edx                 # imm = 0x3FFFFFF
	ja	.LBB51_53
# %bb.29:                               # %"assert succeeded8.lr.ph.lr.ph"
	orq	$4, %rdi
	movslq	%eax, %rbx
	movq	128(%rsp), %r9                  # 8-byte Reload
	leal	(,%r9,8), %ecx
	movl	%ecx, 168(%rsp)                 # 4-byte Spill
	leal	-1(%r13), %ecx
	movq	%rcx, 88(%rsp)                  # 8-byte Spill
	shlq	$5, %rcx
	addq	$32, %rcx
	movq	%rcx, 160(%rsp)                 # 8-byte Spill
	movl	%r8d, %ecx
	movl	%r13d, %edx
	andl	$3, %edx
	movl	%edx, 12(%rsp)                  # 4-byte Spill
	movl	%edx, %r12d
	negl	%r12d
	addl	%eax, %r13d
	movq	136(%rsp), %rax                 # 8-byte Reload
	addq	$12, %rax
	movq	%rax, 192(%rsp)                 # 8-byte Spill
	movq	%rbx, %rax
	shlq	$5, %rax
	movl	$96, %edx
	subq	%rax, %rdx
	movq	%rdx, 152(%rsp)                 # 8-byte Spill
	movq	%rbx, 96(%rsp)                  # 8-byte Spill
	movq	%rbx, %rax
	imulq	%rcx, %rax
	leaq	(%rax,%rax,2), %rax
	shlq	$5, %rax
	addq	%rax, 184(%rsp)                 # 8-byte Folded Spill
	movq	%rcx, 256(%rsp)                 # 8-byte Spill
	movq	%rcx, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %rax
	movq	%rax, 208(%rsp)                 # 8-byte Spill
	leal	(,%r9,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, 236(%rsp)                 # 4-byte Spill
	leaq	(,%r9,4), %rax
	movq	%rax, 200(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movq	%rdi, 104(%rsp)                 # 8-byte Spill
	.p2align	4, 0x90
.LBB51_31:                              # %"assert succeeded8.lr.ph"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB51_32 Depth 2
                                        #       Child Loop BB51_39 Depth 3
                                        #         Child Loop BB51_42 Depth 4
                                        #         Child Loop BB51_46 Depth 4
                                        #       Child Loop BB51_50 Depth 3
	movq	32(%rsp), %rbx
	movq	40(%rsp), %rax
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	movl	%ebp, 20(%rsp)                  # 4-byte Spill
	movq	%rsi, 144(%rsp)                 # 8-byte Spill
	movl	%esi, %eax
	xorl	%ecx, %ecx
	movq	%rcx, 80(%rsp)                  # 8-byte Spill
	.p2align	4, 0x90
.LBB51_32:                              # %"assert succeeded8"
                                        #   Parent Loop BB51_31 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB51_39 Depth 3
                                        #         Child Loop BB51_42 Depth 4
                                        #         Child Loop BB51_46 Depth 4
                                        #       Child Loop BB51_50 Depth 3
	movl	%eax, 176(%rsp)                 # 4-byte Spill
	movl	%eax, %r15d
	cmpq	%rdi, 72(%rsp)                  # 8-byte Folded Reload
	movl	%ebp, 112(%rsp)                 # 4-byte Spill
	jb	.LBB51_33
# %bb.36:                               # %pseudostack_alloc.exit40
                                        #   in Loop: Header=BB51_32 Depth=2
	shlq	$5, %r15
	movslq	%ebp, %rbp
	cmpl	$0, 8(%rsp)                     # 4-byte Folded Reload
	jle	.LBB51_38
.LBB51_37:                              # %"for head2_relu_0_d_def__.s0.n11.preheader"
                                        #   in Loop: Header=BB51_32 Depth=2
	movq	%rbx, %rdi
	xorl	%esi, %esi
	movq	160(%rsp), %rdx                 # 8-byte Reload
	vzeroupper
	callq	memset@PLT
.LBB51_38:                              # %"end for head2_relu_0_d_def__.s0.n12"
                                        #   in Loop: Header=BB51_32 Depth=2
	addq	184(%rsp), %r15                 # 8-byte Folded Reload
	movq	240(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rbp,4), %r10
	movq	80(%rsp), %rax                  # 8-byte Reload
	leaq	8(,%rax,8), %rax
	imulq	%r14, %rax
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	movq	152(%rsp), %rax                 # 8-byte Reload
	addq	%rbx, %rax
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	xorl	%r11d, %r11d
	vxorps	%xmm2, %xmm2, %xmm2
	jmp	.LBB51_39
	.p2align	4, 0x90
.LBB51_47:                              # %"end for head2_relu_0_d_def__.s1.n18"
                                        #   in Loop: Header=BB51_39 Depth=3
	incq	%r11
	cmpq	$32, %r11
	je	.LBB51_48
.LBB51_39:                              # %"for head2_relu_0_d_def__.s1.r955$x14"
                                        #   Parent Loop BB51_31 Depth=1
                                        #     Parent Loop BB51_32 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB51_42 Depth 4
                                        #         Child Loop BB51_46 Depth 4
	cmpl	$0, 8(%rsp)                     # 4-byte Folded Reload
	jle	.LBB51_54
# %bb.40:                               # %"for head2_relu_0_d_def__.s1.n17.preheader"
                                        #   in Loop: Header=BB51_39 Depth=3
	movq	224(%rsp), %rax                 # 8-byte Reload
	leaq	(%r11,%rax), %rcx
	movq	248(%rsp), %r9                  # 8-byte Reload
	leaq	(%r9,%rcx,4), %rdx
	leaq	(%rdx,%r14,4), %rdi
	leaq	(%rdi,%r14,4), %rax
	leaq	(%rax,%r14,4), %rbp
	leaq	(%rbp,%r14,4), %rsi
	leaq	(%rsi,%r14,4), %r8
	vmovss	(%rbp,%r14,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rsi,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%r8,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	leaq	(%r8,%r14,4), %rsi
	vinsertps	$48, (%rsi,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r9,%rcx,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdx,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdi,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rax,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	96(%rsp), %rsi                  # 8-byte Reload
	movq	%rsi, %rcx
	cmpl	$0, 12(%rsp)                    # 4-byte Folded Reload
	je	.LBB51_44
# %bb.41:                               # %"for head2_relu_0_d_def__.s1.n17.prol.preheader"
                                        #   in Loop: Header=BB51_39 Depth=3
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r11d, %eax
	imull	16(%rsp), %eax                  # 4-byte Folded Reload
	cltq
	addq	%rsi, %rax
	movq	136(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	xorl	%edi, %edi
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB51_42:                              # %"for head2_relu_0_d_def__.s1.n17.prol"
                                        #   Parent Loop BB51_31 Depth=1
                                        #     Parent Loop BB51_32 Depth=2
                                        #       Parent Loop BB51_39 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vbroadcastss	(%rcx,%rdi), %ymm1
	vfmadd213ps	(%rbx,%rdi,8), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rbx,%rdi,8)
	decq	%rdx
	addq	$4, %rdi
	cmpl	%edx, %r12d
	jne	.LBB51_42
# %bb.43:                               # %"for head2_relu_0_d_def__.s1.n17.prol.loopexit.loopexit"
                                        #   in Loop: Header=BB51_39 Depth=3
	movq	%rsi, %rcx
	subq	%rdx, %rcx
.LBB51_44:                              # %"for head2_relu_0_d_def__.s1.n17.prol.loopexit"
                                        #   in Loop: Header=BB51_39 Depth=3
	cmpl	$3, 88(%rsp)                    # 4-byte Folded Reload
	jb	.LBB51_47
# %bb.45:                               # %"for head2_relu_0_d_def__.s1.n17.preheader2"
                                        #   in Loop: Header=BB51_39 Depth=3
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	%r11d, %eax
	imull	16(%rsp), %eax                  # 4-byte Folded Reload
	cltq
	movq	192(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rax,4), %rdx
	movq	%rcx, %rdi
	shlq	$5, %rdi
	addq	120(%rsp), %rdi                 # 8-byte Folded Reload
	.p2align	4, 0x90
.LBB51_46:                              # %"for head2_relu_0_d_def__.s1.n17"
                                        #   Parent Loop BB51_31 Depth=1
                                        #     Parent Loop BB51_32 Depth=2
                                        #       Parent Loop BB51_39 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vbroadcastss	-12(%rdx,%rcx,4), %ymm1
	vfmadd213ps	-96(%rdi), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -96(%rdi)
	vbroadcastss	-8(%rdx,%rcx,4), %ymm1
	vfmadd213ps	-64(%rdi), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -64(%rdi)
	vbroadcastss	-4(%rdx,%rcx,4), %ymm1
	vfmadd213ps	-32(%rdi), %ymm0, %ymm1 # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, -32(%rdi)
	vbroadcastss	(%rdx,%rcx,4), %ymm1
	vfmadd213ps	(%rdi), %ymm0, %ymm1    # ymm1 = (ymm0 * ymm1) + mem
	vmovaps	%ymm1, (%rdi)
	addq	$4, %rcx
	subq	$-128, %rdi
	cmpl	%ecx, %r13d
	jne	.LBB51_46
	jmp	.LBB51_47
.LBB51_54:                              # %"end for head2_relu_0_d_def__.s1.n18.thread"
                                        #   in Loop: Header=BB51_39 Depth=3
	incq	%r11
	cmpq	$32, %r11
	jne	.LBB51_39
	jmp	.LBB51_51
	.p2align	4, 0x90
.LBB51_48:                              # %"consume head2_relu_0_d_def__20"
                                        #   in Loop: Header=BB51_32 Depth=2
	cmpl	$0, 8(%rsp)                     # 4-byte Folded Reload
	jle	.LBB51_51
# %bb.49:                               # %"for head2_conv_1_d_def__.s0.n.ni21.preheader"
                                        #   in Loop: Header=BB51_32 Depth=2
	movq	%rbx, %r9
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB51_50:                              # %"for head2_conv_1_d_def__.s0.n.ni21"
                                        #   Parent Loop BB51_31 Depth=1
                                        #     Parent Loop BB51_32 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	leaq	(%r10,%rdx,4), %r8
	movq	200(%rsp), %rcx                 # 8-byte Reload
	leaq	(%r8,%rcx), %rsi
	addq	%rcx, %rsi
	leaq	(%rsi,%rcx), %rdi
	addq	%rcx, %rdi
	leaq	(%rdi,%rcx), %rbp
	movq	128(%rsp), %rax                 # 8-byte Reload
	vmovss	(%rsi,%rax,8), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rdi,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, (%rbp,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	addq	%rcx, %rbp
	vinsertps	$48, (%rbp,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	(%r10,%rdx,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%r8,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%r8,%rax,8), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, (%rsi,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vcmpleps	%ymm0, %ymm2, %ymm0
	vandps	(%r9), %ymm0, %ymm0
	vmovaps	%ymm0, (%r15)
	incq	%rdx
	addq	208(%rsp), %r15                 # 8-byte Folded Reload
	addq	$32, %r9
	cmpq	%rdx, 216(%rsp)                 # 8-byte Folded Reload
	jne	.LBB51_50
.LBB51_51:                              # %"end for head2_conv_1_d_def__.s0.n.ni22"
                                        #   in Loop: Header=BB51_32 Depth=2
	movq	80(%rsp), %rdx                  # 8-byte Reload
	incq	%rdx
	movl	176(%rsp), %eax                 # 4-byte Reload
	incl	%eax
	movl	112(%rsp), %ebp                 # 4-byte Reload
	addl	168(%rsp), %ebp                 # 4-byte Folded Reload
	movq	%rdx, %rcx
	movq	%rdx, 80(%rsp)                  # 8-byte Spill
	cmpq	$3, %rdx
	movq	104(%rsp), %rdi                 # 8-byte Reload
	jne	.LBB51_32
	jmp	.LBB51_52
.LBB51_33:                              #   in Loop: Header=BB51_32 Depth=2
	testq	%rbx, %rbx
	movq	56(%rsp), %rbp                  # 8-byte Reload
	je	.LBB51_35
# %bb.34:                               #   in Loop: Header=BB51_32 Depth=2
	movq	%rbp, %rdi
	movq	%rbx, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB51_35:                              #   in Loop: Header=BB51_32 Depth=2
	movq	%rbp, %rdi
	movq	104(%rsp), %rbx                 # 8-byte Reload
	movq	%rbx, %rsi
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rbx, %rcx
	movq	%rax, %rbx
	movq	%rcx, 72(%rsp)                  # 8-byte Spill
	movl	112(%rsp), %ebp                 # 4-byte Reload
	shlq	$5, %r15
	movslq	%ebp, %rbp
	cmpl	$0, 8(%rsp)                     # 4-byte Folded Reload
	jg	.LBB51_37
	jmp	.LBB51_38
	.p2align	4, 0x90
.LBB51_52:                              # %"end for head2_conv_1_d_def__.s0.c.c5"
                                        #   in Loop: Header=BB51_31 Depth=1
	movq	72(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 40(%rsp)
	movq	%rbx, 32(%rsp)
	movq	64(%rsp), %rax                  # 8-byte Reload
	incq	%rax
	cmpq	256(%rsp), %rax                 # 8-byte Folded Reload
	je	.LBB51_23
# %bb.30:                               # %"for head2_conv_1_d_def__.s0.w1"
                                        #   in Loop: Header=BB51_31 Depth=1
	movq	%rax, 64(%rsp)                  # 8-byte Spill
	movq	24(%rsp), %rax                  # 8-byte Reload
	addl	$32, %eax
	movq	%rax, 24(%rsp)                  # 8-byte Spill
	movq	144(%rsp), %rsi                 # 8-byte Reload
	addl	$3, %esi
	movl	20(%rsp), %ebp                  # 4-byte Reload
	addl	236(%rsp), %ebp                 # 4-byte Folded Reload
	jmp	.LBB51_31
.LBB51_23:                              # %destructor_block.thread43
	movq	$0, 40(%rsp)
	movq	32(%rsp), %rsi
	testq	%rsi, %rsi
	je	.LBB51_25
# %bb.24:
	movq	56(%rsp), %rdi                  # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
.LBB51_25:                              # %pseudostack_free.exit
	movq	$0, 32(%rsp)
.LBB51_26:                              # %call_destructor.exit
	xorl	%eax, %eax
	addq	$264, %rsp                      # imm = 0x108
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.LBB51_53:                              # %"assert failed"
	leaq	.Lstr.138(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	movq	%rdi, %rdx
	movq	56(%rsp), %rdi                  # 8-byte Reload
	addq	$264, %rsp                      # imm = 0x108
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	jmp	halide_error_buffer_allocation_too_large@PLT # TAILCALL
.Lfunc_end51:
	.size	train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n, .Lfunc_end51-train_cost_model.par_for.head2_conv_1_d_def__.s0.n.n
                                        # -- End function
	.section	.text.train_cost_model.par_for.updated_head2_filter.s1.v246.v246.v246,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.updated_head2_filter.s1.v246.v246.v246
	.type	train_cost_model.par_for.updated_head2_filter.s1.v246.v246.v246,@function
train_cost_model.par_for.updated_head2_filter.s1.v246.v246.v246: # @train_cost_model.par_for.updated_head2_filter.s1.v246.v246.v246
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movl	(%rdx), %ebp
	movl	8(%rdx), %eax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	movslq	4(%rdx), %r10
	movq	32(%rdx), %rax
	movq	%rax, -104(%rsp)                # 8-byte Spill
	movq	48(%rdx), %r9
	movq	64(%rdx), %rax
	cmpl	$56, %esi
	movq	%rbp, -40(%rsp)                 # 8-byte Spill
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	jg	.LBB52_15
# %bb.1:                                # %true_bb
	movslq	28(%rdx), %rax
	movq	%rax, -96(%rsp)                 # 8-byte Spill
	movslq	20(%rdx), %rax
	movl	%esi, %ecx
	sarl	$31, %ecx
	movl	%esi, %edi
	movl	%ecx, %esi
	xorl	%edi, %esi
	imulq	$1431655766, %rsi, %rbx         # imm = 0x55555556
	movl	%edi, %esi
	shrq	$32, %rbx
	xorl	%ecx, %ebx
	leal	(%rbx,%rbx,2), %ecx
	subl	%ecx, %esi
	addl	%ebx, %ebx
	movslq	%esi, %rcx
	leaq	(%rax,%rcx,8), %rax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movslq	%ebx, %rax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	testl	%ebp, %ebp
	jle	.LBB52_2
# %bb.3:                                # %"for head2_filter_im_0_d_def__.s1.r1061$y.preheader"
	movl	24(%rdx), %ecx
	movq	%rbx, -24(%rsp)                 # 8-byte Spill
	movl	%ebx, %eax
	imull	%r10d, %eax
	movl	%ecx, -32(%rsp)                 # 4-byte Spill
	subl	%ecx, %eax
	cltq
	movq	-112(%rsp), %rdx                # 8-byte Reload
	leaq	-1(%rdx), %rcx
	movq	%rcx, -80(%rsp)                 # 8-byte Spill
	movl	%edx, %r12d
	andl	$3, %r12d
	movl	%edx, %r8d
	andl	$-4, %r8d
	imulq	$468, %r10, %r14                # imm = 0x1D4
	movq	%r9, -16(%rsp)                  # 8-byte Spill
	movq	%r9, %rbx
	leaq	(%r9,%rax,4), %r9
	imulq	$624, %r10, %rcx                # imm = 0x270
	leal	(%rdx,%rdx,2), %eax
	movl	%eax, -48(%rsp)                 # 4-byte Spill
	imulq	$312, %r10, %rdx                # imm = 0x138
	imulq	$156, %r10, %rdi
	movq	%r10, -8(%rsp)                  # 8-byte Spill
	imulq	$39, %r10, %rax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%r11d, %r11d
	movl	%esi, -28(%rsp)                 # 4-byte Spill
	jmp	.LBB52_4
	.p2align	4, 0x90
.LBB52_12:                              # %"end for head2_filter_im_0_d_def__.s1.r1061$x"
                                        #   in Loop: Header=BB52_4 Depth=1
	incq	%r11
	addq	$4, %r9
	addl	-48(%rsp), %esi                 # 4-byte Folded Reload
	movq	-40(%rsp), %rbp                 # 8-byte Reload
	cmpq	%rbp, %r11
	je	.LBB52_13
.LBB52_4:                               # %"for head2_filter_im_0_d_def__.s1.r1061$y"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB52_8 Depth 2
                                        #     Child Loop BB52_11 Depth 2
	cmpl	$0, -112(%rsp)                  # 4-byte Folded Reload
	jle	.LBB52_12
# %bb.5:                                # %"for head2_filter_im_0_d_def__.s1.r1061$x.preheader"
                                        #   in Loop: Header=BB52_4 Depth=1
	movslq	%esi, %r10
	cmpq	$3, -80(%rsp)                   # 8-byte Folded Reload
	jae	.LBB52_7
# %bb.6:                                #   in Loop: Header=BB52_4 Depth=1
	xorl	%r15d, %r15d
	jmp	.LBB52_9
	.p2align	4, 0x90
.LBB52_7:                               # %"for head2_filter_im_0_d_def__.s1.r1061$x.preheader1"
                                        #   in Loop: Header=BB52_4 Depth=1
	movq	%r10, %r13
	shlq	$5, %r13
	addq	-104(%rsp), %r13                # 8-byte Folded Reload
	movq	%r9, %rbp
	xorl	%r15d, %r15d
	.p2align	4, 0x90
.LBB52_8:                               # %"for head2_filter_im_0_d_def__.s1.r1061$x"
                                        #   Parent Loop BB52_4 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rbp), %ymm1
	vfmadd132ps	(%r13), %ymm0, %ymm1    # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbp,%rdi), %ymm0
	vfmadd132ps	96(%r13), %ymm1, %ymm0  # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	(%rbp,%rdx), %ymm1
	vfmadd132ps	192(%r13), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbp,%r14), %ymm0
	vfmadd132ps	288(%r13), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	addq	$4, %r15
	addq	%rcx, %rbp
	addq	$384, %r13                      # imm = 0x180
	cmpq	%r15, %r8
	jne	.LBB52_8
.LBB52_9:                               # %"end for head2_filter_im_0_d_def__.s1.r1061$x.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB52_4 Depth=1
	testq	%r12, %r12
	je	.LBB52_12
# %bb.10:                               # %"for head2_filter_im_0_d_def__.s1.r1061$x.epil.preheader"
                                        #   in Loop: Header=BB52_4 Depth=1
	shlq	$3, %r10
	movq	-88(%rsp), %rax                 # 8-byte Reload
	imulq	%r15, %rax
	leaq	(%r9,%rax,4), %rbx
	leaq	(%r15,%r15,2), %rax
	leaq	(%r10,%rax,8), %rax
	movq	-104(%rsp), %rbp                # 8-byte Reload
	leaq	(%rbp,%rax,4), %rbp
	movq	%r12, %rax
	.p2align	4, 0x90
.LBB52_11:                              # %"for head2_filter_im_0_d_def__.s1.r1061$x.epil"
                                        #   Parent Loop BB52_4 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rbx), %ymm1
	vfmadd231ps	(%rbp), %ymm1, %ymm0    # ymm0 = (ymm1 * mem) + ymm0
	addq	%rdi, %rbx
	addq	$96, %rbp
	decq	%rax
	jne	.LBB52_11
	jmp	.LBB52_12
.LBB52_15:                              # %"produce head2_filter_im_0_d_def__1"
	movslq	16(%rdx), %rax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	movl	%esi, %eax
	imulq	$1431655766, %rax, %rax         # imm = 0x55555556
	shrq	$32, %rax
	leal	(%rax,%rax,2), %ecx
	movl	%esi, %edi
	subl	%ecx, %edi
	movl	%edi, -96(%rsp)                 # 4-byte Spill
	testl	%ebp, %ebp
	jle	.LBB52_16
# %bb.17:                               # %"for head2_filter_im_0_d_def__.s1.r1061$y2.preheader"
	movslq	12(%rdx), %rcx
	movq	-112(%rsp), %rbp                # 8-byte Reload
	leaq	-1(%rbp), %rdx
	movq	%rdx, -80(%rsp)                 # 8-byte Spill
	movl	%ebp, %r13d
	andl	$3, %r13d
	movl	%ebp, %r14d
	andl	$-4, %r14d
	imulq	$468, %r10, %rdi                # imm = 0x1D4
	leaq	(%r9,%rcx,4), %r9
	imulq	$624, %r10, %rdx                # imm = 0x270
	leal	(%rax,%rax,2), %eax
	subl	%eax, %esi
	leal	(%rbp,%rbp,2), %r12d
	imulq	$312, %r10, %rax                # imm = 0x138
	imulq	$156, %r10, %rbx
	imulq	$39, %r10, %rcx
	movq	%rcx, -88(%rsp)                 # 8-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%r11d, %r11d
	jmp	.LBB52_18
.LBB52_13:                              # %"consume head2_filter_im_0_d_def__"
	movq	-64(%rsp), %rax                 # 8-byte Reload
	imulq	-96(%rsp), %rax                 # 8-byte Folded Reload
	addq	-72(%rsp), %rax                 # 8-byte Folded Reload
	movq	-56(%rsp), %r9                  # 8-byte Reload
	vmovups	%ymm0, (%r9,%rax,4)
	testl	%ebp, %ebp
	jle	.LBB52_14
# %bb.30:                               # %"for head2_filter_im_0_d_def__.s1.r1061$y.preheader.1"
	movq	-24(%rsp), %rax                 # 8-byte Reload
	orl	$1, %eax
	imull	-8(%rsp), %eax                  # 4-byte Folded Reload
	subl	-32(%rsp), %eax                 # 4-byte Folded Reload
	movslq	%eax, %rsi
	movq	-112(%rsp), %rbp                # 8-byte Reload
	movl	%ebp, %r8d
	andl	$3, %r8d
	movl	%ebp, %eax
	andl	$-4, %eax
	movq	-16(%rsp), %rbx                 # 8-byte Reload
	leaq	(%rbx,%rsi,4), %r10
	leal	(%rbp,%rbp,2), %r11d
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%r12d, %r12d
	movl	-28(%rsp), %esi                 # 4-byte Reload
	jmp	.LBB52_31
.LBB52_19:                              #   in Loop: Header=BB52_18 Depth=1
	movl	%esi, %r15d
	.p2align	4, 0x90
.LBB52_27:                              # %"end for head2_filter_im_0_d_def__.s1.r1061$x6"
                                        #   in Loop: Header=BB52_18 Depth=1
	incq	%r11
	addq	$4, %r9
	movl	%r15d, %esi
	addl	%r12d, %esi
	cmpq	-40(%rsp), %r11                 # 8-byte Folded Reload
	je	.LBB52_28
.LBB52_18:                              # %"for head2_filter_im_0_d_def__.s1.r1061$y2"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB52_23 Depth 2
                                        #     Child Loop BB52_26 Depth 2
	cmpl	$0, -112(%rsp)                  # 4-byte Folded Reload
	jle	.LBB52_19
# %bb.20:                               # %"for head2_filter_im_0_d_def__.s1.r1061$x5.preheader"
                                        #   in Loop: Header=BB52_18 Depth=1
	movl	%esi, %r15d
	movslq	%esi, %r10
	cmpq	$3, -80(%rsp)                   # 8-byte Folded Reload
	jae	.LBB52_22
# %bb.21:                               #   in Loop: Header=BB52_18 Depth=1
	xorl	%r8d, %r8d
	jmp	.LBB52_24
	.p2align	4, 0x90
.LBB52_22:                              # %"for head2_filter_im_0_d_def__.s1.r1061$x5.preheader2"
                                        #   in Loop: Header=BB52_18 Depth=1
	movq	%r10, %rsi
	shlq	$5, %rsi
	addq	-104(%rsp), %rsi                # 8-byte Folded Reload
	movq	%r9, %rbp
	xorl	%r8d, %r8d
	.p2align	4, 0x90
.LBB52_23:                              # %"for head2_filter_im_0_d_def__.s1.r1061$x5"
                                        #   Parent Loop BB52_18 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rbp), %ymm1
	vfmadd132ps	(%rsi), %ymm0, %ymm1    # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbp,%rbx), %ymm0
	vfmadd132ps	96(%rsi), %ymm1, %ymm0  # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	(%rbp,%rax), %ymm1
	vfmadd132ps	192(%rsi), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbp,%rdi), %ymm0
	vfmadd132ps	288(%rsi), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	addq	$4, %r8
	addq	%rdx, %rbp
	addq	$384, %rsi                      # imm = 0x180
	cmpq	%r8, %r14
	jne	.LBB52_23
.LBB52_24:                              # %"end for head2_filter_im_0_d_def__.s1.r1061$x6.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB52_18 Depth=1
	testq	%r13, %r13
	je	.LBB52_27
# %bb.25:                               # %"for head2_filter_im_0_d_def__.s1.r1061$x5.epil.preheader"
                                        #   in Loop: Header=BB52_18 Depth=1
	shlq	$3, %r10
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	imulq	%r8, %rcx
	leaq	(%r9,%rcx,4), %rsi
	leaq	(%r8,%r8,2), %rcx
	leaq	(%r10,%rcx,8), %rcx
	movq	-104(%rsp), %rbp                # 8-byte Reload
	leaq	(%rbp,%rcx,4), %rbp
	movq	%r13, %rcx
	.p2align	4, 0x90
.LBB52_26:                              # %"for head2_filter_im_0_d_def__.s1.r1061$x5.epil"
                                        #   Parent Loop BB52_18 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rsi), %ymm1
	vfmadd231ps	(%rbp), %ymm1, %ymm0    # ymm0 = (ymm1 * mem) + ymm0
	addq	%rbx, %rsi
	addq	$96, %rbp
	decq	%rcx
	jne	.LBB52_26
	jmp	.LBB52_27
.LBB52_32:                              #   in Loop: Header=BB52_31 Depth=1
	movl	%esi, %r13d
	.p2align	4, 0x90
.LBB52_40:                              # %"end for head2_filter_im_0_d_def__.s1.r1061$x.1"
                                        #   in Loop: Header=BB52_31 Depth=1
	incq	%r12
	addq	$4, %r10
	movl	%r13d, %esi
	addl	%r11d, %esi
	cmpq	-40(%rsp), %r12                 # 8-byte Folded Reload
	je	.LBB52_41
.LBB52_31:                              # %"for head2_filter_im_0_d_def__.s1.r1061$y.1"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB52_36 Depth 2
                                        #     Child Loop BB52_39 Depth 2
	cmpl	$0, -112(%rsp)                  # 4-byte Folded Reload
	jle	.LBB52_32
# %bb.33:                               # %"for head2_filter_im_0_d_def__.s1.r1061$x.preheader.1"
                                        #   in Loop: Header=BB52_31 Depth=1
	movl	%esi, %r13d
	movslq	%esi, %r15
	cmpq	$3, -80(%rsp)                   # 8-byte Folded Reload
	jae	.LBB52_35
# %bb.34:                               #   in Loop: Header=BB52_31 Depth=1
	xorl	%esi, %esi
	jmp	.LBB52_37
	.p2align	4, 0x90
.LBB52_35:                              # %"for head2_filter_im_0_d_def__.s1.r1061$x.1.preheader"
                                        #   in Loop: Header=BB52_31 Depth=1
	movq	%r15, %rbp
	shlq	$5, %rbp
	addq	-104(%rsp), %rbp                # 8-byte Folded Reload
	movq	%r10, %rbx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB52_36:                              # %"for head2_filter_im_0_d_def__.s1.r1061$x.1"
                                        #   Parent Loop BB52_31 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rbx), %ymm1
	vfmadd132ps	(%rbp), %ymm0, %ymm1    # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbx,%rdi), %ymm0
	vfmadd132ps	96(%rbp), %ymm1, %ymm0  # ymm0 = (ymm0 * mem) + ymm1
	vbroadcastss	(%rbx,%rdx), %ymm1
	vfmadd132ps	192(%rbp), %ymm0, %ymm1 # ymm1 = (ymm1 * mem) + ymm0
	vbroadcastss	(%rbx,%r14), %ymm0
	vfmadd132ps	288(%rbp), %ymm1, %ymm0 # ymm0 = (ymm0 * mem) + ymm1
	addq	$4, %rsi
	addq	%rcx, %rbx
	addq	$384, %rbp                      # imm = 0x180
	cmpq	%rsi, %rax
	jne	.LBB52_36
.LBB52_37:                              # %"end for head2_filter_im_0_d_def__.s1.r1061$x.1.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB52_31 Depth=1
	testq	%r8, %r8
	je	.LBB52_40
# %bb.38:                               # %"for head2_filter_im_0_d_def__.s1.r1061$x.1.epil.preheader"
                                        #   in Loop: Header=BB52_31 Depth=1
	shlq	$3, %r15
	movq	-88(%rsp), %rbp                 # 8-byte Reload
	imulq	%rsi, %rbp
	leaq	(%r10,%rbp,4), %rbp
	leaq	(%rsi,%rsi,2), %rsi
	leaq	(%r15,%rsi,8), %rsi
	movq	-104(%rsp), %rbx                # 8-byte Reload
	leaq	(%rbx,%rsi,4), %rsi
	movq	%r8, %rbx
	.p2align	4, 0x90
.LBB52_39:                              # %"for head2_filter_im_0_d_def__.s1.r1061$x.1.epil"
                                        #   Parent Loop BB52_31 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vbroadcastss	(%rbp), %ymm1
	vfmadd231ps	(%rsi), %ymm1, %ymm0    # ymm0 = (ymm1 * mem) + ymm0
	addq	%rdi, %rbp
	addq	$96, %rsi
	decq	%rbx
	jne	.LBB52_39
	jmp	.LBB52_40
.LBB52_2:                               # %"consume head2_filter_im_0_d_def__.thread"
	movq	-64(%rsp), %rax                 # 8-byte Reload
	imulq	-96(%rsp), %rax                 # 8-byte Folded Reload
	addq	-72(%rsp), %rax                 # 8-byte Folded Reload
	vxorps	%xmm0, %xmm0, %xmm0
	movq	-56(%rsp), %r9                  # 8-byte Reload
	vmovups	%ymm0, (%r9,%rax,4)
	jmp	.LBB52_41
.LBB52_16:
	vxorps	%xmm0, %xmm0, %xmm0
.LBB52_28:                              # %"consume head2_filter_im_0_d_def__8"
	movslq	-96(%rsp), %rax                 # 4-byte Folded Reload
	movq	-48(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,8), %rax
	movq	-56(%rsp), %r9                  # 8-byte Reload
	jmp	.LBB52_29
.LBB52_14:
	vxorps	%xmm0, %xmm0, %xmm0
.LBB52_41:                              # %"consume head2_filter_im_0_d_def__.1"
	movq	-64(%rsp), %rax                 # 8-byte Reload
	orq	$1, %rax
	imulq	-96(%rsp), %rax                 # 8-byte Folded Reload
	addq	-72(%rsp), %rax                 # 8-byte Folded Reload
.LBB52_29:                              # %destructor_block
	vmovups	%ymm0, (%r9,%rax,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end52:
	.size	train_cost_model.par_for.updated_head2_filter.s1.v246.v246.v246, .Lfunc_end52-train_cost_model.par_for.updated_head2_filter.s1.v246.v246.v246
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_head2_filter.s2.v246.v246.v246
.LCPI53_0:
	.long	0x3f666666                      # float 0.899999976
.LCPI53_1:
	.long	0x3dcccccd                      # float 0.100000001
	.section	.text.train_cost_model.par_for.updated_head2_filter.s2.v246.v246.v246,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_head2_filter.s2.v246.v246.v246,@function
train_cost_model.par_for.updated_head2_filter.s2.v246.v246.v246: # @train_cost_model.par_for.updated_head2_filter.s2.v246.v246.v246
# %bb.0:                                # %entry
                                        # kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r8
	movslq	4(%rdx), %rcx
	movq	8(%rdx), %rax
	cmpl	$56, %esi
	jg	.LBB53_2
# %bb.1:                                # %true_bb
	movl	%esi, %edx
	sarl	$31, %edx
	movl	%edx, %edi
	xorl	%esi, %edi
	imulq	$1431655766, %rdi, %rdi         # imm = 0x55555556
	shrq	$32, %rdi
	xorl	%edx, %edi
	leal	(%rdi,%rdi,2), %edx
                                        # kill: def $edi killed $edi killed $rdi
	addl	%edi, %edi
	subl	%edx, %esi
	shll	$3, %esi
	leaq	(%rcx,%rcx,2), %r9
	movslq	%edi, %rdx
	movslq	%esi, %r10
	movq	%rdx, %rdi
	imulq	%r8, %rdi
	addq	%r10, %rdi
	vbroadcastss	.LCPI53_0(%rip), %ymm0  # ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%r9,%rdi), %rsi
	addq	%rcx, %rdi
	vbroadcastss	.LCPI53_1(%rip), %ymm1  # ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rax,%rsi,4), %ymm1, %ymm1
	vfmadd231ps	(%rax,%rdi,4), %ymm0, %ymm1 # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rdi,4)
	orq	$1, %rdx
	imulq	%r8, %rdx
	addq	%r10, %rdx
	addq	%rdx, %rcx
	leaq	(%rax,%rcx,4), %rsi
	addq	%r9, %rdx
	jmp	.LBB53_3
.LBB53_2:                               # %false_bb
	imull	$38, %r8d, %edx
	movl	%esi, %edi
	imulq	$1431655766, %rdi, %rdi         # imm = 0x55555556
	shrq	$32, %rdi
	leal	(%rdi,%rdi,2), %edi
	subl	%edi, %esi
	leal	(%rdx,%rsi,8), %edx
	movslq	%edx, %rdi
	leaq	(%rdi,%rcx), %rdx
	leaq	(%rax,%rdx,4), %rsi
	leaq	(%rcx,%rcx,2), %rdx
	addq	%rdi, %rdx
.LBB53_3:                               # %destructor_block
	vbroadcastss	.LCPI53_0(%rip), %ymm0  # ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	vbroadcastss	.LCPI53_1(%rip), %ymm1  # ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rax,%rdx,4), %ymm1, %ymm1
	vfmadd231ps	(%rsi), %ymm0, %ymm1    # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rsi)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end53:
	.size	train_cost_model.par_for.updated_head2_filter.s2.v246.v246.v246, .Lfunc_end53-train_cost_model.par_for.updated_head2_filter.s2.v246.v246.v246
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_head2_filter.s3.v246.v246.v246
.LCPI54_0:
	.long	0x3f7fbe77                      # float 0.999000012
.LCPI54_1:
	.long	0x3a83126f                      # float 0.00100000005
	.section	.text.train_cost_model.par_for.updated_head2_filter.s3.v246.v246.v246,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_head2_filter.s3.v246.v246.v246,@function
train_cost_model.par_for.updated_head2_filter.s3.v246.v246.v246: # @train_cost_model.par_for.updated_head2_filter.s3.v246.v246.v246
# %bb.0:                                # %entry
                                        # kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %rdi
	movslq	4(%rdx), %r8
	movslq	8(%rdx), %r9
	movq	16(%rdx), %rax
	cmpl	$56, %esi
	jg	.LBB54_2
# %bb.1:                                # %true_bb
	movl	%esi, %edx
	sarl	$31, %edx
	movl	%edx, %ecx
	xorl	%esi, %ecx
	imulq	$1431655766, %rcx, %rcx         # imm = 0x55555556
	shrq	$32, %rcx
	xorl	%edx, %ecx
	leal	(%rcx,%rcx,2), %edx
                                        # kill: def $ecx killed $ecx killed $rcx
	addl	%ecx, %ecx
	subl	%edx, %esi
	shll	$3, %esi
	movslq	%ecx, %rcx
	movslq	%esi, %r10
	movq	%rcx, %rsi
	imulq	%r9, %rsi
	addq	%r10, %rsi
	leaq	(%rsi,%rdi), %rdx
	vmovups	(%rax,%rdx,4), %ymm0
	addq	%r8, %rsi
	vbroadcastss	.LCPI54_0(%rip), %ymm1  # ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	.LCPI54_1(%rip), %ymm2  # ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rax,%rsi,4), %ymm1, %ymm0 # ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rax,%rsi,4)
	orq	$1, %rcx
	imulq	%r9, %rcx
	addq	%r10, %rcx
	addq	%rcx, %rdi
	vmovups	(%rax,%rdi,4), %ymm0
	addq	%r8, %rcx
	jmp	.LBB54_3
.LBB54_2:                               # %false_bb
	imull	$38, %r9d, %ecx
	movl	%esi, %edx
	imulq	$1431655766, %rdx, %rdx         # imm = 0x55555556
	shrq	$32, %rdx
	leal	(%rdx,%rdx,2), %edx
	subl	%edx, %esi
	leal	(%rcx,%rsi,8), %ecx
	movslq	%ecx, %rcx
	addq	%rcx, %rdi
	vmovups	(%rax,%rdi,4), %ymm0
	addl	%r8d, %ecx
	movslq	%ecx, %rcx
.LBB54_3:                               # %destructor_block
	leaq	(%rax,%rcx,4), %rax
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	.LCPI54_0(%rip), %ymm1  # ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vbroadcastss	.LCPI54_1(%rip), %ymm2  # ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rax), %ymm1, %ymm0    # ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rax)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end54:
	.size	train_cost_model.par_for.updated_head2_filter.s3.v246.v246.v246, .Lfunc_end54-train_cost_model.par_for.updated_head2_filter.s3.v246.v246.v246
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_head2_filter.s4.v246.v246.v246
.LCPI55_0:
	.long	0xb727c5ac                      # float -9.99999974E-6
	.section	.text.train_cost_model.par_for.updated_head2_filter.s4.v246.v246.v246,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_head2_filter.s4.v246.v246.v246,@function
train_cost_model.par_for.updated_head2_filter.s4.v246.v246.v246: # @train_cost_model.par_for.updated_head2_filter.s4.v246.v246.v246
# %bb.0:                                # %entry
	pushq	%r15
	pushq	%r14
	pushq	%rbx
                                        # kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r9
	vmovss	4(%rdx), %xmm1                  # xmm1 = mem[0],zero,zero,zero
	vmovss	8(%rdx), %xmm2                  # xmm2 = mem[0],zero,zero,zero
	vmovss	12(%rdx), %xmm0                 # xmm0 = mem[0],zero,zero,zero
	movl	16(%rdx), %r10d
	movslq	20(%rdx), %r14
	movq	24(%rdx), %r8
	movq	40(%rdx), %r15
	cmpl	$56, %esi
	jg	.LBB55_2
# %bb.1:                                # %true_bb
	movl	%esi, %eax
	sarl	$31, %eax
	movl	%eax, %edx
	xorl	%esi, %edx
	imulq	$1431655766, %rdx, %rdx         # imm = 0x55555556
	shrq	$32, %rdx
	xorl	%eax, %edx
	leal	(%rdx,%rdx,2), %r11d
	leal	(%rdx,%rdx), %eax
	incl	%eax
                                        # kill: def $edx killed $edx killed $rdx
	addl	%edx, %edx
	subl	%r11d, %esi
	leal	(,%rsi,8), %ebx
	movslq	%edx, %rdi
	movslq	%ebx, %r11
	vmulss	%xmm2, %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vbroadcastss	%xmm0, %ymm0
	movl	%edi, %edx
	imull	%r10d, %edx
	leal	(%rdx,%rsi,8), %edx
	movq	%rdi, %rbx
	imulq	%r9, %rbx
	movslq	%edx, %rdx
	leaq	(%rdx,%r14), %rcx
	vmulps	(%r15,%rcx,4), %ymm1, %ymm2
	addq	%r11, %rbx
	leaq	(%rdx,%r14,2), %rcx
	vmulps	(%r15,%rcx,4), %ymm0, %ymm3
	vsqrtps	%ymm3, %ymm3
	vbroadcastss	.LCPI55_0(%rip), %ymm4  # ymm4 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm3, %ymm4, %ymm3
	vdivps	%ymm3, %ymm2, %ymm2
	vaddps	(%r8,%rbx,4), %ymm2, %ymm2
	vmovups	%ymm2, (%r15,%rdx,4)
	imull	%r10d, %eax
	leal	(%rax,%rsi,8), %eax
	orq	$1, %rdi
	imulq	%r9, %rdi
	cltq
	leaq	(%rax,%r14), %rcx
	vmulps	(%r15,%rcx,4), %ymm1, %ymm1
	addq	%r11, %rdi
	leaq	(%rax,%r14,2), %rcx
	jmp	.LBB55_3
.LBB55_2:                               # %false_bb
	imull	$38, %r10d, %eax
	movl	%esi, %ecx
	imulq	$1431655766, %rcx, %rcx         # imm = 0x55555556
	shrq	$32, %rcx
	leal	(%rcx,%rcx,2), %ecx
	subl	%ecx, %esi
	leal	(%rax,%rsi,8), %eax
	imulq	$38, %r9, %rcx
	movslq	%esi, %rdx
	leaq	(%rcx,%rdx,8), %rdi
	cltq
	leaq	(%rax,%r14), %rcx
	vmulss	%xmm2, %xmm1, %xmm1
	vbroadcastss	%xmm1, %ymm1
	vmulps	(%r15,%rcx,4), %ymm1, %ymm1
	leaq	(%rax,%r14,2), %rcx
	vbroadcastss	%xmm0, %ymm0
.LBB55_3:                               # %destructor_block
	vmulps	(%r15,%rcx,4), %ymm0, %ymm0
	vsqrtps	%ymm0, %ymm0
	vbroadcastss	.LCPI55_0(%rip), %ymm2  # ymm2 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm0, %ymm2, %ymm0
	vdivps	%ymm0, %ymm1, %ymm0
	vaddps	(%r8,%rdi,4), %ymm0, %ymm0
	vmovups	%ymm0, (%r15,%rax,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	popq	%r15
	vzeroupper
	retq
.Lfunc_end55:
	.size	train_cost_model.par_for.updated_head2_filter.s4.v246.v246.v246, .Lfunc_end55-train_cost_model.par_for.updated_head2_filter.s4.v246.v246.v246
                                        # -- End function
	.section	.text.train_cost_model.par_for.updated_head2_bias.s1.v249.v249,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.updated_head2_bias.s1.v249.v249
	.type	train_cost_model.par_for.updated_head2_bias.s1.v249.v249,@function
train_cost_model.par_for.updated_head2_bias.s1.v249.v249: # @train_cost_model.par_for.updated_head2_bias.s1.v249.v249
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	movl	(%rdx), %r10d
	movslq	8(%rdx), %r8
	movq	32(%rdx), %r9
	testl	%r10d, %r10d
	jle	.LBB56_1
# %bb.2:                                # %"for head2_bias_im_0_d_def__.s1.r1082$y.preheader"
	movl	4(%rdx), %r15d
	movq	16(%rdx), %r12
	leaq	-1(%r15), %r11
	movl	%r15d, %r13d
	andl	$7, %r13d
	movl	%r15d, %edi
	andl	$-8, %edi
	leal	(%r15,%r15,2), %r14d
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%ebx, %ebx
	movl	%esi, %ebp
	jmp	.LBB56_3
	.p2align	4, 0x90
.LBB56_11:                              # %"end for head2_bias_im_0_d_def__.s1.r1082$x"
                                        #   in Loop: Header=BB56_3 Depth=1
	incq	%rbx
	addl	%r14d, %ebp
	cmpq	%r10, %rbx
	je	.LBB56_12
.LBB56_3:                               # %"for head2_bias_im_0_d_def__.s1.r1082$y"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB56_7 Depth 2
                                        #     Child Loop BB56_10 Depth 2
	testl	%r15d, %r15d
	jle	.LBB56_11
# %bb.4:                                # %"for head2_bias_im_0_d_def__.s1.r1082$x.preheader"
                                        #   in Loop: Header=BB56_3 Depth=1
	movslq	%ebp, %rax
	cmpq	$7, %r11
	jae	.LBB56_6
# %bb.5:                                #   in Loop: Header=BB56_3 Depth=1
	xorl	%edx, %edx
	jmp	.LBB56_8
	.p2align	4, 0x90
.LBB56_6:                               # %"for head2_bias_im_0_d_def__.s1.r1082$x.preheader1"
                                        #   in Loop: Header=BB56_3 Depth=1
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r12, %rcx
	xorl	%edx, %edx
	.p2align	4, 0x90
.LBB56_7:                               # %"for head2_bias_im_0_d_def__.s1.r1082$x"
                                        #   Parent Loop BB56_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vaddps	(%rcx), %ymm0, %ymm0
	vaddps	96(%rcx), %ymm0, %ymm0
	vaddps	192(%rcx), %ymm0, %ymm0
	vaddps	288(%rcx), %ymm0, %ymm0
	vaddps	384(%rcx), %ymm0, %ymm0
	vaddps	480(%rcx), %ymm0, %ymm0
	vaddps	576(%rcx), %ymm0, %ymm0
	vaddps	672(%rcx), %ymm0, %ymm0
	addq	$8, %rdx
	addq	$768, %rcx                      # imm = 0x300
	cmpq	%rdx, %rdi
	jne	.LBB56_7
.LBB56_8:                               # %"end for head2_bias_im_0_d_def__.s1.r1082$x.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB56_3 Depth=1
	testq	%r13, %r13
	je	.LBB56_11
# %bb.9:                                # %"for head2_bias_im_0_d_def__.s1.r1082$x.epil.preheader"
                                        #   in Loop: Header=BB56_3 Depth=1
	shlq	$3, %rax
	leaq	(%rdx,%rdx,2), %rcx
	leaq	(%rax,%rcx,8), %rax
	leaq	(%r12,%rax,4), %rax
	movq	%r13, %rcx
	.p2align	4, 0x90
.LBB56_10:                              # %"for head2_bias_im_0_d_def__.s1.r1082$x.epil"
                                        #   Parent Loop BB56_3 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vaddps	(%rax), %ymm0, %ymm0
	addq	$96, %rax
	decq	%rcx
	jne	.LBB56_10
	jmp	.LBB56_11
.LBB56_1:
	vxorps	%xmm0, %xmm0, %xmm0
.LBB56_12:                              # %"consume head2_bias_im_0_d_def__"
	movslq	%esi, %rax
	leaq	(%r8,%rax,8), %rax
	vmovups	%ymm0, (%r9,%rax,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end56:
	.size	train_cost_model.par_for.updated_head2_bias.s1.v249.v249, .Lfunc_end56-train_cost_model.par_for.updated_head2_bias.s1.v249.v249
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_head2_bias.s2.v249.v249
.LCPI57_0:
	.long	0x3f666666                      # float 0.899999976
.LCPI57_1:
	.long	0x3dcccccd                      # float 0.100000001
	.section	.text.train_cost_model.par_for.updated_head2_bias.s2.v249.v249,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_head2_bias.s2.v249.v249,@function
train_cost_model.par_for.updated_head2_bias.s2.v249.v249: # @train_cost_model.par_for.updated_head2_bias.s2.v249.v249
# %bb.0:                                # %entry
	movslq	(%rdx), %rax
	movq	8(%rdx), %rcx
	movslq	%esi, %rdx
	leaq	(%rax,%rdx,8), %rsi
	vbroadcastss	.LCPI57_0(%rip), %ymm0  # ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%rax,%rax,2), %rax
	leaq	(%rax,%rdx,8), %rax
	vbroadcastss	.LCPI57_1(%rip), %ymm1  # ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rcx,%rax,4), %ymm1, %ymm1
	vfmadd231ps	(%rcx,%rsi,4), %ymm0, %ymm1 # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rcx,%rsi,4)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end57:
	.size	train_cost_model.par_for.updated_head2_bias.s2.v249.v249, .Lfunc_end57-train_cost_model.par_for.updated_head2_bias.s2.v249.v249
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_head2_bias.s3.v249.v249
.LCPI58_0:
	.long	0x3f7fbe77                      # float 0.999000012
.LCPI58_1:
	.long	0x3a83126f                      # float 0.00100000005
	.section	.text.train_cost_model.par_for.updated_head2_bias.s3.v249.v249,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_head2_bias.s3.v249.v249,@function
train_cost_model.par_for.updated_head2_bias.s3.v249.v249: # @train_cost_model.par_for.updated_head2_bias.s3.v249.v249
# %bb.0:                                # %entry
	movslq	(%rdx), %rax
	movq	8(%rdx), %rcx
	leaq	(%rax,%rax,2), %rdx
	movslq	%esi, %rsi
	leaq	(%rdx,%rsi,8), %rdx
	vmovups	(%rcx,%rdx,4), %ymm0
	leaq	(%rax,%rsi,4), %rax
	vbroadcastss	.LCPI58_0(%rip), %ymm1  # ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	.LCPI58_1(%rip), %ymm2  # ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rcx,%rax,8), %ymm1, %ymm0 # ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rcx,%rax,8)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end58:
	.size	train_cost_model.par_for.updated_head2_bias.s3.v249.v249, .Lfunc_end58-train_cost_model.par_for.updated_head2_bias.s3.v249.v249
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_head2_bias.s4.v249.v249
.LCPI59_0:
	.long	0xb727c5ac                      # float -9.99999974E-6
	.section	.text.train_cost_model.par_for.updated_head2_bias.s4.v249.v249,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_head2_bias.s4.v249.v249,@function
train_cost_model.par_for.updated_head2_bias.s4.v249.v249: # @train_cost_model.par_for.updated_head2_bias.s4.v249.v249
# %bb.0:                                # %entry
	vmovss	(%rdx), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	movslq	12(%rdx), %rax
	movq	32(%rdx), %rcx
	movslq	%esi, %rsi
	vmulss	4(%rdx), %xmm0, %xmm0
	leaq	(%rax,%rsi,8), %rdi
	vbroadcastss	%xmm0, %ymm0
	vmulps	(%rcx,%rdi,4), %ymm0, %ymm0
	leaq	(%rax,%rsi,4), %rax
	vbroadcastss	8(%rdx), %ymm1
	vmulps	(%rcx,%rax,8), %ymm1, %ymm1
	vsqrtps	%ymm1, %ymm1
	vbroadcastss	.LCPI59_0(%rip), %ymm2  # ymm2 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm1, %ymm2, %ymm1
	vdivps	%ymm1, %ymm0, %ymm0
	movq	16(%rdx), %rax
	shlq	$5, %rsi
	vaddps	(%rax,%rsi), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rsi)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end59:
	.size	train_cost_model.par_for.updated_head2_bias.s4.v249.v249, .Lfunc_end59-train_cost_model.par_for.updated_head2_bias.s4.v249.v249
                                        # -- End function
	.section	.text.train_cost_model.par_for.filter1_im_0_d_def__.s0.v18,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.filter1_im_0_d_def__.s0.v18
	.type	train_cost_model.par_for.filter1_im_0_d_def__.s0.v18,@function
train_cost_model.par_for.filter1_im_0_d_def__.s0.v18: # @train_cost_model.par_for.filter1_im_0_d_def__.s0.v18
# %bb.0:                                # %entry
	movq	(%rdx), %rax
	shll	$2, %esi
	movslq	%esi, %rcx
	shlq	$5, %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%ymm0, 96(%rax,%rcx)
	vmovaps	%ymm0, 64(%rax,%rcx)
	vmovaps	%ymm0, 32(%rax,%rcx)
	vmovaps	%ymm0, (%rax,%rcx)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end60:
	.size	train_cost_model.par_for.filter1_im_0_d_def__.s0.v18, .Lfunc_end60-train_cost_model.par_for.filter1_im_0_d_def__.s0.v18
                                        # -- End function
	.section	.text.train_cost_model.par_for.filter1_im_0_d_def__.s1.v18,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.filter1_im_0_d_def__.s1.v18
	.type	train_cost_model.par_for.filter1_im_0_d_def__.s1.v18,@function
train_cost_model.par_for.filter1_im_0_d_def__.s1.v18: # @train_cost_model.par_for.filter1_im_0_d_def__.s1.v18
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$88, %rsp
	movl	(%rdx), %eax
	movq	%rax, -104(%rsp)                # 8-byte Spill
	movl	12(%rdx), %r9d
	movslq	4(%rdx), %r11
	movslq	8(%rdx), %rbp
	movq	24(%rdx), %rax
	movq	40(%rdx), %r14
	movq	56(%rdx), %r10
	movslq	%esi, %rcx
	shll	$5, %esi
	imulq	%r11, %rcx
	movq	%r11, %rdi
	shlq	$5, %rdi
	movslq	%esi, %rsi
	movq	%rsi, -64(%rsp)                 # 8-byte Spill
	movq	%r9, %rsi
	decq	%rsi
	movq	%rsi, 32(%rsp)                  # 8-byte Spill
	xorl	%esi, %esi
	subl	16(%rdx), %esi
	movl	%esi, -124(%rsp)                # 4-byte Spill
	leaq	(%r11,%r11,2), %rdx
	shlq	$5, %rdx
	addq	%rcx, %rdx
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, -24(%rsp)                 # 8-byte Spill
	movq	%r11, %rdx
	shlq	$6, %rdx
	addq	%rcx, %rdx
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, -32(%rsp)                 # 8-byte Spill
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	leaq	(%rdi,%rcx), %rdx
	leaq	(%rax,%rdx,4), %rdx
	movq	%rdx, -40(%rsp)                 # 8-byte Spill
	leaq	(%rax,%rcx,4), %rax
	movq	%rax, -112(%rsp)                # 8-byte Spill
	movl	%r9d, %eax
	andl	$3, %eax
	movq	%rax, -96(%rsp)                 # 8-byte Spill
	movq	%r9, 48(%rsp)                   # 8-byte Spill
                                        # kill: def $r9d killed $r9d killed $r9 def $r9
	andl	$-4, %r9d
	leaq	(,%rbp,8), %rax
	addq	%rbp, %rax
	shlq	$5, %rax
	addq	%r10, %rax
	movq	%rax, -72(%rsp)                 # 8-byte Spill
	movq	%rbp, %rax
	shlq	$7, %rax
	leaq	(%rax,%rax,2), %r8
	movq	%r11, %rdi
	shlq	$9, %rdi
	leaq	(,%rbp,2), %rax
	addq	%rbp, %rax
	shlq	$6, %rax
	addq	%r10, %rax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	leaq	(,%rbp,8), %rax
	leaq	(%rax,%rax,2), %rcx
	movq	%rbp, -48(%rsp)                 # 8-byte Spill
	movq	%rbp, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %r12
	shlq	$7, %r11
	xorl	%eax, %eax
	movq	%rax, -120(%rsp)                # 8-byte Spill
	movq	%r10, -56(%rsp)                 # 8-byte Spill
	movq	%rcx, 16(%rsp)                  # 8-byte Spill
	leaq	(%r10,%rcx,4), %rax
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	movq	%r14, 40(%rsp)                  # 8-byte Spill
	.p2align	4, 0x90
.LBB61_1:                               # %"for filter1_im_0_d_def__.s1.r977$z"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB61_3 Depth 2
                                        #       Child Loop BB61_7 Depth 3
                                        #       Child Loop BB61_10 Depth 3
	cmpl	$0, -104(%rsp)                  # 4-byte Folded Reload
	jle	.LBB61_13
# %bb.2:                                # %"for filter1_im_0_d_def__.s1.r977$y.preheader"
                                        #   in Loop: Header=BB61_1 Depth=1
	movslq	-124(%rsp), %rax                # 4-byte Folded Reload
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rdx
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	%rcx, (%rsp)                    # 8-byte Spill
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rcx
	movq	%rcx, -8(%rsp)                  # 8-byte Spill
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax,4), %rax
	movq	%rax, -16(%rsp)                 # 8-byte Spill
	movq	-64(%rsp), %rax                 # 8-byte Reload
	movq	-120(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rax), %rax
	movq	-112(%rsp), %rbp                # 8-byte Reload
	movq	%rdx, 8(%rsp)                   # 8-byte Spill
	movq	%rdx, %rbx
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movq	%rax, 56(%rsp)                  # 8-byte Spill
	.p2align	4, 0x90
.LBB61_3:                               # %"for filter1_im_0_d_def__.s1.r977$y"
                                        #   Parent Loop BB61_1 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB61_7 Depth 3
                                        #       Child Loop BB61_10 Depth 3
	cmpl	$0, 48(%rsp)                    # 4-byte Folded Reload
	jle	.LBB61_12
# %bb.4:                                # %"for filter1_im_0_d_def__.s1.r977$x.preheader"
                                        #   in Loop: Header=BB61_3 Depth=2
	vmovss	32(%r14,%rax,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	cmpq	$3, 32(%rsp)                    # 8-byte Folded Reload
	movq	%rbp, 80(%rsp)                  # 8-byte Spill
	movq	%rbx, 72(%rsp)                  # 8-byte Spill
	movq	%rcx, 64(%rsp)                  # 8-byte Spill
	jae	.LBB61_6
# %bb.5:                                #   in Loop: Header=BB61_3 Depth=2
	xorl	%r14d, %r14d
	jmp	.LBB61_8
	.p2align	4, 0x90
.LBB61_6:                               # %"for filter1_im_0_d_def__.s1.r977$x.preheader1"
                                        #   in Loop: Header=BB61_3 Depth=2
	movq	-112(%rsp), %rax                # 8-byte Reload
	movq	8(%rsp), %rbx                   # 8-byte Reload
	movq	-40(%rsp), %rcx                 # 8-byte Reload
	movq	-16(%rsp), %rsi                 # 8-byte Reload
	movq	-32(%rsp), %rbp                 # 8-byte Reload
	movq	-8(%rsp), %r10                  # 8-byte Reload
	movq	-24(%rsp), %r13                 # 8-byte Reload
	movq	(%rsp), %r15                    # 8-byte Reload
	xorl	%r14d, %r14d
	.p2align	4, 0x90
.LBB61_7:                               # %"for filter1_im_0_d_def__.s1.r977$x"
                                        #   Parent Loop BB61_1 Depth=1
                                        #     Parent Loop BB61_3 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovss	(%rax,%rdx), %xmm1              # xmm1 = mem[0],zero,zero,zero
	vfmadd132ss	(%rbx,%rdx), %xmm0, %xmm1 # xmm1 = (xmm1 * mem) + xmm0
	vmovss	(%rcx,%rdx), %xmm0              # xmm0 = mem[0],zero,zero,zero
	vfmadd132ss	(%rsi,%rdx), %xmm1, %xmm0 # xmm0 = (xmm0 * mem) + xmm1
	vmovss	(%rbp,%rdx), %xmm1              # xmm1 = mem[0],zero,zero,zero
	vfmadd132ss	(%r10,%rdx), %xmm0, %xmm1 # xmm1 = (xmm1 * mem) + xmm0
	vmovss	(%r13,%rdx), %xmm0              # xmm0 = mem[0],zero,zero,zero
	vfmadd132ss	(%r15,%rdx), %xmm1, %xmm0 # xmm0 = (xmm0 * mem) + xmm1
	addq	$4, %r14
	addq	%r8, %r15
	addq	%rdi, %r13
	addq	%r8, %r10
	addq	%rdi, %rbp
	addq	%r8, %rsi
	addq	%rdi, %rcx
	addq	%r8, %rbx
	addq	%rdi, %rax
	cmpq	%r14, %r9
	jne	.LBB61_7
.LBB61_8:                               # %"end for filter1_im_0_d_def__.s1.r977$x.loopexit.unr-lcssa"
                                        #   in Loop: Header=BB61_3 Depth=2
	cmpq	$0, -96(%rsp)                   # 8-byte Folded Reload
	movq	80(%rsp), %rbp                  # 8-byte Reload
	movq	72(%rsp), %rbx                  # 8-byte Reload
	je	.LBB61_11
# %bb.9:                                # %"for filter1_im_0_d_def__.s1.r977$x.epil.preheader"
                                        #   in Loop: Header=BB61_3 Depth=2
	movq	16(%rsp), %rax                  # 8-byte Reload
	imulq	%r14, %rax
	leaq	(%rbx,%rax,4), %rax
	imulq	24(%rsp), %r14                  # 8-byte Folded Reload
	leaq	(,%r14,4), %rcx
	addq	%rbp, %rcx
	movq	-96(%rsp), %rsi                 # 8-byte Reload
	.p2align	4, 0x90
.LBB61_10:                              # %"for filter1_im_0_d_def__.s1.r977$x.epil"
                                        #   Parent Loop BB61_1 Depth=1
                                        #     Parent Loop BB61_3 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovss	(%rcx), %xmm1                   # xmm1 = mem[0],zero,zero,zero
	vfmadd231ss	(%rax), %xmm1, %xmm0    # xmm0 = (xmm1 * mem) + xmm0
	addq	%r12, %rax
	addq	%r11, %rcx
	decq	%rsi
	jne	.LBB61_10
.LBB61_11:                              # %"end for filter1_im_0_d_def__.s1.r977$x.loopexit"
                                        #   in Loop: Header=BB61_3 Depth=2
	movq	40(%rsp), %r14                  # 8-byte Reload
	movq	56(%rsp), %rax                  # 8-byte Reload
	vmovss	%xmm0, 32(%r14,%rax,4)
	movq	64(%rsp), %rcx                  # 8-byte Reload
.LBB61_12:                              # %"end for filter1_im_0_d_def__.s1.r977$x"
                                        #   in Loop: Header=BB61_3 Depth=2
	incq	%rcx
	addq	$4, %rdx
	addq	$4, %rbx
	addq	$4, %rbp
	cmpq	-104(%rsp), %rcx                # 8-byte Folded Reload
	jne	.LBB61_3
.LBB61_13:                              # %"end for filter1_im_0_d_def__.s1.r977$y"
                                        #   in Loop: Header=BB61_1 Depth=1
	movq	-120(%rsp), %rcx                # 8-byte Reload
	incq	%rcx
	movl	-124(%rsp), %eax                # 4-byte Reload
	addl	-48(%rsp), %eax                 # 4-byte Folded Reload
	movl	%eax, -124(%rsp)                # 4-byte Spill
	movq	%rcx, %rax
	movq	%rcx, -120(%rsp)                # 8-byte Spill
	cmpq	$24, %rcx
	jne	.LBB61_1
# %bb.14:                               # %destructor_block
	xorl	%eax, %eax
	addq	$88, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	retq
.Lfunc_end61:
	.size	train_cost_model.par_for.filter1_im_0_d_def__.s1.v18, .Lfunc_end61-train_cost_model.par_for.filter1_im_0_d_def__.s1.v18
                                        # -- End function
	.section	.text.train_cost_model.par_for.filter1_im_0_d_def__.s2.v18,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.filter1_im_0_d_def__.s2.v18
	.type	train_cost_model.par_for.filter1_im_0_d_def__.s2.v18,@function
train_cost_model.par_for.filter1_im_0_d_def__.s2.v18: # @train_cost_model.par_for.filter1_im_0_d_def__.s2.v18
# %bb.0:                                # %entry
	pushq	%r15
	pushq	%r14
	pushq	%rbx
	movl	(%rdx), %r15d
	testl	%r15d, %r15d
	jle	.LBB62_65
# %bb.1:                                # %"for filter1_im_0_d_def__.s2.r1173$x.preheader"
	movq	8(%rdx), %r8
	movq	24(%rdx), %r11
	movq	40(%rdx), %r10
	movl	%r15d, %eax
	imull	%esi, %eax
	shll	$5, %esi
	movslq	%eax, %r9
	movslq	%esi, %r14
	vmovss	(%r11,%r14,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %r15d
	jae	.LBB62_3
# %bb.2:
	xorl	%eax, %eax
	jmp	.LBB62_6
.LBB62_3:                               # %vector.ph
	movl	%r15d, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%r15, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         # xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	384(%r10), %rdi
	leaq	(%r8,%r9,4), %rbx
	addq	$48, %rbx
	xorl	%edx, %edx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
.LBB62_4:                               # %vector.body
                                        # =>This Inner Loop Header: Depth=1
	vmovups	-384(%rdi), %xmm4
	vmovups	-288(%rdi), %xmm5
	vshufps	$0, -320(%rdi), %xmm5, %xmm5    # xmm5 = xmm5[0,0],mem[0,0]
	vmovups	-256(%rdi), %xmm6
	vinsertps	$28, -352(%rdi), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vmovups	-160(%rdi), %xmm7
	vshufps	$0, -192(%rdi), %xmm7, %xmm7    # xmm7 = xmm7[0,0],mem[0,0]
	vinsertps	$28, -224(%rdi), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm4, %xmm8        # xmm8 = xmm4[0,1],xmm5[2,0]
	vmovups	-32(%rdi), %xmm5
	vshufps	$0, -64(%rdi), %xmm5, %xmm5     # xmm5 = xmm5[0,0],mem[0,0]
	vshufps	$36, %xmm7, %xmm6, %xmm6        # xmm6 = xmm6[0,1],xmm7[2,0]
	vmovups	-128(%rdi), %xmm7
	vinsertps	$28, -96(%rdi), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm7, %xmm5        # xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rdi), %xmm7
	vshufps	$0, 64(%rdi), %xmm7, %xmm7      # xmm7 = xmm7[0,0],mem[0,0]
	vmovups	(%rdi), %xmm4
	vinsertps	$28, 32(%rdi), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm7, %xmm4, %xmm4        # xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rbx,%rdx,4), %xmm8, %xmm1 # xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rbx,%rdx,4), %xmm6, %xmm0 # xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rbx,%rdx,4), %xmm5, %xmm2 # xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rbx,%rdx,4), %xmm4, %xmm3 # xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rdx
	addq	$512, %rdi                      # imm = 0x200
	cmpq	%rdx, %rax
	jne	.LBB62_4
# %bb.5:                                # %middle.block
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
.LBB62_6:                               # %"for filter1_im_0_d_def__.s2.r1173$x.preheader290"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r10, %rcx
	leaq	(%r8,%r9,4), %rdi
	.p2align	4, 0x90
.LBB62_7:                               # %"for filter1_im_0_d_def__.s2.r1173$x"
                                        # =>This Inner Loop Header: Depth=1
	vmovss	(%rdi,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    # xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %r15
	jne	.LBB62_7
# %bb.8:                                # %"end for filter1_im_0_d_def__.s2.r1173$x"
	vmovss	%xmm1, (%r11,%r14,4)
	testl	%r15d, %r15d
	jle	.LBB62_65
# %bb.9:                                # %"for filter1_im_0_d_def__.s2.r1173$x.preheader.1"
	movl	%esi, %eax
	orl	$1, %eax
	movslq	%eax, %r14
	vmovss	(%r11,%r14,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %r15d
	jae	.LBB62_11
# %bb.10:
	xorl	%eax, %eax
	jmp	.LBB62_14
.LBB62_11:                              # %vector.ph37
	movl	%r15d, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%r15, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         # xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	388(%r10), %rbx
	leaq	(%r8,%r9,4), %rdx
	addq	$48, %rdx
	xorl	%ecx, %ecx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
.LBB62_12:                              # %vector.body35
                                        # =>This Inner Loop Header: Depth=1
	vmovups	-384(%rbx), %xmm4
	vmovups	-288(%rbx), %xmm5
	vshufps	$0, -320(%rbx), %xmm5, %xmm5    # xmm5 = xmm5[0,0],mem[0,0]
	vmovups	-256(%rbx), %xmm6
	vinsertps	$28, -352(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vmovups	-160(%rbx), %xmm7
	vshufps	$0, -192(%rbx), %xmm7, %xmm7    # xmm7 = xmm7[0,0],mem[0,0]
	vinsertps	$28, -224(%rbx), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm4, %xmm8        # xmm8 = xmm4[0,1],xmm5[2,0]
	vmovups	-32(%rbx), %xmm5
	vshufps	$0, -64(%rbx), %xmm5, %xmm5     # xmm5 = xmm5[0,0],mem[0,0]
	vshufps	$36, %xmm7, %xmm6, %xmm6        # xmm6 = xmm6[0,1],xmm7[2,0]
	vmovups	-128(%rbx), %xmm7
	vinsertps	$28, -96(%rbx), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm7, %xmm5        # xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vshufps	$0, 64(%rbx), %xmm7, %xmm7      # xmm7 = xmm7[0,0],mem[0,0]
	vmovups	(%rbx), %xmm4
	vinsertps	$28, 32(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm7, %xmm4, %xmm4        # xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rdx,%rcx,4), %xmm8, %xmm1 # xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rdx,%rcx,4), %xmm6, %xmm0 # xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rdx,%rcx,4), %xmm5, %xmm2 # xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rdx,%rcx,4), %xmm4, %xmm3 # xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rcx
	addq	$512, %rbx                      # imm = 0x200
	cmpq	%rcx, %rax
	jne	.LBB62_12
# %bb.13:                               # %middle.block33
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
.LBB62_14:                              # %"for filter1_im_0_d_def__.s2.r1173$x.1.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r10, %rcx
	addq	$4, %rcx
	.p2align	4, 0x90
.LBB62_15:                              # %"for filter1_im_0_d_def__.s2.r1173$x.1"
                                        # =>This Inner Loop Header: Depth=1
	vmovss	(%rdi,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    # xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %r15
	jne	.LBB62_15
# %bb.16:                               # %"end for filter1_im_0_d_def__.s2.r1173$x.1"
	vmovss	%xmm1, (%r11,%r14,4)
	testl	%r15d, %r15d
	jle	.LBB62_65
# %bb.17:                               # %"for filter1_im_0_d_def__.s2.r1173$x.preheader.2"
	movl	%esi, %eax
	orl	$2, %eax
	movslq	%eax, %r14
	vmovss	(%r11,%r14,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %r15d
	jae	.LBB62_19
# %bb.18:
	xorl	%eax, %eax
	jmp	.LBB62_22
.LBB62_19:                              # %vector.ph68
	movl	%r15d, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%r15, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         # xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	392(%r10), %rbx
	leaq	(%r8,%r9,4), %rdx
	addq	$48, %rdx
	xorl	%ecx, %ecx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
.LBB62_20:                              # %vector.body66
                                        # =>This Inner Loop Header: Depth=1
	vmovups	-384(%rbx), %xmm4
	vmovups	-288(%rbx), %xmm5
	vshufps	$0, -320(%rbx), %xmm5, %xmm5    # xmm5 = xmm5[0,0],mem[0,0]
	vmovups	-256(%rbx), %xmm6
	vinsertps	$28, -352(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vmovups	-160(%rbx), %xmm7
	vshufps	$0, -192(%rbx), %xmm7, %xmm7    # xmm7 = xmm7[0,0],mem[0,0]
	vinsertps	$28, -224(%rbx), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm4, %xmm8        # xmm8 = xmm4[0,1],xmm5[2,0]
	vmovups	-32(%rbx), %xmm5
	vshufps	$0, -64(%rbx), %xmm5, %xmm5     # xmm5 = xmm5[0,0],mem[0,0]
	vshufps	$36, %xmm7, %xmm6, %xmm6        # xmm6 = xmm6[0,1],xmm7[2,0]
	vmovups	-128(%rbx), %xmm7
	vinsertps	$28, -96(%rbx), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm7, %xmm5        # xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vshufps	$0, 64(%rbx), %xmm7, %xmm7      # xmm7 = xmm7[0,0],mem[0,0]
	vmovups	(%rbx), %xmm4
	vinsertps	$28, 32(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm7, %xmm4, %xmm4        # xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rdx,%rcx,4), %xmm8, %xmm1 # xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rdx,%rcx,4), %xmm6, %xmm0 # xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rdx,%rcx,4), %xmm5, %xmm2 # xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rdx,%rcx,4), %xmm4, %xmm3 # xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rcx
	addq	$512, %rbx                      # imm = 0x200
	cmpq	%rcx, %rax
	jne	.LBB62_20
# %bb.21:                               # %middle.block64
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
.LBB62_22:                              # %"for filter1_im_0_d_def__.s2.r1173$x.2.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r10, %rcx
	addq	$8, %rcx
	.p2align	4, 0x90
.LBB62_23:                              # %"for filter1_im_0_d_def__.s2.r1173$x.2"
                                        # =>This Inner Loop Header: Depth=1
	vmovss	(%rdi,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    # xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %r15
	jne	.LBB62_23
# %bb.24:                               # %"end for filter1_im_0_d_def__.s2.r1173$x.2"
	vmovss	%xmm1, (%r11,%r14,4)
	testl	%r15d, %r15d
	jle	.LBB62_65
# %bb.25:                               # %"for filter1_im_0_d_def__.s2.r1173$x.preheader.3"
	movl	%esi, %eax
	orl	$3, %eax
	movslq	%eax, %r14
	vmovss	(%r11,%r14,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %r15d
	jae	.LBB62_27
# %bb.26:
	xorl	%eax, %eax
	jmp	.LBB62_30
.LBB62_27:                              # %vector.ph99
	movl	%r15d, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%r15, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         # xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	396(%r10), %rbx
	leaq	(%r8,%r9,4), %rdx
	addq	$48, %rdx
	xorl	%ecx, %ecx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
.LBB62_28:                              # %vector.body97
                                        # =>This Inner Loop Header: Depth=1
	vmovups	-384(%rbx), %xmm4
	vmovups	-288(%rbx), %xmm5
	vshufps	$0, -320(%rbx), %xmm5, %xmm5    # xmm5 = xmm5[0,0],mem[0,0]
	vmovups	-256(%rbx), %xmm6
	vinsertps	$28, -352(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vmovups	-160(%rbx), %xmm7
	vshufps	$0, -192(%rbx), %xmm7, %xmm7    # xmm7 = xmm7[0,0],mem[0,0]
	vinsertps	$28, -224(%rbx), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm4, %xmm8        # xmm8 = xmm4[0,1],xmm5[2,0]
	vmovups	-32(%rbx), %xmm5
	vshufps	$0, -64(%rbx), %xmm5, %xmm5     # xmm5 = xmm5[0,0],mem[0,0]
	vshufps	$36, %xmm7, %xmm6, %xmm6        # xmm6 = xmm6[0,1],xmm7[2,0]
	vmovups	-128(%rbx), %xmm7
	vinsertps	$28, -96(%rbx), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm7, %xmm5        # xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vshufps	$0, 64(%rbx), %xmm7, %xmm7      # xmm7 = xmm7[0,0],mem[0,0]
	vmovups	(%rbx), %xmm4
	vinsertps	$28, 32(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm7, %xmm4, %xmm4        # xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rdx,%rcx,4), %xmm8, %xmm1 # xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rdx,%rcx,4), %xmm6, %xmm0 # xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rdx,%rcx,4), %xmm5, %xmm2 # xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rdx,%rcx,4), %xmm4, %xmm3 # xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rcx
	addq	$512, %rbx                      # imm = 0x200
	cmpq	%rcx, %rax
	jne	.LBB62_28
# %bb.29:                               # %middle.block95
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
.LBB62_30:                              # %"for filter1_im_0_d_def__.s2.r1173$x.3.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r10, %rcx
	addq	$12, %rcx
	.p2align	4, 0x90
.LBB62_31:                              # %"for filter1_im_0_d_def__.s2.r1173$x.3"
                                        # =>This Inner Loop Header: Depth=1
	vmovss	(%rdi,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    # xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %r15
	jne	.LBB62_31
# %bb.32:                               # %"end for filter1_im_0_d_def__.s2.r1173$x.3"
	vmovss	%xmm1, (%r11,%r14,4)
	testl	%r15d, %r15d
	jle	.LBB62_65
# %bb.33:                               # %"for filter1_im_0_d_def__.s2.r1173$x.preheader.4"
	movl	%esi, %eax
	orl	$4, %eax
	movslq	%eax, %r14
	vmovss	(%r11,%r14,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %r15d
	jae	.LBB62_35
# %bb.34:
	xorl	%eax, %eax
	jmp	.LBB62_38
.LBB62_35:                              # %vector.ph130
	movl	%r15d, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%r15, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         # xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	400(%r10), %rbx
	leaq	(%r8,%r9,4), %rdx
	addq	$48, %rdx
	xorl	%ecx, %ecx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
.LBB62_36:                              # %vector.body128
                                        # =>This Inner Loop Header: Depth=1
	vmovups	-384(%rbx), %xmm4
	vmovups	-288(%rbx), %xmm5
	vshufps	$0, -320(%rbx), %xmm5, %xmm5    # xmm5 = xmm5[0,0],mem[0,0]
	vmovups	-256(%rbx), %xmm6
	vinsertps	$28, -352(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vmovups	-160(%rbx), %xmm7
	vshufps	$0, -192(%rbx), %xmm7, %xmm7    # xmm7 = xmm7[0,0],mem[0,0]
	vinsertps	$28, -224(%rbx), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm4, %xmm8        # xmm8 = xmm4[0,1],xmm5[2,0]
	vmovups	-32(%rbx), %xmm5
	vshufps	$0, -64(%rbx), %xmm5, %xmm5     # xmm5 = xmm5[0,0],mem[0,0]
	vshufps	$36, %xmm7, %xmm6, %xmm6        # xmm6 = xmm6[0,1],xmm7[2,0]
	vmovups	-128(%rbx), %xmm7
	vinsertps	$28, -96(%rbx), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm7, %xmm5        # xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vshufps	$0, 64(%rbx), %xmm7, %xmm7      # xmm7 = xmm7[0,0],mem[0,0]
	vmovups	(%rbx), %xmm4
	vinsertps	$28, 32(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm7, %xmm4, %xmm4        # xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rdx,%rcx,4), %xmm8, %xmm1 # xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rdx,%rcx,4), %xmm6, %xmm0 # xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rdx,%rcx,4), %xmm5, %xmm2 # xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rdx,%rcx,4), %xmm4, %xmm3 # xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rcx
	addq	$512, %rbx                      # imm = 0x200
	cmpq	%rcx, %rax
	jne	.LBB62_36
# %bb.37:                               # %middle.block126
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
.LBB62_38:                              # %"for filter1_im_0_d_def__.s2.r1173$x.4.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r10, %rcx
	addq	$16, %rcx
	.p2align	4, 0x90
.LBB62_39:                              # %"for filter1_im_0_d_def__.s2.r1173$x.4"
                                        # =>This Inner Loop Header: Depth=1
	vmovss	(%rdi,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    # xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %r15
	jne	.LBB62_39
# %bb.40:                               # %"end for filter1_im_0_d_def__.s2.r1173$x.4"
	vmovss	%xmm1, (%r11,%r14,4)
	testl	%r15d, %r15d
	jle	.LBB62_65
# %bb.41:                               # %"for filter1_im_0_d_def__.s2.r1173$x.preheader.5"
	movl	%esi, %eax
	orl	$5, %eax
	movslq	%eax, %r14
	vmovss	(%r11,%r14,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %r15d
	jae	.LBB62_43
# %bb.42:
	xorl	%eax, %eax
	jmp	.LBB62_46
.LBB62_43:                              # %vector.ph161
	movl	%r15d, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%r15, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         # xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	404(%r10), %rbx
	leaq	(%r8,%r9,4), %rdx
	addq	$48, %rdx
	xorl	%ecx, %ecx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
.LBB62_44:                              # %vector.body159
                                        # =>This Inner Loop Header: Depth=1
	vmovups	-384(%rbx), %xmm4
	vmovups	-288(%rbx), %xmm5
	vshufps	$0, -320(%rbx), %xmm5, %xmm5    # xmm5 = xmm5[0,0],mem[0,0]
	vmovups	-256(%rbx), %xmm6
	vinsertps	$28, -352(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vmovups	-160(%rbx), %xmm7
	vshufps	$0, -192(%rbx), %xmm7, %xmm7    # xmm7 = xmm7[0,0],mem[0,0]
	vinsertps	$28, -224(%rbx), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm4, %xmm8        # xmm8 = xmm4[0,1],xmm5[2,0]
	vmovups	-32(%rbx), %xmm5
	vshufps	$0, -64(%rbx), %xmm5, %xmm5     # xmm5 = xmm5[0,0],mem[0,0]
	vshufps	$36, %xmm7, %xmm6, %xmm6        # xmm6 = xmm6[0,1],xmm7[2,0]
	vmovups	-128(%rbx), %xmm7
	vinsertps	$28, -96(%rbx), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm7, %xmm5        # xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vshufps	$0, 64(%rbx), %xmm7, %xmm7      # xmm7 = xmm7[0,0],mem[0,0]
	vmovups	(%rbx), %xmm4
	vinsertps	$28, 32(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm7, %xmm4, %xmm4        # xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rdx,%rcx,4), %xmm8, %xmm1 # xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rdx,%rcx,4), %xmm6, %xmm0 # xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rdx,%rcx,4), %xmm5, %xmm2 # xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rdx,%rcx,4), %xmm4, %xmm3 # xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rcx
	addq	$512, %rbx                      # imm = 0x200
	cmpq	%rcx, %rax
	jne	.LBB62_44
# %bb.45:                               # %middle.block157
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
.LBB62_46:                              # %"for filter1_im_0_d_def__.s2.r1173$x.5.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r10, %rcx
	addq	$20, %rcx
	.p2align	4, 0x90
.LBB62_47:                              # %"for filter1_im_0_d_def__.s2.r1173$x.5"
                                        # =>This Inner Loop Header: Depth=1
	vmovss	(%rdi,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    # xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %r15
	jne	.LBB62_47
# %bb.48:                               # %"end for filter1_im_0_d_def__.s2.r1173$x.5"
	vmovss	%xmm1, (%r11,%r14,4)
	testl	%r15d, %r15d
	jle	.LBB62_65
# %bb.49:                               # %"for filter1_im_0_d_def__.s2.r1173$x.preheader.6"
	movl	%esi, %eax
	orl	$6, %eax
	movslq	%eax, %r14
	vmovss	(%r11,%r14,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %r15d
	jae	.LBB62_51
# %bb.50:
	xorl	%eax, %eax
	jmp	.LBB62_54
.LBB62_51:                              # %vector.ph192
	movl	%r15d, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%r15, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         # xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	408(%r10), %rbx
	leaq	(%r8,%r9,4), %rdx
	addq	$48, %rdx
	xorl	%ecx, %ecx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
.LBB62_52:                              # %vector.body190
                                        # =>This Inner Loop Header: Depth=1
	vmovups	-384(%rbx), %xmm4
	vmovups	-288(%rbx), %xmm5
	vshufps	$0, -320(%rbx), %xmm5, %xmm5    # xmm5 = xmm5[0,0],mem[0,0]
	vmovups	-256(%rbx), %xmm6
	vinsertps	$28, -352(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vmovups	-160(%rbx), %xmm7
	vshufps	$0, -192(%rbx), %xmm7, %xmm7    # xmm7 = xmm7[0,0],mem[0,0]
	vinsertps	$28, -224(%rbx), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm4, %xmm8        # xmm8 = xmm4[0,1],xmm5[2,0]
	vmovups	-32(%rbx), %xmm5
	vshufps	$0, -64(%rbx), %xmm5, %xmm5     # xmm5 = xmm5[0,0],mem[0,0]
	vshufps	$36, %xmm7, %xmm6, %xmm6        # xmm6 = xmm6[0,1],xmm7[2,0]
	vmovups	-128(%rbx), %xmm7
	vinsertps	$28, -96(%rbx), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm7, %xmm5        # xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rbx), %xmm7
	vshufps	$0, 64(%rbx), %xmm7, %xmm7      # xmm7 = xmm7[0,0],mem[0,0]
	vmovups	(%rbx), %xmm4
	vinsertps	$28, 32(%rbx), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm7, %xmm4, %xmm4        # xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rdx,%rcx,4), %xmm8, %xmm1 # xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rdx,%rcx,4), %xmm6, %xmm0 # xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rdx,%rcx,4), %xmm5, %xmm2 # xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rdx,%rcx,4), %xmm4, %xmm3 # xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rcx
	addq	$512, %rbx                      # imm = 0x200
	cmpq	%rcx, %rax
	jne	.LBB62_52
# %bb.53:                               # %middle.block188
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
.LBB62_54:                              # %"for filter1_im_0_d_def__.s2.r1173$x.6.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r10, %rcx
	addq	$24, %rcx
	.p2align	4, 0x90
.LBB62_55:                              # %"for filter1_im_0_d_def__.s2.r1173$x.6"
                                        # =>This Inner Loop Header: Depth=1
	vmovss	(%rdi,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    # xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %r15
	jne	.LBB62_55
# %bb.56:                               # %"end for filter1_im_0_d_def__.s2.r1173$x.6"
	vmovss	%xmm1, (%r11,%r14,4)
	testl	%r15d, %r15d
	jle	.LBB62_65
# %bb.57:                               # %"for filter1_im_0_d_def__.s2.r1173$x.preheader.7"
	orl	$7, %esi
	movslq	%esi, %rdx
	vmovss	(%r11,%rdx,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	cmpl	$17, %r15d
	jae	.LBB62_59
# %bb.58:
	xorl	%eax, %eax
	jmp	.LBB62_62
.LBB62_59:                              # %vector.ph223
	movl	%r15d, %eax
	andl	$15, %eax
	testq	%rax, %rax
	movl	$16, %ecx
	cmovneq	%rax, %rcx
	movq	%r15, %rax
	subq	%rcx, %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vblendps	$1, %xmm1, %xmm0, %xmm1         # xmm1 = xmm1[0],xmm0[1,2,3]
	leaq	412(%r10), %rsi
	leaq	(%r8,%r9,4), %rbx
	addq	$48, %rbx
	xorl	%ecx, %ecx
	vxorps	%xmm2, %xmm2, %xmm2
	vxorps	%xmm3, %xmm3, %xmm3
	.p2align	4, 0x90
.LBB62_60:                              # %vector.body221
                                        # =>This Inner Loop Header: Depth=1
	vmovups	-384(%rsi), %xmm4
	vmovups	-288(%rsi), %xmm5
	vshufps	$0, -320(%rsi), %xmm5, %xmm5    # xmm5 = xmm5[0,0],mem[0,0]
	vmovups	-256(%rsi), %xmm6
	vinsertps	$28, -352(%rsi), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vmovups	-160(%rsi), %xmm7
	vshufps	$0, -192(%rsi), %xmm7, %xmm7    # xmm7 = xmm7[0,0],mem[0,0]
	vinsertps	$28, -224(%rsi), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm4, %xmm8        # xmm8 = xmm4[0,1],xmm5[2,0]
	vmovups	-32(%rsi), %xmm5
	vshufps	$0, -64(%rsi), %xmm5, %xmm5     # xmm5 = xmm5[0,0],mem[0,0]
	vshufps	$36, %xmm7, %xmm6, %xmm6        # xmm6 = xmm6[0,1],xmm7[2,0]
	vmovups	-128(%rsi), %xmm7
	vinsertps	$28, -96(%rsi), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],zero,zero
	vshufps	$36, %xmm5, %xmm7, %xmm5        # xmm5 = xmm7[0,1],xmm5[2,0]
	vmovups	96(%rsi), %xmm7
	vshufps	$0, 64(%rsi), %xmm7, %xmm7      # xmm7 = xmm7[0,0],mem[0,0]
	vmovups	(%rsi), %xmm4
	vinsertps	$28, 32(%rsi), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm7, %xmm4, %xmm4        # xmm4 = xmm4[0,1],xmm7[2,0]
	vfmadd231ps	-48(%rbx,%rcx,4), %xmm8, %xmm1 # xmm1 = (xmm8 * mem) + xmm1
	vfmadd231ps	-32(%rbx,%rcx,4), %xmm6, %xmm0 # xmm0 = (xmm6 * mem) + xmm0
	vfmadd231ps	-16(%rbx,%rcx,4), %xmm5, %xmm2 # xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ps	(%rbx,%rcx,4), %xmm4, %xmm3 # xmm3 = (xmm4 * mem) + xmm3
	addq	$16, %rcx
	addq	$512, %rsi                      # imm = 0x200
	cmpq	%rcx, %rax
	jne	.LBB62_60
# %bb.61:                               # %middle.block219
	vaddps	%xmm1, %xmm0, %xmm0
	vaddps	%xmm0, %xmm2, %xmm0
	vaddps	%xmm0, %xmm3, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm1
.LBB62_62:                              # %"for filter1_im_0_d_def__.s2.r1173$x.7.preheader"
	movq	%rax, %rcx
	shlq	$5, %rcx
	addq	%r10, %rcx
	addq	$28, %rcx
	.p2align	4, 0x90
.LBB62_63:                              # %"for filter1_im_0_d_def__.s2.r1173$x.7"
                                        # =>This Inner Loop Header: Depth=1
	vmovss	(%rdi,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vfmadd231ss	(%rcx), %xmm0, %xmm1    # xmm1 = (xmm0 * mem) + xmm1
	incq	%rax
	addq	$32, %rcx
	cmpq	%rax, %r15
	jne	.LBB62_63
# %bb.64:                               # %"end for filter1_im_0_d_def__.s2.r1173$x.loopexit.7"
	vmovss	%xmm1, (%r11,%rdx,4)
.LBB62_65:                              # %"end for filter1_im_0_d_def__.s2.r1173$x.7"
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	popq	%r15
	retq
.Lfunc_end62:
	.size	train_cost_model.par_for.filter1_im_0_d_def__.s2.v18, .Lfunc_end62-train_cost_model.par_for.filter1_im_0_d_def__.s2.v18
                                        # -- End function
	.section	.text.train_cost_model.par_for.updated_filter1.s1.v251.v251.v251,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.updated_filter1.s1.v251.v251.v251
	.type	train_cost_model.par_for.updated_filter1.s1.v251.v251.v251,@function
train_cost_model.par_for.updated_filter1.s1.v251.v251.v251: # @train_cost_model.par_for.updated_filter1.s1.v251.v251.v251
# %bb.0:                                # %entry
	pushq	%r15
	pushq	%r14
	pushq	%rbx
                                        # kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r8
	movslq	4(%rdx), %r9
	movq	8(%rdx), %rax
	movl	%esi, %ecx
	andl	$-2, %ecx
	leal	(%rsi,%rsi), %r10d
	andl	$2, %r10d
	movslq	%ecx, %rdi
	movq	%rdi, %r15
	imulq	%r9, %r15
	addq	%r8, %r15
	leaq	(%r15,%r10,8), %r14
	leaq	1(%r10), %r11
	movq	%r10, %rcx
	shlq	$8, %rcx
	leaq	(%rcx,%rdi), %rbx
	vmovss	(%rax,%rbx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovss	512(%rax,%rbx,4), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 128(%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 256(%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 384(%rax,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 640(%rax,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 768(%rax,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 896(%rax,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	leaq	8(%r15,%r10,8), %rbx
	movq	24(%rdx), %rdx
	vmovups	%xmm1, 16(%rdx,%r14,4)
	vmovups	%xmm0, (%rdx,%r14,4)
	shlq	$8, %r11
	addq	%r11, %rdi
	vmovss	(%rax,%rdi,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovss	512(%rax,%rdi,4), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 128(%rax,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 256(%rax,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 384(%rax,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 640(%rax,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 768(%rax,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 896(%rax,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovups	%xmm1, 16(%rdx,%rbx,4)
	orl	$1, %esi
	movslq	%esi, %rsi
	imulq	%rsi, %r9
	addq	%r8, %r9
	leaq	(%r9,%r10,8), %rdi
	vmovups	%xmm0, (%rdx,%rbx,4)
	addq	%rsi, %rcx
	vmovss	(%rax,%rcx,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovss	512(%rax,%rcx,4), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 128(%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 256(%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 384(%rax,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 640(%rax,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 768(%rax,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 896(%rax,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovups	%xmm1, 16(%rdx,%rdi,4)
	leaq	(%r9,%r10,8), %rcx
	addq	$8, %rcx
	vmovups	%xmm0, (%rdx,%rdi,4)
	addq	%rsi, %r11
	vmovss	(%rax,%r11,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovss	512(%rax,%r11,4), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 128(%rax,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 256(%rax,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 384(%rax,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 640(%rax,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 768(%rax,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 896(%rax,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovups	%xmm1, 16(%rdx,%rcx,4)
	vmovups	%xmm0, (%rdx,%rcx,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	popq	%r15
	retq
.Lfunc_end63:
	.size	train_cost_model.par_for.updated_filter1.s1.v251.v251.v251, .Lfunc_end63-train_cost_model.par_for.updated_filter1.s1.v251.v251.v251
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_filter1.s2.v251.v251.v251
.LCPI64_0:
	.long	0x3f666666                      # float 0.899999976
.LCPI64_1:
	.long	0x3dcccccd                      # float 0.100000001
	.section	.text.train_cost_model.par_for.updated_filter1.s2.v251.v251.v251,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_filter1.s2.v251.v251.v251,@function
train_cost_model.par_for.updated_filter1.s2.v251.v251.v251: # @train_cost_model.par_for.updated_filter1.s2.v251.v251.v251
# %bb.0:                                # %entry
	pushq	%rbx
                                        # kill: def $esi killed $esi def $rsi
	movl	(%rdx), %r8d
	movslq	4(%rdx), %r10
	movq	8(%rdx), %rax
	movl	%esi, %edx
	andl	$-2, %edx
	leaq	(%r10,%r10,2), %r9
	imull	%r8d, %edx
	movl	%esi, %edi
	shll	$4, %edi
	andl	$16, %edi
	leal	(%rdi,%rdx), %r11d
	leal	(%r11,%r10), %ecx
	movslq	%ecx, %rbx
	vbroadcastss	.LCPI64_0(%rip), %ymm0  # ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	movslq	%r11d, %rcx
	addq	%r9, %rcx
	vbroadcastss	.LCPI64_1(%rip), %ymm1  # ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rax,%rcx,4), %ymm1, %ymm2
	vfmadd231ps	(%rax,%rbx,4), %ymm0, %ymm2 # ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rax,%rbx,4)
	leal	8(%rdi,%rdx), %ecx
	leal	(%rcx,%r10), %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	addq	%r9, %rcx
	vmulps	(%rax,%rcx,4), %ymm1, %ymm2
	vfmadd231ps	(%rax,%rdx,4), %ymm0, %ymm2 # ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rax,%rdx,4)
	orl	$1, %esi
	imull	%r8d, %esi
	leal	(%rdi,%rsi), %ecx
	addl	$8, %ecx
                                        # kill: def $edi killed $edi killed $rdi def $rdi
	addl	%esi, %edi
	leal	(%rdi,%r10), %edx
	movslq	%edx, %rdx
	movslq	%edi, %rsi
	addq	%r9, %rsi
	vmulps	(%rax,%rsi,4), %ymm1, %ymm2
	vfmadd231ps	(%rax,%rdx,4), %ymm0, %ymm2 # ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rax,%rdx,4)
	leal	(%rcx,%r10), %edx
	movslq	%edx, %rdx
	movslq	%ecx, %rcx
	addq	%r9, %rcx
	vmulps	(%rax,%rcx,4), %ymm1, %ymm1
	vfmadd231ps	(%rax,%rdx,4), %ymm0, %ymm1 # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rdx,4)
	xorl	%eax, %eax
	popq	%rbx
	vzeroupper
	retq
.Lfunc_end64:
	.size	train_cost_model.par_for.updated_filter1.s2.v251.v251.v251, .Lfunc_end64-train_cost_model.par_for.updated_filter1.s2.v251.v251.v251
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_filter1.s3.v251.v251.v251
.LCPI65_0:
	.long	0x3f7fbe77                      # float 0.999000012
.LCPI65_1:
	.long	0x3a83126f                      # float 0.00100000005
	.section	.text.train_cost_model.par_for.updated_filter1.s3.v251.v251.v251,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_filter1.s3.v251.v251.v251,@function
train_cost_model.par_for.updated_filter1.s3.v251.v251.v251: # @train_cost_model.par_for.updated_filter1.s3.v251.v251.v251
# %bb.0:                                # %entry
                                        # kill: def $esi killed $esi def $rsi
	movslq	(%rdx), %r10
	movl	4(%rdx), %r9d
	movl	8(%rdx), %r8d
	movq	16(%rdx), %rax
	movl	%esi, %ecx
	andl	$-2, %ecx
	imull	%r8d, %ecx
	movl	%esi, %r11d
	shll	$4, %r11d
	andl	$16, %r11d
	leal	(%r11,%rcx), %edi
	movslq	%edi, %rdi
	leaq	(%rdi,%r10), %rdx
	vmovups	(%rax,%rdx,4), %ymm1
	addl	%r9d, %edi
	movslq	%edi, %rdx
	vbroadcastss	.LCPI65_0(%rip), %ymm0  # ymm0 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm1, %ymm1, %ymm2
	vbroadcastss	.LCPI65_1(%rip), %ymm1  # ymm1 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm1, %ymm2, %ymm2
	vfmadd231ps	(%rax,%rdx,4), %ymm0, %ymm2 # ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rax,%rdx,4)
	leal	8(%r11,%rcx), %ecx
	movslq	%ecx, %rcx
	leaq	(%rcx,%r10), %rdx
	vmovups	(%rax,%rdx,4), %ymm2
	addl	%r9d, %ecx
	movslq	%ecx, %rcx
	vmulps	%ymm2, %ymm2, %ymm2
	vmulps	%ymm1, %ymm2, %ymm2
	vfmadd231ps	(%rax,%rcx,4), %ymm0, %ymm2 # ymm2 = (ymm0 * mem) + ymm2
	vmovups	%ymm2, (%rax,%rcx,4)
	orl	$1, %esi
	imull	%r8d, %esi
	movl	%r11d, %ecx
	addl	%esi, %ecx
	movslq	%ecx, %rcx
	leaq	(%rcx,%r10), %rdx
	vmovups	(%rax,%rdx,4), %ymm2
	addl	%r9d, %ecx
	movslq	%ecx, %rcx
	vmulps	%ymm2, %ymm2, %ymm2
	vmulps	%ymm1, %ymm2, %ymm2
	vfmadd231ps	(%rax,%rcx,4), %ymm0, %ymm2 # ymm2 = (ymm0 * mem) + ymm2
	leal	(%r11,%rsi), %edx
	addl	$8, %edx
	vmovups	%ymm2, (%rax,%rcx,4)
	movslq	%edx, %rcx
	addq	%rcx, %r10
	vmovups	(%rax,%r10,4), %ymm2
	addl	%r9d, %ecx
	movslq	%ecx, %rcx
	vmulps	%ymm2, %ymm2, %ymm2
	vmulps	%ymm1, %ymm2, %ymm1
	vfmadd231ps	(%rax,%rcx,4), %ymm0, %ymm1 # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rax,%rcx,4)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end65:
	.size	train_cost_model.par_for.updated_filter1.s3.v251.v251.v251, .Lfunc_end65-train_cost_model.par_for.updated_filter1.s3.v251.v251.v251
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_filter1.s4.v251.v251.v251
.LCPI66_0:
	.long	0x3727c5ac                      # float 9.99999974E-6
	.section	.text.train_cost_model.par_for.updated_filter1.s4.v251.v251.v251,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_filter1.s4.v251.v251.v251,@function
train_cost_model.par_for.updated_filter1.s4.v251.v251.v251: # @train_cost_model.par_for.updated_filter1.s4.v251.v251.v251
# %bb.0:                                # %entry
	pushq	%r14
	pushq	%rbx
                                        # kill: def $esi killed $esi def $rsi
	movl	(%rdx), %r10d
	vmovss	4(%rdx), %xmm0                  # xmm0 = mem[0],zero,zero,zero
	movl	16(%rdx), %r9d
	movslq	20(%rdx), %r11
	movq	24(%rdx), %r8
	movq	40(%rdx), %rax
	movl	%esi, %edi
	andl	$-2, %edi
	leal	(%rsi,%rsi), %ecx
	vmulss	8(%rdx), %xmm0, %xmm0
	andl	$2, %ecx
	vbroadcastss	%xmm0, %ymm1
	vbroadcastss	12(%rdx), %ymm0
	movl	%edi, %edx
	imull	%r9d, %edx
	imull	%r10d, %edi
	movslq	%edi, %r14
	leal	(%rdx,%rcx,8), %ebx
	movslq	%ebx, %rbx
	leaq	(%rbx,%r11), %rdi
	vmulps	(%rax,%rdi,4), %ymm1, %ymm3
	leaq	(%r14,%rcx,8), %rdi
	vmovups	(%r8,%rdi,4), %ymm4
	leaq	(%rbx,%r11,2), %rdi
	vmulps	(%rax,%rdi,4), %ymm0, %ymm2
	vsqrtps	%ymm2, %ymm5
	vbroadcastss	.LCPI66_0(%rip), %ymm2  # ymm2 = [9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6,9.99999974E-6]
	vaddps	%ymm2, %ymm5, %ymm5
	vdivps	%ymm5, %ymm3, %ymm3
	vsubps	%ymm3, %ymm4, %ymm3
	vmovups	%ymm3, (%rax,%rbx,4)
	leal	8(%rdx,%rcx,8), %edx
	movslq	%edx, %rdx
	leaq	(%rdx,%r11), %rdi
	vmulps	(%rax,%rdi,4), %ymm1, %ymm3
	leaq	8(%r14,%rcx,8), %rdi
	leaq	(%rdx,%r11,2), %rbx
	vmulps	(%rax,%rbx,4), %ymm0, %ymm4
	vmovups	(%r8,%rdi,4), %ymm5
	vsqrtps	%ymm4, %ymm4
	vaddps	%ymm2, %ymm4, %ymm4
	vdivps	%ymm4, %ymm3, %ymm3
	vsubps	%ymm3, %ymm5, %ymm3
	vmovups	%ymm3, (%rax,%rdx,4)
	orl	$1, %esi
	imull	%esi, %r9d
	imull	%r10d, %esi
	movslq	%esi, %rdx
	leal	(%r9,%rcx,8), %esi
	movslq	%esi, %rsi
	leaq	(%rsi,%r11), %rdi
	vmulps	(%rax,%rdi,4), %ymm1, %ymm3
	leaq	(%rdx,%rcx,8), %rdi
	leaq	(%rsi,%r11,2), %rbx
	vmulps	(%rax,%rbx,4), %ymm0, %ymm4
	vmovups	(%r8,%rdi,4), %ymm5
	vsqrtps	%ymm4, %ymm4
	vaddps	%ymm2, %ymm4, %ymm4
	vdivps	%ymm4, %ymm3, %ymm3
	vsubps	%ymm3, %ymm5, %ymm3
	vmovups	%ymm3, (%rax,%rsi,4)
	leal	8(%r9,%rcx,8), %esi
	movslq	%esi, %rsi
	leaq	(%rsi,%r11), %rdi
	vmulps	(%rax,%rdi,4), %ymm1, %ymm1
	leaq	8(%rdx,%rcx,8), %rcx
	leaq	(%rsi,%r11,2), %rdx
	vmulps	(%rax,%rdx,4), %ymm0, %ymm0
	vmovups	(%r8,%rcx,4), %ymm3
	vsqrtps	%ymm0, %ymm0
	vaddps	%ymm2, %ymm0, %ymm0
	vdivps	%ymm0, %ymm1, %ymm0
	vsubps	%ymm0, %ymm3, %ymm0
	vmovups	%ymm0, (%rax,%rsi,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	vzeroupper
	retq
.Lfunc_end66:
	.size	train_cost_model.par_for.updated_filter1.s4.v251.v251.v251, .Lfunc_end66-train_cost_model.par_for.updated_filter1.s4.v251.v251.v251
                                        # -- End function
	.section	.text.train_cost_model.par_for.updated_bias1.s1.v254.v254,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.updated_bias1.s1.v254.v254
	.type	train_cost_model.par_for.updated_bias1.s1.v254.v254,@function
train_cost_model.par_for.updated_bias1.s1.v254.v254: # @train_cost_model.par_for.updated_bias1.s1.v254.v254
# %bb.0:                                # %entry
	pushq	%r14
	pushq	%rbx
	movl	(%rdx), %edi
	movslq	4(%rdx), %r8
	movq	24(%rdx), %r9
	testl	%edi, %edi
	jle	.LBB67_1
# %bb.2:                                # %"for bias1_im_0_d_def__.s1.r1193$x.preheader"
	movq	8(%rdx), %rax
	movl	%esi, %ecx
	imull	%edi, %ecx
	shll	$3, %ecx
	movslq	%ecx, %rcx
	leaq	(%rax,%rcx,4), %r10
	leaq	(,%rdi,4), %r11
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB67_3:                               # %"for bias1_im_0_d_def__.s1.r1193$x"
                                        # =>This Inner Loop Header: Depth=1
	leaq	(%r10,%rcx,4), %r14
	leaq	(%r14,%r11), %rax
	addq	%r11, %rax
	leaq	(%rax,%r11), %rbx
	addq	%r11, %rbx
	leaq	(%rbx,%r11), %rdx
	vmovss	(%rax,%rdi,8), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, (%rbx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	addq	%r11, %rdx
	vinsertps	$48, (%rdx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovss	(%r10,%rcx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, (%r14,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, (%r14,%rdi,8), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, (%rax,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vaddps	%ymm1, %ymm0, %ymm0
	incq	%rcx
	cmpq	%rcx, %rdi
	jne	.LBB67_3
.LBB67_4:                               # %"consume bias1_im_0_d_def__"
	movslq	%esi, %rax
	leaq	(%r8,%rax,8), %rax
	vmovups	%ymm0, (%r9,%rax,4)
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	vzeroupper
	retq
.LBB67_1:
	vxorps	%xmm0, %xmm0, %xmm0
	jmp	.LBB67_4
.Lfunc_end67:
	.size	train_cost_model.par_for.updated_bias1.s1.v254.v254, .Lfunc_end67-train_cost_model.par_for.updated_bias1.s1.v254.v254
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_bias1.s2.v254.v254
.LCPI68_0:
	.long	0x3f666666                      # float 0.899999976
.LCPI68_1:
	.long	0x3dcccccd                      # float 0.100000001
	.section	.text.train_cost_model.par_for.updated_bias1.s2.v254.v254,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_bias1.s2.v254.v254,@function
train_cost_model.par_for.updated_bias1.s2.v254.v254: # @train_cost_model.par_for.updated_bias1.s2.v254.v254
# %bb.0:                                # %entry
	movslq	(%rdx), %rax
	movq	8(%rdx), %rcx
	movslq	%esi, %rdx
	leaq	(%rax,%rdx,8), %rsi
	vbroadcastss	.LCPI68_0(%rip), %ymm0  # ymm0 = [8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1,8.99999976E-1]
	leaq	(%rax,%rax,2), %rax
	leaq	(%rax,%rdx,8), %rax
	vbroadcastss	.LCPI68_1(%rip), %ymm1  # ymm1 = [1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1,1.00000001E-1]
	vmulps	(%rcx,%rax,4), %ymm1, %ymm1
	vfmadd231ps	(%rcx,%rsi,4), %ymm0, %ymm1 # ymm1 = (ymm0 * mem) + ymm1
	vmovups	%ymm1, (%rcx,%rsi,4)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end68:
	.size	train_cost_model.par_for.updated_bias1.s2.v254.v254, .Lfunc_end68-train_cost_model.par_for.updated_bias1.s2.v254.v254
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_bias1.s3.v254.v254
.LCPI69_0:
	.long	0x3f7fbe77                      # float 0.999000012
.LCPI69_1:
	.long	0x3a83126f                      # float 0.00100000005
	.section	.text.train_cost_model.par_for.updated_bias1.s3.v254.v254,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_bias1.s3.v254.v254,@function
train_cost_model.par_for.updated_bias1.s3.v254.v254: # @train_cost_model.par_for.updated_bias1.s3.v254.v254
# %bb.0:                                # %entry
	movslq	(%rdx), %rax
	movq	8(%rdx), %rcx
	leaq	(%rax,%rax,2), %rdx
	movslq	%esi, %rsi
	leaq	(%rdx,%rsi,8), %rdx
	vmovups	(%rcx,%rdx,4), %ymm0
	leaq	(%rax,%rsi,4), %rax
	vbroadcastss	.LCPI69_0(%rip), %ymm1  # ymm1 = [9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1,9.99000012E-1]
	vmulps	%ymm0, %ymm0, %ymm0
	vbroadcastss	.LCPI69_1(%rip), %ymm2  # ymm2 = [1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3,1.00000005E-3]
	vmulps	%ymm2, %ymm0, %ymm0
	vfmadd231ps	(%rcx,%rax,8), %ymm1, %ymm0 # ymm0 = (ymm1 * mem) + ymm0
	vmovups	%ymm0, (%rcx,%rax,8)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end69:
	.size	train_cost_model.par_for.updated_bias1.s3.v254.v254, .Lfunc_end69-train_cost_model.par_for.updated_bias1.s3.v254.v254
                                        # -- End function
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2                               # -- Begin function train_cost_model.par_for.updated_bias1.s4.v254.v254
.LCPI70_0:
	.long	0xb727c5ac                      # float -9.99999974E-6
	.section	.text.train_cost_model.par_for.updated_bias1.s4.v254.v254,"ax",@progbits
	.p2align	4, 0x90
	.type	train_cost_model.par_for.updated_bias1.s4.v254.v254,@function
train_cost_model.par_for.updated_bias1.s4.v254.v254: # @train_cost_model.par_for.updated_bias1.s4.v254.v254
# %bb.0:                                # %entry
	vmovss	(%rdx), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	movslq	12(%rdx), %rax
	movq	32(%rdx), %rcx
	movslq	%esi, %rsi
	vmulss	4(%rdx), %xmm0, %xmm0
	leaq	(%rax,%rsi,8), %rdi
	vbroadcastss	%xmm0, %ymm0
	vmulps	(%rcx,%rdi,4), %ymm0, %ymm0
	leaq	(%rax,%rsi,4), %rax
	vbroadcastss	8(%rdx), %ymm1
	vmulps	(%rcx,%rax,8), %ymm1, %ymm1
	vsqrtps	%ymm1, %ymm1
	vbroadcastss	.LCPI70_0(%rip), %ymm2  # ymm2 = [-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6,-9.99999974E-6]
	vsubps	%ymm1, %ymm2, %ymm1
	vdivps	%ymm1, %ymm0, %ymm0
	movq	16(%rdx), %rax
	shlq	$5, %rsi
	vaddps	(%rax,%rsi), %ymm0, %ymm0
	vmovups	%ymm0, (%rcx,%rsi)
	xorl	%eax, %eax
	vzeroupper
	retq
.Lfunc_end70:
	.size	train_cost_model.par_for.updated_bias1.s4.v254.v254, .Lfunc_end70-train_cost_model.par_for.updated_bias1.s4.v254.v254
                                        # -- End function
	.section	.text.train_cost_model.par_for.prediction_output.s0.n.n,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.prediction_output.s0.n.n
	.type	train_cost_model.par_for.prediction_output.s0.n.n,@function
train_cost_model.par_for.prediction_output.s0.n.n: # @train_cost_model.par_for.prediction_output.s0.n.n
# %bb.0:                                # %entry
	pushq	%r14
	pushq	%rbx
	movslq	8(%rdx), %rax
	movq	16(%rdx), %r11
	movq	32(%rdx), %r10
	cmpl	%esi, 4(%rdx)
	jle	.LBB71_2
# %bb.1:                                # %true_bb
	movslq	%esi, %rcx
	leaq	(%rax,%rcx,8), %rax
	vmovups	(%r11,%rax,4), %ymm0
	shlq	$5, %rcx
	vmovups	%ymm0, (%r10,%rcx)
	jmp	.LBB71_15
.LBB71_2:                               # %false_bb
	movl	(%rdx), %ecx
	shll	$3, %esi
	subl	%esi, %ecx
	jle	.LBB71_15
# %bb.3:                                # %"for prediction_output.s0.n.ni.preheader"
	cmpl	$9, %ecx
	movl	$8, %r14d
	cmovll	%ecx, %r14d
	addl	%esi, %eax
	movslq	%eax, %r8
	movslq	%esi, %r9
	cmpl	$32, %r14d
	jae	.LBB71_5
# %bb.4:
	xorl	%esi, %esi
	jmp	.LBB71_13
.LBB71_5:                               # %vector.ph
	movl	%r14d, %esi
	andl	$-32, %esi
	leaq	-32(%rsi), %rcx
	movq	%rcx, %rdi
	shrq	$5, %rdi
	incq	%rdi
	movl	%edi, %eax
	andl	$3, %eax
	cmpq	$96, %rcx
	jae	.LBB71_7
# %bb.6:
	xorl	%ecx, %ecx
	jmp	.LBB71_9
.LBB71_7:                               # %vector.ph.new
	leaq	(%r10,%r9,4), %rbx
	addq	$480, %rbx                      # imm = 0x1E0
	leaq	(%r11,%r8,4), %rdx
	addq	$480, %rdx                      # imm = 0x1E0
	andq	$-4, %rdi
	negq	%rdi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB71_8:                               # %vector.body
                                        # =>This Inner Loop Header: Depth=1
	vmovups	-480(%rdx,%rcx,4), %ymm0
	vmovups	-448(%rdx,%rcx,4), %ymm1
	vmovups	-416(%rdx,%rcx,4), %ymm2
	vmovups	-384(%rdx,%rcx,4), %ymm3
	vmovups	%ymm0, -480(%rbx,%rcx,4)
	vmovups	%ymm1, -448(%rbx,%rcx,4)
	vmovups	%ymm2, -416(%rbx,%rcx,4)
	vmovups	%ymm3, -384(%rbx,%rcx,4)
	vmovups	-352(%rdx,%rcx,4), %ymm0
	vmovups	-320(%rdx,%rcx,4), %ymm1
	vmovups	-288(%rdx,%rcx,4), %ymm2
	vmovups	-256(%rdx,%rcx,4), %ymm3
	vmovups	%ymm0, -352(%rbx,%rcx,4)
	vmovups	%ymm1, -320(%rbx,%rcx,4)
	vmovups	%ymm2, -288(%rbx,%rcx,4)
	vmovups	%ymm3, -256(%rbx,%rcx,4)
	vmovups	-224(%rdx,%rcx,4), %ymm0
	vmovups	-192(%rdx,%rcx,4), %ymm1
	vmovups	-160(%rdx,%rcx,4), %ymm2
	vmovups	-128(%rdx,%rcx,4), %ymm3
	vmovups	%ymm0, -224(%rbx,%rcx,4)
	vmovups	%ymm1, -192(%rbx,%rcx,4)
	vmovups	%ymm2, -160(%rbx,%rcx,4)
	vmovups	%ymm3, -128(%rbx,%rcx,4)
	vmovups	-96(%rdx,%rcx,4), %ymm0
	vmovups	-64(%rdx,%rcx,4), %ymm1
	vmovups	-32(%rdx,%rcx,4), %ymm2
	vmovups	(%rdx,%rcx,4), %ymm3
	vmovups	%ymm0, -96(%rbx,%rcx,4)
	vmovups	%ymm1, -64(%rbx,%rcx,4)
	vmovups	%ymm2, -32(%rbx,%rcx,4)
	vmovups	%ymm3, (%rbx,%rcx,4)
	subq	$-128, %rcx
	addq	$4, %rdi
	jne	.LBB71_8
.LBB71_9:                               # %middle.block.unr-lcssa
	testq	%rax, %rax
	je	.LBB71_12
# %bb.10:                               # %vector.body.epil.preheader
	leaq	(%r10,%r9,4), %rdx
	leaq	96(,%rcx,4), %rcx
	leaq	(%r11,%r8,4), %rdi
	negq	%rax
	.p2align	4, 0x90
.LBB71_11:                              # %vector.body.epil
                                        # =>This Inner Loop Header: Depth=1
	vmovups	-96(%rdi,%rcx), %ymm0
	vmovups	-64(%rdi,%rcx), %ymm1
	vmovups	-32(%rdi,%rcx), %ymm2
	vmovups	(%rdi,%rcx), %ymm3
	vmovups	%ymm0, -96(%rdx,%rcx)
	vmovups	%ymm1, -64(%rdx,%rcx)
	vmovups	%ymm2, -32(%rdx,%rcx)
	vmovups	%ymm3, (%rdx,%rcx)
	subq	$-128, %rcx
	incq	%rax
	jne	.LBB71_11
.LBB71_12:                              # %middle.block
	cmpq	%r14, %rsi
	je	.LBB71_15
.LBB71_13:                              # %"for prediction_output.s0.n.ni.preheader5"
	leaq	(%r10,%r9,4), %rax
	leaq	(%r11,%r8,4), %rcx
	.p2align	4, 0x90
.LBB71_14:                              # %"for prediction_output.s0.n.ni"
                                        # =>This Inner Loop Header: Depth=1
	movl	(%rcx,%rsi,4), %edx
	movl	%edx, (%rax,%rsi,4)
	incq	%rsi
	cmpq	%rsi, %r14
	jne	.LBB71_14
.LBB71_15:                              # %destructor_block
	xorl	%eax, %eax
	popq	%rbx
	popq	%r14
	vzeroupper
	retq
.Lfunc_end71:
	.size	train_cost_model.par_for.prediction_output.s0.n.n, .Lfunc_end71-train_cost_model.par_for.prediction_output.s0.n.n
                                        # -- End function
	.section	.text.train_cost_model.par_for.sum.s0.n.n,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.sum.s0.n.n
	.type	train_cost_model.par_for.sum.s0.n.n,@function
train_cost_model.par_for.sum.s0.n.n:    # @train_cost_model.par_for.sum.s0.n.n
# %bb.0:                                # %entry
	pushq	%rax
	movq	8(%rdx), %rax
	cmpl	%esi, 4(%rdx)
	jle	.LBB72_2
# %bb.1:                                # %true_bb
	movslq	%esi, %rcx
	shlq	$5, %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovaps	%ymm0, (%rax,%rcx)
.LBB72_4:                               # %destructor_block
	xorl	%eax, %eax
	popq	%rcx
	vzeroupper
	retq
.LBB72_2:                               # %false_bb
	movl	(%rdx), %ecx
	shll	$3, %esi
	subl	%esi, %ecx
	jle	.LBB72_4
# %bb.3:                                # %"for sum.s0.n.ni.preheader"
	cmpl	$9, %ecx
	movl	$8, %edx
	cmovll	%ecx, %edx
	movslq	%esi, %rcx
	leaq	(%rax,%rcx,4), %rdi
	decl	%edx
	leaq	4(,%rdx,4), %rdx
	xorl	%esi, %esi
	callq	memset@PLT
	xorl	%eax, %eax
	popq	%rcx
	retq
.Lfunc_end72:
	.size	train_cost_model.par_for.sum.s0.n.n, .Lfunc_end72-train_cost_model.par_for.sum.s0.n.n
                                        # -- End function
	.section	.text.train_cost_model.par_for.sum.s1.n.n,"ax",@progbits
	.p2align	4, 0x90                         # -- Begin function train_cost_model.par_for.sum.s1.n.n
	.type	train_cost_model.par_for.sum.s1.n.n,@function
train_cost_model.par_for.sum.s1.n.n:    # @train_cost_model.par_for.sum.s1.n.n
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$136, %rsp
                                        # kill: def $esi killed $esi def $rsi
	movslq	4(%rdx), %r11
	movl	8(%rdx), %ecx
	movl	16(%rdx), %edi
	movq	24(%rdx), %r9
	movq	40(%rdx), %r13
	leal	(,%rsi,8), %eax
	cmpl	%esi, 12(%rdx)
	jle	.LBB73_5
# %bb.1:                                # %true_bb
	testl	%ecx, %ecx
	jle	.LBB73_22
# %bb.2:                                # %"for sum.s1.r107$y.preheader"
	movl	%r11d, %edx
	shll	$5, %edx
	movl	%edx, -48(%rsp)                 # 4-byte Spill
	movslq	%eax, %rdx
	movq	%rdx, -40(%rsp)                 # 8-byte Spill
	vmovaps	(%r13,%rdx,4), %ymm0
	vmovups	%ymm0, 64(%rsp)                 # 32-byte Spill
	leaq	(%r11,%r11), %rdx
	leaq	(%r11,%r11,2), %rsi
	leaq	(,%r11,4), %r12
	leaq	(%r11,%r11,4), %rbx
	leaq	(%rdx,%rdx,2), %rbp
	movq	%rbp, -80(%rsp)                 # 8-byte Spill
	leaq	(,%r11,8), %r8
	movl	%edi, %r10d
	movq	%r8, %rdi
	subq	%r11, %rdi
	movq	%rdi, -96(%rsp)                 # 8-byte Spill
	leaq	(%r11,%r11,8), %r15
	movq	%r11, %rdi
	shlq	$4, %rdi
	movq	%rdi, %rbp
	subq	%r11, %rbp
	subq	%r11, %rbp
	movq	%rbp, -112(%rsp)                # 8-byte Spill
	leaq	(,%rsi,8), %rbp
	subq	%r11, %rbp
	movq	%rbp, -120(%rsp)                # 8-byte Spill
	movq	%r11, %r14
	shlq	$5, %r14
	subq	%r11, %r14
	subl	%r10d, %eax
	movq	%r14, %r10
	subq	%r11, %r10
	vxorps	%xmm1, %xmm1, %xmm1
	leaq	(%rdx,%rdx,4), %rbp
	movq	%rbp, -128(%rsp)                # 8-byte Spill
	leaq	(%r11,%rbx,2), %rbp
	movq	%rbp, 48(%rsp)                  # 8-byte Spill
	leaq	(%r12,%r12,2), %rbp
	movq	%rbp, 40(%rsp)                  # 8-byte Spill
	movq	%rsi, -64(%rsp)                 # 8-byte Spill
	leaq	(%r11,%rsi,4), %rsi
	movq	%rsi, 32(%rsp)                  # 8-byte Spill
	leaq	(%rbx,%rbx,2), %rsi
	movq	%rsi, 24(%rsp)                  # 8-byte Spill
	movq	%rdi, -104(%rsp)                # 8-byte Spill
	leaq	(%r11,%rdi), %rsi
	movq	%rsi, 16(%rsp)                  # 8-byte Spill
	movq	%rdx, -56(%rsp)                 # 8-byte Spill
	leaq	(%rdx,%rdx,8), %rdx
	movq	%rdx, 8(%rsp)                   # 8-byte Spill
	leaq	(%r11,%r15,2), %rdx
	movq	%rdx, (%rsp)                    # 8-byte Spill
	movq	%r12, -72(%rsp)                 # 8-byte Spill
	leaq	(%r12,%r12,4), %rdx
	movq	%rdx, -8(%rsp)                  # 8-byte Spill
	leaq	(%r11,%rbx,4), %r12
	leaq	(%r11,%r12), %rdx
	movq	%rdx, -16(%rsp)                 # 8-byte Spill
	movq	%rcx, %rdi
	movq	%r8, -88(%rsp)                  # 8-byte Spill
	leaq	(%r8,%r8,2), %rcx
	movq	%rcx, -24(%rsp)                 # 8-byte Spill
	movq	%rbx, %r8
	leaq	(%rbx,%rbx,4), %rbp
	leaq	(%rbp,%r11), %rcx
	movq	%rcx, -32(%rsp)                 # 8-byte Spill
	movq	%r15, %rbx
	leaq	(%r15,%r15,2), %rdx
	leaq	(%rdx,%r11), %rsi
	leaq	(%r11,%rsi), %r15
	.p2align	4, 0x90
.LBB73_3:                               # %"for sum.s1.r107$y"
                                        # =>This Inner Loop Header: Depth=1
	cltq
	vmovups	(%r9,%rax,4), %ymm2
	vminps	%ymm1, %ymm2, %ymm2
	leaq	(%r11,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm2
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	movq	-64(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm2
	movq	-72(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	leaq	(%r8,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	-80(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm2
	movq	-96(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	movq	-88(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	leaq	(%rbx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	-128(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm2
	movq	48(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	movq	40(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	32(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	-112(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	24(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vaddps	%ymm3, %ymm2, %ymm0
	vmovups	%ymm0, 96(%rsp)                 # 32-byte Spill
	movq	-104(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm3
	vminps	%ymm1, %ymm3, %ymm3
	movq	16(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	movq	8(%rsp), %rcx                   # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm4
	movq	(%rsp), %rcx                    # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm5
	movq	-8(%rsp), %rcx                  # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm6
	leaq	(%r12,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm7
	movq	-16(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm8
	movq	-120(%rsp), %rcx                # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm9
	movq	-24(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm10
	leaq	(%rbp,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm11
	movq	-32(%rsp), %rcx                 # 8-byte Reload
	leaq	(%rcx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm12
	leaq	(%rdx,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm13
	leaq	(%rsi,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm14
	leaq	(%r15,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm15
	leaq	(%r10,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm2
	leaq	(%r14,%rax), %rcx
	vmovups	(%r9,%rcx,4), %ymm0
	vminps	%ymm1, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vminps	%ymm1, %ymm5, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vminps	%ymm1, %ymm6, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vminps	%ymm1, %ymm7, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vaddps	96(%rsp), %ymm3, %ymm3          # 32-byte Folded Reload
	vminps	%ymm1, %ymm8, %ymm4
	vminps	%ymm1, %ymm9, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vminps	%ymm1, %ymm10, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vminps	%ymm1, %ymm11, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vminps	%ymm1, %ymm12, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vminps	%ymm1, %ymm13, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vminps	%ymm1, %ymm14, %ymm5
	vaddps	%ymm5, %ymm4, %ymm4
	vaddps	%ymm4, %ymm3, %ymm3
	vminps	%ymm1, %ymm15, %ymm4
	vminps	%ymm1, %ymm2, %ymm2
	vaddps	%ymm2, %ymm4, %ymm2
	vminps	%ymm1, %ymm0, %ymm0
	vaddps	%ymm0, %ymm2, %ymm0
	vaddps	%ymm0, %ymm3, %ymm0
	vmovups	64(%rsp), %ymm2                 # 32-byte Reload
	vsubps	%ymm0, %ymm2, %ymm2
	vmovups	%ymm2, 64(%rsp)                 # 32-byte Spill
	addl	-48(%rsp), %eax                 # 4-byte Folded Reload
	decq	%rdi
	jne	.LBB73_3
# %bb.4:                                # %destructor_block.loopexit
	movq	-40(%rsp), %rax                 # 8-byte Reload
	vmovups	64(%rsp), %ymm0                 # 32-byte Reload
	vmovaps	%ymm0, (%r13,%rax,4)
.LBB73_22:                              # %destructor_block
	xorl	%eax, %eax
	addq	$136, %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.LBB73_5:                               # %false_bb
	testl	%ecx, %ecx
	jle	.LBB73_22
# %bb.6:                                # %"for sum.s1.r107$y1.preheader"
	movl	%eax, %r10d
	subl	%edi, %r10d
	movl	(%rdx), %edi
	subl	%eax, %edi
	cmpl	$9, %edi
	movl	$8, %ebx
	cmovll	%edi, %ebx
	xorl	%edx, %edx
	testl	%edi, %edi
	cmovlel	%edx, %ebx
	movslq	%eax, %r14
	movl	%ebx, %eax
	andl	$2147483616, %eax               # imm = 0x7FFFFFE0
	movq	%rax, -56(%rsp)                 # 8-byte Spill
	addq	$-32, %rax
	movq	%rax, -64(%rsp)                 # 8-byte Spill
	movq	%rax, %rsi
	shrq	$5, %rsi
	incq	%rsi
	leaq	224(%r9), %rax
	movq	%rax, -80(%rsp)                 # 8-byte Spill
	movl	%r11d, %eax
	shll	$5, %eax
	movl	%eax, -128(%rsp)                # 4-byte Spill
	leaq	224(,%r14,4), %rbp
	addq	%r13, %rbp
	movq	%rsi, %rax
	movq	%rsi, -72(%rsp)                 # 8-byte Spill
	andq	$-2, %rsi
	negq	%rsi
	movq	%rsi, -88(%rsp)                 # 8-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vxorps	%xmm1, %xmm1, %xmm1
	movl	%r10d, -104(%rsp)               # 4-byte Spill
	movq	%r11, 64(%rsp)                  # 8-byte Spill
	movl	%edi, 96(%rsp)                  # 4-byte Spill
	movq	%r14, -48(%rsp)                 # 8-byte Spill
	movq	%rcx, 56(%rsp)                  # 8-byte Spill
	jmp	.LBB73_7
	.p2align	4, 0x90
.LBB73_21:                              # %"end for sum.s1.r107$x5"
                                        #   in Loop: Header=BB73_7 Depth=1
	movq	-112(%rsp), %rdx                # 8-byte Reload
	incq	%rdx
	movl	-120(%rsp), %r10d               # 4-byte Reload
	addl	-128(%rsp), %r10d               # 4-byte Folded Reload
	movq	56(%rsp), %rcx                  # 8-byte Reload
	cmpq	%rcx, %rdx
	je	.LBB73_22
.LBB73_7:                               # %"for sum.s1.r107$y1"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB73_8 Depth 2
                                        #       Child Loop BB73_16 Depth 3
                                        #       Child Loop BB73_12 Depth 3
	movq	%rdx, -112(%rsp)                # 8-byte Spill
	shlq	$5, %rdx
	movq	%rdx, -96(%rsp)                 # 8-byte Spill
	movl	%r10d, -120(%rsp)               # 4-byte Spill
	xorl	%r8d, %r8d
	jmp	.LBB73_8
	.p2align	4, 0x90
.LBB73_20:                              # %"end for sum.s1.n.ni"
                                        #   in Loop: Header=BB73_8 Depth=2
	incq	%r8
	addl	%r11d, %r10d
	cmpq	$32, %r8
	je	.LBB73_21
.LBB73_8:                               # %"for sum.s1.r107$x4"
                                        #   Parent Loop BB73_7 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB73_16 Depth 3
                                        #       Child Loop BB73_12 Depth 3
	testl	%edi, %edi
	jle	.LBB73_20
# %bb.9:                                # %"for sum.s1.n.ni.preheader"
                                        #   in Loop: Header=BB73_8 Depth=2
	movslq	%r10d, %r15
	cmpl	$31, %ebx
	ja	.LBB73_13
# %bb.10:                               #   in Loop: Header=BB73_8 Depth=2
	xorl	%eax, %eax
	jmp	.LBB73_11
	.p2align	4, 0x90
.LBB73_13:                              # %vector.ph
                                        #   in Loop: Header=BB73_8 Depth=2
	cmpq	$0, -64(%rsp)                   # 8-byte Folded Reload
	je	.LBB73_14
# %bb.15:                               # %vector.body.preheader
                                        #   in Loop: Header=BB73_8 Depth=2
	movq	-80(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%r15,4), %r11
	movq	-88(%rsp), %r14                 # 8-byte Reload
	xorl	%r12d, %r12d
	.p2align	4, 0x90
.LBB73_16:                              # %vector.body
                                        #   Parent Loop BB73_7 Depth=1
                                        #     Parent Loop BB73_8 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	-224(%rbp,%r12,4), %ymm2
	vmovups	-192(%rbp,%r12,4), %ymm3
	vmovups	-160(%rbp,%r12,4), %ymm4
	vmovups	-128(%rbp,%r12,4), %ymm5
	vmovups	-224(%r11,%r12,4), %ymm6
	vmovups	-192(%r11,%r12,4), %ymm7
	vmovups	-160(%r11,%r12,4), %ymm8
	vmovups	-128(%r11,%r12,4), %ymm9
	vminps	%ymm1, %ymm6, %ymm6
	vsubps	%ymm6, %ymm2, %ymm2
	vminps	%ymm1, %ymm7, %ymm6
	vsubps	%ymm6, %ymm3, %ymm3
	vminps	%ymm1, %ymm8, %ymm6
	vsubps	%ymm6, %ymm4, %ymm4
	vminps	%ymm1, %ymm9, %ymm6
	vsubps	%ymm6, %ymm5, %ymm5
	vmovups	%ymm2, -224(%rbp,%r12,4)
	vmovups	%ymm3, -192(%rbp,%r12,4)
	vmovups	%ymm4, -160(%rbp,%r12,4)
	vmovups	%ymm5, -128(%rbp,%r12,4)
	vmovups	-96(%rbp,%r12,4), %ymm2
	vmovups	-64(%rbp,%r12,4), %ymm3
	vmovups	-32(%rbp,%r12,4), %ymm4
	vmovups	(%rbp,%r12,4), %ymm5
	vmovups	-96(%r11,%r12,4), %ymm6
	vmovups	-64(%r11,%r12,4), %ymm7
	vmovups	-32(%r11,%r12,4), %ymm8
	vmovups	(%r11,%r12,4), %ymm9
	vminps	%ymm1, %ymm6, %ymm6
	vsubps	%ymm6, %ymm2, %ymm2
	vminps	%ymm1, %ymm7, %ymm6
	vsubps	%ymm6, %ymm3, %ymm3
	vminps	%ymm1, %ymm8, %ymm6
	vsubps	%ymm6, %ymm4, %ymm4
	vminps	%ymm1, %ymm9, %ymm6
	vsubps	%ymm6, %ymm5, %ymm5
	vmovups	%ymm2, -96(%rbp,%r12,4)
	vmovups	%ymm3, -64(%rbp,%r12,4)
	vmovups	%ymm4, -32(%rbp,%r12,4)
	vmovups	%ymm5, (%rbp,%r12,4)
	addq	$64, %r12
	addq	$2, %r14
	jne	.LBB73_16
# %bb.17:                               # %middle.block.unr-lcssa
                                        #   in Loop: Header=BB73_8 Depth=2
	testb	$1, -72(%rsp)                   # 1-byte Folded Reload
	movq	64(%rsp), %r11                  # 8-byte Reload
	movq	-48(%rsp), %r14                 # 8-byte Reload
	je	.LBB73_19
.LBB73_18:                              # %vector.body.epil
                                        #   in Loop: Header=BB73_8 Depth=2
	movq	-96(%rsp), %rax                 # 8-byte Reload
	addl	%r8d, %eax
	imull	%r11d, %eax
	addl	-104(%rsp), %eax                # 4-byte Folded Reload
	cltq
	leaq	(%r12,%r14), %rcx
	vmovups	(%r13,%rcx,4), %ymm2
	vmovups	32(%r13,%rcx,4), %ymm3
	vmovups	64(%r13,%rcx,4), %ymm4
	vmovups	96(%r13,%rcx,4), %ymm5
	addq	%r12, %rax
	vmovups	(%r9,%rax,4), %ymm6
	vmovups	32(%r9,%rax,4), %ymm7
	vmovups	64(%r9,%rax,4), %ymm8
	vmovups	96(%r9,%rax,4), %ymm9
	vminps	%ymm1, %ymm6, %ymm6
	vsubps	%ymm6, %ymm2, %ymm2
	vminps	%ymm1, %ymm7, %ymm6
	vsubps	%ymm6, %ymm3, %ymm3
	vminps	%ymm1, %ymm8, %ymm6
	vsubps	%ymm6, %ymm4, %ymm4
	vminps	%ymm1, %ymm9, %ymm6
	vsubps	%ymm6, %ymm5, %ymm5
	vmovups	%ymm2, (%r13,%rcx,4)
	vmovups	%ymm3, 32(%r13,%rcx,4)
	vmovups	%ymm4, 64(%r13,%rcx,4)
	vmovups	%ymm5, 96(%r13,%rcx,4)
.LBB73_19:                              # %middle.block
                                        #   in Loop: Header=BB73_8 Depth=2
	movq	-56(%rsp), %rcx                 # 8-byte Reload
	movq	%rcx, %rax
	cmpq	%rbx, %rcx
	movl	96(%rsp), %edi                  # 4-byte Reload
	je	.LBB73_20
.LBB73_11:                              # %"for sum.s1.n.ni.preheader36"
                                        #   in Loop: Header=BB73_8 Depth=2
	movq	%rbx, %rsi
	subq	%rax, %rsi
	addq	%rax, %r15
	leaq	(%r9,%r15,4), %rdx
	addq	%r14, %rax
	leaq	(,%rax,4), %rax
	addq	%r13, %rax
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB73_12:                              # %"for sum.s1.n.ni"
                                        #   Parent Loop BB73_7 Depth=1
                                        #     Parent Loop BB73_8 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovss	(%rax,%rcx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmovss	(%rdx,%rcx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vminss	%xmm0, %xmm3, %xmm3
	vsubss	%xmm3, %xmm2, %xmm2
	vmovss	%xmm2, (%rax,%rcx,4)
	incq	%rcx
	cmpq	%rcx, %rsi
	jne	.LBB73_12
	jmp	.LBB73_20
.LBB73_14:                              #   in Loop: Header=BB73_8 Depth=2
	xorl	%r12d, %r12d
	testb	$1, -72(%rsp)                   # 1-byte Folded Reload
	movq	64(%rsp), %r11                  # 8-byte Reload
	movq	-48(%rsp), %r14                 # 8-byte Reload
	jne	.LBB73_18
	jmp	.LBB73_19
.Lfunc_end73:
	.size	train_cost_model.par_for.sum.s1.n.n, .Lfunc_end73-train_cost_model.par_for.sum.s1.n.n
                                        # -- End function
	.section	.text.train_cost_model_argv,"ax",@progbits
	.globl	train_cost_model_argv           # -- Begin function train_cost_model_argv
	.p2align	4, 0x90
	.type	train_cost_model_argv,@function
train_cost_model_argv:                  # @train_cost_model_argv
	.cfi_startproc
# %bb.0:                                # %entry
	pushq	%rbx
	.cfi_def_cfa_offset 16
	subq	$160, %rsp
	.cfi_def_cfa_offset 176
	.cfi_offset %rbx, -16
	movq	(%rdi), %rax
	movq	8(%rdi), %rcx
	movl	(%rax), %r10d
	movl	(%rcx), %esi
	movq	16(%rdi), %rcx
	movl	(%rcx), %edx
	movq	24(%rdi), %rcx
	movq	32(%rdi), %r8
	movq	40(%rdi), %r9
	vmovups	48(%rdi), %ymm1
	movq	80(%rdi), %r11
	movq	88(%rdi), %rax
	vmovss	(%rax), %xmm0                   # xmm0 = mem[0],zero,zero,zero
	movq	96(%rdi), %rax
	movl	(%rax), %eax
	movq	104(%rdi), %rbx
	movl	(%rbx), %ebx
	vmovups	112(%rdi), %ymm2
	vmovups	144(%rdi), %ymm3
	vmovups	176(%rdi), %ymm4
	vmovups	%ymm4, 120(%rsp)
	vmovups	%ymm3, 88(%rsp)
	vmovups	%ymm2, 56(%rsp)
	movl	%ebx, 48(%rsp)
	movl	%eax, 40(%rsp)
	movq	%r11, 32(%rsp)
	vmovups	%ymm1, (%rsp)
	movl	%r10d, %edi
	vzeroupper
	callq	train_cost_model@PLT
	addq	$160, %rsp
	.cfi_def_cfa_offset 16
	popq	%rbx
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end74:
	.size	train_cost_model_argv, .Lfunc_end74-train_cost_model_argv
	.cfi_endproc
                                        # -- End function
	.section	.text.train_cost_model_metadata,"ax",@progbits
	.globl	train_cost_model_metadata       # -- Begin function train_cost_model_metadata
	.p2align	4, 0x90
	.type	train_cost_model_metadata,@function
train_cost_model_metadata:              # @train_cost_model_metadata
# %bb.0:                                # %entry
	leaq	.Ltrain_cost_model_metadata_storage(%rip), %rax
	retq
.Lfunc_end75:
	.size	train_cost_model_metadata, .Lfunc_end75-train_cost_model_metadata
                                        # -- End function
	.type	.Lstr,@object                   # @str
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr:
	.asciz	"updated_head2_filter"
	.size	.Lstr, 21

	.type	.Lstr.3,@object                 # @str.3
	.p2align	5
.Lstr.3:
	.asciz	"updated_head2_bias"
	.size	.Lstr.3, 19

	.type	.Lstr.4,@object                 # @str.4
	.p2align	5
.Lstr.4:
	.asciz	"updated_head1_filter"
	.size	.Lstr.4, 21

	.type	.Lstr.5,@object                 # @str.5
	.p2align	5
.Lstr.5:
	.asciz	"updated_head1_bias"
	.size	.Lstr.5, 19

	.type	.Lstr.6,@object                 # @str.6
	.p2align	5
.Lstr.6:
	.asciz	"updated_filter1"
	.size	.Lstr.6, 16

	.type	.Lstr.7,@object                 # @str.7
	.p2align	5
.Lstr.7:
	.asciz	"updated_bias1"
	.size	.Lstr.7, 14

	.type	.Lstr.8,@object                 # @str.8
	.p2align	5
.Lstr.8:
	.asciz	"true_runtime"
	.size	.Lstr.8, 13

	.type	.Lstr.9,@object                 # @str.9
	.p2align	5
.Lstr.9:
	.asciz	"store_cost_output"
	.size	.Lstr.9, 18

	.type	.Lstr.10,@object                # @str.10
	.p2align	5
.Lstr.10:
	.asciz	"schedule_features"
	.size	.Lstr.10, 18

	.type	.Lstr.11,@object                # @str.11
	.p2align	5
.Lstr.11:
	.asciz	"prediction_output"
	.size	.Lstr.11, 18

	.type	.Lstr.12,@object                # @str.12
	.p2align	5
.Lstr.12:
	.asciz	"pipeline_features"
	.size	.Lstr.12, 18

	.type	.Lstr.13,@object                # @str.13
	.p2align	5
.Lstr.13:
	.asciz	"loss_output"
	.size	.Lstr.13, 12

	.type	.Lstr.14,@object                # @str.14
	.p2align	5
.Lstr.14:
	.asciz	"load_cost_output"
	.size	.Lstr.14, 17

	.type	.Lstr.15,@object                # @str.15
	.p2align	5
.Lstr.15:
	.asciz	"head2_filter"
	.size	.Lstr.15, 13

	.type	.Lstr.16,@object                # @str.16
	.p2align	5
.Lstr.16:
	.asciz	"head2_bias"
	.size	.Lstr.16, 11

	.type	.Lstr.17,@object                # @str.17
	.p2align	5
.Lstr.17:
	.asciz	"head1_filter"
	.size	.Lstr.17, 13

	.type	.Lstr.18,@object                # @str.18
	.p2align	5
.Lstr.18:
	.asciz	"head1_bias"
	.size	.Lstr.18, 11

	.type	.Lstr.19,@object                # @str.19
	.p2align	5
.Lstr.19:
	.asciz	"filter1"
	.size	.Lstr.19, 8

	.type	.Lstr.20,@object                # @str.20
	.p2align	5
.Lstr.20:
	.asciz	"compute_cost_output"
	.size	.Lstr.20, 20

	.type	.Lstr.21,@object                # @str.21
	.p2align	5
.Lstr.21:
	.asciz	"bias1"
	.size	.Lstr.21, 6

	.type	.Lstr.22,@object                # @str.22
	.p2align	5
.Lstr.22:
	.asciz	"Input buffer filter1"
	.size	.Lstr.22, 21

	.type	.Lstr.23,@object                # @str.23
	.p2align	5
.Lstr.23:
	.asciz	"Input buffer bias1"
	.size	.Lstr.23, 19

	.type	.Lstr.24,@object                # @str.24
	.p2align	5
.Lstr.24:
	.asciz	"Output buffer compute_cost_output"
	.size	.Lstr.24, 34

	.type	.Lstr.25,@object                # @str.25
	.p2align	5
.Lstr.25:
	.asciz	"Input buffer head1_bias"
	.size	.Lstr.25, 24

	.type	.Lstr.26,@object                # @str.26
	.p2align	5
.Lstr.26:
	.asciz	"Input buffer head1_filter"
	.size	.Lstr.26, 26

	.type	.Lstr.27,@object                # @str.27
	.p2align	5
.Lstr.27:
	.asciz	"Input buffer head2_bias"
	.size	.Lstr.27, 24

	.type	.Lstr.28,@object                # @str.28
	.p2align	5
.Lstr.28:
	.asciz	"Input buffer head2_filter"
	.size	.Lstr.28, 26

	.type	.Lstr.29,@object                # @str.29
	.p2align	5
.Lstr.29:
	.asciz	"Output buffer load_cost_output"
	.size	.Lstr.29, 31

	.type	.Lstr.30,@object                # @str.30
	.p2align	5
.Lstr.30:
	.asciz	"Output buffer loss_output"
	.size	.Lstr.30, 26

	.type	.Lstr.31,@object                # @str.31
	.p2align	5
.Lstr.31:
	.asciz	"Input buffer pipeline_features"
	.size	.Lstr.31, 31

	.type	.Lstr.32,@object                # @str.32
	.p2align	5
.Lstr.32:
	.asciz	"Output buffer prediction_output"
	.size	.Lstr.32, 32

	.type	.Lstr.33,@object                # @str.33
	.p2align	5
.Lstr.33:
	.asciz	"Input buffer schedule_features"
	.size	.Lstr.33, 31

	.type	.Lstr.34,@object                # @str.34
	.p2align	5
.Lstr.34:
	.asciz	"Output buffer store_cost_output"
	.size	.Lstr.34, 32

	.type	.Lstr.35,@object                # @str.35
	.p2align	5
.Lstr.35:
	.asciz	"Input buffer true_runtime"
	.size	.Lstr.35, 26

	.type	.Lstr.36,@object                # @str.36
	.p2align	5
.Lstr.36:
	.asciz	"Output buffer updated_bias1"
	.size	.Lstr.36, 28

	.type	.Lstr.37,@object                # @str.37
	.p2align	5
.Lstr.37:
	.asciz	"Output buffer updated_filter1"
	.size	.Lstr.37, 30

	.type	.Lstr.38,@object                # @str.38
	.p2align	5
.Lstr.38:
	.asciz	"Output buffer updated_head1_bias"
	.size	.Lstr.38, 33

	.type	.Lstr.39,@object                # @str.39
	.p2align	5
.Lstr.39:
	.asciz	"Output buffer updated_head1_filter"
	.size	.Lstr.39, 35

	.type	.Lstr.40,@object                # @str.40
	.p2align	5
.Lstr.40:
	.asciz	"Output buffer updated_head2_bias"
	.size	.Lstr.40, 33

	.type	.Lstr.41,@object                # @str.41
	.p2align	5
.Lstr.41:
	.asciz	"Output buffer updated_head2_filter"
	.size	.Lstr.41, 35

	.type	.Lstr.42,@object                # @str.42
	.p2align	5
.Lstr.42:
	.asciz	"bias1.stride.0"
	.size	.Lstr.42, 15

	.type	.Lstr.43,@object                # @str.43
	.p2align	5
.Lstr.43:
	.asciz	"1"
	.size	.Lstr.43, 2

	.type	.Lstr.44,@object                # @str.44
	.p2align	5
.Lstr.44:
	.asciz	"bias1.min.0"
	.size	.Lstr.44, 12

	.type	.Lstr.45,@object                # @str.45
	.p2align	5
.Lstr.45:
	.asciz	"0"
	.size	.Lstr.45, 2

	.type	.Lstr.46,@object                # @str.46
	.p2align	5
.Lstr.46:
	.asciz	"bias1.extent.0"
	.size	.Lstr.46, 15

	.type	.Lstr.47,@object                # @str.47
	.p2align	5
.Lstr.47:
	.asciz	"32"
	.size	.Lstr.47, 3

	.type	.Lstr.48,@object                # @str.48
	.p2align	5
.Lstr.48:
	.asciz	"compute_cost_output.stride.0"
	.size	.Lstr.48, 29

	.type	.Lstr.49,@object                # @str.49
	.p2align	5
.Lstr.49:
	.asciz	"filter1.stride.0"
	.size	.Lstr.49, 17

	.type	.Lstr.50,@object                # @str.50
	.p2align	5
.Lstr.50:
	.asciz	"filter1.min.0"
	.size	.Lstr.50, 14

	.type	.Lstr.51,@object                # @str.51
	.p2align	5
.Lstr.51:
	.asciz	"filter1.extent.0"
	.size	.Lstr.51, 17

	.type	.Lstr.52,@object                # @str.52
	.p2align	5
.Lstr.52:
	.asciz	"filter1.min.1"
	.size	.Lstr.52, 14

	.type	.Lstr.53,@object                # @str.53
	.p2align	5
.Lstr.53:
	.asciz	"filter1.extent.1"
	.size	.Lstr.53, 17

	.type	.Lstr.54,@object                # @str.54
	.p2align	5
.Lstr.54:
	.asciz	"head1_bias.stride.0"
	.size	.Lstr.54, 20

	.type	.Lstr.55,@object                # @str.55
	.p2align	5
.Lstr.55:
	.asciz	"head1_bias.min.0"
	.size	.Lstr.55, 17

	.type	.Lstr.56,@object                # @str.56
	.p2align	5
.Lstr.56:
	.asciz	"head1_bias.extent.0"
	.size	.Lstr.56, 20

	.type	.Lstr.57,@object                # @str.57
	.p2align	5
.Lstr.57:
	.asciz	"8"
	.size	.Lstr.57, 2

	.type	.Lstr.58,@object                # @str.58
	.p2align	5
.Lstr.58:
	.asciz	"head1_filter.stride.0"
	.size	.Lstr.58, 22

	.type	.Lstr.59,@object                # @str.59
	.p2align	5
.Lstr.59:
	.asciz	"head1_filter.min.0"
	.size	.Lstr.59, 19

	.type	.Lstr.60,@object                # @str.60
	.p2align	5
.Lstr.60:
	.asciz	"head1_filter.extent.0"
	.size	.Lstr.60, 22

	.type	.Lstr.61,@object                # @str.61
	.p2align	5
.Lstr.61:
	.asciz	"head1_filter.min.1"
	.size	.Lstr.61, 19

	.type	.Lstr.62,@object                # @str.62
	.p2align	5
.Lstr.62:
	.asciz	"head1_filter.extent.1"
	.size	.Lstr.62, 22

	.type	.Lstr.63,@object                # @str.63
	.p2align	5
.Lstr.63:
	.asciz	"40"
	.size	.Lstr.63, 3

	.type	.Lstr.64,@object                # @str.64
	.p2align	5
.Lstr.64:
	.asciz	"head1_filter.min.2"
	.size	.Lstr.64, 19

	.type	.Lstr.65,@object                # @str.65
	.p2align	5
.Lstr.65:
	.asciz	"head1_filter.extent.2"
	.size	.Lstr.65, 22

	.type	.Lstr.66,@object                # @str.66
	.p2align	5
.Lstr.66:
	.asciz	"7"
	.size	.Lstr.66, 2

	.type	.Lstr.67,@object                # @str.67
	.p2align	5
.Lstr.67:
	.asciz	"head2_bias.stride.0"
	.size	.Lstr.67, 20

	.type	.Lstr.68,@object                # @str.68
	.p2align	5
.Lstr.68:
	.asciz	"head2_bias.min.0"
	.size	.Lstr.68, 17

	.type	.Lstr.69,@object                # @str.69
	.p2align	5
.Lstr.69:
	.asciz	"head2_bias.extent.0"
	.size	.Lstr.69, 20

	.type	.Lstr.70,@object                # @str.70
	.p2align	5
.Lstr.70:
	.asciz	"24"
	.size	.Lstr.70, 3

	.type	.Lstr.71,@object                # @str.71
	.p2align	5
.Lstr.71:
	.asciz	"head2_filter.stride.0"
	.size	.Lstr.71, 22

	.type	.Lstr.72,@object                # @str.72
	.p2align	5
.Lstr.72:
	.asciz	"head2_filter.min.0"
	.size	.Lstr.72, 19

	.type	.Lstr.73,@object                # @str.73
	.p2align	5
.Lstr.73:
	.asciz	"head2_filter.extent.0"
	.size	.Lstr.73, 22

	.type	.Lstr.74,@object                # @str.74
	.p2align	5
.Lstr.74:
	.asciz	"head2_filter.min.1"
	.size	.Lstr.74, 19

	.type	.Lstr.75,@object                # @str.75
	.p2align	5
.Lstr.75:
	.asciz	"head2_filter.extent.1"
	.size	.Lstr.75, 22

	.type	.Lstr.76,@object                # @str.76
	.p2align	5
.Lstr.76:
	.asciz	"39"
	.size	.Lstr.76, 3

	.type	.Lstr.77,@object                # @str.77
	.p2align	5
.Lstr.77:
	.asciz	"load_cost_output.stride.0"
	.size	.Lstr.77, 26

	.type	.Lstr.78,@object                # @str.78
	.p2align	5
.Lstr.78:
	.asciz	"pipeline_features.stride.0"
	.size	.Lstr.78, 27

	.type	.Lstr.79,@object                # @str.79
	.p2align	5
.Lstr.79:
	.asciz	"prediction_output.stride.0"
	.size	.Lstr.79, 27

	.type	.Lstr.80,@object                # @str.80
	.p2align	5
.Lstr.80:
	.asciz	"schedule_features.stride.0"
	.size	.Lstr.80, 27

	.type	.Lstr.81,@object                # @str.81
	.p2align	5
.Lstr.81:
	.asciz	"store_cost_output.stride.0"
	.size	.Lstr.81, 27

	.type	.Lstr.82,@object                # @str.82
	.p2align	5
.Lstr.82:
	.asciz	"true_runtime.stride.0"
	.size	.Lstr.82, 22

	.type	.Lstr.83,@object                # @str.83
	.p2align	5
.Lstr.83:
	.asciz	"updated_bias1.stride.0"
	.size	.Lstr.83, 23

	.type	.Lstr.84,@object                # @str.84
	.p2align	5
.Lstr.84:
	.asciz	"updated_bias1.min.0"
	.size	.Lstr.84, 20

	.type	.Lstr.85,@object                # @str.85
	.p2align	5
.Lstr.85:
	.asciz	"updated_bias1.extent.0"
	.size	.Lstr.85, 23

	.type	.Lstr.86,@object                # @str.86
	.p2align	5
.Lstr.86:
	.asciz	"updated_bias1.min.1"
	.size	.Lstr.86, 20

	.type	.Lstr.87,@object                # @str.87
	.p2align	5
.Lstr.87:
	.asciz	"updated_bias1.extent.1"
	.size	.Lstr.87, 23

	.type	.Lstr.88,@object                # @str.88
	.p2align	5
.Lstr.88:
	.asciz	"4"
	.size	.Lstr.88, 2

	.type	.Lstr.89,@object                # @str.89
	.p2align	5
.Lstr.89:
	.asciz	"updated_filter1.stride.0"
	.size	.Lstr.89, 25

	.type	.Lstr.90,@object                # @str.90
	.p2align	5
.Lstr.90:
	.asciz	"updated_filter1.min.0"
	.size	.Lstr.90, 22

	.type	.Lstr.91,@object                # @str.91
	.p2align	5
.Lstr.91:
	.asciz	"updated_filter1.extent.0"
	.size	.Lstr.91, 25

	.type	.Lstr.92,@object                # @str.92
	.p2align	5
.Lstr.92:
	.asciz	"updated_filter1.min.1"
	.size	.Lstr.92, 22

	.type	.Lstr.93,@object                # @str.93
	.p2align	5
.Lstr.93:
	.asciz	"updated_filter1.extent.1"
	.size	.Lstr.93, 25

	.type	.Lstr.94,@object                # @str.94
	.p2align	5
.Lstr.94:
	.asciz	"updated_filter1.min.2"
	.size	.Lstr.94, 22

	.type	.Lstr.95,@object                # @str.95
	.p2align	5
.Lstr.95:
	.asciz	"updated_filter1.extent.2"
	.size	.Lstr.95, 25

	.type	.Lstr.96,@object                # @str.96
	.p2align	5
.Lstr.96:
	.asciz	"updated_head1_bias.stride.0"
	.size	.Lstr.96, 28

	.type	.Lstr.97,@object                # @str.97
	.p2align	5
.Lstr.97:
	.asciz	"updated_head1_bias.min.0"
	.size	.Lstr.97, 25

	.type	.Lstr.98,@object                # @str.98
	.p2align	5
.Lstr.98:
	.asciz	"updated_head1_bias.extent.0"
	.size	.Lstr.98, 28

	.type	.Lstr.99,@object                # @str.99
	.p2align	5
.Lstr.99:
	.asciz	"updated_head1_bias.min.1"
	.size	.Lstr.99, 25

	.type	.Lstr.100,@object               # @str.100
	.p2align	5
.Lstr.100:
	.asciz	"updated_head1_bias.extent.1"
	.size	.Lstr.100, 28

	.type	.Lstr.101,@object               # @str.101
	.p2align	5
.Lstr.101:
	.asciz	"updated_head1_filter.stride.0"
	.size	.Lstr.101, 30

	.type	.Lstr.102,@object               # @str.102
	.p2align	5
.Lstr.102:
	.asciz	"updated_head1_filter.min.0"
	.size	.Lstr.102, 27

	.type	.Lstr.103,@object               # @str.103
	.p2align	5
.Lstr.103:
	.asciz	"updated_head1_filter.extent.0"
	.size	.Lstr.103, 30

	.type	.Lstr.104,@object               # @str.104
	.p2align	5
.Lstr.104:
	.asciz	"updated_head1_filter.min.1"
	.size	.Lstr.104, 27

	.type	.Lstr.105,@object               # @str.105
	.p2align	5
.Lstr.105:
	.asciz	"updated_head1_filter.extent.1"
	.size	.Lstr.105, 30

	.type	.Lstr.106,@object               # @str.106
	.p2align	5
.Lstr.106:
	.asciz	"updated_head1_filter.min.2"
	.size	.Lstr.106, 27

	.type	.Lstr.107,@object               # @str.107
	.p2align	5
.Lstr.107:
	.asciz	"updated_head1_filter.extent.2"
	.size	.Lstr.107, 30

	.type	.Lstr.108,@object               # @str.108
	.p2align	5
.Lstr.108:
	.asciz	"updated_head1_filter.min.3"
	.size	.Lstr.108, 27

	.type	.Lstr.109,@object               # @str.109
	.p2align	5
.Lstr.109:
	.asciz	"updated_head1_filter.extent.3"
	.size	.Lstr.109, 30

	.type	.Lstr.110,@object               # @str.110
	.p2align	5
.Lstr.110:
	.asciz	"updated_head2_bias.stride.0"
	.size	.Lstr.110, 28

	.type	.Lstr.111,@object               # @str.111
	.p2align	5
.Lstr.111:
	.asciz	"updated_head2_bias.min.0"
	.size	.Lstr.111, 25

	.type	.Lstr.112,@object               # @str.112
	.p2align	5
.Lstr.112:
	.asciz	"updated_head2_bias.extent.0"
	.size	.Lstr.112, 28

	.type	.Lstr.113,@object               # @str.113
	.p2align	5
.Lstr.113:
	.asciz	"updated_head2_bias.min.1"
	.size	.Lstr.113, 25

	.type	.Lstr.114,@object               # @str.114
	.p2align	5
.Lstr.114:
	.asciz	"updated_head2_bias.extent.1"
	.size	.Lstr.114, 28

	.type	.Lstr.115,@object               # @str.115
	.p2align	5
.Lstr.115:
	.asciz	"updated_head2_filter.stride.0"
	.size	.Lstr.115, 30

	.type	.Lstr.116,@object               # @str.116
	.p2align	5
.Lstr.116:
	.asciz	"updated_head2_filter.min.0"
	.size	.Lstr.116, 27

	.type	.Lstr.117,@object               # @str.117
	.p2align	5
.Lstr.117:
	.asciz	"updated_head2_filter.extent.0"
	.size	.Lstr.117, 30

	.type	.Lstr.118,@object               # @str.118
	.p2align	5
.Lstr.118:
	.asciz	"updated_head2_filter.min.1"
	.size	.Lstr.118, 27

	.type	.Lstr.119,@object               # @str.119
	.p2align	5
.Lstr.119:
	.asciz	"updated_head2_filter.extent.1"
	.size	.Lstr.119, 30

	.type	.Lstr.120,@object               # @str.120
	.p2align	5
.Lstr.120:
	.asciz	"updated_head2_filter.min.2"
	.size	.Lstr.120, 27

	.type	.Lstr.121,@object               # @str.121
	.p2align	5
.Lstr.121:
	.asciz	"updated_head2_filter.extent.2"
	.size	.Lstr.121, 30

	.type	.Lstr.122,@object               # @str.122
	.p2align	5
.Lstr.122:
	.asciz	"head1_conv"
	.size	.Lstr.122, 11

	.type	.Lstr.123,@object               # @str.123
	.p2align	5
.Lstr.123:
	.asciz	"normalized_schedule_features"
	.size	.Lstr.123, 29

	.type	.Lstr.124,@object               # @str.124
	.p2align	5
.Lstr.124:
	.asciz	"head2_conv"
	.size	.Lstr.124, 11

	.type	.Lstr.125,@object               # @str.125
	.p2align	5
.Lstr.125:
	.asciz	"head2_relu"
	.size	.Lstr.125, 11

	.type	.Lstr.126,@object               # @str.126
	.p2align	5
.Lstr.126:
	.asciz	"conv1_stage2"
	.size	.Lstr.126, 13

	.type	.Lstr.127,@object               # @str.127
	.p2align	5
.Lstr.127:
	.asciz	"conv1_stage1"
	.size	.Lstr.127, 13

	.type	.Lstr.128,@object               # @str.128
	.p2align	5
.Lstr.128:
	.asciz	"f7"
	.size	.Lstr.128, 3

	.type	.Lstr.129,@object               # @str.129
	.p2align	5
.Lstr.129:
	.asciz	"f6_0_d_def__"
	.size	.Lstr.129, 13

	.type	.Lstr.130,@object               # @str.130
	.p2align	5
.Lstr.130:
	.asciz	"f7_1_d_def__"
	.size	.Lstr.130, 13

	.type	.Lstr.131,@object               # @str.131
	.p2align	5
.Lstr.131:
	.asciz	"relu1_0_d_def__"
	.size	.Lstr.131, 16

	.type	.Lstr.132,@object               # @str.132
	.p2align	5
.Lstr.132:
	.asciz	"sum_1_d_def__"
	.size	.Lstr.132, 14

	.type	.Lstr.133,@object               # @str.133
	.p2align	5
.Lstr.133:
	.asciz	"conv1_stage2_1_d_def__"
	.size	.Lstr.133, 23

	.type	.Lstr.134,@object               # @str.134
	.p2align	5
.Lstr.134:
	.asciz	"conv1_stage1_1_d_def__"
	.size	.Lstr.134, 23

	.type	.Lstr.135,@object               # @str.135
	.p2align	5
.Lstr.135:
	.asciz	"conv1_stage2_0_d_def__$1"
	.size	.Lstr.135, 25

	.type	.Lstr.137,@object               # @str.137
	.p2align	5
.Lstr.137:
	.asciz	"head2_conv_1_d_def__"
	.size	.Lstr.137, 21

	.type	.Lstr.138,@object               # @str.138
	.p2align	5
.Lstr.138:
	.asciz	"head2_relu_0_d_def__"
	.size	.Lstr.138, 21

	.type	.Lstr.139,@object               # @str.139
	.p2align	5
.Lstr.139:
	.asciz	"sum"
	.size	.Lstr.139, 4

	.type	.Lstr.140,@object               # @str.140
	.p2align	5
.Lstr.140:
	.asciz	"num_stages"
	.size	.Lstr.140, 11

	.type	.L__unnamed_1,@object           # @0
	.p2align	3
.L__unnamed_1:
	.long	1                               # 0x1
	.long	0                               # 0x0
	.size	.L__unnamed_1, 8

	.type	.L__unnamed_2,@object           # @1
	.p2align	3
.L__unnamed_2:
	.long	13                              # 0xd
	.long	0                               # 0x0
	.size	.L__unnamed_2, 8

	.type	.Lstr.141,@object               # @str.141
	.p2align	5
.Lstr.141:
	.asciz	"batch_size"
	.size	.Lstr.141, 11

	.type	.L__unnamed_3,@object           # @2
	.p2align	3
.L__unnamed_3:
	.long	1                               # 0x1
	.long	0                               # 0x0
	.size	.L__unnamed_3, 8

	.type	.L__unnamed_4,@object           # @3
	.p2align	3
.L__unnamed_4:
	.long	80                              # 0x50
	.long	0                               # 0x0
	.size	.L__unnamed_4, 8

	.type	.Lstr.142,@object               # @str.142
	.p2align	5
.Lstr.142:
	.asciz	"num_cores"
	.size	.Lstr.142, 10

	.type	.L__unnamed_5,@object           # @4
	.p2align	3
.L__unnamed_5:
	.long	1                               # 0x1
	.long	0                               # 0x0
	.size	.L__unnamed_5, 8

	.type	.L__unnamed_6,@object           # @5
	.p2align	3
.L__unnamed_6:
	.long	32                              # 0x20
	.long	0                               # 0x0
	.size	.L__unnamed_6, 8

	.type	.L__unnamed_7,@object           # @6
	.p2align	3
.L__unnamed_7:
	.quad	0                               # 0x0
	.size	.L__unnamed_7, 8

	.type	.L__unnamed_8,@object           # @7
	.p2align	3
.L__unnamed_8:
	.quad	40                              # 0x28
	.size	.L__unnamed_8, 8

	.type	.L__unnamed_9,@object           # @8
	.p2align	3
.L__unnamed_9:
	.quad	0                               # 0x0
	.size	.L__unnamed_9, 8

	.type	.L__unnamed_10,@object          # @9
	.p2align	3
.L__unnamed_10:
	.quad	7                               # 0x7
	.size	.L__unnamed_10, 8

	.type	.L__unnamed_11,@object          # @10
	.p2align	3
.L__unnamed_11:
	.quad	0                               # 0x0
	.size	.L__unnamed_11, 8

	.type	.L__unnamed_12,@object          # @11
	.p2align	3
.L__unnamed_12:
	.quad	13                              # 0xd
	.size	.L__unnamed_12, 8

	.type	.L__unnamed_13,@object          # @12
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_13:
	.quad	.L__unnamed_7
	.quad	.L__unnamed_8
	.quad	.L__unnamed_9
	.quad	.L__unnamed_10
	.quad	.L__unnamed_11
	.quad	.L__unnamed_12
	.size	.L__unnamed_13, 48

	.type	.L__unnamed_14,@object          # @13
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_14:
	.quad	0                               # 0x0
	.size	.L__unnamed_14, 8

	.type	.L__unnamed_15,@object          # @14
	.p2align	3
.L__unnamed_15:
	.quad	80                              # 0x50
	.size	.L__unnamed_15, 8

	.type	.L__unnamed_16,@object          # @15
	.p2align	3
.L__unnamed_16:
	.quad	0                               # 0x0
	.size	.L__unnamed_16, 8

	.type	.L__unnamed_17,@object          # @16
	.p2align	3
.L__unnamed_17:
	.quad	39                              # 0x27
	.size	.L__unnamed_17, 8

	.type	.L__unnamed_18,@object          # @17
	.p2align	3
.L__unnamed_18:
	.quad	0                               # 0x0
	.size	.L__unnamed_18, 8

	.type	.L__unnamed_19,@object          # @18
	.p2align	3
.L__unnamed_19:
	.quad	13                              # 0xd
	.size	.L__unnamed_19, 8

	.type	.L__unnamed_20,@object          # @19
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_20:
	.quad	.L__unnamed_14
	.quad	.L__unnamed_15
	.quad	.L__unnamed_16
	.quad	.L__unnamed_17
	.quad	.L__unnamed_18
	.quad	.L__unnamed_19
	.size	.L__unnamed_20, 48

	.type	.L__unnamed_21,@object          # @20
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_21:
	.quad	0                               # 0x0
	.size	.L__unnamed_21, 8

	.type	.L__unnamed_22,@object          # @21
	.p2align	3
.L__unnamed_22:
	.quad	8                               # 0x8
	.size	.L__unnamed_22, 8

	.type	.L__unnamed_23,@object          # @22
	.p2align	3
.L__unnamed_23:
	.quad	0                               # 0x0
	.size	.L__unnamed_23, 8

	.type	.L__unnamed_24,@object          # @23
	.p2align	3
.L__unnamed_24:
	.quad	40                              # 0x28
	.size	.L__unnamed_24, 8

	.type	.L__unnamed_25,@object          # @24
	.p2align	3
.L__unnamed_25:
	.quad	0                               # 0x0
	.size	.L__unnamed_25, 8

	.type	.L__unnamed_26,@object          # @25
	.p2align	3
.L__unnamed_26:
	.quad	7                               # 0x7
	.size	.L__unnamed_26, 8

	.type	.L__unnamed_27,@object          # @26
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_27:
	.quad	.L__unnamed_21
	.quad	.L__unnamed_22
	.quad	.L__unnamed_23
	.quad	.L__unnamed_24
	.quad	.L__unnamed_25
	.quad	.L__unnamed_26
	.size	.L__unnamed_27, 48

	.type	.L__unnamed_28,@object          # @27
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_28:
	.quad	0                               # 0x0
	.size	.L__unnamed_28, 8

	.type	.L__unnamed_29,@object          # @28
	.p2align	3
.L__unnamed_29:
	.quad	8                               # 0x8
	.size	.L__unnamed_29, 8

	.type	.L__unnamed_30,@object          # @29
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__unnamed_30:
	.quad	.L__unnamed_28
	.quad	.L__unnamed_29
	.size	.L__unnamed_30, 16

	.type	.L__unnamed_31,@object          # @30
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_31:
	.quad	0                               # 0x0
	.size	.L__unnamed_31, 8

	.type	.L__unnamed_32,@object          # @31
	.p2align	3
.L__unnamed_32:
	.quad	24                              # 0x18
	.size	.L__unnamed_32, 8

	.type	.L__unnamed_33,@object          # @32
	.p2align	3
.L__unnamed_33:
	.quad	0                               # 0x0
	.size	.L__unnamed_33, 8

	.type	.L__unnamed_34,@object          # @33
	.p2align	3
.L__unnamed_34:
	.quad	39                              # 0x27
	.size	.L__unnamed_34, 8

	.type	.L__unnamed_35,@object          # @34
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_35:
	.quad	.L__unnamed_31
	.quad	.L__unnamed_32
	.quad	.L__unnamed_33
	.quad	.L__unnamed_34
	.size	.L__unnamed_35, 32

	.type	.L__unnamed_36,@object          # @35
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_36:
	.quad	0                               # 0x0
	.size	.L__unnamed_36, 8

	.type	.L__unnamed_37,@object          # @36
	.p2align	3
.L__unnamed_37:
	.quad	24                              # 0x18
	.size	.L__unnamed_37, 8

	.type	.L__unnamed_38,@object          # @37
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__unnamed_38:
	.quad	.L__unnamed_36
	.quad	.L__unnamed_37
	.size	.L__unnamed_38, 16

	.type	.L__unnamed_39,@object          # @38
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_39:
	.quad	0                               # 0x0
	.size	.L__unnamed_39, 8

	.type	.L__unnamed_40,@object          # @39
	.p2align	3
.L__unnamed_40:
	.quad	32                              # 0x20
	.size	.L__unnamed_40, 8

	.type	.L__unnamed_41,@object          # @40
	.p2align	3
.L__unnamed_41:
	.quad	0                               # 0x0
	.size	.L__unnamed_41, 8

	.type	.L__unnamed_42,@object          # @41
	.p2align	3
.L__unnamed_42:
	.quad	32                              # 0x20
	.size	.L__unnamed_42, 8

	.type	.L__unnamed_43,@object          # @42
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_43:
	.quad	.L__unnamed_39
	.quad	.L__unnamed_40
	.quad	.L__unnamed_41
	.quad	.L__unnamed_42
	.size	.L__unnamed_43, 32

	.type	.L__unnamed_44,@object          # @43
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_44:
	.quad	0                               # 0x0
	.size	.L__unnamed_44, 8

	.type	.L__unnamed_45,@object          # @44
	.p2align	3
.L__unnamed_45:
	.quad	32                              # 0x20
	.size	.L__unnamed_45, 8

	.type	.L__unnamed_46,@object          # @45
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__unnamed_46:
	.quad	.L__unnamed_44
	.quad	.L__unnamed_45
	.size	.L__unnamed_46, 16

	.type	.Lstr.143,@object               # @str.143
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr.143:
	.asciz	"learning_rate"
	.size	.Lstr.143, 14

	.type	.L__unnamed_47,@object          # @46
	.p2align	3
.L__unnamed_47:
	.long	0x3f800000                      # float 1
	.long	0x00000000                      # float 0
	.size	.L__unnamed_47, 8

	.type	.L__unnamed_48,@object          # @47
	.p2align	3
.L__unnamed_48:
	.long	0x3a83126f                      # float 0.00100000005
	.long	0x00000000                      # float 0
	.size	.L__unnamed_48, 8

	.type	.Lstr.144,@object               # @str.144
	.p2align	5
.Lstr.144:
	.asciz	"timestep"
	.size	.Lstr.144, 9

	.type	.L__unnamed_49,@object          # @48
	.p2align	3
.L__unnamed_49:
	.zero	8
	.size	.L__unnamed_49, 8

	.type	.L__unnamed_50,@object          # @49
	.p2align	3
.L__unnamed_50:
	.long	37                              # 0x25
	.long	0                               # 0x0
	.size	.L__unnamed_50, 8

	.type	.Lstr.145,@object               # @str.145
	.p2align	5
.Lstr.145:
	.asciz	"reference"
	.size	.Lstr.145, 10

	.type	.L__unnamed_51,@object          # @50
	.p2align	3
.L__unnamed_51:
	.zero	8
	.size	.L__unnamed_51, 8

	.type	.L__unnamed_52,@object          # @51
	.p2align	3
.L__unnamed_52:
	.zero	8
	.size	.L__unnamed_52, 8

	.type	.L__unnamed_53,@object          # @52
	.p2align	3
.L__unnamed_53:
	.quad	0                               # 0x0
	.size	.L__unnamed_53, 8

	.type	.L__unnamed_54,@object          # @53
	.p2align	3
.L__unnamed_54:
	.quad	80                              # 0x50
	.size	.L__unnamed_54, 8

	.type	.L__unnamed_55,@object          # @54
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__unnamed_55:
	.quad	.L__unnamed_53
	.quad	.L__unnamed_54
	.size	.L__unnamed_55, 16

	.type	.L__unnamed_56,@object          # @55
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_56:
	.quad	0                               # 0x0
	.size	.L__unnamed_56, 8

	.type	.L__unnamed_57,@object          # @56
	.p2align	3
.L__unnamed_57:
	.quad	8                               # 0x8
	.size	.L__unnamed_57, 8

	.type	.L__unnamed_58,@object          # @57
	.p2align	3
.L__unnamed_58:
	.quad	0                               # 0x0
	.size	.L__unnamed_58, 8

	.type	.L__unnamed_59,@object          # @58
	.p2align	3
.L__unnamed_59:
	.quad	40                              # 0x28
	.size	.L__unnamed_59, 8

	.type	.L__unnamed_60,@object          # @59
	.p2align	3
.L__unnamed_60:
	.quad	0                               # 0x0
	.size	.L__unnamed_60, 8

	.type	.L__unnamed_61,@object          # @60
	.p2align	3
.L__unnamed_61:
	.quad	7                               # 0x7
	.size	.L__unnamed_61, 8

	.type	.L__unnamed_62,@object          # @61
	.p2align	3
.L__unnamed_62:
	.quad	0                               # 0x0
	.size	.L__unnamed_62, 8

	.type	.L__unnamed_63,@object          # @62
	.p2align	3
.L__unnamed_63:
	.quad	4                               # 0x4
	.size	.L__unnamed_63, 8

	.type	.L__unnamed_64,@object          # @63
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_64:
	.quad	.L__unnamed_56
	.quad	.L__unnamed_57
	.quad	.L__unnamed_58
	.quad	.L__unnamed_59
	.quad	.L__unnamed_60
	.quad	.L__unnamed_61
	.quad	.L__unnamed_62
	.quad	.L__unnamed_63
	.size	.L__unnamed_64, 64

	.type	.L__unnamed_65,@object          # @64
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_65:
	.quad	0                               # 0x0
	.size	.L__unnamed_65, 8

	.type	.L__unnamed_66,@object          # @65
	.p2align	3
.L__unnamed_66:
	.quad	8                               # 0x8
	.size	.L__unnamed_66, 8

	.type	.L__unnamed_67,@object          # @66
	.p2align	3
.L__unnamed_67:
	.quad	0                               # 0x0
	.size	.L__unnamed_67, 8

	.type	.L__unnamed_68,@object          # @67
	.p2align	3
.L__unnamed_68:
	.quad	4                               # 0x4
	.size	.L__unnamed_68, 8

	.type	.L__unnamed_69,@object          # @68
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_69:
	.quad	.L__unnamed_65
	.quad	.L__unnamed_66
	.quad	.L__unnamed_67
	.quad	.L__unnamed_68
	.size	.L__unnamed_69, 32

	.type	.L__unnamed_70,@object          # @69
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_70:
	.quad	0                               # 0x0
	.size	.L__unnamed_70, 8

	.type	.L__unnamed_71,@object          # @70
	.p2align	3
.L__unnamed_71:
	.quad	24                              # 0x18
	.size	.L__unnamed_71, 8

	.type	.L__unnamed_72,@object          # @71
	.p2align	3
.L__unnamed_72:
	.quad	0                               # 0x0
	.size	.L__unnamed_72, 8

	.type	.L__unnamed_73,@object          # @72
	.p2align	3
.L__unnamed_73:
	.quad	39                              # 0x27
	.size	.L__unnamed_73, 8

	.type	.L__unnamed_74,@object          # @73
	.p2align	3
.L__unnamed_74:
	.quad	0                               # 0x0
	.size	.L__unnamed_74, 8

	.type	.L__unnamed_75,@object          # @74
	.p2align	3
.L__unnamed_75:
	.quad	4                               # 0x4
	.size	.L__unnamed_75, 8

	.type	.L__unnamed_76,@object          # @75
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_76:
	.quad	.L__unnamed_70
	.quad	.L__unnamed_71
	.quad	.L__unnamed_72
	.quad	.L__unnamed_73
	.quad	.L__unnamed_74
	.quad	.L__unnamed_75
	.size	.L__unnamed_76, 48

	.type	.L__unnamed_77,@object          # @76
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_77:
	.quad	0                               # 0x0
	.size	.L__unnamed_77, 8

	.type	.L__unnamed_78,@object          # @77
	.p2align	3
.L__unnamed_78:
	.quad	24                              # 0x18
	.size	.L__unnamed_78, 8

	.type	.L__unnamed_79,@object          # @78
	.p2align	3
.L__unnamed_79:
	.quad	0                               # 0x0
	.size	.L__unnamed_79, 8

	.type	.L__unnamed_80,@object          # @79
	.p2align	3
.L__unnamed_80:
	.quad	4                               # 0x4
	.size	.L__unnamed_80, 8

	.type	.L__unnamed_81,@object          # @80
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_81:
	.quad	.L__unnamed_77
	.quad	.L__unnamed_78
	.quad	.L__unnamed_79
	.quad	.L__unnamed_80
	.size	.L__unnamed_81, 32

	.type	.L__unnamed_82,@object          # @81
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_82:
	.quad	0                               # 0x0
	.size	.L__unnamed_82, 8

	.type	.L__unnamed_83,@object          # @82
	.p2align	3
.L__unnamed_83:
	.quad	32                              # 0x20
	.size	.L__unnamed_83, 8

	.type	.L__unnamed_84,@object          # @83
	.p2align	3
.L__unnamed_84:
	.quad	0                               # 0x0
	.size	.L__unnamed_84, 8

	.type	.L__unnamed_85,@object          # @84
	.p2align	3
.L__unnamed_85:
	.quad	32                              # 0x20
	.size	.L__unnamed_85, 8

	.type	.L__unnamed_86,@object          # @85
	.p2align	3
.L__unnamed_86:
	.quad	0                               # 0x0
	.size	.L__unnamed_86, 8

	.type	.L__unnamed_87,@object          # @86
	.p2align	3
.L__unnamed_87:
	.quad	4                               # 0x4
	.size	.L__unnamed_87, 8

	.type	.L__unnamed_88,@object          # @87
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_88:
	.quad	.L__unnamed_82
	.quad	.L__unnamed_83
	.quad	.L__unnamed_84
	.quad	.L__unnamed_85
	.quad	.L__unnamed_86
	.quad	.L__unnamed_87
	.size	.L__unnamed_88, 48

	.type	.L__unnamed_89,@object          # @88
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_89:
	.quad	0                               # 0x0
	.size	.L__unnamed_89, 8

	.type	.L__unnamed_90,@object          # @89
	.p2align	3
.L__unnamed_90:
	.quad	32                              # 0x20
	.size	.L__unnamed_90, 8

	.type	.L__unnamed_91,@object          # @90
	.p2align	3
.L__unnamed_91:
	.quad	0                               # 0x0
	.size	.L__unnamed_91, 8

	.type	.L__unnamed_92,@object          # @91
	.p2align	3
.L__unnamed_92:
	.quad	4                               # 0x4
	.size	.L__unnamed_92, 8

	.type	.L__unnamed_93,@object          # @92
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_93:
	.quad	.L__unnamed_89
	.quad	.L__unnamed_90
	.quad	.L__unnamed_91
	.quad	.L__unnamed_92
	.size	.L__unnamed_93, 32

	.type	.L__unnamed_94,@object          # @93
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_94:
	.quad	0                               # 0x0
	.size	.L__unnamed_94, 8

	.type	.L__unnamed_95,@object          # @94
	.p2align	3
.L__unnamed_95:
	.quad	80                              # 0x50
	.size	.L__unnamed_95, 8

	.type	.L__unnamed_96,@object          # @95
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__unnamed_96:
	.quad	.L__unnamed_94
	.quad	.L__unnamed_95
	.size	.L__unnamed_96, 16

	.type	.L__unnamed_97,@object          # @96
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_97:
	.zero	16
	.size	.L__unnamed_97, 16

	.type	.L__unnamed_98,@object          # @97
	.p2align	3
.L__unnamed_98:
	.zero	16
	.size	.L__unnamed_98, 16

	.type	.L__unnamed_99,@object          # @98
	.p2align	3
.L__unnamed_99:
	.zero	16
	.size	.L__unnamed_99, 16

	.type	.L__unnamed_100,@object         # @99
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_100:
	.quad	.Lstr.140
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	0                               # 0x0
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_1
	.quad	0
	.quad	0
	.quad	.L__unnamed_2
	.quad	0
	.quad	.Lstr.141
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	0                               # 0x0
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_3
	.quad	0
	.quad	0
	.quad	.L__unnamed_4
	.quad	0
	.quad	.Lstr.142
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	0                               # 0x0
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_5
	.quad	0
	.quad	0
	.quad	.L__unnamed_6
	.quad	0
	.quad	.Lstr.12
	.long	1                               # 0x1
	.long	3                               # 0x3
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_13
	.quad	.Lstr.10
	.long	1                               # 0x1
	.long	3                               # 0x3
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_20
	.quad	.Lstr.17
	.long	1                               # 0x1
	.long	3                               # 0x3
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_27
	.quad	.Lstr.18
	.long	1                               # 0x1
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_30
	.quad	.Lstr.15
	.long	1                               # 0x1
	.long	2                               # 0x2
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_35
	.quad	.Lstr.16
	.long	1                               # 0x1
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_38
	.quad	.Lstr.19
	.long	1                               # 0x1
	.long	2                               # 0x2
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_43
	.quad	.Lstr.21
	.long	1                               # 0x1
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_46
	.quad	.Lstr.143
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_47
	.quad	0
	.quad	0
	.quad	.L__unnamed_48
	.quad	0
	.quad	.Lstr.144
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	0                               # 0x0
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_49
	.quad	0
	.quad	0
	.quad	.L__unnamed_50
	.quad	0
	.quad	.Lstr.145
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	0                               # 0x0
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_51
	.quad	0
	.quad	0
	.quad	.L__unnamed_52
	.quad	0
	.quad	.Lstr.8
	.long	1                               # 0x1
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_55
	.quad	.Lstr.4
	.long	2                               # 0x2
	.long	4                               # 0x4
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_64
	.quad	.Lstr.5
	.long	2                               # 0x2
	.long	2                               # 0x2
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_69
	.quad	.Lstr
	.long	2                               # 0x2
	.long	3                               # 0x3
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_76
	.quad	.Lstr.3
	.long	2                               # 0x2
	.long	2                               # 0x2
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_81
	.quad	.Lstr.6
	.long	2                               # 0x2
	.long	3                               # 0x3
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_88
	.quad	.Lstr.7
	.long	2                               # 0x2
	.long	2                               # 0x2
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_93
	.quad	.Lstr.11
	.long	2                               # 0x2
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_96
	.quad	.Lstr.13
	.long	2                               # 0x2
	.long	0                               # 0x0
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.14
	.long	2                               # 0x2
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_97
	.quad	.Lstr.9
	.long	2                               # 0x2
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_98
	.quad	.Lstr.20
	.long	2                               # 0x2
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_99
	.size	.L__unnamed_100, 1664

	.type	.Lstr.146,@object               # @str.146
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr.146:
	.asciz	"x86-64-linux-avx-avx2-f16c-fma-no_runtime-sse41"
	.size	.Lstr.146, 48

	.type	.Lstr.147,@object               # @str.147
	.p2align	5
.Lstr.147:
	.asciz	"train_cost_model"
	.size	.Lstr.147, 17

	.type	.Ltrain_cost_model_metadata_storage,@object # @train_cost_model_metadata_storage
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.Ltrain_cost_model_metadata_storage:
	.long	1                               # 0x1
	.long	26                              # 0x1a
	.quad	.L__unnamed_100
	.quad	.Lstr.146
	.quad	.Lstr.147
	.size	.Ltrain_cost_model_metadata_storage, 32

	.ident	"clang version 11.0.0"
	.ident	"clang version 11.0.0"
	.ident	"clang version 11.0.0"
	.ident	"clang version 11.0.0"
	.ident	"clang version 11.0.0"
	.section	".note.GNU-stack","",@progbits
