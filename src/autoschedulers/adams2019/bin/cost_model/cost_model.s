	.text
	.file	"halide_buffer_t.cpp"
	.section	.rodata.cst16,"aM",@progbits,16
	.p2align	4                               # -- Begin function cost_model
.LCPI0_0:
	.quad	-9223372036854775808            # 0x8000000000000000
	.quad	0                               # 0x0
.LCPI0_3:
	.long	0                               # 0x0
	.long	32                              # 0x20
	.long	1                               # 0x1
	.long	0                               # 0x0
.LCPI0_4:
	.long	0                               # 0x0
	.long	32                              # 0x20
	.long	32                              # 0x20
	.long	0                               # 0x0
.LCPI0_5:
	.long	0                               # 0x0
	.long	8                               # 0x8
	.long	1                               # 0x1
	.long	0                               # 0x0
.LCPI0_6:
	.long	0                               # 0x0
	.long	40                              # 0x28
	.long	8                               # 0x8
	.long	0                               # 0x0
.LCPI0_7:
	.long	0                               # 0x0
	.long	7                               # 0x7
	.long	320                             # 0x140
	.long	0                               # 0x0
.LCPI0_8:
	.long	0                               # 0x0
	.long	24                              # 0x18
	.long	1                               # 0x1
	.long	0                               # 0x0
.LCPI0_9:
	.long	0                               # 0x0
	.long	39                              # 0x27
	.long	24                              # 0x18
	.long	0                               # 0x0
.LCPI0_10:
	.long	0                               # 0x0
	.long	40                              # 0x28
	.long	1                               # 0x1
	.long	0                               # 0x0
.LCPI0_11:
	.long	0                               # 0x0
	.long	7                               # 0x7
	.long	40                              # 0x28
	.long	0                               # 0x0
.LCPI0_14:
	.long	524288                          # 0x80000
	.long	0                               # 0x0
	.long	0                               # 0x0
	.long	0                               # 0x0
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5
.LCPI0_1:
	.quad	-9223372036854775800            # 0x8000000000000008
	.quad	2                               # 0x2
	.quad	256                             # 0x100
	.quad	512                             # 0x200
.LCPI0_2:
	.quad	64                              # 0x40
	.quad	128                             # 0x80
	.quad	1024                            # 0x400
	.quad	4096                            # 0x1000
.LCPI0_12:
	.quad	0                               # 0x0
	.quad	0                               # 0x0
	.quad	2199023255552                   # 0x20000000000
	.quad	0                               # 0x0
.LCPI0_13:
	.quad	1024                            # 0x400
	.quad	2048                            # 0x800
	.quad	0                               # 0x0
	.quad	4096                            # 0x1000
.LCPI0_15:
	.quad	0                               # 0x0
	.quad	281474976710656                 # 0x1000000000000
	.quad	1125899906842624                # 0x4000000000000
	.quad	4503599627370496                # 0x10000000000000
.LCPI0_16:
	.quad	34359738368                     # 0x800000000
	.quad	2147483648                      # 0x80000000
	.quad	137438953472                    # 0x2000000000
	.quad	549755813888                    # 0x8000000000
.LCPI0_17:
	.quad	0                               # 0x0
	.quad	0                               # 0x0
	.quad	536870912                       # 0x20000000
	.quad	0                               # 0x0
.LCPI0_18:
	.quad	16                              # 0x10
	.quad	32                              # 0x20
	.quad	0                               # 0x0
	.quad	64                              # 0x40
.LCPI0_19:
	.quad	0                               # 0x0
	.quad	8796093022208                   # 0x80000000000
	.quad	35184372088832                  # 0x200000000000
	.quad	0                               # 0x0
.LCPI0_20:
	.quad	8192                            # 0x2000
	.quad	0                               # 0x0
	.quad	0                               # 0x0
	.quad	16384                           # 0x4000
.LCPI0_21:
	.quad	0                               # 0x0
	.quad	67108864                        # 0x4000000
	.quad	0                               # 0x0
	.quad	0                               # 0x0
.LCPI0_22:
	.quad	2                               # 0x2
	.quad	0                               # 0x0
	.quad	4                               # 0x4
	.quad	8                               # 0x8
.LCPI0_23:
	.quad	0                               # 0x0
	.quad	8589934592                      # 0x200000000
	.quad	0                               # 0x0
	.quad	0                               # 0x0
.LCPI0_24:
	.quad	128                             # 0x80
	.quad	0                               # 0x0
	.quad	256                             # 0x100
	.quad	512                             # 0x200
.LCPI0_25:
	.quad	32768                           # 0x8000
	.quad	65536                           # 0x10000
	.quad	131072                          # 0x20000
	.quad	262144                          # 0x40000
.LCPI0_26:
	.long	0                               # 0x0
	.long	32                              # 0x20
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	32                              # 0x20
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	32                              # 0x20
.LCPI0_27:
	.long	0                               # 0x0
	.long	8                               # 0x8
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	8                               # 0x8
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	40                              # 0x28
.LCPI0_28:
	.long	0                               # 0x0
	.long	7                               # 0x7
	.long	0                               # 0x0
	.long	24                              # 0x18
	.long	1                               # 0x1
	.long	0                               # 0x0
	.long	24                              # 0x18
	.long	1                               # 0x1
.LCPI0_29:
	.quad	524288                          # 0x80000
	.quad	8388608                         # 0x800000
	.quad	16777216                        # 0x1000000
	.quad	4194304                         # 0x400000
.LCPI0_30:
	.quad	131072                          # 0x20000
	.quad	262144                          # 0x40000
	.quad	1048576                         # 0x100000
	.quad	2097152                         # 0x200000
.LCPI0_31:
	.quad	-9223372036854775806            # 0x8000000000000002
	.quad	4                               # 0x4
	.quad	8                               # 0x8
	.quad	32                              # 0x20
.LCPI0_32:
	.quad	1024                            # 0x400
	.quad	2048                            # 0x800
	.quad	512                             # 0x200
	.quad	8192                            # 0x2000
.LCPI0_33:
	.quad	16384                           # 0x4000
	.quad	4096                            # 0x1000
	.quad	32768                           # 0x8000
	.quad	65536                           # 0x10000
.LCPI0_34:
	.quad	64                              # 0x40
	.quad	16                              # 0x10
	.quad	128                             # 0x80
	.quad	256                             # 0x100
.LCPI0_35:
	.quad	8                               # 0x8
	.quad	137438953472                    # 0x2000000000
	.quad	274877906944                    # 0x4000000000
	.quad	16                              # 0x10
.LCPI0_36:
	.quad	32                              # 0x20
	.quad	549755813888                    # 0x8000000000
	.quad	1099511627776                   # 0x10000000000
	.quad	64                              # 0x40
.LCPI0_37:
	.quad	2                               # 0x2
	.quad	34359738368                     # 0x800000000
	.quad	4                               # 0x4
	.quad	68719476736                     # 0x1000000000
.LCPI0_38:
	.quad	-9223372034707292160            # 0x8000000080000000
	.quad	4294967296                      # 0x100000000
	.quad	8589934592                      # 0x200000000
	.quad	17179869184                     # 0x400000000
.LCPI0_61:
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	6                               # 0x6
	.quad	7                               # 0x7
.LCPI0_62:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
.LCPI0_70:
	.zero	32
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI0_39:
	.long	0xbfb8aa3b                      # float -1.44269502
.LCPI0_40:
	.long	0x3f317200                      # float 0.693145751
.LCPI0_41:
	.long	0x35bfbe8e                      # float 1.42860677E-6
.LCPI0_42:
	.long	4294967169                      # 0xffffff81
.LCPI0_43:
	.long	128                             # 0x80
.LCPI0_44:
	.long	0xb9a797f3                      # float -3.19659332E-4
.LCPI0_45:
	.long	0xbc0b192a                      # float -0.00848988629
.LCPI0_46:
	.long	0xbe2aae1f                      # float -0.166679844
.LCPI0_47:
	.long	0xbf800000                      # float -1
.LCPI0_48:
	.long	0x3f800000                      # float 1
.LCPI0_49:
	.long	0x3a9c2e66                      # float 0.00119156833
.LCPI0_50:
	.long	0x3d2a66bc                      # float 0.0416018814
.LCPI0_51:
	.long	0x3effffde                      # float 0.499998987
.LCPI0_52:
	.long	2155872255                      # 0x807fffff
.LCPI0_53:
	.long	0x3f317218                      # float 0.693147182
.LCPI0_54:
	.long	0x3d9c7946                      # float 0.0764031857
.LCPI0_55:
	.long	0x3e5333c6                      # float 0.206252187
.LCPI0_56:
	.long	0x3eaa99cd                      # float 0.333204657
.LCPI0_57:
	.long	0x3e266e2a                      # float 0.162529618
.LCPI0_58:
	.long	0xbe809085                      # float -0.251102597
.LCPI0_59:
	.long	0xbefffcbe                      # float -0.499975145
.LCPI0_60:
	.long	0xbe266e2a                      # float -0.162529618
.LCPI0_66:
	.long	0x45800000                      # float 4096
.LCPI0_67:
	.long	0x40000000                      # float 2
.LCPI0_68:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI0_63:
	.quad	8                               # 0x8
.LCPI0_64:
	.quad	16                              # 0x10
.LCPI0_65:
	.quad	24                              # 0x18
.LCPI0_69:
	.quad	32                              # 0x20
	.section	.text.cost_model,"ax",@progbits
	.globl	cost_model
	.p2align	4, 0x90
	.type	cost_model,@function
cost_model:                             # @cost_model
	.cfi_startproc
# %bb.0:                                # %entry
	pushq	%rbp
	.cfi_def_cfa_offset 16
	.cfi_offset %rbp, -16
	movq	%rsp, %rbp
	.cfi_def_cfa_register %rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	andq	$-32, %rsp
	subq	$12416, %rsp                    # imm = 0x3080
	.cfi_offset %rbx, -56
	.cfi_offset %r12, -48
	.cfi_offset %r13, -40
	.cfi_offset %r14, -32
	.cfi_offset %r15, -24
	movl	%edx, 760(%rsp)                 # 4-byte Spill
                                        # kill: def $esi killed $esi def $rsi
	movq	%rsi, 1088(%rsp)                # 8-byte Spill
                                        # kill: def $edi killed $edi def $rdi
	movq	%rdi, 72(%rsp)                  # 8-byte Spill
	xorl	%eax, %eax
	cmpq	$0, 104(%rbp)
	sete	%al
	xorl	%edx, %edx
	cmpq	$0, 80(%rbp)
	sete	%dl
	shlq	$2, %rdx
	xorl	%ebx, %ebx
	cmpq	$0, 88(%rbp)
	sete	%bl
	shlq	$4, %rbx
	xorl	%esi, %esi
	cmpq	$0, 96(%rbp)
	sete	%sil
	shlq	$5, %rsi
	movq	112(%rbp), %r14
	xorl	%edi, %edi
	testq	%r14, %r14
	sete	%dil
	orq	%rdx, %rbx
	orq	%rsi, %rbx
	shlq	$11, %rdi
	orq	%rdi, %rbx
	movq	16(%rbp), %r12
	vmovq	%r12, %xmm0
	vmovq	%r9, %xmm1
	vpunpcklqdq	%xmm0, %xmm1, %xmm0     # xmm0 = xmm1[0],xmm0[0]
	movq	%r8, 1096(%rsp)                 # 8-byte Spill
	vmovq	%r8, %xmm1
	movq	%rcx, 128(%rsp)                 # 8-byte Spill
	vmovq	%rcx, %xmm2
	vpunpcklqdq	%xmm1, %xmm2, %xmm1     # xmm1 = xmm2[0],xmm1[0]
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	vpxor	%xmm1, %xmm1, %xmm1
	vpcmpeqq	%ymm1, %ymm0, %ymm0
	vpcmpeqq	24(%rbp), %ymm1, %ymm1
	vmovapd	.LCPI0_0(%rip), %xmm2           # xmm2 = [9223372036854775808,0]
	vblendvpd	%ymm0, .LCPI0_1(%rip), %ymm2, %ymm0
	orq	%rax, %rbx
	vpand	.LCPI0_2(%rip), %ymm1, %ymm1
	vorpd	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm1
	vorpd	%xmm1, %xmm0, %xmm0
	vpermilps	$78, %xmm0, %xmm1       # xmm1 = xmm0[2,3,0,1]
	vorpd	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rax
	orq	%rax, %rbx
	xorl	%eax, %eax
	tzcntq	%rbx, %rax
	cmpl	$12, %eax
	jbe	.LBB0_208
# %bb.1:                                # %no_errors_bb
	movq	%r9, %r15
	movq	48(%rbp), %rbx
	movq	40(%rbp), %r13
	movq	16(%rbx), %rax
	movq	%rax, 256(%rsp)                 # 8-byte Spill
	leaq	32(%rbx), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	vzeroupper
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 2400(%rsp)                # 4-byte Spill
	movq	24(%rbx), %rax
	movq	%rax, 2528(%rsp)                # 8-byte Spill
	movl	36(%rbx), %eax
	movl	%eax, 1888(%rsp)                # 4-byte Spill
	movq	40(%rbx), %rax
	vmovsd	(%rax), %xmm0                   # xmm0 = mem[0],zero
	vmovaps	%xmm0, 3008(%rsp)               # 16-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 816(%rsp)                 # 4-byte Spill
	movq	16(%r14), %rax
	movq	%rax, 2680(%rsp)                # 8-byte Spill
	leaq	32(%r14), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 2112(%rsp)                # 4-byte Spill
	movq	24(%r14), %rax
	movq	%rax, 456(%rsp)                 # 8-byte Spill
	movl	36(%r14), %eax
	movl	%eax, 2272(%rsp)                # 4-byte Spill
	movq	40(%r14), %rax
	movl	(%rax), %ecx
	movq	%rcx, 192(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1024(%rsp)                # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 2176(%rsp)                # 4-byte Spill
	movq	16(%r13), %rax
	movq	%rax, 2640(%rsp)                # 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 2080(%rsp)                # 4-byte Spill
	movq	24(%r13), %rax
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	movl	36(%r13), %eax
	movl	%eax, 2048(%rsp)                # 4-byte Spill
	movq	40(%r13), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1152(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 368(%rsp)                 # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 704(%rsp)                 # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 768(%rsp)                 # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 352(%rsp)                 # 8-byte Spill
	movslq	24(%rax), %r14
	movq	16(%r12), %rbx
	leaq	32(%r12), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 2208(%rsp)                # 4-byte Spill
	movq	24(%r12), %rax
	movq	%rax, 384(%rsp)                 # 8-byte Spill
	movl	36(%r12), %eax
	movl	%eax, 976(%rsp)                 # 4-byte Spill
	movq	40(%r12), %rax
	movl	(%rax), %ecx
	movq	%rcx, 864(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1184(%rsp)                # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 1792(%rsp)                # 4-byte Spill
	movq	16(%r15), %rax
	movq	%rax, 1344(%rsp)                # 8-byte Spill
	leaq	32(%r15), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 960(%rsp)                 # 4-byte Spill
	movq	24(%r15), %rax
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	movl	36(%r15), %eax
	movl	%eax, 800(%rsp)                 # 4-byte Spill
	movq	%r15, 480(%rsp)                 # 8-byte Spill
	movq	40(%r15), %rax
	movl	(%rax), %ecx
	movq	%rcx, 832(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 224(%rsp)                 # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 1920(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 896(%rsp)                 # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 1312(%rsp)                # 8-byte Spill
	movslq	24(%rax), %r12
	vmovsd	32(%rax), %xmm0                 # xmm0 = mem[0],zero
	vmovaps	%xmm0, 2336(%rsp)               # 16-byte Spill
	movslq	40(%rax), %rax
	movq	%rax, 1536(%rsp)                # 8-byte Spill
	movq	32(%rbp), %r15
	movq	16(%r15), %rax
	movq	%rax, 1784(%rsp)                # 8-byte Spill
	leaq	32(%r15), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 2592(%rsp)                # 4-byte Spill
	movq	24(%r15), %rax
	movq	%rax, 512(%rsp)                 # 8-byte Spill
	movl	36(%r15), %eax
	movl	%eax, 2560(%rsp)                # 4-byte Spill
	movq	40(%r15), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1120(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1504(%rsp)                # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 2304(%rsp)                # 4-byte Spill
	movq	24(%rbp), %r15
	movq	16(%r15), %rax
	movq	%rax, 3296(%rsp)                # 8-byte Spill
	leaq	32(%r15), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 2976(%rsp)                # 4-byte Spill
	movq	24(%r15), %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movl	36(%r15), %eax
	movl	%eax, 2944(%rsp)                # 4-byte Spill
	movq	40(%r15), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1216(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1440(%rsp)                # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 2496(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 1472(%rsp)                # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 560(%rsp)                 # 8-byte Spill
	movslq	24(%rax), %rax
	movq	%rax, 464(%rsp)                 # 8-byte Spill
	movq	96(%rbp), %r13
	movq	16(%r13), %rax
	movq	%rax, 2664(%rsp)                # 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 3136(%rsp)                # 4-byte Spill
	movq	24(%r13), %rax
	movq	%rax, 576(%rsp)                 # 8-byte Spill
	movl	36(%r13), %eax
	movl	%eax, 3104(%rsp)                # 4-byte Spill
	movq	40(%r13), %rax
	movl	(%rax), %r15d
	movl	4(%rax), %ecx
	movq	%rcx, 1016(%rsp)                # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 2464(%rsp)                # 4-byte Spill
	movq	88(%rbp), %r13
	movq	16(%r13), %rax
	movq	%rax, 2656(%rsp)                # 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 3072(%rsp)                # 4-byte Spill
	movq	24(%r13), %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movl	36(%r13), %eax
	movl	%eax, 2816(%rsp)                # 4-byte Spill
	movq	128(%rsp), %r13                 # 8-byte Reload
	movq	16(%r13), %rax
	movq	%rax, 1424(%rsp)                # 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 2784(%rsp)                # 4-byte Spill
	movq	24(%r13), %rax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	movl	36(%r13), %eax
	movl	%eax, 2368(%rsp)                # 4-byte Spill
	movq	40(%r13), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1112(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 48(%rsp)                  # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 2432(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 928(%rsp)                 # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 1824(%rsp)                # 8-byte Spill
	movslq	24(%rax), %rcx
	movq	%rcx, 1280(%rsp)                # 8-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 1104(%rsp)                # 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 608(%rsp)                 # 8-byte Spill
	movslq	40(%rax), %rax
	movq	%rax, 1776(%rsp)                # 8-byte Spill
	movq	80(%rbp), %r13
	movq	16(%r13), %rax
	movq	%rax, 1976(%rsp)                # 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 2752(%rsp)                # 4-byte Spill
	movq	24(%r13), %rax
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	movl	36(%r13), %eax
	movl	%eax, 3040(%rsp)                # 4-byte Spill
	movq	40(%r13), %rax
	movl	(%rax), %ecx
	movq	%rcx, 744(%rsp)                 # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 1248(%rsp)                # 8-byte Spill
	movl	8(%rax), %eax
	movl	%eax, 2144(%rsp)                # 4-byte Spill
	movq	1096(%rsp), %r13                # 8-byte Reload
	movq	16(%r13), %rax
	movq	%rax, 440(%rsp)                 # 8-byte Spill
	leaq	32(%r13), %rsi
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movl	12(%rsp), %eax
	movl	%eax, 2720(%rsp)                # 4-byte Spill
	movq	24(%r13), %rax
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	movl	36(%r13), %eax
	movl	%eax, 3232(%rsp)                # 4-byte Spill
	movq	40(%r13), %rax
	movl	(%rax), %ecx
	movq	%rcx, 1056(%rsp)                # 8-byte Spill
	movl	4(%rax), %ecx
	movq	%rcx, 640(%rsp)                 # 8-byte Spill
	movl	8(%rax), %ecx
	movl	%ecx, 2240(%rsp)                # 4-byte Spill
	movl	16(%rax), %ecx
	movq	%rcx, 472(%rsp)                 # 8-byte Spill
	movl	20(%rax), %ecx
	movq	%rcx, 1600(%rsp)                # 8-byte Spill
	movl	24(%rax), %ecx
	movq	%rcx, 632(%rsp)                 # 8-byte Spill
	movl	32(%rax), %ecx
	movq	%rcx, 1048(%rsp)                # 8-byte Spill
	movl	36(%rax), %ecx
	movq	%rcx, 1664(%rsp)                # 8-byte Spill
	movl	40(%rax), %eax
	movl	%eax, 184(%rsp)                 # 4-byte Spill
	movq	104(%rbp), %rcx
	movq	16(%rcx), %rax
	movq	%rax, 2672(%rsp)                # 8-byte Spill
	leaq	32(%rcx), %rsi
	movq	%rcx, %r13
	leaq	12(%rsp), %rdi
	movl	$4, %edx
	callq	memcpy@PLT
	movq	40(%r13), %rax
	movq	%rax, 1568(%rsp)                # 8-byte Spill
	movl	(%rax), %r9d
	movl	4(%rax), %eax
	movq	1016(%rsp), %rcx                # 8-byte Reload
	leal	(%rcx,%r15), %edi
	movq	%rax, 1392(%rsp)                # 8-byte Spill
	leal	(%rax,%r9), %r11d
	cmpl	%r11d, %edi
	movl	%r11d, %ecx
	cmovgel	%edi, %ecx
	movq	192(%rsp), %rax                 # 8-byte Reload
	movq	1024(%rsp), %rdx                # 8-byte Reload
	addl	%eax, %edx
	cmpl	%edx, %ecx
	cmovll	%edx, %ecx
	movq	1088(%rsp), %r8                 # 8-byte Reload
	cmpl	%r8d, %ecx
	cmovll	%r8d, %ecx
	cmpl	%r15d, %r9d
	movl	%r9d, %r10d
	cmovgl	%r15d, %r10d
	cmpl	%r10d, %eax
	cmovlel	%eax, %r10d
	movq	224(%rsp), %rsi                 # 8-byte Reload
	cmpl	$-17, %esi
	movl	$-16, %eax
	cmovgl	%esi, %eax
	movq	%rax, 1632(%rsp)                # 8-byte Spill
	movl	%esi, %eax
	sarl	$31, %eax
	andl	%esi, %eax
	movl	%eax, 176(%rsp)                 # 4-byte Spill
	leal	-1(%r8), %r13d
	movl	%r13d, %esi
	andl	$-8, %esi
	leal	-8(%r8), %eax
	cmpl	%esi, %eax
	cmovlel	%eax, %esi
	addl	$7, %esi
	cmpl	$8, %r8d
	cmovll	%r13d, %esi
	cmpl	%ecx, %edi
	movq	%rcx, 1432(%rsp)                # 8-byte Spill
	movl	%ecx, %eax
	movl	%edi, 2036(%rsp)                # 4-byte Spill
	cmovgel	%edi, %eax
	cmpl	%r11d, %eax
	movl	%r11d, 2040(%rsp)               # 4-byte Spill
	cmovll	%r11d, %eax
	movl	%eax, %r11d
	cmpl	%edx, %eax
	movl	%edx, 2044(%rsp)                # 4-byte Spill
	cmovll	%edx, %r11d
	movq	%rsi, 1064(%rsp)                # 8-byte Spill
	leal	1(%rsi), %eax
	cmpl	%eax, %r11d
	movl	%eax, 2880(%rsp)                # 4-byte Spill
	cmovll	%eax, %r11d
	movl	%r10d, %eax
	sarl	$31, %eax
	movl	%r10d, 1744(%rsp)               # 4-byte Spill
	andl	%r10d, %eax
	cmpl	%eax, %r15d
	movq	%r15, 1080(%rsp)                # 8-byte Spill
	movl	%r15d, %edi
	movl	%eax, 1400(%rsp)                # 4-byte Spill
	cmovgl	%eax, %edi
	cmpl	%edi, %r9d
	movq	%r9, 1072(%rsp)                 # 8-byte Spill
	cmovlel	%r9d, %edi
	movq	192(%rsp), %rax                 # 8-byte Reload
	cmpl	%edi, %eax
	cmovlel	%eax, %edi
	movq	40(%rbp), %rax
	cmpq	$0, 16(%rax)
	movq	1568(%rsp), %rax                # 8-byte Reload
	movl	8(%rax), %r10d
	movl	12(%rsp), %eax
	movq	104(%rbp), %r9
	movq	24(%r9), %rcx
	movl	36(%r9), %esi
	jne	.LBB0_4
# %bb.2:                                # %_halide_buffer_is_bounds_query.exit
	cmpl	$0, 224(%rsp)                   # 4-byte Folded Reload
	setns	%r8b
	cmpl	$9, 1632(%rsp)                  # 4-byte Folded Reload
	setl	%dl
	testb	%r8b, %dl
	jne	.LBB0_4
# %bb.3:                                # %_halide_buffer_is_bounds_query.exit
	movq	40(%rbp), %rdx
	cmpq	$0, (%rdx)
	je	.LBB0_228
.LBB0_4:                                # %"assert succeeded"
	movl	%esi, 104(%rsp)                 # 4-byte Spill
	movl	%eax, 2848(%rsp)                # 4-byte Spill
	movl	%edi, 1856(%rsp)                # 4-byte Spill
	movl	%r10d, 2912(%rsp)               # 4-byte Spill
	movq	%rcx, 448(%rsp)                 # 8-byte Spill
	movl	%r13d, 1736(%rsp)               # 4-byte Spill
	movq	48(%rbp), %rax
	cmpq	$0, 16(%rax)
	jne	.LBB0_7
# %bb.5:                                # %_halide_buffer_is_bounds_query.exit339
	cmpq	$0, (%rax)
	jne	.LBB0_7
# %bb.6:                                # %true_bb
	movq	40(%rax), %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rax)
	movq	$0, 16(%rax)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%rax)
	vmovaps	.LCPI0_3(%rip), %xmm0           # xmm0 = [0,32,1,0]
	vmovups	%xmm0, (%rcx)
	movq	$0, 24(%rax)
.LBB0_7:                                # %after_bb
	movq	112(%rbp), %rdi
	cmpq	$0, 16(%rdi)
	movq	1096(%rsp), %rsi                # 8-byte Reload
	movq	96(%rbp), %r8
	movq	88(%rbp), %r13
	jne	.LBB0_9
# %bb.8:                                # %_halide_buffer_is_bounds_query.exit343
	cmpq	$0, (%rdi)
	je	.LBB0_12
.LBB0_9:                                # %after_bb15
	movq	40(%rbp), %rax
	cmpq	$0, 16(%rax)
	jne	.LBB0_13
.LBB0_10:                               # %_halide_buffer_is_bounds_query.exit345
	cmpq	$0, (%rax)
	jne	.LBB0_13
# %bb.11:                               # %true_bb16
	movq	40(%rax), %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rax)
	movq	$0, 16(%rax)
	movabsq	$8590008322, %rdx               # imm = 0x200012002
	movq	%rdx, 32(%rax)
	vmovaps	.LCPI0_3(%rip), %xmm0           # xmm0 = [0,32,1,0]
	vmovups	%xmm0, (%rcx)
	movq	40(%rax), %rcx
	vmovaps	.LCPI0_4(%rip), %xmm0           # xmm0 = [0,32,32,0]
	vmovups	%xmm0, 16(%rcx)
	movq	$0, 24(%rax)
	jmp	.LBB0_13
.LBB0_12:                               # %true_bb13
	movq	40(%rdi), %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rdi)
	movq	$0, 16(%rdi)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%rdi)
	movq	192(%rsp), %rax                 # 8-byte Reload
	movl	%eax, (%rcx)
	movq	1024(%rsp), %rdx                # 8-byte Reload
	movl	%edx, 4(%rcx)
	movq	$1, 8(%rcx)
	movq	$0, 24(%rdi)
	movq	40(%rbp), %rax
	cmpq	$0, 16(%rax)
	je	.LBB0_10
.LBB0_13:                               # %after_bb18
	movq	16(%rbp), %rax
	cmpq	$0, 16(%rax)
	movq	480(%rsp), %rdi                 # 8-byte Reload
	jne	.LBB0_15
# %bb.14:                               # %_halide_buffer_is_bounds_query.exit347
	cmpq	$0, (%rax)
	je	.LBB0_18
.LBB0_15:                               # %after_bb21
	movq	%r9, %r10
	cmpq	$0, 16(%rdi)
	jne	.LBB0_19
.LBB0_16:                               # %_halide_buffer_is_bounds_query.exit349
	cmpq	$0, (%rdi)
	jne	.LBB0_19
# %bb.17:                               # %true_bb22
	movq	40(%rdi), %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rdi)
	movq	$0, 16(%rdi)
	movabsq	$12884975618, %rdx              # imm = 0x300012002
	movq	%rdx, 32(%rdi)
	vmovaps	.LCPI0_5(%rip), %xmm0           # xmm0 = [0,8,1,0]
	vmovups	%xmm0, (%rcx)
	movq	40(%rdi), %rcx
	vmovaps	.LCPI0_6(%rip), %xmm0           # xmm0 = [0,40,8,0]
	vmovups	%xmm0, 16(%rcx)
	movq	40(%rdi), %rcx
	vmovaps	.LCPI0_7(%rip), %xmm0           # xmm0 = [0,7,320,0]
	vmovups	%xmm0, 32(%rcx)
	movq	$0, 24(%rdi)
	jmp	.LBB0_19
.LBB0_18:                               # %true_bb19
	movq	40(%rax), %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rax)
	movq	$0, 16(%rax)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%rax)
	vmovaps	.LCPI0_5(%rip), %xmm0           # xmm0 = [0,8,1,0]
	vmovups	%xmm0, (%rcx)
	movq	$0, 24(%rax)
	movq	%r9, %r10
	cmpq	$0, 16(%rdi)
	je	.LBB0_16
.LBB0_19:                               # %after_bb24
	movq	32(%rbp), %r9
	cmpq	$0, 16(%r9)
	movq	128(%rsp), %rax                 # 8-byte Reload
	jne	.LBB0_21
# %bb.20:                               # %_halide_buffer_is_bounds_query.exit352
	cmpq	$0, (%r9)
	je	.LBB0_24
.LBB0_21:                               # %after_bb27
	movq	24(%rbp), %rcx
	cmpq	$0, 16(%rcx)
	movq	%r10, %r9
	jne	.LBB0_25
.LBB0_22:                               # %_halide_buffer_is_bounds_query.exit354
	cmpq	$0, (%rcx)
	jne	.LBB0_25
# %bb.23:                               # %true_bb28
	movq	24(%rbp), %rcx
	movq	40(%rcx), %r10
	vxorps	%xmm0, %xmm0, %xmm0
	movq	24(%rbp), %rdx
	vmovups	%xmm0, (%rdx)
	movq	24(%rbp), %rdx
	movq	$0, 16(%rdx)
	movabsq	$8590008322, %rdx               # imm = 0x200012002
	movq	24(%rbp), %rcx
	movq	%rdx, 32(%rcx)
	vmovaps	.LCPI0_8(%rip), %xmm0           # xmm0 = [0,24,1,0]
	vmovups	%xmm0, (%r10)
	movq	24(%rbp), %rcx
	movq	40(%rcx), %rcx
	vmovaps	.LCPI0_9(%rip), %xmm0           # xmm0 = [0,39,24,0]
	vmovups	%xmm0, 16(%rcx)
	movq	24(%rbp), %rcx
	movq	$0, 24(%rcx)
	jmp	.LBB0_25
.LBB0_24:                               # %true_bb25
	movq	40(%r9), %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r9)
	movq	$0, 16(%r9)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%r9)
	vmovaps	.LCPI0_8(%rip), %xmm0           # xmm0 = [0,24,1,0]
	vmovups	%xmm0, (%rcx)
	movq	$0, 24(%r9)
	movq	24(%rbp), %rcx
	cmpq	$0, 16(%rcx)
	movq	%r10, %r9
	je	.LBB0_22
.LBB0_25:                               # %after_bb30
	cmpq	$0, 16(%r8)
	jne	.LBB0_27
# %bb.26:                               # %_halide_buffer_is_bounds_query.exit357
	cmpq	$0, (%r8)
	je	.LBB0_30
.LBB0_27:                               # %after_bb33
	cmpq	$0, 16(%r13)
	movq	80(%rbp), %r10
	jne	.LBB0_31
.LBB0_28:                               # %_halide_buffer_is_bounds_query.exit359
	cmpq	$0, (%r13)
	jne	.LBB0_31
# %bb.29:                               # %true_bb34
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, 16(%r13)
	vmovups	%xmm0, (%r13)
	movq	$73730, 32(%r13)                # imm = 0x12002
	jmp	.LBB0_31
.LBB0_30:                               # %true_bb31
	movq	40(%r8), %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r8)
	movq	$0, 16(%r8)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%r8)
	movq	1080(%rsp), %rax                # 8-byte Reload
	movl	%eax, (%rcx)
	movq	128(%rsp), %rax                 # 8-byte Reload
	movq	1016(%rsp), %rdx                # 8-byte Reload
	movl	%edx, 4(%rcx)
	movq	$1, 8(%rcx)
	movq	24(%rbp), %rcx
	movq	$0, 24(%r8)
	cmpq	$0, 16(%r13)
	movq	80(%rbp), %r10
	je	.LBB0_28
.LBB0_31:                               # %after_bb36
	cmpq	$0, 16(%rax)
	jne	.LBB0_33
# %bb.32:                               # %_halide_buffer_is_bounds_query.exit360
	cmpq	$0, (%rax)
	je	.LBB0_36
.LBB0_33:                               # %after_bb39
	cmpq	$0, 16(%r10)
	jne	.LBB0_37
.LBB0_34:                               # %_halide_buffer_is_bounds_query.exit363
	cmpq	$0, (%r10)
	jne	.LBB0_37
# %bb.35:                               # %true_bb40
	movq	40(%r10), %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r10)
	movq	$0, 16(%r10)
	movabsq	$4295041026, %rdx               # imm = 0x100012002
	movq	%rdx, 32(%r10)
	movl	$0, (%rcx)
	movl	2880(%rsp), %edx                # 4-byte Reload
	movl	%edx, 4(%rcx)
	movq	$1, 8(%rcx)
	movq	24(%rbp), %rcx
	movq	$0, 24(%r10)
	jmp	.LBB0_37
.LBB0_36:                               # %true_bb37
	movq	40(%rax), %rcx
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rax)
	movq	$0, 16(%rax)
	movabsq	$12884975618, %rdx              # imm = 0x300012002
	movq	%rdx, 32(%rax)
	vmovaps	.LCPI0_10(%rip), %xmm0          # xmm0 = [0,40,1,0]
	vmovups	%xmm0, (%rcx)
	movq	40(%rax), %rcx
	vmovaps	.LCPI0_11(%rip), %xmm0          # xmm0 = [0,7,40,0]
	vmovups	%xmm0, 16(%rcx)
	movq	40(%rax), %rcx
	movl	$0, 32(%rcx)
	movq	72(%rsp), %rdx                  # 8-byte Reload
	movl	%edx, 36(%rcx)
	movq	$280, 40(%rcx)                  # imm = 0x118
	movq	24(%rbp), %rcx
	movq	$0, 24(%rax)
	cmpq	$0, 16(%r10)
	je	.LBB0_34
.LBB0_37:                               # %after_bb42
	cmpq	$0, 16(%rsi)
	jne	.LBB0_39
# %bb.38:                               # %_halide_buffer_is_bounds_query.exit365
	cmpq	$0, (%rsi)
	je	.LBB0_41
.LBB0_39:                               # %after_bb45
	cmpq	$0, 16(%r9)
	je	.LBB0_42
.LBB0_40:
	movl	$0, 480(%rsp)                   # 4-byte Folded Spill
	cmpq	$0, 16(%rsi)
	je	.LBB0_148
.LBB0_44:
	movl	$0, 128(%rsp)                   # 4-byte Folded Spill
	movq	32(%rbp), %rdx
	cmpq	$0, 16(%r10)
	je	.LBB0_149
.LBB0_45:
	movl	$0, 1568(%rsp)                  # 4-byte Folded Spill
	movq	48(%rbp), %r10
	movl	%r11d, 2880(%rsp)               # 4-byte Spill
	cmpq	$0, 16(%rax)
	je	.LBB0_150
.LBB0_46:
	xorl	%r15d, %r15d
	cmpq	$0, 16(%r13)
	je	.LBB0_151
.LBB0_47:
	xorl	%r13d, %r13d
	cmpq	$0, 16(%r8)
	je	.LBB0_152
.LBB0_48:
	xorl	%eax, %eax
	movq	40(%rbp), %r8
	cmpq	$0, 16(%rcx)
	je	.LBB0_153
.LBB0_49:
	xorl	%ecx, %ecx
	cmpq	$0, 16(%rdx)
	je	.LBB0_154
.LBB0_50:
	xorl	%edx, %edx
	cmpq	$0, 16(%rdi)
	je	.LBB0_155
.LBB0_51:
	xorl	%esi, %esi
	movq	16(%rbp), %rdi
	cmpq	$0, 16(%rdi)
	je	.LBB0_156
.LBB0_52:
	xorl	%edi, %edi
	cmpq	$0, 16(%r8)
	je	.LBB0_157
.LBB0_53:
	xorl	%r8d, %r8d
	cmpq	$0, 16(%r10)
	je	.LBB0_158
.LBB0_54:
	xorl	%r9d, %r9d
	movq	112(%rbp), %r11
	cmpq	$0, 16(%r11)
	movl	$0, %r10d
	jne	.LBB0_56
.LBB0_55:
	cmpq	$0, (%r11)
	sete	%r10b
.LBB0_56:                               # %_halide_buffer_is_bounds_query.exit382
	orb	%r10b, %r9b
	orb	%r9b, %r8b
	orb	%r8b, %dil
	orb	%dil, %sil
	orb	%sil, %dl
	orb	%dl, %cl
	orb	%cl, %al
	orb	%al, %r13b
	orb	%r13b, %r15b
	movl	1568(%rsp), %eax                # 4-byte Reload
	orb	%r15b, %al
	movl	128(%rsp), %ecx                 # 4-byte Reload
	orb	%al, %cl
	movl	480(%rsp), %eax                 # 4-byte Reload
	orb	%cl, %al
	testb	$1, %al
	je	.LBB0_58
# %bb.57:
	xorl	%r14d, %r14d
	jmp	.LBB0_189
.LBB0_58:                               # %true_bb49
	xorl	%eax, %eax
	cmpl	$73730, 2400(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	setne	%al
	movq	%rax, 1568(%rsp)                # 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 1888(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 2688(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 2112(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 696(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 2272(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 688(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 2080(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 736(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$2, 2048(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1040(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 2208(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 1728(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 976(%rsp)                   # 4-byte Folded Reload
	sete	%al
	movl	%eax, 680(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 960(%rsp)               # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 432(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$3, 800(%rsp)                   # 4-byte Folded Reload
	sete	%al
	movl	%eax, 672(%rsp)                 # 4-byte Spill
	xorl	%r9d, %r9d
	cmpl	$73730, 2592(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%r9b
	xorl	%edi, %edi
	cmpl	$1, 2560(%rsp)                  # 4-byte Folded Reload
	sete	%dil
	xorl	%r13d, %r13d
	cmpl	$73730, 2976(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%r13b
	xorl	%eax, %eax
	cmpl	$2, 2944(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1968(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 3136(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 764(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 3104(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 728(%rsp)                 # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 3072(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 1720(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$0, 2816(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1712(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 2784(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	sete	%al
	movl	%eax, 1704(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$3, 2368(%rsp)                  # 4-byte Folded Reload
	sete	%al
	movl	%eax, 1032(%rsp)                # 4-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 2752(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	setne	%al
	shlq	$20, %rax
	movq	%rax, 3264(%rsp)                # 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 3040(%rsp)                  # 4-byte Folded Reload
	setne	%al
	shlq	$21, %rax
	movq	%rax, 1416(%rsp)                # 8-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 2720(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	setne	%al
	shlq	$22, %rax
	movq	%rax, 280(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	cmpl	$3, 3232(%rsp)                  # 4-byte Folded Reload
	setne	%al
	shlq	$23, %rax
	movq	%rax, 1760(%rsp)                # 8-byte Spill
	xorl	%eax, %eax
	cmpl	$73730, 2848(%rsp)              # 4-byte Folded Reload
                                        # imm = 0x12002
	setne	%al
	shlq	$24, %rax
	movq	%rax, 1768(%rsp)                # 8-byte Spill
	xorl	%eax, %eax
	cmpl	$1, 104(%rsp)                   # 4-byte Folded Reload
	setne	%al
	shlq	$25, %rax
	movq	%rax, 1752(%rsp)                # 8-byte Spill
	vmovdqa	3008(%rsp), %xmm0               # 16-byte Reload
	vmovd	%xmm0, %ecx
	testl	%ecx, %ecx
	setg	%al
	vpextrd	$1, %xmm0, %edx
	movq	%rcx, 128(%rsp)                 # 8-byte Spill
	addl	%edx, %ecx
	movl	%ecx, 2032(%rsp)                # 4-byte Spill
	cmpl	$32, %ecx
	setl	%cl
	orb	%al, %cl
	movb	%cl, 1960(%rsp)                 # 1-byte Spill
	movq	%rdx, 480(%rsp)                 # 8-byte Spill
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	shrl	$4, %edx
	andl	$134217728, %edx                # imm = 0x8000000
	movq	%rdx, 1408(%rsp)                # 8-byte Spill
	movq	1024(%rsp), %rax                # 8-byte Reload
	shrq	$3, %rax
	andl	$268435456, %eax                # imm = 0x10000000
	movq	%rax, 3008(%rsp)                # 8-byte Spill
	movq	1152(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	%al
	movq	368(%rsp), %rsi                 # 8-byte Reload
	addl	%esi, %ecx
	movl	%ecx, 2028(%rsp)                # 4-byte Spill
	cmpl	$32, %ecx
	setl	%cl
	orb	%al, %cl
	movb	%cl, 1008(%rsp)                 # 1-byte Spill
	shrq	%rsi
	andl	$1073741824, %esi               # imm = 0x40000000
	movq	768(%rsp), %rcx                 # 8-byte Reload
	cmpl	176(%rsp), %ecx                 # 4-byte Folded Reload
	setg	%r8b
	movq	1632(%rsp), %rax                # 8-byte Reload
	leal	24(%rax), %edx
	movq	352(%rsp), %rax                 # 8-byte Reload
	addl	%eax, %ecx
	movl	%ecx, 2024(%rsp)                # 4-byte Spill
	cmpl	%ecx, %edx
	vmovd	%r9d, %xmm0
	vpinsrb	$4, %edi, %xmm0, %xmm0
	setg	%cl
	orb	%r8b, %cl
	movb	%cl, 190(%rsp)                  # 1-byte Spill
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-2147483648, %eax              # imm = 0x80000000
	addq	%rax, %rax
	movq	%rax, 1952(%rsp)                # 8-byte Spill
	movq	864(%rsp), %rax                 # 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	1184(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %eax
	movl	%eax, 2020(%rsp)                # 4-byte Spill
	cmpl	$8, %eax
	setl	%al
	orb	%cl, %al
	movb	%al, 191(%rsp)                  # 1-byte Spill
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              # imm = 0x80000000
	shlq	$3, %rdx
	movq	%rdx, 1000(%rsp)                # 8-byte Spill
	movq	832(%rsp), %rax                 # 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	224(%rsp), %rdx                 # 8-byte Reload
	addl	%edx, %eax
	movl	%eax, 2016(%rsp)                # 4-byte Spill
	cmpl	$8, %eax
	setl	%r9b
	orb	%cl, %r9b
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              # imm = 0x80000000
	shlq	$5, %rdx
	movq	%rdx, 3224(%rsp)                # 8-byte Spill
	movq	896(%rsp), %rax                 # 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	1312(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %eax
	movl	%eax, 2012(%rsp)                # 4-byte Spill
	cmpl	$40, %eax
	setl	%r10b
	orb	%cl, %r10b
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              # imm = 0x80000000
	shlq	$7, %rdx
	movq	%rdx, 3216(%rsp)                # 8-byte Spill
	vmovdqa	2336(%rsp), %xmm1               # 16-byte Reload
	vmovd	%xmm1, %eax
	testl	%eax, %eax
	vpextrd	$1, %xmm1, %ecx
	setg	%dl
	movq	%rax, 2336(%rsp)                # 8-byte Spill
	addl	%ecx, %eax
	movl	%eax, 2008(%rsp)                # 4-byte Spill
	cmpl	$7, %eax
	setl	%r11b
	orb	%dl, %r11b
	movq	%rcx, 752(%rsp)                 # 8-byte Spill
                                        # kill: def $ecx killed $ecx killed $rcx def $rcx
	shrl	$31, %ecx
	shlq	$40, %rcx
	movq	%rcx, 3208(%rsp)                # 8-byte Spill
	movq	1120(%rsp), %rax                # 8-byte Reload
	testl	%eax, %eax
	setg	%dl
	movq	1504(%rsp), %rdi                # 8-byte Reload
	addl	%edi, %eax
	movl	%eax, 2004(%rsp)                # 4-byte Spill
	cmpl	$24, %eax
	setl	%cl
	orb	%dl, %cl
	movzbl	%cl, %ecx
	vpinsrb	$8, %ecx, %xmm0, %xmm0
	vpinsrb	$12, %r13d, %xmm0, %xmm0
	vmovd	1032(%rsp), %xmm1               # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
                                        # kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              # imm = 0x80000000
	shlq	$11, %rdi
	movq	%rdi, 1032(%rsp)                # 8-byte Spill
	movq	1216(%rsp), %rax                # 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	1440(%rsp), %rdx                # 8-byte Reload
	addl	%edx, %eax
	movl	%eax, 1988(%rsp)                # 4-byte Spill
	cmpl	$24, %eax
	setl	%r13b
	orb	%cl, %r13b
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              # imm = 0x80000000
	shlq	$13, %rdx
	movq	%rdx, 3200(%rsp)                # 8-byte Spill
	movq	1472(%rsp), %rax                # 8-byte Reload
	testl	%eax, %eax
	setg	%cl
	movq	560(%rsp), %rdx                 # 8-byte Reload
	addl	%edx, %eax
	movl	%eax, 1984(%rsp)                # 4-byte Spill
	cmpl	$39, %eax
	setl	%r8b
	orb	%cl, %r8b
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-2147483648, %edx              # imm = 0x80000000
	shlq	$15, %rdx
	movq	%rdx, 3192(%rsp)                # 8-byte Spill
	movq	1016(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx def $rcx
	andl	$-2147483648, %ecx              # imm = 0x80000000
	shlq	$16, %rcx
	movq	%rcx, 3184(%rsp)                # 8-byte Spill
	movq	1112(%rsp), %rdx                # 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	48(%rsp), %rax                  # 8-byte Reload
	addl	%eax, %edx
	movl	%edx, 2000(%rsp)                # 4-byte Spill
	cmpl	$40, %edx
	setl	%dil
	orb	%cl, %dil
	movzbl	%dil, %ecx
	vpinsrb	$4, %ecx, %xmm1, %xmm1
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-2147483648, %eax              # imm = 0x80000000
	shlq	$18, %rax
	movq	%rax, 3176(%rsp)                # 8-byte Spill
	movq	928(%rsp), %rdx                 # 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	1824(%rsp), %rax                # 8-byte Reload
	addl	%eax, %edx
	movl	%edx, 1996(%rsp)                # 4-byte Spill
	cmpl	$7, %edx
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	vpinsrb	$8, %ecx, %xmm1, %xmm1
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-2147483648, %eax              # imm = 0x80000000
	shlq	$20, %rax
	movq	%rax, 3168(%rsp)                # 8-byte Spill
	movq	1104(%rsp), %rdx                # 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	608(%rsp), %rax                 # 8-byte Reload
	addl	%eax, %edx
	movq	72(%rsp), %r15                  # 8-byte Reload
	movl	%edx, 1992(%rsp)                # 4-byte Spill
	cmpl	%r15d, %edx
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %ecx
	vpinsrb	$12, %ecx, %xmm1, %xmm2
	movzbl	%r9b, %ecx
	vmovd	%ecx, %xmm1
	movzbl	190(%rsp), %ecx                 # 1-byte Folded Reload
	vpinsrb	$4, %ecx, %xmm1, %xmm1
	movzbl	%r10b, %ecx
	vpinsrb	$8, %ecx, %xmm1, %xmm1
	movzbl	%r11b, %ecx
	vpinsrb	$12, %ecx, %xmm1, %xmm1
	vmovd	736(%rsp), %xmm3                # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vpinsrb	$4, 1040(%rsp), %xmm3, %xmm3    # 4-byte Folded Reload
	movzbl	1008(%rsp), %ecx                # 1-byte Folded Reload
	vpinsrb	$8, %ecx, %xmm3, %xmm3
	vpinsrb	$12, 1728(%rsp), %xmm3, %xmm3   # 4-byte Folded Reload
	vmovd	1968(%rsp), %xmm4               # 4-byte Folded Reload
                                        # xmm4 = mem[0],zero,zero,zero
	movzbl	%r13b, %ecx
	vpinsrb	$4, %ecx, %xmm4, %xmm4
	movzbl	%r8b, %ecx
	vpinsrb	$8, %ecx, %xmm4, %xmm4
	vpinsrb	$12, 764(%rsp), %xmm4, %xmm4    # 4-byte Folded Reload
	vmovd	2688(%rsp), %xmm5               # 4-byte Folded Reload
                                        # xmm5 = mem[0],zero,zero,zero
	movzbl	1960(%rsp), %ecx                # 1-byte Folded Reload
	vpinsrb	$4, %ecx, %xmm5, %xmm5
	vpinsrb	$8, 696(%rsp), %xmm5, %xmm5     # 4-byte Folded Reload
	vpinsrb	$12, 688(%rsp), %xmm5, %xmm5    # 4-byte Folded Reload
	vmovd	680(%rsp), %xmm6                # 4-byte Folded Reload
                                        # xmm6 = mem[0],zero,zero,zero
	movzbl	191(%rsp), %ecx                 # 1-byte Folded Reload
	vpinsrb	$4, %ecx, %xmm6, %xmm6
	vpinsrb	$8, 432(%rsp), %xmm6, %xmm6     # 4-byte Folded Reload
	vpinsrb	$12, 672(%rsp), %xmm6, %xmm6    # 4-byte Folded Reload
	vmovd	728(%rsp), %xmm7                # 4-byte Folded Reload
                                        # xmm7 = mem[0],zero,zero,zero
	vpinsrb	$4, 1720(%rsp), %xmm7, %xmm7    # 4-byte Folded Reload
	vpinsrb	$8, 1712(%rsp), %xmm7, %xmm7    # 4-byte Folded Reload
	vpinsrb	$12, 1704(%rsp), %xmm7, %xmm7   # 4-byte Folded Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-2147483648, %eax              # imm = 0x80000000
	shlq	$22, %rax
	movq	%rax, 696(%rsp)                 # 8-byte Spill
	movq	744(%rsp), %rdx                 # 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	1248(%rsp), %r10                # 8-byte Reload
	addl	%r10d, %edx
	movl	%edx, 728(%rsp)                 # 4-byte Spill
	cmpl	%edx, 1064(%rsp)                # 4-byte Folded Reload
	setge	%dl
	orb	%cl, %dl
	movzbl	%dl, %r11d
	shlq	$54, %r11
                                        # kill: def $r10d killed $r10d killed $r10 def $r10
	andl	$-2147483648, %r10d             # imm = 0x80000000
	shlq	$24, %r10
	movq	1056(%rsp), %rdx                # 8-byte Reload
	cmpl	1856(%rsp), %edx                # 4-byte Folded Reload
	setg	%cl
	movq	640(%rsp), %r9                  # 8-byte Reload
	leal	(%r9,%rdx), %eax
	movl	%eax, 680(%rsp)                 # 4-byte Spill
	cmpl	%eax, 2880(%rsp)                # 4-byte Folded Reload
	setg	%dl
	orb	%cl, %dl
	movzbl	%dl, %r13d
	shlq	$56, %r13
                                        # kill: def $r9d killed $r9d killed $r9 def $r9
	andl	$-2147483648, %r9d              # imm = 0x80000000
	shlq	$26, %r9
	movq	472(%rsp), %rdx                 # 8-byte Reload
	testl	%edx, %edx
	setg	%cl
	movq	1600(%rsp), %rdi                # 8-byte Reload
	leal	(%rdi,%rdx), %eax
	movl	%eax, 432(%rsp)                 # 4-byte Spill
	cmpl	$39, %eax
	setl	%dl
	orb	%cl, %dl
	movzbl	%dl, %edx
	shlq	$58, %rdx
                                        # kill: def $edi killed $edi killed $rdi def $rdi
	andl	$-2147483648, %edi              # imm = 0x80000000
	shlq	$28, %rdi
	movq	1048(%rsp), %rcx                # 8-byte Reload
	testl	%ecx, %ecx
	setg	688(%rsp)                       # 1-byte Folded Spill
	leal	3(%r15), %eax
	movl	%eax, 672(%rsp)                 # 4-byte Spill
	andl	$-4, %eax
	movq	1664(%rsp), %r8                 # 8-byte Reload
	leal	(%r8,%rcx), %r15d
	movl	%eax, 2688(%rsp)                # 4-byte Spill
	cmpl	%r15d, %eax
	setg	%cl
	movq	1416(%rsp), %rax                # 8-byte Reload
	orq	3264(%rsp), %rax                # 8-byte Folded Reload
	orq	%r11, %rax
	orq	280(%rsp), %rax                 # 8-byte Folded Reload
	orq	1760(%rsp), %rax                # 8-byte Folded Reload
	orq	%rdx, %rax
	vpslld	$31, %xmm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vmovapd	.LCPI0_13(%rip), %ymm8          # ymm8 = [1024,2048,0,4096]
	vblendvpd	%ymm0, .LCPI0_12(%rip), %ymm8, %ymm0
	orb	688(%rsp), %cl                  # 1-byte Folded Reload
	vpslld	$31, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vmovapd	.LCPI0_14(%rip), %xmm8          # xmm8 = [524288,0,0,0]
	vblendvpd	%ymm2, .LCPI0_15(%rip), %ymm8, %ymm2
	movzbl	%cl, %ecx
	shlq	$60, %rcx
	orq	%rcx, %rax
	vpslld	$31, %xmm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vmovapd	.LCPI0_18(%rip), %ymm8          # ymm8 = [16,32,0,64]
	vblendvpd	%ymm3, .LCPI0_17(%rip), %ymm8, %ymm3
	orq	1768(%rsp), %rax                # 8-byte Folded Reload
	orq	1752(%rsp), %rax                # 8-byte Folded Reload
	vpslld	$31, %xmm4, %xmm4
	vpmovsxdq	%xmm4, %ymm4
	vmovapd	.LCPI0_20(%rip), %ymm8          # ymm8 = [8192,0,0,16384]
	vblendvpd	%ymm4, .LCPI0_19(%rip), %ymm8, %ymm4
	orq	%r13, %rax
	orq	1568(%rsp), %rax                # 8-byte Folded Reload
	vpslld	$31, %xmm5, %xmm5
	vpmovsxdq	%xmm5, %ymm5
	vmovapd	.LCPI0_22(%rip), %ymm8          # ymm8 = [2,0,4,8]
	vblendvpd	%ymm5, .LCPI0_21(%rip), %ymm8, %ymm5
	vpslld	$31, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vpand	.LCPI0_16(%rip), %ymm1, %ymm1
	orq	1408(%rsp), %rax                # 8-byte Folded Reload
	vpor	%ymm2, %ymm1, %ymm1
	vpslld	$31, %xmm6, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vmovapd	.LCPI0_24(%rip), %ymm6          # ymm6 = [128,0,256,512]
	vblendvpd	%ymm2, .LCPI0_23(%rip), %ymm6, %ymm2
	vorpd	%ymm4, %ymm3, %ymm3
	vorpd	%ymm1, %ymm3, %ymm1
	vpslld	$31, %xmm7, %xmm3
	vpsrad	$31, %xmm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vpandn	.LCPI0_25(%rip), %ymm3, %ymm3
	vorpd	%ymm3, %ymm2, %ymm2
	vorpd	%ymm2, %ymm0, %ymm0
	vorpd	%ymm0, %ymm5, %ymm0
	vorpd	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm1
	vorpd	%xmm1, %xmm0, %xmm0
	vpermilps	$78, %xmm0, %xmm1       # xmm1 = xmm0[2,3,0,1]
	vorpd	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rcx
	orq	%rcx, %rax
	orq	3008(%rsp), %rsi                # 8-byte Folded Reload
	orq	1952(%rsp), %rsi                # 8-byte Folded Reload
	orq	1000(%rsp), %rsi                # 8-byte Folded Reload
	orq	3224(%rsp), %rsi                # 8-byte Folded Reload
	orq	3216(%rsp), %rsi                # 8-byte Folded Reload
	orq	3208(%rsp), %rsi                # 8-byte Folded Reload
	orq	1032(%rsp), %rsi                # 8-byte Folded Reload
	orq	3200(%rsp), %rsi                # 8-byte Folded Reload
	orq	3192(%rsp), %rsi                # 8-byte Folded Reload
	orq	3184(%rsp), %rsi                # 8-byte Folded Reload
	orq	3176(%rsp), %rsi                # 8-byte Folded Reload
	orq	3168(%rsp), %rsi                # 8-byte Folded Reload
	orq	696(%rsp), %rsi                 # 8-byte Folded Reload
	orq	%r10, %rsi
	orq	%r9, %rsi
	orq	%rdi, %rsi
	movl	%r8d, %ecx
	andl	$-2147483648, %ecx              # imm = 0x80000000
	shlq	$30, %rcx
	orq	%rcx, %rsi
	movq	1392(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx def $rcx
	andl	$-2147483648, %ecx              # imm = 0x80000000
	shlq	$31, %rcx
	orq	%rcx, %rsi
	movabsq	$-9223372036854775808, %rcx     # imm = 0x8000000000000000
	orq	%rsi, %rcx
	orq	%rax, %rcx
	xorl	%eax, %eax
	tzcntq	%rcx, %rax
	cmpl	$62, %eax
	jbe	.LBB0_213
# %bb.59:                               # %no_errors_bb52
	xorl	%r9d, %r9d
	cmpl	$39, 560(%rsp)                  # 4-byte Folded Reload
	setne	%r9b
	vmovd	224(%rsp), %xmm0                # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	vpinsrd	$1, 1920(%rsp), %xmm0, %xmm0    # 4-byte Folded Reload
	vpinsrd	$2, 896(%rsp), %xmm0, %xmm0     # 4-byte Folded Reload
	vpinsrd	$3, 1312(%rsp), %xmm0, %xmm0    # 4-byte Folded Reload
	xorl	%r15d, %r15d
	vmovd	864(%rsp), %xmm1                # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	vpinsrd	$1, 1184(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vpinsrd	$2, 1792(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vpinsrd	$3, 832(%rsp), %xmm1, %xmm1     # 4-byte Folded Reload
	cmpl	$1, 816(%rsp)                   # 4-byte Folded Reload
	vmovd	2304(%rsp), %xmm2               # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 1216(%rsp), %xmm2, %xmm2    # 4-byte Folded Reload
	vpinsrd	$2, 1440(%rsp), %xmm2, %xmm2    # 4-byte Folded Reload
	vpinsrd	$3, 2496(%rsp), %xmm2, %xmm2    # 4-byte Folded Reload
	setne	%r15b
	vmovd	2336(%rsp), %xmm3               # 4-byte Folded Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vpinsrd	$1, 752(%rsp), %xmm3, %xmm3     # 4-byte Folded Reload
	vpinsrd	$2, 1120(%rsp), %xmm3, %xmm3    # 4-byte Folded Reload
	vpinsrd	$3, 1504(%rsp), %xmm3, %xmm3    # 4-byte Folded Reload
	vinserti128	$1, %xmm2, %ymm3, %ymm2
	vpcmpeqd	.LCPI0_28(%rip), %ymm2, %ymm2
	vpmovsxdq	%xmm2, %ymm3
	vextracti128	$1, %ymm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vpandn	.LCPI0_29(%rip), %ymm2, %ymm2
	vpandn	.LCPI0_30(%rip), %ymm3, %ymm3
	vpor	%ymm2, %ymm3, %ymm2
	xorl	%r11d, %r11d
	cmpl	$0, 1472(%rsp)                  # 4-byte Folded Reload
	setne	%r11b
	shlq	$25, %r11
	shlq	$26, %r9
	xorl	%r10d, %r10d
	cmpl	$1, 2464(%rsp)                  # 4-byte Folded Reload
	setne	%r10b
	shlq	$27, %r10
	xorl	%esi, %esi
	cmpl	$1, 2432(%rsp)                  # 4-byte Folded Reload
	setne	%sil
	shlq	$28, %rsi
	xorl	%r13d, %r13d
	cmpl	$1, 2144(%rsp)                  # 4-byte Folded Reload
	setne	%r13b
	shlq	$29, %r13
	xorl	%eax, %eax
	cmpl	$1, 2240(%rsp)                  # 4-byte Folded Reload
	setne	%al
	shlq	$30, %rax
	xorl	%edx, %edx
	cmpl	$1, 2912(%rsp)                  # 4-byte Folded Reload
	setne	%dl
	vextracti128	$1, %ymm2, %xmm3
	vpor	%xmm3, %xmm2, %xmm2
	vpshufd	$78, %xmm2, %xmm3               # xmm3 = xmm2[2,3,0,1]
	vpor	%xmm3, %xmm2, %xmm2
	vmovq	%xmm2, %rdi
	orq	%r11, %rdi
	vmovd	368(%rsp), %xmm2                # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrd	$1, 704(%rsp), %xmm2, %xmm2     # 4-byte Folded Reload
	vpinsrd	$2, 768(%rsp), %xmm2, %xmm2     # 4-byte Folded Reload
	vpinsrd	$3, 352(%rsp), %xmm2, %xmm2     # 4-byte Folded Reload
	vinserti128	$1, %xmm0, %ymm1, %ymm0
	movq	128(%rsp), %r11                 # 8-byte Reload
	vmovd	%r11d, %xmm1
	movq	480(%rsp), %r8                  # 8-byte Reload
	vpinsrd	$1, %r8d, %xmm1, %xmm1
	vpinsrd	$2, 2176(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vpinsrd	$3, 1152(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vinserti128	$1, %xmm2, %ymm1, %ymm1
	vpcmpeqd	.LCPI0_26(%rip), %ymm1, %ymm1
	vextracti128	$1, %ymm1, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vpcmpeqd	.LCPI0_27(%rip), %ymm0, %ymm0
	vextracti128	$1, %ymm0, %xmm3
	vpmovsxdq	%xmm0, %ymm0
	vpmovsxdq	%xmm1, %ymm1
	vmovapd	.LCPI0_0(%rip), %xmm4           # xmm4 = [9223372036854775808,0]
	vmovapd	.LCPI0_31(%rip), %ymm5          # ymm5 = [9223372036854775810,4,8,32]
	vblendvpd	%ymm1, %ymm4, %ymm5, %ymm1
	vpandn	.LCPI0_32(%rip), %ymm0, %ymm0
	vpmovsxdq	%xmm3, %ymm3
	vorpd	%ymm0, %ymm1, %ymm0
	vpandn	.LCPI0_33(%rip), %ymm3, %ymm1
	vpandn	.LCPI0_34(%rip), %ymm2, %ymm2
	vpor	%ymm1, %ymm2, %ymm1
	vorpd	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm1
	vorpd	%xmm1, %xmm0, %xmm0
	vpermilps	$78, %xmm0, %xmm1       # xmm1 = xmm0[2,3,0,1]
	vorpd	%xmm1, %xmm0, %xmm0
	orq	%r9, %rdi
	vmovq	%xmm0, %rcx
	orq	%rcx, %rdi
	orq	%r10, %rsi
	orq	%r13, %rsi
	orq	%rax, %rsi
	shlq	$31, %rdx
	orq	%rdx, %rsi
	orq	%r15, %rsi
	orq	%rdi, %rsi
	xorl	%eax, %eax
	tzcntq	%rsi, %rax
	cmpl	$31, %eax
	jbe	.LBB0_215
# %bb.60:                               # %no_errors_bb116
	movslq	1824(%rsp), %rsi                # 4-byte Folded Reload
	movslq	48(%rsp), %r9                   # 4-byte Folded Reload
	movq	%rsi, %rdi
	imulq	%r9, %rdi
	movslq	608(%rsp), %r8                  # 4-byte Folded Reload
	movq	%rdi, %r10
	imulq	%r8, %r10
	movslq	1600(%rsp), %r13                # 4-byte Folded Reload
	movslq	640(%rsp), %r15                 # 4-byte Folded Reload
	movq	%r13, %r11
	imulq	%r15, %r11
	movslq	1664(%rsp), %rcx                # 4-byte Folded Reload
	movq	%rcx, 1120(%rsp)                # 8-byte Spill
	movq	%r11, %rax
	imulq	%rcx, %rax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movslq	1024(%rsp), %rcx                # 4-byte Folded Reload
	movq	%rcx, %rax
	negq	%rax
	cmovlq	%rcx, %rax
	xorl	%ecx, %ecx
	movq	%rax, 560(%rsp)                 # 8-byte Spill
	cmpq	$2147483647, %rax               # imm = 0x7FFFFFFF
	seta	%cl
	movq	%rcx, 224(%rsp)                 # 8-byte Spill
	movq	%r14, %rax
	shlq	$5, %rax
	movq	%rax, %rcx
	negq	%rcx
	testl	%r14d, %r14d
	movq	%rax, 2336(%rsp)                # 8-byte Spill
	cmovnsq	%rax, %rcx
	xorl	%eax, %eax
	movq	%rcx, 608(%rsp)                 # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 480(%rsp)                 # 4-byte Spill
	leaq	(,%r12,8), %rax
	leaq	(%rax,%rax,4), %rax
	movq	%rax, %rcx
	negq	%rcx
	testl	%r12d, %r12d
	cmovnsq	%rax, %rcx
	xorl	%eax, %eax
	movq	%rcx, 1824(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 1152(%rsp)                # 4-byte Spill
	movq	1536(%rsp), %rcx                # 8-byte Reload
	leaq	(,%rcx,8), %rdx
	subq	%rcx, %rdx
	movq	%rdx, %rax
	negq	%rax
	testl	%ecx, %ecx
	cmovnsq	%rdx, %rax
	xorl	%ecx, %ecx
	movq	%rax, 1664(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rax               # imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 832(%rsp)                 # 4-byte Spill
	movq	464(%rsp), %rdx                 # 8-byte Reload
	imulq	$39, %rdx, %rax
	movq	%rax, %rcx
	negq	%rcx
	testl	%edx, %edx
	cmovnsq	%rax, %rcx
	xorl	%eax, %eax
	movq	%rcx, 1600(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 768(%rsp)                 # 4-byte Spill
	movslq	1016(%rsp), %rax                # 4-byte Folded Reload
	movq	%rax, %rcx
	negq	%rcx
	cmovlq	%rax, %rcx
	xorl	%eax, %eax
	movq	%rcx, 1632(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%al
	movl	%eax, 352(%rsp)                 # 4-byte Spill
	movq	%r9, %rax
	negq	%rax
	cmpl	$0, 48(%rsp)                    # 4-byte Folded Reload
	cmovnsq	%r9, %rax
	xorl	%ecx, %ecx
	movq	%rax, 1568(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rax               # imm = 0x7FFFFFFF
	seta	%cl
	movl	%ecx, 864(%rsp)                 # 4-byte Spill
	imulq	1280(%rsp), %rsi                # 8-byte Folded Reload
	movq	%rsi, %rax
	negq	%rax
	cmovlq	%rsi, %rax
	movq	%rax, 1856(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rax               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$7, %rax
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	xorl	%esi, %esi
	movq	%rdi, 1216(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rdi               # imm = 0x7FFFFFFF
	setg	%sil
	shlq	$8, %rsi
	imulq	1776(%rsp), %r8                 # 8-byte Folded Reload
	movq	%r8, %rax
	negq	%rax
	cmovlq	%r8, %rax
	movq	%rax, 816(%rsp)                 # 8-byte Spill
	cmpq	$2147483647, %rax               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$9, %rax
	movq	%rax, 368(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%r10, 1472(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %r10               # imm = 0x7FFFFFFF
	setg	%al
	shlq	$10, %rax
	movq	%rax, 1184(%rsp)                # 8-byte Spill
	movq	1248(%rsp), %rdx                # 8-byte Reload
	movslq	%edx, %rax
	movq	%rax, %rcx
	negq	%rcx
	testl	%edx, %edx
	cmovnsq	%rax, %rcx
	movq	%rcx, 2176(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$11, %rax
	movq	%rax, 1248(%rsp)                # 8-byte Spill
	movq	%r15, %rax
	negq	%rax
	cmpl	$0, 640(%rsp)                   # 4-byte Folded Reload
	cmovnsq	%r15, %rax
	movq	%rax, 704(%rsp)                 # 8-byte Spill
	cmpq	$2147483647, %rax               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$12, %rax
	movq	%rax, 640(%rsp)                 # 8-byte Spill
	movslq	632(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 3008(%rsp)                # 8-byte Spill
	imulq	%rax, %r13
	movq	%r13, %rax
	negq	%rax
	cmovlq	%r13, %rax
	movq	%rax, 1792(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rax               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$13, %rax
	movq	%rax, 896(%rsp)                 # 8-byte Spill
	xorl	%eax, %eax
	movq	%r11, 1440(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %r11               # imm = 0x7FFFFFFF
	setg	%al
	shlq	$14, %rax
	movq	%rax, 1312(%rsp)                # 8-byte Spill
	movslq	184(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 176(%rsp)                 # 8-byte Spill
	movq	1120(%rsp), %rcx                # 8-byte Reload
	imulq	%rax, %rcx
	movq	%rcx, %rax
	negq	%rax
	cmovlq	%rcx, %rax
	movq	%rax, 1920(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rax               # imm = 0x7FFFFFFF
	movl	$0, %eax
	seta	%al
	shlq	$15, %rax
	movq	%rax, 1120(%rsp)                # 8-byte Spill
	xorl	%eax, %eax
	cmpq	$2147483647, 128(%rsp)          # 8-byte Folded Reload
                                        # imm = 0x7FFFFFFF
	setg	%al
	shlq	$16, %rax
	movq	%rax, 1504(%rsp)                # 8-byte Spill
	movslq	1392(%rsp), %rax                # 4-byte Folded Reload
	movq	%rax, %rcx
	negq	%rcx
	cmovlq	%rax, %rcx
	xorl	%eax, %eax
	movq	%rcx, 2304(%rsp)                # 8-byte Spill
	cmpq	$2147483647, %rcx               # imm = 0x7FFFFFFF
	seta	%al
	vmovd	832(%rsp), %xmm0                # 4-byte Folded Reload
                                        # xmm0 = mem[0],zero,zero,zero
	shlq	$17, %rax
	movq	%rax, 832(%rsp)                 # 8-byte Spill
	movq	2528(%rsp), %r11                # 8-byte Reload
	andl	$2, %r11d
	shlq	$17, %r11
	movq	456(%rsp), %r13                 # 8-byte Reload
	andl	$2, %r13d
	shlq	$18, %r13
	movq	320(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$19, %rax
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	movq	384(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$20, %rax
	movq	%rax, 384(%rsp)                 # 8-byte Spill
	movq	288(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$21, %rax
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	movq	512(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$22, %rax
	movq	%rax, 512(%rsp)                 # 8-byte Spill
	movq	16(%rsp), %rax                  # 8-byte Reload
	andl	$2, %eax
	shlq	$23, %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movq	576(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$24, %rax
	movq	%rax, 576(%rsp)                 # 8-byte Spill
	movq	32(%rsp), %rax                  # 8-byte Reload
	andl	$2, %eax
	shlq	$25, %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	80(%rsp), %rax                  # 8-byte Reload
	andl	$2, %eax
	shlq	$26, %rax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	movq	160(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$27, %rax
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	movq	112(%rsp), %rax                 # 8-byte Reload
	andl	$2, %eax
	shlq	$28, %rax
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	movq	448(%rsp), %r15                 # 8-byte Reload
	andl	$2, %r15d
	shlq	$29, %r15
	xorl	%r9d, %r9d
	cmpq	$0, 256(%rsp)                   # 8-byte Folded Reload
	sete	%r9b
	xorl	%r8d, %r8d
	cmpq	$0, 2680(%rsp)                  # 8-byte Folded Reload
	sete	%r8b
	xorl	%edi, %edi
	cmpq	$0, 2640(%rsp)                  # 8-byte Folded Reload
	sete	%dil
	xorl	%r10d, %r10d
	testq	%rbx, %rbx
	sete	%r10b
	xorl	%ecx, %ecx
	cmpq	$0, 1344(%rsp)                  # 8-byte Folded Reload
	sete	%cl
	xorl	%edx, %edx
	cmpq	$0, 1784(%rsp)                  # 8-byte Folded Reload
	sete	%dl
	xorl	%eax, %eax
	cmpq	$0, 3296(%rsp)                  # 8-byte Folded Reload
	sete	%al
	vpinsrb	$4, %eax, %xmm0, %xmm0
	xorl	%eax, %eax
	cmpq	$0, 2664(%rsp)                  # 8-byte Folded Reload
	sete	%al
	vpinsrb	$8, %eax, %xmm0, %xmm0
	vpinsrb	$12, 768(%rsp), %xmm0, %xmm0    # 4-byte Folded Reload
	vmovd	352(%rsp), %xmm1                # 4-byte Folded Reload
                                        # xmm1 = mem[0],zero,zero,zero
	xorl	%eax, %eax
	cmpq	$0, 2656(%rsp)                  # 8-byte Folded Reload
	sete	%al
	vpinsrb	$4, %eax, %xmm1, %xmm1
	xorl	%eax, %eax
	cmpq	$0, 1424(%rsp)                  # 8-byte Folded Reload
	sete	%al
	vpinsrb	$8, %eax, %xmm1, %xmm1
	vpinsrb	$12, 864(%rsp), %xmm1, %xmm1    # 4-byte Folded Reload
	vmovd	480(%rsp), %xmm2                # 4-byte Folded Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vpinsrb	$4, %ecx, %xmm2, %xmm2
	vpinsrb	$8, 1152(%rsp), %xmm2, %xmm2    # 4-byte Folded Reload
	vpinsrb	$12, %edx, %xmm2, %xmm2
	vmovd	%r9d, %xmm3
	vpinsrb	$4, %r8d, %xmm3, %xmm3
	vpinsrb	$8, %edi, %xmm3, %xmm3
	vpinsrb	$12, %r10d, %xmm3, %xmm3
	xorl	%eax, %eax
	cmpq	$0, 1976(%rsp)                  # 8-byte Folded Reload
	sete	%al
	shlq	$41, %rax
	xorl	%ecx, %ecx
	cmpq	$0, 440(%rsp)                   # 8-byte Folded Reload
	sete	%cl
	shlq	$42, %rcx
	xorl	%edx, %edx
	cmpq	$0, 2672(%rsp)                  # 8-byte Folded Reload
	sete	%dl
	orq	48(%rsp), %rsi                  # 8-byte Folded Reload
	orq	1184(%rsp), %rsi                # 8-byte Folded Reload
	orq	%rax, %rsi
	orq	368(%rsp), %rsi                 # 8-byte Folded Reload
	orq	1248(%rsp), %rsi                # 8-byte Folded Reload
	orq	%rcx, %rsi
	orq	640(%rsp), %rsi                 # 8-byte Folded Reload
	orq	1312(%rsp), %rsi                # 8-byte Folded Reload
	orq	896(%rsp), %rsi                 # 8-byte Folded Reload
	orq	1504(%rsp), %rsi                # 8-byte Folded Reload
	shlq	$43, %rdx
	orq	%rdx, %rsi
	orq	1120(%rsp), %rsi                # 8-byte Folded Reload
	orq	832(%rsp), %rsi                 # 8-byte Folded Reload
	orq	%r11, %rsi
	orq	%r13, %rsi
	orq	224(%rsp), %rsi                 # 8-byte Folded Reload
	orq	320(%rsp), %rsi                 # 8-byte Folded Reload
	orq	384(%rsp), %rsi                 # 8-byte Folded Reload
	orq	288(%rsp), %rsi                 # 8-byte Folded Reload
	orq	512(%rsp), %rsi                 # 8-byte Folded Reload
	orq	16(%rsp), %rsi                  # 8-byte Folded Reload
	orq	576(%rsp), %rsi                 # 8-byte Folded Reload
	orq	32(%rsp), %rsi                  # 8-byte Folded Reload
	orq	80(%rsp), %rsi                  # 8-byte Folded Reload
	orq	160(%rsp), %rsi                 # 8-byte Folded Reload
	vpslld	$31, %xmm3, %xmm3
	vpmovsxdq	%xmm3, %ymm3
	vmovapd	.LCPI0_0(%rip), %xmm4           # xmm4 = [9223372036854775808,0]
	vblendvpd	%ymm3, .LCPI0_38(%rip), %ymm4, %ymm3
	orq	112(%rsp), %rsi                 # 8-byte Folded Reload
	vpslld	$31, %xmm0, %xmm0
	vpsrad	$31, %xmm0, %xmm0
	vpmovsxdq	%xmm0, %ymm0
	vpand	.LCPI0_35(%rip), %ymm0, %ymm0
	vpslld	$31, %xmm1, %xmm1
	vpsrad	$31, %xmm1, %xmm1
	vpmovsxdq	%xmm1, %ymm1
	vpand	.LCPI0_36(%rip), %ymm1, %ymm1
	vpslld	$31, %xmm2, %xmm2
	vpsrad	$31, %xmm2, %xmm2
	vpmovsxdq	%xmm2, %ymm2
	vpand	.LCPI0_37(%rip), %ymm2, %ymm2
	orq	%r15, %rsi
	vpor	%ymm1, %ymm2, %ymm1
	vorpd	%ymm0, %ymm3, %ymm0
	vorpd	%ymm1, %ymm0, %ymm0
	vextractf128	$1, %ymm0, %xmm1
	vorpd	%xmm1, %xmm0, %xmm0
	vpermilps	$78, %xmm0, %xmm1       # xmm1 = xmm0[2,3,0,1]
	vorpd	%xmm1, %xmm0, %xmm0
	vmovq	%xmm0, %rax
	orq	%rax, %rsi
	xorl	%eax, %eax
	tzcntq	%rsi, %rax
	cmpl	$43, %eax
	jbe	.LBB0_217
# %bb.61:                               # %"for squashed_head1_filter.s0.n.preheader"
	movslq	192(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 1768(%rsp)                # 8-byte Spill
	movslq	1080(%rsp), %rax                # 4-byte Folded Reload
	movq	%rax, 1752(%rsp)                # 8-byte Spill
	movslq	1072(%rsp), %rax                # 4-byte Folded Reload
	movq	%rax, 1760(%rsp)                # 8-byte Spill
	movl	672(%rsp), %r9d                 # 4-byte Reload
	sarl	$2, %r9d
	movq	1536(%rsp), %r8                 # 8-byte Reload
	shlq	$2, %r8
	shlq	$2, %r12
	leaq	3424(%rsp), %rax
	xorl	%ecx, %ecx
	vbroadcastss	.LCPI0_39(%rip), %ymm2  # ymm2 = [-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0,-1.44269502E+0]
	vbroadcastss	.LCPI0_40(%rip), %ymm13 # ymm13 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vbroadcastss	.LCPI0_41(%rip), %ymm3  # ymm3 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vpbroadcastd	.LCPI0_42(%rip), %ymm12 # ymm12 = [4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169,4294967169]
	vpbroadcastd	.LCPI0_43(%rip), %ymm4  # ymm4 = [128,128,128,128,128,128,128,128]
	vbroadcastss	.LCPI0_44(%rip), %ymm14 # ymm14 = [-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4,-3.19659332E-4]
	vbroadcastss	.LCPI0_45(%rip), %ymm6  # ymm6 = [-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3,-8.48988629E-3]
	vbroadcastss	.LCPI0_46(%rip), %ymm8  # ymm8 = [-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1,-1.66679844E-1]
	vbroadcastss	.LCPI0_47(%rip), %ymm0  # ymm0 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vmovaps	%ymm0, 3232(%rsp)               # 32-byte Spill
	vbroadcastss	.LCPI0_48(%rip), %ymm15 # ymm15 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	.LCPI0_49(%rip), %ymm0  # ymm0 = [1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3,1.19156833E-3]
	vmovaps	%ymm0, 192(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI0_50(%rip), %ymm0  # ymm0 = [4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2,4.16018814E-2]
	vmovaps	%ymm0, 128(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI0_51(%rip), %ymm0  # ymm0 = [4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1,4.99998987E-1]
	vmovaps	%ymm0, 224(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI0_48(%rip), %ymm0  # ymm0 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vmovaps	%ymm0, 2848(%rsp)               # 32-byte Spill
	movq	1344(%rsp), %rdi                # 8-byte Reload
	vmovdqa	%ymm12, 2880(%rsp)              # 32-byte Spill
	vmovaps	192(%rsp), %ymm9                # 32-byte Reload
	vmovaps	128(%rsp), %ymm0                # 32-byte Reload
	vmovaps	224(%rsp), %ymm1                # 32-byte Reload
	.p2align	4, 0x90
.LBB0_62:                               # %"for squashed_head1_filter.s0.n"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_63 Depth 2
	movq	%rdi, %rdx
	xorl	%esi, %esi
	.p2align	4, 0x90
.LBB0_63:                               # %"for squashed_head1_filter.s0.s"
                                        #   Parent Loop BB0_62 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovups	(%rdx), %ymm10
	vmulps	%ymm2, %ymm10, %ymm11
	vroundps	$1, %ymm11, %ymm11
	vfmadd231ps	%ymm13, %ymm11, %ymm10  # ymm10 = (ymm11 * ymm13) + ymm10
	vfmadd231ps	%ymm11, %ymm3, %ymm10   # ymm10 = (ymm3 * ymm11) + ymm10
	vmulps	%ymm10, %ymm10, %ymm12
	vmovaps	%ymm3, %ymm5
	vmovaps	%ymm13, %ymm3
	vmovaps	%ymm14, %ymm13
	vfmadd213ps	%ymm6, %ymm12, %ymm13   # ymm13 = (ymm12 * ymm13) + ymm6
	vfmadd213ps	%ymm8, %ymm12, %ymm13   # ymm13 = (ymm12 * ymm13) + ymm8
	vfmadd213ps	3232(%rsp), %ymm12, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm12 * ymm13) + mem
	vmovaps	%ymm6, %ymm7
	vmovaps	%ymm14, %ymm6
	vmovaps	%ymm9, %ymm14
	vfmadd213ps	%ymm0, %ymm12, %ymm14   # ymm14 = (ymm12 * ymm14) + ymm0
	vfmadd213ps	%ymm1, %ymm12, %ymm14   # ymm14 = (ymm12 * ymm14) + ymm1
	vfmadd213ps	%ymm15, %ymm12, %ymm14  # ymm14 = (ymm12 * ymm14) + ymm15
	vfmadd231ps	%ymm13, %ymm10, %ymm14  # ymm14 = (ymm10 * ymm13) + ymm14
	vmovaps	%ymm3, %ymm13
	vmovaps	%ymm5, %ymm3
	vcvttps2dq	%ymm11, %ymm10
	vpslld	$23, %ymm10, %ymm11
	vpaddd	2848(%rsp), %ymm11, %ymm11      # 32-byte Folded Reload
	vfmadd213ps	%ymm15, %ymm14, %ymm11  # ymm11 = (ymm14 * ymm11) + ymm15
	vmovaps	%ymm6, %ymm14
	vmovaps	%ymm7, %ymm6
	vdivps	%ymm11, %ymm15, %ymm11
	vpcmpgtd	%ymm10, %ymm4, %ymm12
	vpand	%ymm11, %ymm12, %ymm11
	vmovdqa	2880(%rsp), %ymm12              # 32-byte Reload
	vpcmpgtd	%ymm12, %ymm10, %ymm10
	vblendvps	%ymm10, %ymm11, %ymm15, %ymm10
	vmovaps	%ymm10, (%rax,%rsi)
	addq	$32, %rsi
	addq	%r12, %rdx
	cmpq	$1280, %rsi                     # imm = 0x500
	jne	.LBB0_63
# %bb.64:                               # %"end for squashed_head1_filter.s0.s"
                                        #   in Loop: Header=BB0_62 Depth=1
	incq	%rcx
	addq	$1280, %rax                     # imm = 0x500
	addq	%r8, %rdi
	cmpq	$7, %rcx
	jne	.LBB0_62
# %bb.65:                               # %"end for squashed_head1_filter.s0.n"
	vmovaps	%ymm15, 2720(%rsp)              # 32-byte Spill
	movq	72(%rsp), %rax                  # 8-byte Reload
	leal	-1(%rax), %ecx
	sarl	$2, %ecx
	cmpl	$-2, %ecx
	movl	$-1, %eax
	cmovgl	%ecx, %eax
	leal	4(,%rax,4), %r12d
	cmpl	$16777216, %r12d                # imm = 0x1000000
	jae	.LBB0_219
# %bb.66:                               # %"assert succeeded195"
	movl	%r9d, 672(%rsp)                 # 4-byte Spill
	movq	%rcx, 432(%rsp)                 # 8-byte Spill
	movq	%r12, %rsi
	shlq	$7, %rsi
	movq	%rsi, 1040(%rsp)                # 8-byte Spill
	orq	$4, %rsi
	xorl	%edi, %edi
	vzeroupper
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB0_220
# %bb.67:                               # %"assert succeeded197"
	movq	%r12, 736(%rsp)                 # 8-byte Spill
	movq	72(%rsp), %rdx                  # 8-byte Reload
	movl	%edx, %ecx
	movq	%rcx, 104(%rsp)                 # 8-byte Spill
	testl	%edx, %edx
	movq	%rax, 192(%rsp)                 # 8-byte Spill
	jle	.LBB0_81
# %bb.68:                               # %"for conv1_stage1.s0.w.preheader"
	movq	256(%rsp), %rcx                 # 8-byte Reload
	vmovups	(%rcx), %ymm0
	vmovups	32(%rcx), %ymm1
	vmovups	64(%rcx), %ymm2
	vmovups	96(%rcx), %ymm3
	movq	104(%rsp), %rdx                 # 8-byte Reload
	leaq	-1(%rdx), %rcx
	movl	%edx, %edi
	andl	$7, %edi
	cmpq	$7, %rcx
	jae	.LBB0_70
# %bb.69:
	xorl	%ecx, %ecx
	jmp	.LBB0_72
.LBB0_70:                               # %"for conv1_stage1.s0.w.preheader.new"
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	andl	$-8, %edx
	leaq	992(%rax), %rsi
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB0_71:                               # %"for conv1_stage1.s0.w"
                                        # =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, -992(%rsi)
	vmovaps	%ymm1, -960(%rsi)
	vmovaps	%ymm2, -928(%rsi)
	vmovaps	%ymm3, -896(%rsi)
	vmovaps	%ymm0, -864(%rsi)
	vmovaps	%ymm1, -832(%rsi)
	vmovaps	%ymm2, -800(%rsi)
	vmovaps	%ymm3, -768(%rsi)
	vmovaps	%ymm0, -736(%rsi)
	vmovaps	%ymm1, -704(%rsi)
	vmovaps	%ymm2, -672(%rsi)
	vmovaps	%ymm3, -640(%rsi)
	vmovaps	%ymm0, -608(%rsi)
	vmovaps	%ymm1, -576(%rsi)
	vmovaps	%ymm2, -544(%rsi)
	vmovaps	%ymm3, -512(%rsi)
	vmovaps	%ymm0, -480(%rsi)
	vmovaps	%ymm1, -448(%rsi)
	vmovaps	%ymm2, -416(%rsi)
	vmovaps	%ymm3, -384(%rsi)
	vmovaps	%ymm0, -352(%rsi)
	vmovaps	%ymm1, -320(%rsi)
	vmovaps	%ymm2, -288(%rsi)
	vmovaps	%ymm3, -256(%rsi)
	vmovaps	%ymm0, -224(%rsi)
	vmovaps	%ymm1, -192(%rsi)
	vmovaps	%ymm2, -160(%rsi)
	vmovaps	%ymm3, -128(%rsi)
	vmovaps	%ymm0, -96(%rsi)
	vmovaps	%ymm1, -64(%rsi)
	vmovaps	%ymm2, -32(%rsi)
	vmovaps	%ymm3, (%rsi)
	addq	$8, %rcx
	addq	$1024, %rsi                     # imm = 0x400
	cmpq	%rcx, %rdx
	jne	.LBB0_71
.LBB0_72:                               # %"for conv1_stage1.s1.w.preheader.unr-lcssa"
	testq	%rdi, %rdi
	je	.LBB0_75
# %bb.73:                               # %"for conv1_stage1.s0.w.epil.preheader"
	shlq	$7, %rcx
	addq	%rax, %rcx
	addq	$96, %rcx
	.p2align	4, 0x90
.LBB0_74:                               # %"for conv1_stage1.s0.w.epil"
                                        # =>This Inner Loop Header: Depth=1
	vmovaps	%ymm0, -96(%rcx)
	vmovaps	%ymm1, -64(%rcx)
	vmovaps	%ymm2, -32(%rcx)
	vmovaps	%ymm3, (%rcx)
	subq	$-128, %rcx
	decq	%rdi
	jne	.LBB0_74
.LBB0_75:                               # %"for conv1_stage1.s1.w.preheader"
	movq	928(%rsp), %rax                 # 8-byte Reload
	movl	%eax, %edx
	movq	1280(%rsp), %rcx                # 8-byte Reload
	imull	%ecx, %edx
	movl	%edx, 1416(%rsp)                # 4-byte Spill
	movl	$1, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movq	%rdx, 280(%rsp)                 # 8-byte Spill
	movl	$2, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movq	%rdx, 1408(%rsp)                # 8-byte Spill
	movl	$3, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movq	%rdx, 752(%rsp)                 # 8-byte Spill
	movl	$4, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movq	%rdx, 696(%rsp)                 # 8-byte Spill
	movl	$5, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movq	%rdx, 688(%rsp)                 # 8-byte Spill
	movl	$6, %edx
	subl	%eax, %edx
	imull	%ecx, %edx
	movl	%edx, 680(%rsp)                 # 4-byte Spill
	leaq	(,%r14,4), %rdx
	xorl	%esi, %esi
	vxorps	%xmm13, %xmm13, %xmm13
	.p2align	4, 0x90
.LBB0_76:                               # %"for conv1_stage1.s1.w"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_77 Depth 2
                                        #       Child Loop BB0_78 Depth 3
	movq	%rsi, %r15
	shlq	$5, %r15
	movq	%rsi, 3264(%rsp)                # 8-byte Spill
                                        # kill: def $esi killed $esi killed $rsi def $rsi
	subl	1104(%rsp), %esi                # 4-byte Folded Reload
	imull	1776(%rsp), %esi                # 4-byte Folded Reload
	subl	1112(%rsp), %esi                # 4-byte Folded Reload
	movl	%esi, %eax
	subl	1416(%rsp), %eax                # 4-byte Folded Reload
	movslq	%eax, %rdi
	movq	280(%rsp), %rax                 # 8-byte Reload
	addl	%esi, %eax
	movslq	%eax, %r13
	movq	1408(%rsp), %rax                # 8-byte Reload
	leal	(%rsi,%rax), %eax
	movslq	%eax, %rcx
	movq	752(%rsp), %rax                 # 8-byte Reload
	leal	(%rsi,%rax), %eax
	movslq	%eax, %r8
	movq	696(%rsp), %rax                 # 8-byte Reload
	leal	(%rsi,%rax), %eax
	movslq	%eax, %r12
	movq	688(%rsp), %rax                 # 8-byte Reload
	addl	%esi, %eax
	movslq	%eax, %r11
	addl	680(%rsp), %esi                 # 4-byte Folded Reload
	movslq	%esi, %r9
	movq	1424(%rsp), %rax                # 8-byte Reload
	vmovups	(%rax,%rdi,4), %xmm5
	vmovups	16(%rax,%rdi,4), %xmm6
	vmovups	32(%rax,%rdi,4), %xmm1
	vmovaps	%xmm1, 128(%rsp)                # 16-byte Spill
	vmovups	48(%rax,%rdi,4), %xmm1
	vmovaps	%xmm1, 224(%rsp)                # 16-byte Spill
	vmovups	64(%rax,%rdi,4), %xmm7
	vmovups	80(%rax,%rdi,4), %xmm1
	vmovups	96(%rax,%rdi,4), %xmm0
	vmovups	112(%rax,%rdi,4), %xmm10
	vmovups	128(%rax,%rdi,4), %xmm2
	vmovaps	%xmm2, 320(%rsp)                # 16-byte Spill
	vmovss	144(%rax,%rdi,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 384(%rsp)                # 4-byte Spill
	vmovss	148(%rax,%rdi,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 288(%rsp)                # 4-byte Spill
	vmovss	152(%rax,%rdi,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 512(%rsp)                # 4-byte Spill
	vmovss	156(%rax,%rdi,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 16(%rsp)                 # 4-byte Spill
	vmovups	(%rax,%r13,4), %xmm2
	vmovaps	%xmm2, 576(%rsp)                # 16-byte Spill
	vmovups	16(%rax,%r13,4), %xmm2
	vmovaps	%xmm2, 32(%rsp)                 # 16-byte Spill
	vmovups	32(%rax,%r13,4), %xmm2
	vmovaps	%xmm2, 80(%rsp)                 # 16-byte Spill
	movq	2640(%rsp), %rdi                # 8-byte Reload
	xorl	%r10d, %r10d
	vmovups	48(%rax,%r13,4), %xmm2
	vmovaps	%xmm2, 160(%rsp)                # 16-byte Spill
	vmovups	64(%rax,%r13,4), %xmm2
	vmovaps	%xmm2, 112(%rsp)                # 16-byte Spill
	vmovups	80(%rax,%r13,4), %xmm2
	vmovaps	%xmm2, 640(%rsp)                # 16-byte Spill
	vmovups	96(%rax,%r13,4), %xmm2
	vmovaps	%xmm2, 48(%rsp)                 # 16-byte Spill
	vmovups	112(%rax,%r13,4), %xmm2
	vmovaps	%xmm2, 480(%rsp)                # 16-byte Spill
	vmovups	128(%rax,%r13,4), %xmm2
	vmovaps	%xmm2, 256(%rsp)                # 16-byte Spill
	vmovss	144(%rax,%r13,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 928(%rsp)                # 4-byte Spill
	vmovss	148(%rax,%r13,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1280(%rsp)               # 4-byte Spill
	vmovss	152(%rax,%r13,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1344(%rsp)               # 4-byte Spill
	vmovss	156(%rax,%r13,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1536(%rsp)               # 4-byte Spill
	vmovups	(%rax,%rcx,4), %xmm2
	vmovaps	%xmm2, 1152(%rsp)               # 16-byte Spill
	vmovups	16(%rax,%rcx,4), %xmm2
	vmovaps	%xmm2, 768(%rsp)                # 16-byte Spill
	vmovups	32(%rax,%rcx,4), %xmm2
	vmovaps	%xmm2, 368(%rsp)                # 16-byte Spill
	vmovups	48(%rax,%rcx,4), %xmm2
	vmovaps	%xmm2, 352(%rsp)                # 16-byte Spill
	vmovups	64(%rax,%rcx,4), %xmm2
	vmovaps	%xmm2, 1248(%rsp)               # 16-byte Spill
	vmovups	80(%rax,%rcx,4), %xmm2
	vmovaps	%xmm2, 864(%rsp)                # 16-byte Spill
	vmovups	96(%rax,%rcx,4), %xmm2
	vmovaps	%xmm2, 1184(%rsp)               # 16-byte Spill
	vmovups	112(%rax,%rcx,4), %xmm2
	vmovaps	%xmm2, 832(%rsp)                # 16-byte Spill
	vmovups	128(%rax,%rcx,4), %xmm2
	vmovaps	%xmm2, 896(%rsp)                # 16-byte Spill
	vmovss	144(%rax,%rcx,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1312(%rsp)               # 4-byte Spill
	vmovss	148(%rax,%rcx,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1120(%rsp)               # 4-byte Spill
	vmovss	152(%rax,%rcx,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1504(%rsp)               # 4-byte Spill
	vmovss	156(%rax,%rcx,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1216(%rsp)               # 4-byte Spill
	vmovups	(%rax,%r8,4), %xmm2
	vmovaps	%xmm2, 1472(%rsp)               # 16-byte Spill
	vmovups	16(%rax,%r8,4), %xmm2
	vmovaps	%xmm2, 1440(%rsp)               # 16-byte Spill
	vmovups	32(%rax,%r8,4), %xmm2
	vmovaps	%xmm2, 560(%rsp)                # 16-byte Spill
	vmovups	48(%rax,%r8,4), %xmm2
	vmovaps	%xmm2, 608(%rsp)                # 16-byte Spill
	vmovups	64(%rax,%r8,4), %xmm2
	vmovaps	%xmm2, 1824(%rsp)               # 16-byte Spill
	vmovups	80(%rax,%r8,4), %xmm2
	vmovaps	%xmm2, 1664(%rsp)               # 16-byte Spill
	vmovups	96(%rax,%r8,4), %xmm2
	vmovaps	%xmm2, 1600(%rsp)               # 16-byte Spill
	vmovups	112(%rax,%r8,4), %xmm2
	vmovaps	%xmm2, 1568(%rsp)               # 16-byte Spill
	vmovups	128(%rax,%r8,4), %xmm2
	vmovaps	%xmm2, 1632(%rsp)               # 16-byte Spill
	vmovss	144(%rax,%r8,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1856(%rsp)               # 4-byte Spill
	vmovss	148(%rax,%r8,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 2528(%rsp)               # 4-byte Spill
	vmovss	152(%rax,%r8,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 456(%rsp)                # 4-byte Spill
	vmovss	156(%rax,%r8,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 448(%rsp)                # 4-byte Spill
	movq	%r15, %r8
	vmovups	(%rax,%r12,4), %xmm2
	vmovaps	%xmm2, 816(%rsp)                # 16-byte Spill
	vmovups	16(%rax,%r12,4), %xmm2
	vmovaps	%xmm2, 2176(%rsp)               # 16-byte Spill
	vmovups	32(%rax,%r12,4), %xmm2
	vmovaps	%xmm2, 704(%rsp)                # 16-byte Spill
	vmovups	48(%rax,%r12,4), %xmm2
	vmovaps	%xmm2, 1792(%rsp)               # 16-byte Spill
	vmovups	64(%rax,%r12,4), %xmm2
	vmovaps	%xmm2, 1920(%rsp)               # 16-byte Spill
	vmovups	80(%rax,%r12,4), %xmm2
	vmovaps	%xmm2, 2304(%rsp)               # 16-byte Spill
	vmovups	96(%rax,%r12,4), %xmm2
	vmovaps	%xmm2, 2496(%rsp)               # 16-byte Spill
	vmovups	112(%rax,%r12,4), %xmm2
	vmovaps	%xmm2, 2464(%rsp)               # 16-byte Spill
	vmovups	128(%rax,%r12,4), %xmm2
	vmovaps	%xmm2, 2432(%rsp)               # 16-byte Spill
	vmovss	144(%rax,%r12,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 1888(%rsp)               # 4-byte Spill
	vmovss	148(%rax,%r12,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 2400(%rsp)               # 4-byte Spill
	vmovss	152(%rax,%r12,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 2144(%rsp)               # 4-byte Spill
	vmovss	156(%rax,%r12,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 2112(%rsp)               # 4-byte Spill
	movq	192(%rsp), %rsi                 # 8-byte Reload
	vmovups	(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 2272(%rsp)               # 16-byte Spill
	vmovups	16(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 2240(%rsp)               # 16-byte Spill
	vmovups	32(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 2080(%rsp)               # 16-byte Spill
	vmovups	48(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 2048(%rsp)               # 16-byte Spill
	vmovups	64(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 2208(%rsp)               # 16-byte Spill
	vmovups	80(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 976(%rsp)                # 16-byte Spill
	vmovups	96(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 960(%rsp)                # 16-byte Spill
	vmovups	112(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 800(%rsp)                # 16-byte Spill
	vmovups	128(%rax,%r11,4), %xmm2
	vmovaps	%xmm2, 2592(%rsp)               # 16-byte Spill
	vmovss	144(%rax,%r11,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 2560(%rsp)               # 4-byte Spill
	vmovss	148(%rax,%r11,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 2976(%rsp)               # 4-byte Spill
	vmovss	152(%rax,%r11,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 2944(%rsp)               # 4-byte Spill
	vmovss	156(%rax,%r11,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vmovss	%xmm2, 3136(%rsp)               # 4-byte Spill
	vmovups	(%rax,%r9,4), %xmm2
	vmovaps	%xmm2, 2912(%rsp)               # 16-byte Spill
	vmovups	16(%rax,%r9,4), %xmm2
	vmovaps	%xmm2, 3104(%rsp)               # 16-byte Spill
	vmovups	32(%rax,%r9,4), %xmm2
	vmovaps	%xmm2, 3072(%rsp)               # 16-byte Spill
	vmovups	48(%rax,%r9,4), %xmm2
	vmovaps	%xmm2, 2816(%rsp)               # 16-byte Spill
	vmovups	64(%rax,%r9,4), %xmm2
	vmovaps	%xmm2, 2784(%rsp)               # 16-byte Spill
	vmovups	80(%rax,%r9,4), %xmm2
	vmovaps	%xmm2, 2368(%rsp)               # 16-byte Spill
	vmovups	96(%rax,%r9,4), %xmm2
	vmovaps	%xmm2, 2752(%rsp)               # 16-byte Spill
	vmovups	112(%rax,%r9,4), %xmm2
	vmovaps	%xmm2, 3040(%rsp)               # 16-byte Spill
	vmovups	128(%rax,%r9,4), %xmm12
	vmovss	144(%rax,%r9,4), %xmm8          # xmm8 = mem[0],zero,zero,zero
	vmovss	148(%rax,%r9,4), %xmm14         # xmm14 = mem[0],zero,zero,zero
	vmovss	152(%rax,%r9,4), %xmm15         # xmm15 = mem[0],zero,zero,zero
	vmovss	156(%rax,%r9,4), %xmm11         # xmm11 = mem[0],zero,zero,zero
	movq	104(%rsp), %rcx                 # 8-byte Reload
	.p2align	4, 0x90
.LBB0_77:                               # %"for conv1_stage1.s1.c"
                                        #   Parent Loop BB0_76 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB0_78 Depth 3
	leaq	(%r10,%r8), %r11
	vmovss	(%rsi,%r11,4), %xmm9            # xmm9 = mem[0],zero,zero,zero
	movq	%rdi, %r9
	xorl	%eax, %eax
	.p2align	4, 0x90
.LBB0_78:                               # %"for conv1_stage1.s1.r54$x"
                                        #   Parent Loop BB0_76 Depth=1
                                        #     Parent Loop BB0_77 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovaps	%xmm10, %xmm3
	vmovups	3520(%rsp,%rax,4), %xmm10
	vshufps	$0, 3488(%rsp,%rax,4), %xmm10, %xmm2 # xmm2 = xmm10[0,0],mem[0,0]
	vmovaps	%xmm3, %xmm10
	vmovups	3424(%rsp,%rax,4), %xmm3
	vinsertps	$28, 3456(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vmovss	(%rbx,%rax,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vshufps	$36, %xmm2, %xmm3, %xmm2        # xmm2 = xmm3[0,1],xmm2[2,0]
	vmovups	3552(%rsp,%rax,4), %xmm3
	vfmadd213ps	%xmm4, %xmm5, %xmm2     # xmm2 = (xmm5 * xmm2) + xmm4
	vmovups	3648(%rsp,%rax,4), %xmm4
	vshufps	$0, 3616(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vinsertps	$28, 3584(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	3776(%rsp,%rax,4), %xmm4
	vshufps	$0, 3744(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd213ps	%xmm2, %xmm6, %xmm3     # xmm3 = (xmm6 * xmm3) + xmm2
	vmovups	3680(%rsp,%rax,4), %xmm2
	vinsertps	$28, 3712(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	3904(%rsp,%rax,4), %xmm4
	vshufps	$0, 3872(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	128(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	3808(%rsp,%rax,4), %xmm3
	vinsertps	$28, 3840(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	4032(%rsp,%rax,4), %xmm4
	vshufps	$0, 4000(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	224(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	3936(%rsp,%rax,4), %xmm2
	vinsertps	$28, 3968(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	4160(%rsp,%rax,4), %xmm4
	vshufps	$0, 4128(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd213ps	%xmm3, %xmm7, %xmm2     # xmm2 = (xmm7 * xmm2) + xmm3
	vmovups	4064(%rsp,%rax,4), %xmm3
	vinsertps	$28, 4096(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	4288(%rsp,%rax,4), %xmm4
	vshufps	$0, 4256(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd213ps	%xmm2, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm3) + xmm2
	vmovups	4192(%rsp,%rax,4), %xmm2
	vinsertps	$28, 4224(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	4416(%rsp,%rax,4), %xmm4
	vshufps	$0, 4384(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd213ps	%xmm3, %xmm0, %xmm2     # xmm2 = (xmm0 * xmm2) + xmm3
	vmovups	4320(%rsp,%rax,4), %xmm3
	vinsertps	$28, 4352(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	4544(%rsp,%rax,4), %xmm4
	vshufps	$0, 4512(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd213ps	%xmm2, %xmm10, %xmm3    # xmm3 = (xmm10 * xmm3) + xmm2
	vmovups	4448(%rsp,%rax,4), %xmm2
	vinsertps	$28, 4480(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vfmadd132ps	320(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vpermilpd	$1, %xmm2, %xmm3        # xmm3 = xmm2[1,0]
	vaddps	%xmm3, %xmm2, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	384(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	4576(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	288(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	4608(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	512(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	4640(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	16(%rsp), %xmm3                 # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	4672(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovups	4800(%rsp,%rax,4), %xmm3
	vshufps	$0, 4768(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	4704(%rsp,%rax,4), %xmm4
	vblendps	$1, %xmm2, %xmm13, %xmm2        # xmm2 = xmm2[0],xmm13[1,2,3]
	vinsertps	$28, 4736(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	576(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	4928(%rsp,%rax,4), %xmm2
	vshufps	$0, 4896(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	4832(%rsp,%rax,4), %xmm4
	vinsertps	$28, 4864(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	32(%rsp), %xmm3, %xmm2  # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	5056(%rsp,%rax,4), %xmm3
	vshufps	$0, 5024(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	4960(%rsp,%rax,4), %xmm4
	vinsertps	$28, 4992(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	80(%rsp), %xmm2, %xmm3  # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	5184(%rsp,%rax,4), %xmm2
	vshufps	$0, 5152(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	5088(%rsp,%rax,4), %xmm4
	vinsertps	$28, 5120(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	160(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	5312(%rsp,%rax,4), %xmm3
	vshufps	$0, 5280(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	5216(%rsp,%rax,4), %xmm4
	vinsertps	$28, 5248(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	112(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	5440(%rsp,%rax,4), %xmm2
	vshufps	$0, 5408(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	5344(%rsp,%rax,4), %xmm4
	vinsertps	$28, 5376(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	640(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	5568(%rsp,%rax,4), %xmm3
	vshufps	$0, 5536(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	5472(%rsp,%rax,4), %xmm4
	vinsertps	$28, 5504(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	48(%rsp), %xmm2, %xmm3  # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	5696(%rsp,%rax,4), %xmm2
	vshufps	$0, 5664(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	5600(%rsp,%rax,4), %xmm4
	vinsertps	$28, 5632(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	480(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	5824(%rsp,%rax,4), %xmm3
	vshufps	$0, 5792(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	5728(%rsp,%rax,4), %xmm4
	vinsertps	$28, 5760(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	256(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        # xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	928(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	5856(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	1280(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	5888(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	1344(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	5920(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovups	6080(%rsp,%rax,4), %xmm3
	vmovups	5984(%rsp,%rax,4), %xmm4
	vinsertps	$28, 6016(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$0, 6048(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vmovss	1536(%rsp), %xmm4               # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vfmadd231ss	5952(%rsp,%rax,4), %xmm4, %xmm2 # xmm2 = (xmm4 * mem) + xmm2
	vmovups	6208(%rsp,%rax,4), %xmm4
	vshufps	$0, 6176(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vblendps	$1, %xmm2, %xmm13, %xmm2        # xmm2 = xmm2[0],xmm13[1,2,3]
	vfmadd132ps	1152(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	6112(%rsp,%rax,4), %xmm2
	vinsertps	$28, 6144(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	6336(%rsp,%rax,4), %xmm4
	vshufps	$0, 6304(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	768(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	6240(%rsp,%rax,4), %xmm3
	vinsertps	$28, 6272(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	6464(%rsp,%rax,4), %xmm4
	vshufps	$0, 6432(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	368(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	6368(%rsp,%rax,4), %xmm2
	vinsertps	$28, 6400(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	6592(%rsp,%rax,4), %xmm4
	vshufps	$0, 6560(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	352(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	6496(%rsp,%rax,4), %xmm3
	vinsertps	$28, 6528(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	6720(%rsp,%rax,4), %xmm4
	vshufps	$0, 6688(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	1248(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	6624(%rsp,%rax,4), %xmm2
	vinsertps	$28, 6656(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	6848(%rsp,%rax,4), %xmm4
	vshufps	$0, 6816(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	864(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	6752(%rsp,%rax,4), %xmm3
	vinsertps	$28, 6784(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	6976(%rsp,%rax,4), %xmm4
	vshufps	$0, 6944(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	1184(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	6880(%rsp,%rax,4), %xmm2
	vinsertps	$28, 6912(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	7104(%rsp,%rax,4), %xmm4
	vshufps	$0, 7072(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	832(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	7008(%rsp,%rax,4), %xmm3
	vinsertps	$28, 7040(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vfmadd132ps	896(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        # xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	1312(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	7136(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	1120(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	7168(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	1504(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	7200(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	1216(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	7232(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovups	7360(%rsp,%rax,4), %xmm3
	vshufps	$0, 7328(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	7264(%rsp,%rax,4), %xmm4
	vblendps	$1, %xmm2, %xmm13, %xmm2        # xmm2 = xmm2[0],xmm13[1,2,3]
	vinsertps	$28, 7296(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	1472(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	7488(%rsp,%rax,4), %xmm2
	vshufps	$0, 7456(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	7392(%rsp,%rax,4), %xmm4
	vinsertps	$28, 7424(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	1440(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	7616(%rsp,%rax,4), %xmm3
	vshufps	$0, 7584(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	7520(%rsp,%rax,4), %xmm4
	vinsertps	$28, 7552(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	560(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	7744(%rsp,%rax,4), %xmm2
	vshufps	$0, 7712(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	7648(%rsp,%rax,4), %xmm4
	vinsertps	$28, 7680(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	608(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	7872(%rsp,%rax,4), %xmm3
	vshufps	$0, 7840(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	7776(%rsp,%rax,4), %xmm4
	vinsertps	$28, 7808(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	1824(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	8000(%rsp,%rax,4), %xmm2
	vshufps	$0, 7968(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	7904(%rsp,%rax,4), %xmm4
	vinsertps	$28, 7936(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	1664(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	8128(%rsp,%rax,4), %xmm3
	vshufps	$0, 8096(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	8032(%rsp,%rax,4), %xmm4
	vinsertps	$28, 8064(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	1600(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	8256(%rsp,%rax,4), %xmm2
	vshufps	$0, 8224(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	8160(%rsp,%rax,4), %xmm4
	vinsertps	$28, 8192(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	1568(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	8384(%rsp,%rax,4), %xmm3
	vshufps	$0, 8352(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	8288(%rsp,%rax,4), %xmm4
	vinsertps	$28, 8320(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	1632(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        # xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	1856(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	8416(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	2528(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	8448(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	456(%rsp), %xmm3                # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	8480(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovups	8640(%rsp,%rax,4), %xmm3
	vmovups	8544(%rsp,%rax,4), %xmm4
	vinsertps	$28, 8576(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$0, 8608(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vmovss	448(%rsp), %xmm4                # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vfmadd231ss	8512(%rsp,%rax,4), %xmm4, %xmm2 # xmm2 = (xmm4 * mem) + xmm2
	vmovups	8768(%rsp,%rax,4), %xmm4
	vshufps	$0, 8736(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vblendps	$1, %xmm2, %xmm13, %xmm2        # xmm2 = xmm2[0],xmm13[1,2,3]
	vfmadd132ps	816(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	8672(%rsp,%rax,4), %xmm2
	vinsertps	$28, 8704(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	8896(%rsp,%rax,4), %xmm4
	vshufps	$0, 8864(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	2176(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	8800(%rsp,%rax,4), %xmm3
	vinsertps	$28, 8832(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	9024(%rsp,%rax,4), %xmm4
	vshufps	$0, 8992(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	704(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	8928(%rsp,%rax,4), %xmm2
	vinsertps	$28, 8960(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	9152(%rsp,%rax,4), %xmm4
	vshufps	$0, 9120(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	1792(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	9056(%rsp,%rax,4), %xmm3
	vinsertps	$28, 9088(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	9280(%rsp,%rax,4), %xmm4
	vshufps	$0, 9248(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	1920(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	9184(%rsp,%rax,4), %xmm2
	vinsertps	$28, 9216(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	9408(%rsp,%rax,4), %xmm4
	vshufps	$0, 9376(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	2304(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	9312(%rsp,%rax,4), %xmm3
	vinsertps	$28, 9344(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	9536(%rsp,%rax,4), %xmm4
	vshufps	$0, 9504(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	2496(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	9440(%rsp,%rax,4), %xmm2
	vinsertps	$28, 9472(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	9664(%rsp,%rax,4), %xmm4
	vshufps	$0, 9632(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	2464(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	9568(%rsp,%rax,4), %xmm3
	vinsertps	$28, 9600(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vfmadd132ps	2432(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        # xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	1888(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	9696(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	2400(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	9728(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	2144(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	9760(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	2112(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	9792(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovups	9920(%rsp,%rax,4), %xmm3
	vshufps	$0, 9888(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	9824(%rsp,%rax,4), %xmm4
	vblendps	$1, %xmm2, %xmm13, %xmm2        # xmm2 = xmm2[0],xmm13[1,2,3]
	vinsertps	$28, 9856(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	2272(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	10048(%rsp,%rax,4), %xmm2
	vshufps	$0, 10016(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	9952(%rsp,%rax,4), %xmm4
	vinsertps	$28, 9984(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	2240(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	10176(%rsp,%rax,4), %xmm3
	vshufps	$0, 10144(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	10080(%rsp,%rax,4), %xmm4
	vinsertps	$28, 10112(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	2080(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	10304(%rsp,%rax,4), %xmm2
	vshufps	$0, 10272(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	10208(%rsp,%rax,4), %xmm4
	vinsertps	$28, 10240(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	2048(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	10432(%rsp,%rax,4), %xmm3
	vshufps	$0, 10400(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	10336(%rsp,%rax,4), %xmm4
	vinsertps	$28, 10368(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	2208(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	10560(%rsp,%rax,4), %xmm2
	vshufps	$0, 10528(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	10464(%rsp,%rax,4), %xmm4
	vinsertps	$28, 10496(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	976(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	10688(%rsp,%rax,4), %xmm3
	vshufps	$0, 10656(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	10592(%rsp,%rax,4), %xmm4
	vinsertps	$28, 10624(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	960(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	10816(%rsp,%rax,4), %xmm2
	vshufps	$0, 10784(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,0],mem[0,0]
	vmovups	10720(%rsp,%rax,4), %xmm4
	vinsertps	$28, 10752(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm2, %xmm4, %xmm2        # xmm2 = xmm4[0,1],xmm2[2,0]
	vfmadd132ps	800(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	10944(%rsp,%rax,4), %xmm3
	vshufps	$0, 10912(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vmovups	10848(%rsp,%rax,4), %xmm4
	vinsertps	$28, 10880(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vfmadd132ps	2592(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        # xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vmovss	2560(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	10976(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	2976(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	11008(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovss	2944(%rsp), %xmm3               # 4-byte Reload
                                        # xmm3 = mem[0],zero,zero,zero
	vfmadd231ss	11040(%rsp,%rax,4), %xmm3, %xmm2 # xmm2 = (xmm3 * mem) + xmm2
	vmovups	11200(%rsp,%rax,4), %xmm3
	vmovups	11104(%rsp,%rax,4), %xmm4
	vinsertps	$28, 11136(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],zero,zero
	vshufps	$0, 11168(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,0],mem[0,0]
	vshufps	$36, %xmm3, %xmm4, %xmm3        # xmm3 = xmm4[0,1],xmm3[2,0]
	vmovss	3136(%rsp), %xmm4               # 4-byte Reload
                                        # xmm4 = mem[0],zero,zero,zero
	vfmadd231ss	11072(%rsp,%rax,4), %xmm4, %xmm2 # xmm2 = (xmm4 * mem) + xmm2
	vmovups	11328(%rsp,%rax,4), %xmm4
	vshufps	$0, 11296(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vblendps	$1, %xmm2, %xmm13, %xmm2        # xmm2 = xmm2[0],xmm13[1,2,3]
	vfmadd132ps	2912(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	11232(%rsp,%rax,4), %xmm2
	vinsertps	$28, 11264(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	11456(%rsp,%rax,4), %xmm4
	vshufps	$0, 11424(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	3104(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	11360(%rsp,%rax,4), %xmm3
	vinsertps	$28, 11392(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	11584(%rsp,%rax,4), %xmm4
	vshufps	$0, 11552(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	3072(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	11488(%rsp,%rax,4), %xmm2
	vinsertps	$28, 11520(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	11712(%rsp,%rax,4), %xmm4
	vshufps	$0, 11680(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	2816(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	11616(%rsp,%rax,4), %xmm3
	vinsertps	$28, 11648(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	11840(%rsp,%rax,4), %xmm4
	vshufps	$0, 11808(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	2784(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	11744(%rsp,%rax,4), %xmm2
	vinsertps	$28, 11776(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	11968(%rsp,%rax,4), %xmm4
	vshufps	$0, 11936(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	2368(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	11872(%rsp,%rax,4), %xmm3
	vinsertps	$28, 11904(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vmovups	12096(%rsp,%rax,4), %xmm4
	vshufps	$0, 12064(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	2752(%rsp), %xmm2, %xmm3 # 16-byte Folded Reload
                                        # xmm3 = (xmm3 * mem) + xmm2
	vmovups	12000(%rsp,%rax,4), %xmm2
	vinsertps	$28, 12032(%rsp,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm2, %xmm2        # xmm2 = xmm2[0,1],xmm4[2,0]
	vmovups	12224(%rsp,%rax,4), %xmm4
	vshufps	$0, 12192(%rsp,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,0],mem[0,0]
	vfmadd132ps	3040(%rsp), %xmm3, %xmm2 # 16-byte Folded Reload
                                        # xmm2 = (xmm2 * mem) + xmm3
	vmovups	12128(%rsp,%rax,4), %xmm3
	vinsertps	$28, 12160(%rsp,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],zero,zero
	vshufps	$36, %xmm4, %xmm3, %xmm3        # xmm3 = xmm3[0,1],xmm4[2,0]
	vfmadd213ps	%xmm2, %xmm12, %xmm3    # xmm3 = (xmm12 * xmm3) + xmm2
	vpermilpd	$1, %xmm3, %xmm2        # xmm2 = xmm3[1,0]
	vaddps	%xmm2, %xmm3, %xmm2
	vmovshdup	%xmm2, %xmm3            # xmm3 = xmm2[1,1,3,3]
	vaddss	%xmm3, %xmm2, %xmm2
	vfmadd231ss	12256(%rsp,%rax,4), %xmm8, %xmm2 # xmm2 = (xmm8 * mem) + xmm2
	vfmadd231ss	12288(%rsp,%rax,4), %xmm14, %xmm2 # xmm2 = (xmm14 * mem) + xmm2
	vfmadd231ss	12320(%rsp,%rax,4), %xmm15, %xmm2 # xmm2 = (xmm15 * mem) + xmm2
	vfmadd231ss	12352(%rsp,%rax,4), %xmm11, %xmm2 # xmm2 = (xmm11 * mem) + xmm2
	vfmadd231ss	(%r9), %xmm2, %xmm9     # xmm9 = (xmm2 * mem) + xmm9
	incq	%rax
	addq	%rdx, %r9
	cmpq	$8, %rax
	jne	.LBB0_78
# %bb.79:                               # %"end for conv1_stage1.s1.r54$x"
                                        #   in Loop: Header=BB0_77 Depth=2
	vmovss	%xmm9, (%rsi,%r11,4)
	incq	%r10
	addq	$4, %rdi
	cmpq	$32, %r10
	jne	.LBB0_77
# %bb.80:                               # %"end for conv1_stage1.s1.c"
                                        #   in Loop: Header=BB0_76 Depth=1
	movq	3264(%rsp), %rsi                # 8-byte Reload
	incq	%rsi
	cmpq	%rcx, %rsi
	jne	.LBB0_76
.LBB0_81:                               # %"end for conv1_stage1.s1.w"
	movq	1432(%rsp), %r12                # 8-byte Reload
	movl	%r12d, %eax
	movl	1400(%rsp), %r15d               # 4-byte Reload
	subl	%r15d, %eax
	movl	%eax, %ecx
	sarl	$31, %ecx
	andnl	%eax, %ecx, %eax
	movq	736(%rsp), %rcx                 # 8-byte Reload
	shlq	$5, %rcx
	leaq	(%rcx,%rcx,2), %rcx
	movq	%rcx, %rsi
	imulq	%rax, %rsi
	movl	$2147483648, %edx               # imm = 0x80000000
	cmpq	%rdx, %rsi
	jae	.LBB0_210
# %bb.82:                               # %"end for conv1_stage1.s1.w"
	movq	%rcx, %rdx
	shrq	$32, %rdx
	andl	$-128, %ecx
	imulq	%rax, %rcx
	shrq	$32, %rcx
	imulq	%rax, %rdx
	addq	%rcx, %rdx
	movabsq	$545460846592, %rcx             # imm = 0x7F00000000
	andq	%rdx, %rcx
	jne	.LBB0_210
# %bb.83:                               # %"assert succeeded199"
	orq	$4, %rsi
	xorl	%edi, %edi
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	testq	%rax, %rax
	movq	432(%rsp), %r13                 # 8-byte Reload
	je	.LBB0_221
# %bb.84:                               # %"assert succeeded201"
	movl	%r13d, %eax
	shll	$5, %eax
	leal	(%rax,%rax,2), %eax
	movq	%rax, 768(%rsp)                 # 8-byte Spill
	movq	1048(%rsp), %rax                # 8-byte Reload
	imull	184(%rsp), %eax                 # 4-byte Folded Reload
	movq	472(%rsp), %rcx                 # 8-byte Reload
	imull	632(%rsp), %ecx                 # 4-byte Folded Reload
	addl	1056(%rsp), %ecx                # 4-byte Folded Reload
	addl	%eax, %ecx
	movl	672(%rsp), %eax                 # 4-byte Reload
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	cmpl	%r15d, %r12d
	movq	%rcx, 472(%rsp)                 # 8-byte Spill
	movq	464(%rsp), %r8                  # 8-byte Reload
	jle	.LBB0_110
# %bb.85:                               # %"for head2_relu.s0.n.preheader"
	xorl	%eax, %eax
	cmpl	$0, 72(%rsp)                    # 4-byte Folded Reload
	movl	2688(%rsp), %edx                # 4-byte Reload
	cmovgl	%edx, %eax
	imulq	$156, %rax, %rsi
	movl	$2147483648, %eax               # imm = 0x80000000
	cmpq	%rax, %rsi
	jae	.LBB0_223
# %bb.86:                               # %"for head2_relu.s0.n.preheader470"
	orq	$4, %rsi
	movq	768(%rsp), %rax                 # 8-byte Reload
	addl	$96, %eax
	movl	%edx, %edx
	movq	%rdx, 512(%rsp)                 # 8-byte Spill
	movslq	%r15d, %rdx
	cltq
	movq	%rax, 1344(%rsp)                # 8-byte Spill
	vmovaps	3424(%rsp), %ymm0
	vmovaps	%ymm0, 480(%rsp)                # 32-byte Spill
	vmovaps	3456(%rsp), %ymm0
	vmovaps	%ymm0, 288(%rsp)                # 32-byte Spill
	vmovaps	3488(%rsp), %ymm0
	vmovaps	%ymm0, 384(%rsp)                # 32-byte Spill
	vmovaps	3520(%rsp), %ymm0
	vmovaps	%ymm0, 320(%rsp)                # 32-byte Spill
	movq	3008(%rsp), %rax                # 8-byte Reload
	leaq	(,%rax,8), %rdi
	movq	%rax, %rbx
	shlq	$4, %rbx
	movq	%rbx, 160(%rsp)                 # 8-byte Spill
	movq	%rdi, 80(%rsp)                  # 8-byte Spill
	leaq	(%rdi,%rdi,2), %rdi
	movq	%rdi, 112(%rsp)                 # 8-byte Spill
	movl	%r15d, %r12d
	subl	%ecx, %r12d
	movl	%r12d, 256(%rsp)                # 4-byte Spill
	leaq	(,%rax,4), %r13
	leaq	(,%r8,8), %r12
	movq	3296(%rsp), %rax                # 8-byte Reload
	leaq	32(%rax), %rcx
	movq	%rcx, 16(%rsp)                  # 8-byte Spill
	leaq	64(%rax), %rax
	movq	%rax, 576(%rsp)                 # 8-byte Spill
	vbroadcastss	.LCPI0_52(%rip), %ymm0  # ymm0 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vmovaps	%ymm0, 640(%rsp)                # 32-byte Spill
	vbroadcastss	.LCPI0_53(%rip), %ymm0  # ymm0 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	vmovaps	%ymm0, 1536(%rsp)               # 32-byte Spill
	vbroadcastss	.LCPI0_54(%rip), %ymm0  # ymm0 = [7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2,7.64031857E-2]
	vmovaps	%ymm0, 1152(%rsp)               # 32-byte Spill
	movq	%rdx, 1280(%rsp)                # 8-byte Spill
	movq	%rdx, 48(%rsp)                  # 8-byte Spill
	movq	%rsi, 928(%rsp)                 # 8-byte Spill
	jmp	.LBB0_88
	.p2align	4, 0x90
.LBB0_87:                               # %call_destructor.exit383
                                        #   in Loop: Header=BB0_88 Depth=1
	vmovaps	%ymm11, 480(%rsp)               # 32-byte Spill
	vmovaps	%ymm10, 288(%rsp)               # 32-byte Spill
	vmovaps	%ymm9, 384(%rsp)                # 32-byte Spill
	vmovaps	%ymm8, 320(%rsp)                # 32-byte Spill
	xorl	%edi, %edi
	movq	32(%rsp), %rsi                  # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	movq	48(%rsp), %rax                  # 8-byte Reload
	incq	%rax
	incl	256(%rsp)                       # 4-byte Folded Spill
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	cmpl	%eax, 1432(%rsp)                # 4-byte Folded Reload
	movq	928(%rsp), %rsi                 # 8-byte Reload
	je	.LBB0_109
.LBB0_88:                               # %"for head2_relu.s0.n"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_91 Depth 2
                                        #       Child Loop BB0_95 Depth 3
                                        #     Child Loop BB0_100 Depth 2
                                        #       Child Loop BB0_101 Depth 3
                                        #       Child Loop BB0_104 Depth 3
                                        #       Child Loop BB0_107 Depth 3
	xorl	%edi, %edi
	vzeroupper
	callq	halide_malloc@PLT
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	testq	%rax, %rax
	je	.LBB0_159
# %bb.89:                               # %"assert succeeded205"
                                        #   in Loop: Header=BB0_88 Depth=1
	cmpl	$0, 2688(%rsp)                  # 4-byte Folded Reload
	vmovaps	2720(%rsp), %ymm9               # 32-byte Reload
	vmovaps	1536(%rsp), %ymm14              # 32-byte Reload
	vmovaps	1152(%rsp), %ymm15              # 32-byte Reload
	vmovss	.LCPI0_48(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmovss	.LCPI0_47(%rip), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmovss	.LCPI0_54(%rip), %xmm11         # xmm11 = mem[0],zero,zero,zero
	vmovss	.LCPI0_55(%rip), %xmm10         # xmm10 = mem[0],zero,zero,zero
	vmovss	.LCPI0_56(%rip), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vmovss	.LCPI0_60(%rip), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vmovss	.LCPI0_58(%rip), %xmm8          # xmm8 = mem[0],zero,zero,zero
	movq	128(%rsp), %rcx                 # 8-byte Reload
	jle	.LBB0_97
# %bb.90:                               # %"for normalized_schedule_features.s0.s.preheader"
                                        #   in Loop: Header=BB0_88 Depth=1
	movslq	256(%rsp), %r10                 # 4-byte Folded Reload
	movq	48(%rsp), %rax                  # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax
	subl	472(%rsp), %eax                 # 4-byte Folded Reload
	movslq	%eax, %r8
	movq	32(%rsp), %rsi                  # 8-byte Reload
	xorl	%r11d, %r11d
	.p2align	4, 0x90
.LBB0_91:                               # %"for normalized_schedule_features.s0.s"
                                        #   Parent Loop BB0_88 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB0_95 Depth 3
	cmpl	$1, 632(%rsp)                   # 4-byte Folded Reload
	movq	440(%rsp), %rcx                 # 8-byte Reload
	jne	.LBB0_93
# %bb.92:                               # %vector.body895
                                        #   in Loop: Header=BB0_91 Depth=2
	movq	%r11, %rdi
	imulq	176(%rsp), %rdi                 # 8-byte Folded Reload
	addq	%r8, %rdi
	vaddps	(%rcx,%rdi,4), %ymm9, %ymm0
	imulq	$39, %r11, %r9
	vmovaps	640(%rsp), %ymm13               # 32-byte Reload
	vandps	%ymm0, %ymm13, %ymm1
	vpsrad	$22, %ymm1, %ymm2
	vpslld	$23, %ymm2, %ymm3
	vpsubd	%ymm3, %ymm1, %ymm1
	vmovdqa	2848(%rsp), %ymm12              # 32-byte Reload
	vpaddd	%ymm1, %ymm12, %ymm1
	vmovaps	3232(%rsp), %ymm10              # 32-byte Reload
	vaddps	%ymm1, %ymm10, %ymm1
	vmulps	%ymm1, %ymm1, %ymm5
	vpsrad	$23, %ymm0, %ymm0
	vmovdqa	2880(%rsp), %ymm11              # 32-byte Reload
	vpaddd	%ymm0, %ymm11, %ymm0
	vpaddd	%ymm0, %ymm2, %ymm0
	vcvtdq2ps	%ymm0, %ymm6
	vbroadcastss	.LCPI0_55(%rip), %ymm0  # ymm0 = [2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1,2.06252187E-1]
	vmovaps	%ymm15, %ymm3
	vfmadd213ps	%ymm0, %ymm5, %ymm3     # ymm3 = (ymm5 * ymm3) + ymm0
	vbroadcastss	.LCPI0_56(%rip), %ymm2  # ymm2 = [3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1,3.33204657E-1]
	vfmadd213ps	%ymm2, %ymm5, %ymm3     # ymm3 = (ymm5 * ymm3) + ymm2
	vfmadd213ps	%ymm9, %ymm5, %ymm3     # ymm3 = (ymm5 * ymm3) + ymm9
	vmulps	%ymm3, %ymm1, %ymm7
	vbroadcastss	.LCPI0_57(%rip), %ymm1  # ymm1 = [1.62529618E-1,1.62529618E-1,1.62529618E-1,1.62529618E-1,1.62529618E-1,1.62529618E-1,1.62529618E-1,1.62529618E-1]
	vbroadcastss	.LCPI0_58(%rip), %ymm3  # ymm3 = [-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1,-2.51102597E-1]
	vmovaps	%ymm1, %ymm8
	vfmsub213ps	%ymm3, %ymm5, %ymm8     # ymm8 = (ymm5 * ymm8) - ymm3
	vbroadcastss	.LCPI0_59(%rip), %ymm4  # ymm4 = [-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1,-4.99975145E-1]
	vfmsub213ps	%ymm4, %ymm5, %ymm8     # ymm8 = (ymm5 * ymm8) - ymm4
	vfmsub213ps	%ymm7, %ymm5, %ymm8     # ymm8 = (ymm5 * ymm8) - ymm7
	vfmsub231ps	%ymm6, %ymm14, %ymm8    # ymm8 = (ymm14 * ymm6) - ymm8
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vmovups	%ymm8, (%rdx,%r9,4)
	movq	80(%rsp), %rax                  # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	vaddps	(%rcx,%rax,4), %ymm9, %ymm5
	vandps	%ymm5, %ymm13, %ymm6
	vpsrad	$22, %ymm6, %ymm7
	vpslld	$23, %ymm7, %ymm8
	vpsubd	%ymm8, %ymm6, %ymm6
	vpaddd	%ymm6, %ymm12, %ymm6
	vaddps	%ymm6, %ymm10, %ymm6
	vmulps	%ymm6, %ymm6, %ymm8
	vpsrad	$23, %ymm5, %ymm5
	vpaddd	%ymm5, %ymm11, %ymm5
	vpaddd	%ymm5, %ymm7, %ymm5
	vcvtdq2ps	%ymm5, %ymm5
	vmovaps	%ymm15, %ymm7
	vfmadd213ps	%ymm0, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) + ymm0
	vfmadd213ps	%ymm2, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) + ymm2
	vfmadd213ps	%ymm9, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) + ymm9
	vmulps	%ymm7, %ymm6, %ymm6
	vmovaps	%ymm1, %ymm7
	vfmsub213ps	%ymm3, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) - ymm3
	vfmsub213ps	%ymm4, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) - ymm4
	vfmsub213ps	%ymm6, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) - ymm6
	vfmsub231ps	%ymm5, %ymm14, %ymm7    # ymm7 = (ymm14 * ymm5) - ymm7
	vmovups	%ymm7, 32(%rdx,%r9,4)
	movq	160(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rdi), %rax
	vaddps	(%rcx,%rax,4), %ymm9, %ymm5
	vandps	%ymm5, %ymm13, %ymm6
	vpsrad	$22, %ymm6, %ymm7
	vpslld	$23, %ymm7, %ymm8
	vpsubd	%ymm8, %ymm6, %ymm6
	vpaddd	%ymm6, %ymm12, %ymm6
	vaddps	%ymm6, %ymm10, %ymm6
	vmulps	%ymm6, %ymm6, %ymm8
	vpsrad	$23, %ymm5, %ymm5
	vpaddd	%ymm5, %ymm11, %ymm5
	vpaddd	%ymm5, %ymm7, %ymm5
	vcvtdq2ps	%ymm5, %ymm5
	vmovaps	%ymm15, %ymm7
	vfmadd213ps	%ymm0, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) + ymm0
	vfmadd213ps	%ymm2, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) + ymm2
	vfmadd213ps	%ymm9, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) + ymm9
	vmulps	%ymm7, %ymm6, %ymm6
	vmovaps	%ymm1, %ymm7
	vfmsub213ps	%ymm3, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) - ymm3
	vfmsub213ps	%ymm4, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) - ymm4
	vfmsub213ps	%ymm6, %ymm8, %ymm7     # ymm7 = (ymm8 * ymm7) - ymm6
	vfmsub231ps	%ymm5, %ymm14, %ymm7    # ymm7 = (ymm14 * ymm5) - ymm7
	vmovups	%ymm7, 64(%rdx,%r9,4)
	addq	112(%rsp), %rdi                 # 8-byte Folded Reload
	vaddps	(%rcx,%rdi,4), %ymm9, %ymm5
	vandps	%ymm5, %ymm13, %ymm6
	vpsrad	$22, %ymm6, %ymm7
	vpslld	$23, %ymm7, %ymm8
	vpsubd	%ymm8, %ymm6, %ymm6
	vpaddd	%ymm6, %ymm12, %ymm6
	vaddps	%ymm6, %ymm10, %ymm6
	vmulps	%ymm6, %ymm6, %ymm8
	vpsrad	$23, %ymm5, %ymm5
	vpaddd	%ymm5, %ymm11, %ymm5
	vpaddd	%ymm5, %ymm7, %ymm5
	vmovss	.LCPI0_55(%rip), %xmm10         # xmm10 = mem[0],zero,zero,zero
	vcvtdq2ps	%ymm5, %ymm5
	vfmadd231ps	%ymm15, %ymm8, %ymm0    # ymm0 = (ymm8 * ymm15) + ymm0
	vfmadd213ps	%ymm2, %ymm8, %ymm0     # ymm0 = (ymm8 * ymm0) + ymm2
	vfmadd213ps	%ymm9, %ymm8, %ymm0     # ymm0 = (ymm8 * ymm0) + ymm9
	vmulps	%ymm0, %ymm6, %ymm0
	vmovss	.LCPI0_54(%rip), %xmm11         # xmm11 = mem[0],zero,zero,zero
	vfmsub213ps	%ymm3, %ymm8, %ymm1     # ymm1 = (ymm8 * ymm1) - ymm3
	vfmsub213ps	%ymm4, %ymm8, %ymm1     # ymm1 = (ymm8 * ymm1) - ymm4
	vmovss	.LCPI0_48(%rip), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vfmsub213ps	%ymm0, %ymm8, %ymm1     # ymm1 = (ymm8 * ymm1) - ymm0
	vmovss	.LCPI0_58(%rip), %xmm8          # xmm8 = mem[0],zero,zero,zero
	vmovss	.LCPI0_60(%rip), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vmovss	.LCPI0_56(%rip), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vfmsub231ps	%ymm5, %ymm14, %ymm1    # ymm1 = (ymm14 * ymm5) - ymm1
	vmovss	.LCPI0_47(%rip), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmovups	%ymm1, 96(%rdx,%r9,4)
	movl	$32, %eax
	jmp	.LBB0_94
	.p2align	4, 0x90
.LBB0_93:                               #   in Loop: Header=BB0_91 Depth=2
	xorl	%eax, %eax
.LBB0_94:                               # %"for normalized_schedule_features.s0.c.preheader"
                                        #   in Loop: Header=BB0_91 Depth=2
	movq	3008(%rsp), %rdi                # 8-byte Reload
	imulq	%rax, %rdi
	addq	%r10, %rdi
	leaq	(%rcx,%rdi,4), %rdi
	vmovss	.LCPI0_59(%rip), %xmm12         # xmm12 = mem[0],zero,zero,zero
	vmovss	.LCPI0_53(%rip), %xmm13         # xmm13 = mem[0],zero,zero,zero
	.p2align	4, 0x90
.LBB0_95:                               # %"for normalized_schedule_features.s0.c"
                                        #   Parent Loop BB0_88 Depth=1
                                        #     Parent Loop BB0_91 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vaddss	(%rdi), %xmm4, %xmm0
	vmovd	%xmm0, %ecx
	movl	%ecx, %ebx
	andl	$-2139095041, %ebx              # imm = 0x807FFFFF
	movl	%ebx, %edx
	sarl	$22, %edx
	sarl	$23, %ecx
	addl	%edx, %ecx
	addl	$-127, %ecx
                                        # kill: def $edx killed $edx killed $rdx def $rdx
	shll	$23, %edx
	negl	%edx
	addl	%ebx, %edx
	addl	$1065353216, %edx               # imm = 0x3F800000
	vmovd	%edx, %xmm0
	vaddss	%xmm5, %xmm0, %xmm0
	vmulss	%xmm0, %xmm0, %xmm1
	vmovaps	%xmm11, %xmm2
	vfmadd213ss	%xmm10, %xmm1, %xmm2    # xmm2 = (xmm1 * xmm2) + xmm10
	vfmadd213ss	%xmm7, %xmm1, %xmm2     # xmm2 = (xmm1 * xmm2) + xmm7
	vfmadd213ss	%xmm4, %xmm1, %xmm2     # xmm2 = (xmm1 * xmm2) + xmm4
	vmovaps	%xmm6, %xmm3
	vmulss	%xmm2, %xmm0, %xmm0
	vfmadd213ss	%xmm8, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm3) + xmm8
	vfmadd213ss	%xmm12, %xmm1, %xmm3    # xmm3 = (xmm1 * xmm3) + xmm12
	vcvtsi2ss	%ecx, %xmm9, %xmm2
	vfmadd213ss	%xmm0, %xmm1, %xmm3     # xmm3 = (xmm1 * xmm3) + xmm0
	vfmadd231ss	%xmm13, %xmm2, %xmm3    # xmm3 = (xmm2 * xmm13) + xmm3
	vmovss	%xmm3, (%rsi,%rax,4)
	incq	%rax
	addq	%r13, %rdi
	cmpq	$39, %rax
	jne	.LBB0_95
# %bb.96:                               # %"end for normalized_schedule_features.s0.c"
                                        #   in Loop: Header=BB0_91 Depth=2
	incq	%r11
	addq	$156, %rsi
	addq	176(%rsp), %r10                 # 8-byte Folded Reload
	cmpq	512(%rsp), %r11                 # 8-byte Folded Reload
	movq	128(%rsp), %rcx                 # 8-byte Reload
	jne	.LBB0_91
.LBB0_97:                               # %"consume normalized_schedule_features"
                                        #   in Loop: Header=BB0_88 Depth=1
	cmpl	$0, 72(%rsp)                    # 4-byte Folded Reload
	vpxor	%xmm12, %xmm12, %xmm12
	vmovaps	320(%rsp), %ymm8                # 32-byte Reload
	vmovaps	384(%rsp), %ymm9                # 32-byte Reload
	vmovaps	288(%rsp), %ymm10               # 32-byte Reload
	vmovaps	480(%rsp), %ymm11               # 32-byte Reload
	movq	464(%rsp), %rbx                 # 8-byte Reload
	jle	.LBB0_87
# %bb.98:                               # %"for head2_relu.s0.w.w.preheader"
                                        #   in Loop: Header=BB0_88 Depth=1
	movq	48(%rsp), %r8                   # 8-byte Reload
	subq	1280(%rsp), %r8                 # 8-byte Folded Reload
	imulq	1344(%rsp), %r8                 # 8-byte Folded Reload
	movq	1784(%rsp), %rax                # 8-byte Reload
	vmovups	(%rax), %ymm0
	vmovups	32(%rax), %ymm1
	vmovups	64(%rax), %ymm2
	movq	32(%rsp), %rdx                  # 8-byte Reload
	addq	$472, %rdx                      # imm = 0x1D8
	xorl	%eax, %eax
	jmp	.LBB0_100
	.p2align	4, 0x90
.LBB0_99:                               # %"for head2_relu.s0.w.v15.preheader.2"
                                        #   in Loop: Header=BB0_100 Depth=2
	vmaxps	%ymm12, %ymm11, %ymm3
	orq	$16, %rsi
	addq	%r8, %rsi
	vmovaps	%ymm3, (%rcx,%rsi,4)
	vmaxps	%ymm12, %ymm10, %ymm3
	vmovaps	%ymm3, 64(%rcx,%r15,4)
	vmaxps	%ymm12, %ymm9, %ymm3
	movq	384(%rsp), %rax                 # 8-byte Reload
	vmovaps	%ymm3, 64(%rcx,%rax,4)
	vmaxps	%ymm12, %ymm8, %ymm3
	shlq	$3, %r9
	leaq	(%r9,%r9,2), %rax
	orq	$16, %rax
	addq	%r8, %rax
	vmovaps	%ymm3, (%rcx,%rax,4)
	movq	320(%rsp), %rax                 # 8-byte Reload
	incq	%rax
	addq	$624, %rdx                      # imm = 0x270
	cmpq	224(%rsp), %rax                 # 8-byte Folded Reload
	je	.LBB0_87
.LBB0_100:                              # %"for head2_relu.s0.w.w"
                                        #   Parent Loop BB0_88 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB0_101 Depth 3
                                        #       Child Loop BB0_104 Depth 3
                                        #       Child Loop BB0_107 Depth 3
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	leal	(,%rax,4), %r10d
	movq	$-38, %rax
	movq	3296(%rsp), %rsi                # 8-byte Reload
	vmovaps	%ymm0, %ymm3
	vmovaps	%ymm0, %ymm4
	vmovaps	%ymm0, %ymm6
	vmovaps	%ymm0, %ymm7
	.p2align	4, 0x90
.LBB0_101:                              # %"for head2_conv.s1.r40$x"
                                        #   Parent Loop BB0_88 Depth=1
                                        #     Parent Loop BB0_100 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	(%rsi), %ymm11
	vbroadcastss	-320(%rdx,%rax,4), %ymm10
	vfmadd213ps	%ymm7, %ymm11, %ymm10   # ymm10 = (ymm11 * ymm10) + ymm7
	vbroadcastss	-164(%rdx,%rax,4), %ymm9
	vfmadd213ps	%ymm6, %ymm11, %ymm9    # ymm9 = (ymm11 * ymm9) + ymm6
	vbroadcastss	-8(%rdx,%rax,4), %ymm8
	vfmadd213ps	%ymm4, %ymm11, %ymm8    # ymm8 = (ymm11 * ymm8) + ymm4
	vbroadcastss	148(%rdx,%rax,4), %ymm5
	vfmadd213ps	%ymm3, %ymm11, %ymm5    # ymm5 = (ymm11 * ymm5) + ymm3
	testq	%rax, %rax
	je	.LBB0_103
# %bb.102:                              # %"for head2_conv.s1.r40$x.11382"
                                        #   in Loop: Header=BB0_101 Depth=3
	vmovups	(%rsi,%rbx,4), %ymm11
	vbroadcastss	-316(%rdx,%rax,4), %ymm7
	vfmadd213ps	%ymm10, %ymm11, %ymm7   # ymm7 = (ymm11 * ymm7) + ymm10
	vbroadcastss	-160(%rdx,%rax,4), %ymm6
	vfmadd213ps	%ymm9, %ymm11, %ymm6    # ymm6 = (ymm11 * ymm6) + ymm9
	vbroadcastss	-4(%rdx,%rax,4), %ymm4
	vfmadd213ps	%ymm8, %ymm11, %ymm4    # ymm4 = (ymm11 * ymm4) + ymm8
	vbroadcastss	152(%rdx,%rax,4), %ymm3
	vfmadd213ps	%ymm5, %ymm11, %ymm3    # ymm3 = (ymm11 * ymm3) + ymm5
	addq	$2, %rax
	addq	%r12, %rsi
	jmp	.LBB0_101
	.p2align	4, 0x90
.LBB0_103:                              # %"for head2_relu.s0.w.v15.preheader"
                                        #   in Loop: Header=BB0_100 Depth=2
	vmaxps	%ymm12, %ymm10, %ymm3
	leaq	(%r10,%r10,2), %rax
	leaq	(%r8,%rax,8), %rax
	vmovaps	%ymm3, (%rcx,%rax,4)
	vmaxps	%ymm12, %ymm9, %ymm3
	movq	%r10, %rax
	orq	$1, %rax
	leaq	(%rax,%rax,2), %rax
	leaq	(%r8,%rax,8), %rax
	movq	%rax, %r15
	vmovaps	%ymm3, (%rcx,%rax,4)
	vmaxps	%ymm12, %ymm8, %ymm3
	movq	%r10, %rdi
	orq	$2, %rdi
	leaq	(%rdi,%rdi,2), %rax
	leaq	(%r8,%rax,8), %rax
	movq	%rax, 384(%rsp)                 # 8-byte Spill
	vmovaps	%ymm3, (%rcx,%rax,4)
	vmaxps	%ymm12, %ymm5, %ymm3
	movq	%r10, %r9
	orq	$3, %r9
	leaq	(%r9,%r9,2), %rax
	movq	%r8, %r11
	leaq	(%r8,%rax,8), %rax
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	vmovaps	%ymm3, (%rcx,%rax,4)
	movq	$-38, %rax
	movq	16(%rsp), %r8                   # 8-byte Reload
	vmovaps	%ymm1, %ymm4
	vmovaps	%ymm1, %ymm5
	vmovaps	%ymm1, %ymm6
	vmovaps	%ymm1, %ymm7
	.p2align	4, 0x90
.LBB0_104:                              # %"for head2_conv.s1.r40$x.1"
                                        #   Parent Loop BB0_88 Depth=1
                                        #     Parent Loop BB0_100 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	(%r8), %ymm11
	vbroadcastss	-320(%rdx,%rax,4), %ymm10
	vfmadd213ps	%ymm7, %ymm11, %ymm10   # ymm10 = (ymm11 * ymm10) + ymm7
	vbroadcastss	-164(%rdx,%rax,4), %ymm9
	vfmadd213ps	%ymm6, %ymm11, %ymm9    # ymm9 = (ymm11 * ymm9) + ymm6
	vbroadcastss	-8(%rdx,%rax,4), %ymm8
	vfmadd213ps	%ymm5, %ymm11, %ymm8    # ymm8 = (ymm11 * ymm8) + ymm5
	vbroadcastss	148(%rdx,%rax,4), %ymm3
	vfmadd213ps	%ymm4, %ymm11, %ymm3    # ymm3 = (ymm11 * ymm3) + ymm4
	testq	%rax, %rax
	je	.LBB0_106
# %bb.105:                              # %"for head2_conv.s1.r40$x.1.1"
                                        #   in Loop: Header=BB0_104 Depth=3
	movq	464(%rsp), %rbx                 # 8-byte Reload
	vmovups	(%r8,%rbx,4), %ymm11
	vbroadcastss	-316(%rdx,%rax,4), %ymm7
	vfmadd213ps	%ymm10, %ymm11, %ymm7   # ymm7 = (ymm11 * ymm7) + ymm10
	vbroadcastss	-160(%rdx,%rax,4), %ymm6
	vfmadd213ps	%ymm9, %ymm11, %ymm6    # ymm6 = (ymm11 * ymm6) + ymm9
	vbroadcastss	-4(%rdx,%rax,4), %ymm5
	vfmadd213ps	%ymm8, %ymm11, %ymm5    # ymm5 = (ymm11 * ymm5) + ymm8
	vbroadcastss	152(%rdx,%rax,4), %ymm4
	vfmadd213ps	%ymm3, %ymm11, %ymm4    # ymm4 = (ymm11 * ymm4) + ymm3
	addq	$2, %rax
	addq	%r12, %r8
	jmp	.LBB0_104
	.p2align	4, 0x90
.LBB0_106:                              # %"for head2_relu.s0.w.v15.preheader.1"
                                        #   in Loop: Header=BB0_100 Depth=2
	vmaxps	%ymm12, %ymm10, %ymm4
	shlq	$3, %r10
	leaq	(%r10,%r10,2), %rsi
	movq	%rsi, %rax
	orq	$8, %rax
	movq	%r11, %r8
	addq	%r11, %rax
	movq	128(%rsp), %rcx                 # 8-byte Reload
	vmovaps	%ymm4, (%rcx,%rax,4)
	vmaxps	%ymm12, %ymm9, %ymm4
	vmovaps	%ymm4, 32(%rcx,%r15,4)
	vmaxps	%ymm12, %ymm8, %ymm4
	shlq	$3, %rdi
	leaq	(%rdi,%rdi,2), %rax
	orq	$8, %rax
	addq	%r11, %rax
	vmovaps	%ymm4, (%rcx,%rax,4)
	vmaxps	%ymm12, %ymm3, %ymm3
	movq	288(%rsp), %rax                 # 8-byte Reload
	vmovaps	%ymm3, 32(%rcx,%rax,4)
	movq	$-38, %rax
	movq	576(%rsp), %rdi                 # 8-byte Reload
	vmovaps	%ymm2, %ymm3
	vmovaps	%ymm2, %ymm4
	vmovaps	%ymm2, %ymm5
	vmovaps	%ymm2, %ymm6
	movq	464(%rsp), %rbx                 # 8-byte Reload
	.p2align	4, 0x90
.LBB0_107:                              # %"for head2_conv.s1.r40$x.2"
                                        #   Parent Loop BB0_88 Depth=1
                                        #     Parent Loop BB0_100 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vmovups	(%rdi), %ymm7
	vbroadcastss	-320(%rdx,%rax,4), %ymm11
	vfmadd213ps	%ymm6, %ymm7, %ymm11    # ymm11 = (ymm7 * ymm11) + ymm6
	vbroadcastss	-164(%rdx,%rax,4), %ymm10
	vfmadd213ps	%ymm5, %ymm7, %ymm10    # ymm10 = (ymm7 * ymm10) + ymm5
	vbroadcastss	-8(%rdx,%rax,4), %ymm9
	vfmadd213ps	%ymm4, %ymm7, %ymm9     # ymm9 = (ymm7 * ymm9) + ymm4
	vbroadcastss	148(%rdx,%rax,4), %ymm8
	vfmadd213ps	%ymm3, %ymm7, %ymm8     # ymm8 = (ymm7 * ymm8) + ymm3
	testq	%rax, %rax
	je	.LBB0_99
# %bb.108:                              # %"for head2_conv.s1.r40$x.2.1"
                                        #   in Loop: Header=BB0_107 Depth=3
	vmovups	(%rdi,%rbx,4), %ymm7
	vbroadcastss	-316(%rdx,%rax,4), %ymm6
	vfmadd213ps	%ymm11, %ymm7, %ymm6    # ymm6 = (ymm7 * ymm6) + ymm11
	vbroadcastss	-160(%rdx,%rax,4), %ymm5
	vfmadd213ps	%ymm10, %ymm7, %ymm5    # ymm5 = (ymm7 * ymm5) + ymm10
	vbroadcastss	-4(%rdx,%rax,4), %ymm4
	vfmadd213ps	%ymm9, %ymm7, %ymm4     # ymm4 = (ymm7 * ymm4) + ymm9
	vbroadcastss	152(%rdx,%rax,4), %ymm3
	vfmadd213ps	%ymm8, %ymm7, %ymm3     # ymm3 = (ymm7 * ymm3) + ymm8
	addq	$2, %rax
	addq	%r12, %rdi
	jmp	.LBB0_107
.LBB0_109:                              # %"end for head2_relu.s0.n.loopexit"
	vmovaps	480(%rsp), %ymm0                # 32-byte Reload
	vmovaps	%ymm0, 3424(%rsp)
	vmovaps	288(%rsp), %ymm0                # 32-byte Reload
	vmovaps	%ymm0, 3456(%rsp)
	vmovaps	384(%rsp), %ymm0                # 32-byte Reload
	vmovaps	%ymm0, 3488(%rsp)
	vmovaps	320(%rsp), %ymm0                # 32-byte Reload
	vmovaps	%ymm0, 3520(%rsp)
	movq	1432(%rsp), %r12                # 8-byte Reload
	movl	1400(%rsp), %r15d               # 4-byte Reload
	movq	432(%rsp), %r13                 # 8-byte Reload
.LBB0_110:                              # %"end for head2_relu.s0.n"
	movq	1080(%rsp), %rax                # 8-byte Reload
	movl	%eax, %ebx
	sarl	$31, %ebx
	andl	%eax, %ebx
	movq	1072(%rsp), %rax                # 8-byte Reload
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	movl	1744(%rsp), %eax                # 4-byte Reload
	cmpl	%ebx, %eax
	cmovlel	%eax, %ebx
	leal	-1(%r12), %eax
	movq	1064(%rsp), %rcx                # 8-byte Reload
	cmpl	%r12d, %ecx
	cmovgel	%ecx, %eax
	subl	%ebx, %eax
	cmpl	$-2, %eax
	movl	$-1, %ecx
	cmovgl	%eax, %ecx
	incl	%ecx
	movq	1040(%rsp), %rdi                # 8-byte Reload
	movq	%rdi, %rsi
	imulq	%rcx, %rsi
	movl	$2147483648, %edx               # imm = 0x80000000
	cmpq	%rdx, %rsi
	jae	.LBB0_211
# %bb.111:                              # %"end for head2_relu.s0.n"
	movq	736(%rsp), %rdx                 # 8-byte Reload
	shrq	$25, %rdx
	andl	$-512, %edi                     # imm = 0xFE00
	imulq	%rcx, %rdi
	shrq	$32, %rdi
	imulq	%rcx, %rdx
	addq	%rdi, %rdx
	movabsq	$1095216660480, %rcx            # imm = 0xFF00000000
	andq	%rdx, %rcx
	jne	.LBB0_211
# %bb.112:                              # %"assert succeeded207"
	orq	$4, %rsi
	xorl	%edi, %edi
	vzeroupper
	callq	halide_malloc@PLT
	testq	%rax, %rax
	je	.LBB0_160
# %bb.113:                              # %"assert succeeded209"
	movq	%rax, 464(%rsp)                 # 8-byte Spill
	shll	$7, %r13d
	movl	%ebx, 80(%rsp)                  # 4-byte Spill
	movslq	%ebx, %rax
	movq	%rax, 280(%rsp)                 # 8-byte Spill
	cmpl	%r15d, %r12d
	jle	.LBB0_124
# %bb.114:                              # %"for relu1.s0.n.preheader"
	movslq	%r15d, %rax
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	movq	%r13, 432(%rsp)                 # 8-byte Spill
	leal	128(%r13), %eax
	movslq	768(%rsp), %rcx                 # 4-byte Folded Reload
	cltq
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movslq	%r12d, %rax
	movq	%rax, 576(%rsp)                 # 8-byte Spill
	vmovaps	3424(%rsp), %ymm1
	vmovaps	3456(%rsp), %ymm2
	vmovaps	3488(%rsp), %ymm3
	vmovaps	3520(%rsp), %ymm4
	leaq	(%r14,%r14,8), %rax
	movq	2640(%rsp), %rdx                # 8-byte Reload
	leaq	(%rdx,%rax,4), %rax
	movq	%rax, 384(%rsp)                 # 8-byte Spill
	shlq	$3, %r14
	movq	128(%rsp), %rax                 # 8-byte Reload
	addq	$292, %rax                      # imm = 0x124
	movq	%rax, 512(%rsp)                 # 8-byte Spill
	leaq	384(,%rcx,4), %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	addq	%rdx, 2336(%rsp)                # 8-byte Folded Spill
	vxorps	%xmm0, %xmm0, %xmm0
	movq	192(%rsp), %rbx                 # 8-byte Reload
	.p2align	4, 0x90
.LBB0_115:                              # %"for relu1.s0.n"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_117 Depth 2
                                        #       Child Loop BB0_118 Depth 3
                                        #         Child Loop BB0_119 Depth 4
	cmpl	$0, 72(%rsp)                    # 4-byte Folded Reload
	jle	.LBB0_122
# %bb.116:                              # %"for relu1.s0.w.w.preheader"
                                        #   in Loop: Header=BB0_115 Depth=1
	movq	288(%rsp), %rax                 # 8-byte Reload
	subq	280(%rsp), %rax                 # 8-byte Folded Reload
	imulq	16(%rsp), %rax                  # 8-byte Folded Reload
	movq	512(%rsp), %rdi                 # 8-byte Reload
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB0_117:                              # %"for relu1.s0.w.w"
                                        #   Parent Loop BB0_115 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB0_118 Depth 3
                                        #         Child Loop BB0_119 Depth 4
	movq	%rcx, %r15
	shlq	$4, %r15
	movq	%rcx, 320(%rsp)                 # 8-byte Spill
	leal	(,%rcx,4), %ecx
	leaq	(,%rcx,4), %r13
	leaq	4(,%rcx,4), %r8
	leaq	8(,%rcx,4), %r9
	leaq	12(,%rcx,4), %r10
	movq	2336(%rsp), %r11                # 8-byte Reload
	movq	384(%rsp), %rsi                 # 8-byte Reload
	xorl	%ecx, %ecx
	.p2align	4, 0x90
.LBB0_118:                              # %"for relu1.s0.c.c"
                                        #   Parent Loop BB0_115 Depth=1
                                        #     Parent Loop BB0_117 Depth=2
                                        # =>    This Loop Header: Depth=3
                                        #         Child Loop BB0_119 Depth 4
	leaq	(%rcx,%r15), %rdx
	shlq	$5, %rdx
	vmovaps	(%rbx,%rdx), %ymm1
	vmovaps	128(%rbx,%rdx), %ymm2
	vmovaps	256(%rbx,%rdx), %ymm3
	vmovaps	384(%rbx,%rdx), %ymm4
	movq	$-96, %rdx
	movl	$0, %r12d
	.p2align	4, 0x90
.LBB0_119:                              # %"for conv1_stage2.s1.r63$x"
                                        #   Parent Loop BB0_115 Depth=1
                                        #     Parent Loop BB0_117 Depth=2
                                        #       Parent Loop BB0_118 Depth=3
                                        # =>      This Inner Loop Header: Depth=4
	vmovups	(%r11,%r12), %ymm5
	vbroadcastss	-196(%rdi,%rdx), %ymm6
	vfmadd213ps	%ymm1, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm1
	vbroadcastss	-100(%rdi,%rdx), %ymm7
	vfmadd213ps	%ymm2, %ymm5, %ymm7     # ymm7 = (ymm5 * ymm7) + ymm2
	vbroadcastss	-4(%rdi,%rdx), %ymm8
	vfmadd213ps	%ymm3, %ymm5, %ymm8     # ymm8 = (ymm5 * ymm8) + ymm3
	vbroadcastss	92(%rdi,%rdx), %ymm9
	vfmadd213ps	%ymm4, %ymm5, %ymm9     # ymm9 = (ymm5 * ymm9) + ymm4
	vmovups	(%rsi,%r12), %ymm5
	vbroadcastss	-192(%rdi,%rdx), %ymm1
	vfmadd213ps	%ymm6, %ymm5, %ymm1     # ymm1 = (ymm5 * ymm1) + ymm6
	vbroadcastss	-96(%rdi,%rdx), %ymm2
	vfmadd213ps	%ymm7, %ymm5, %ymm2     # ymm2 = (ymm5 * ymm2) + ymm7
	vbroadcastss	(%rdi,%rdx), %ymm3
	vfmadd213ps	%ymm8, %ymm5, %ymm3     # ymm3 = (ymm5 * ymm3) + ymm8
	vbroadcastss	96(%rdi,%rdx), %ymm4
	vfmadd213ps	%ymm9, %ymm5, %ymm4     # ymm4 = (ymm5 * ymm4) + ymm9
	addq	%r14, %r12
	addq	$8, %rdx
	jne	.LBB0_119
# %bb.120:                              # %"for relu1.s0.w.v18.preheader"
                                        #   in Loop: Header=BB0_118 Depth=3
	vmaxps	%ymm0, %ymm1, %ymm5
	leaq	(%rcx,%r13), %rdx
	leaq	(%rax,%rdx,8), %rdx
	movq	464(%rsp), %rbx                 # 8-byte Reload
	vmovaps	%ymm5, (%rbx,%rdx,4)
	vmaxps	%ymm0, %ymm2, %ymm5
	leaq	(%r8,%rcx), %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovaps	%ymm5, (%rbx,%rdx,4)
	vmaxps	%ymm0, %ymm3, %ymm5
	leaq	(%r9,%rcx), %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovaps	%ymm5, (%rbx,%rdx,4)
	vmaxps	%ymm0, %ymm4, %ymm5
	leaq	(%r10,%rcx), %rdx
	leaq	(%rax,%rdx,8), %rdx
	vmovaps	%ymm5, (%rbx,%rdx,4)
	incq	%rcx
	addq	$32, %rsi
	addq	$32, %r11
	cmpq	$4, %rcx
	movq	192(%rsp), %rbx                 # 8-byte Reload
	jne	.LBB0_118
# %bb.121:                              # %"end for relu1.s0.c.c"
                                        #   in Loop: Header=BB0_117 Depth=2
	movq	320(%rsp), %rcx                 # 8-byte Reload
	incq	%rcx
	addq	$384, %rdi                      # imm = 0x180
	cmpq	224(%rsp), %rcx                 # 8-byte Folded Reload
	jne	.LBB0_117
.LBB0_122:                              # %"end for relu1.s0.w.w"
                                        #   in Loop: Header=BB0_115 Depth=1
	movq	288(%rsp), %rax                 # 8-byte Reload
	incq	%rax
	movq	32(%rsp), %rcx                  # 8-byte Reload
	addq	%rcx, 512(%rsp)                 # 8-byte Folded Spill
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	cmpq	576(%rsp), %rax                 # 8-byte Folded Reload
	jne	.LBB0_115
# %bb.123:                              # %call_destructor.exit385.loopexit
	vmovaps	%ymm1, 3424(%rsp)
	vmovaps	%ymm2, 3456(%rsp)
	vmovaps	%ymm3, 3488(%rsp)
	vmovaps	%ymm4, 3520(%rsp)
	movq	432(%rsp), %r13                 # 8-byte Reload
.LBB0_124:                              # %call_destructor.exit385
	xorl	%r12d, %r12d
	xorl	%edi, %edi
	movq	192(%rsp), %rsi                 # 8-byte Reload
	vzeroupper
	callq	halide_free@PLT
	xorl	%edi, %edi
	movq	128(%rsp), %rsi                 # 8-byte Reload
	callq	halide_free@PLT
	cmpl	$0, 744(%rsp)                   # 4-byte Folded Reload
	movq	1088(%rsp), %rsi                # 8-byte Reload
	movl	728(%rsp), %eax                 # 4-byte Reload
	movq	464(%rsp), %r15                 # 8-byte Reload
	js	.LBB0_212
# %bb.125:                              # %call_destructor.exit385
	cmpl	%esi, %eax
	jg	.LBB0_212
# %bb.126:                              # %"consume relu1"
	movslq	%r13d, %rax
	movq	%rax, 1008(%rsp)                # 8-byte Spill
	movq	472(%rsp), %rdx                 # 8-byte Reload
	movslq	%edx, %rax
	movq	%rax, 1000(%rsp)                # 8-byte Spill
	cmpl	$7, %esi
	movq	440(%rsp), %rcx                 # 8-byte Reload
	movq	1096(%rsp), %rdi                # 8-byte Reload
	movq	80(%rbp), %rbx
	jg	.LBB0_129
# %bb.127:                              # %true_bb212
	testl	%esi, %esi
	jle	.LBB0_130
# %bb.128:                              # %"for prediction_output.s0.n.v13.preheader"
	movq	632(%rsp), %rsi                 # 8-byte Reload
	movl	%esi, %r8d
	shll	$5, %r8d
	leal	(%r8,%rsi,2), %r12d
	movq	472(%rsp), %rdx                 # 8-byte Reload
	subl	%edx, %r12d
	leal	(%rsi,%r8), %eax
	subl	%edx, %eax
	movl	%eax, 128(%rsp)                 # 4-byte Spill
	movl	%r8d, %eax
	subl	%edx, %eax
	movl	%eax, 192(%rsp)                 # 4-byte Spill
	subl	%esi, %r8d
	movl	%r8d, %eax
	subl	%edx, %eax
	movl	%eax, 224(%rsp)                 # 4-byte Spill
	subl	%esi, %r8d
	subl	%edx, %r8d
	leal	(%rsi,%rsi,8), %ebx
	leal	(%rbx,%rbx,2), %eax
	leal	(%rax,%rsi), %ecx
	leal	(%rsi,%rcx), %edi
	subl	%edx, %edi
	movl	%edi, 320(%rsp)                 # 4-byte Spill
	subl	%edx, %ecx
	movq	%rcx, 384(%rsp)                 # 8-byte Spill
	subl	%edx, %eax
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	leal	(%rsi,%rsi,4), %eax
	leal	(%rax,%rax,4), %ecx
	leal	(%rcx,%rsi), %edi
	subl	%edx, %edi
	movl	%edi, 512(%rsp)                 # 4-byte Spill
	subl	%edx, %ecx
	movq	%rcx, 16(%rsp)                  # 8-byte Spill
	leal	(,%rsi,8), %ecx
	leal	(%rcx,%rcx,2), %ecx
	subl	%edx, %ecx
	movl	%ecx, 576(%rsp)                 # 4-byte Spill
	leal	(%rsi,%rax,4), %ecx
	leal	(%rsi,%rcx), %edi
	subl	%edx, %edi
	movl	%edi, 32(%rsp)                  # 4-byte Spill
	subl	%edx, %ecx
	movq	%rcx, 80(%rsp)                  # 8-byte Spill
	leal	(%rsi,%rbx,2), %ecx
	subl	%edx, %ecx
	movl	%ecx, 160(%rsp)                 # 4-byte Spill
	leal	(%rsi,%rsi), %r11d
	leal	(%r11,%r11,8), %ecx
	subl	%edx, %ecx
	movl	%ecx, 112(%rsp)                 # 4-byte Spill
	movl	%esi, %r9d
	shll	$4, %r9d
	addl	%esi, %r9d
	subl	%edx, %r9d
	leal	(,%rsi,4), %edi
	leal	(%rdi,%rdi,2), %ecx
	subl	%edx, %ecx
	movl	%ecx, 640(%rsp)                 # 4-byte Spill
	leal	(%rsi,%rax,2), %eax
	subl	%edx, %eax
	movl	%eax, 48(%rsp)                  # 4-byte Spill
	leal	(%r11,%r11,4), %eax
	subl	%edx, %eax
	movl	%eax, 480(%rsp)                 # 4-byte Spill
	subl	%edx, %ebx
	movl	%esi, %r11d
	subl	%edx, %r11d
	cmpl	$31, 72(%rsp)                   # 4-byte Folded Reload
	seta	%r13b
	cmpl	$1, 184(%rsp)                   # 4-byte Folded Reload
	sete	%cl
	movq	1000(%rsp), %r10                # 8-byte Reload
	leaq	(,%r10,4), %rdx
	movl	$96, %edi
	subq	%rdx, %rdi
	movq	%rdi, 456(%rsp)                 # 8-byte Spill
	movq	1008(%rsp), %r14                # 8-byte Reload
	leaq	128(%r14), %rax
	movq	280(%rsp), %rdx                 # 8-byte Reload
	movq	%rax, 1952(%rsp)                # 8-byte Spill
	imulq	%rax, %rdx
	shlq	$2, %rdx
	movq	%r15, %rax
	subq	%rdx, %rax
	movq	%rax, 696(%rsp)                 # 8-byte Spill
	andb	%r13b, %cl
	movb	%cl, 764(%rsp)                  # 1-byte Spill
	vcvtsi2ssl	760(%rsp), %xmm0, %xmm0 # 4-byte Folded Reload
	movslq	%r12d, %rax
	movq	%rax, 1568(%rsp)                # 8-byte Spill
	movslq	128(%rsp), %r13                 # 4-byte Folded Reload
	movq	%r13, 448(%rsp)                 # 8-byte Spill
	movslq	192(%rsp), %r12                 # 4-byte Folded Reload
	movq	%r12, 560(%rsp)                 # 8-byte Spill
	movslq	224(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 1504(%rsp)                # 8-byte Spill
	movslq	%r8d, %r8
	movq	%r8, 816(%rsp)                  # 8-byte Spill
	movslq	320(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 1632(%rsp)                # 8-byte Spill
	movslq	384(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 1216(%rsp)                # 8-byte Spill
	movslq	288(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 1472(%rsp)                # 8-byte Spill
	movslq	512(%rsp), %rdi                 # 4-byte Folded Reload
	movslq	16(%rsp), %rax                  # 4-byte Folded Reload
	movq	%rax, 1112(%rsp)                # 8-byte Spill
	movslq	576(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 960(%rsp)                 # 8-byte Spill
	movslq	32(%rsp), %rax                  # 4-byte Folded Reload
	movq	%rax, 1104(%rsp)                # 8-byte Spill
	movslq	80(%rsp), %rsi                  # 4-byte Folded Reload
	movslq	160(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 1440(%rsp)                # 8-byte Spill
	movslq	112(%rsp), %rax                 # 4-byte Folded Reload
	movslq	%r9d, %rcx
	movq	%rcx, 800(%rsp)                 # 8-byte Spill
	movq	%rax, %r9
	movq	%rax, 1824(%rsp)                # 8-byte Spill
	movslq	640(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 1856(%rsp)                # 8-byte Spill
	movslq	48(%rsp), %rcx                  # 4-byte Folded Reload
	movslq	480(%rsp), %rdx                 # 4-byte Folded Reload
	movslq	%ebx, %rax
	movq	%rax, 2528(%rsp)                # 8-byte Spill
	movslq	%r11d, %rax
	movq	%rax, 1424(%rsp)                # 8-byte Spill
	movl	1088(%rsp), %eax                # 4-byte Reload
	movq	%rax, 1968(%rsp)                # 8-byte Spill
	movq	104(%rsp), %rax                 # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-32, %eax
	movq	%rax, 1032(%rsp)                # 8-byte Spill
	vmovaps	%xmm0, 2640(%rsp)               # 16-byte Spill
	vbroadcastss	%xmm0, %ymm0
	vmovaps	%ymm0, 2688(%rsp)               # 32-byte Spill
	movq	176(%rsp), %rax                 # 8-byte Reload
	movq	%rax, %rbx
	shlq	$7, %rbx
	movq	%rbx, 1416(%rsp)                # 8-byte Spill
	movq	%r10, %rbx
	movq	%rcx, %r10
	movq	%rcx, 2176(%rsp)                # 8-byte Spill
	movq	%rdi, %rcx
	movq	%rdx, 704(%rsp)                 # 8-byte Spill
	movq	%rdi, 608(%rsp)                 # 8-byte Spill
	negq	%rbx
	movq	%rbx, 688(%rsp)                 # 8-byte Spill
	vmovss	.LCPI0_48(%rip), %xmm6          # xmm6 = mem[0],zero,zero,zero
	leaq	(,%rax,4), %rax
	movq	%rax, 1432(%rsp)                # 8-byte Spill
	leaq	512(,%r14,4), %rdi
	movq	%rdi, 1960(%rsp)                # 8-byte Spill
	movq	%r9, 680(%rsp)                  # 8-byte Spill
	movq	1440(%rsp), %rax                # 8-byte Reload
	movq	%rax, 1096(%rsp)                # 8-byte Spill
	movq	%rcx, 1088(%rsp)                # 8-byte Spill
	movq	1216(%rsp), %rax                # 8-byte Reload
	movq	%rax, 432(%rsp)                 # 8-byte Spill
	movq	1472(%rsp), %rax                # 8-byte Reload
	movq	%rax, 1080(%rsp)                # 8-byte Spill
	movq	%r13, 1072(%rsp)                # 8-byte Spill
	movq	1568(%rsp), %rdi                # 8-byte Reload
	movq	%rdi, 744(%rsp)                 # 8-byte Spill
	movq	%r10, 1400(%rsp)                # 8-byte Spill
	movq	1632(%rsp), %rax                # 8-byte Reload
	movq	%rax, 1064(%rsp)                # 8-byte Spill
	movq	1856(%rsp), %rax                # 8-byte Reload
	movq	%rax, 1056(%rsp)                # 8-byte Spill
	movq	800(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 1048(%rsp)                # 8-byte Spill
	movq	%rsi, 672(%rsp)                 # 8-byte Spill
	movq	1424(%rsp), %rdi                # 8-byte Reload
	movq	%rdi, 1744(%rsp)                # 8-byte Spill
	movq	1504(%rsp), %rax                # 8-byte Reload
	movq	%rax, 736(%rsp)                 # 8-byte Spill
	movq	%r12, 1736(%rsp)                # 8-byte Spill
	movq	%r8, 728(%rsp)                  # 8-byte Spill
	movq	%rdx, 1040(%rsp)                # 8-byte Spill
	movq	2528(%rsp), %rdx                # 8-byte Reload
	movq	%rdx, 1728(%rsp)                # 8-byte Spill
	movq	1112(%rsp), %rax                # 8-byte Reload
	movq	%rax, 1720(%rsp)                # 8-byte Spill
	movq	1104(%rsp), %rax                # 8-byte Reload
	movq	%rax, 1712(%rsp)                # 8-byte Spill
	xorl	%ecx, %ecx
	movq	960(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 1704(%rsp)                # 8-byte Spill
	movq	440(%rsp), %r12                 # 8-byte Reload
	movq	%rsi, 1776(%rsp)                # 8-byte Spill
	jmp	.LBB0_198
.LBB0_129:                              # %false_bb213
	movl	%esi, 3344(%rsp)
	movq	72(%rsp), %rax                  # 8-byte Reload
	movl	%eax, 3348(%rsp)
	movl	80(%rsp), %eax                  # 4-byte Reload
	movl	%eax, 3352(%rsp)
	movq	632(%rsp), %rax                 # 8-byte Reload
	movl	%eax, 3356(%rsp)
	movl	184(%rsp), %eax                 # 4-byte Reload
	movl	%eax, 3360(%rsp)
	movl	%edx, 3364(%rsp)
	movl	%r13d, 3368(%rsp)
	vcvtsi2ssl	760(%rsp), %xmm13, %xmm0 # 4-byte Folded Reload
	vmovss	%xmm0, 3372(%rsp)
	movq	1976(%rsp), %rax                # 8-byte Reload
	movq	%rax, 3376(%rsp)
	movq	%rbx, 3384(%rsp)
	movq	%r15, 3392(%rsp)
	movq	$0, 3400(%rsp)
	movq	%rcx, 3408(%rsp)
	movq	%rdi, 3416(%rsp)
	leal	7(%rsi), %ecx
	sarl	$3, %ecx
	leaq	cost_model.par_for.prediction_output.s0.n.v13(%rip), %rsi
	leaq	3344(%rsp), %r8
	xorl	%edi, %edi
	xorl	%edx, %edx
	callq	halide_do_par_for@PLT
	testl	%eax, %eax
	jne	.LBB0_226
.LBB0_130:                              # %"produce loss_output"
	movq	2656(%rsp), %rax                # 8-byte Reload
	movl	$0, (%rax)
	movq	632(%rsp), %rcx                 # 8-byte Reload
	leal	(%rcx,%rcx,4), %eax
	leal	(%rax,%rax,4), %eax
	movl	%eax, 2816(%rsp)                # 4-byte Spill
	leal	(,%rcx,8), %eax
	leal	(%rax,%rax,2), %eax
	movl	%eax, 2784(%rsp)                # 4-byte Spill
	leal	(%rcx,%rcx), %eax
	leal	(%rax,%rax,4), %edx
	movl	%edx, 2368(%rsp)                # 4-byte Spill
	leal	(%rcx,%rcx,8), %ecx
	movl	%ecx, 2752(%rsp)                # 4-byte Spill
	cmpl	$0, 1016(%rsp)                  # 4-byte Folded Reload
	jle	.LBB0_144
# %bb.131:                              # %"for load_cost_output.s0.n.preheader"
	leal	(%rax,%rax,8), %r14d
	movq	632(%rsp), %rdi                 # 8-byte Reload
	leal	(%rdi,%rdi,8), %eax
	leal	(%rdi,%rax,2), %r12d
	leal	(%rdi,%rdi,4), %ecx
	leal	(%rcx,%rcx,4), %r9d
	addl	%edi, %r9d
	leal	(%rax,%rax,2), %r11d
	leal	(%r11,%rdi), %r13d
	movl	%edi, %edx
	shll	$5, %edx
	movl	%edx, %ebx
	subl	%edi, %ebx
	leal	(%rdi,%rdx), %eax
	leal	(%rdx,%rdi,2), %ecx
	movq	472(%rsp), %r8                  # 8-byte Reload
	subl	%r8d, %r14d
	subl	%r8d, %r12d
	subl	%r8d, %r9d
	subl	%r8d, %r13d
	subl	%r8d, %r11d
	subl	%r8d, %eax
	movl	%eax, 192(%rsp)                 # 4-byte Spill
	subl	%r8d, %ecx
	movl	%ecx, 512(%rsp)                 # 4-byte Spill
	subl	%r8d, %ebx
	movl	2816(%rsp), %eax                # 4-byte Reload
	subl	%r8d, %eax
	movl	%eax, 384(%rsp)                 # 4-byte Spill
	subl	%r8d, %edx
	movl	2784(%rsp), %eax                # 4-byte Reload
	subl	%r8d, %eax
	movl	%eax, 128(%rsp)                 # 4-byte Spill
	movl	2368(%rsp), %r10d               # 4-byte Reload
	subl	%r8d, %r10d
	movl	2752(%rsp), %eax                # 4-byte Reload
	subl	%r8d, %eax
	movl	%eax, 288(%rsp)                 # 4-byte Spill
	cmpl	$31, 72(%rsp)                   # 4-byte Folded Reload
	seta	224(%rsp)                       # 1-byte Folded Spill
	cmpl	$1, 184(%rsp)                   # 4-byte Folded Reload
	sete	%dil
	movq	1752(%rsp), %rcx                # 8-byte Reload
	leaq	(,%rcx,4), %rax
	movq	1000(%rsp), %rsi                # 8-byte Reload
	leaq	(,%rsi,4), %r8
	subq	%r8, %rax
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	andb	224(%rsp), %dil                 # 1-byte Folded Reload
	movb	%dil, 960(%rsp)                 # 1-byte Spill
	movslq	%r14d, %rax
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	movslq	%r12d, %r14
	movslq	%r9d, %r12
	movslq	%r13d, %r8
	movslq	%r11d, %rax
	movq	%rax, 640(%rsp)                 # 8-byte Spill
	movslq	192(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 896(%rsp)                 # 8-byte Spill
	movslq	512(%rsp), %rdi                 # 4-byte Folded Reload
	movslq	%ebx, %rsi
	movslq	384(%rsp), %r13                 # 4-byte Folded Reload
	movslq	%edx, %r11
	movslq	128(%rsp), %rdx                 # 4-byte Folded Reload
	movq	%rdx, 112(%rsp)                 # 8-byte Spill
	movslq	%r10d, %r10
	movq	%rdi, %rdx
	movslq	288(%rsp), %r9                  # 4-byte Folded Reload
	movq	224(%rsp), %rbx                 # 8-byte Reload
	movq	104(%rsp), %rax                 # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-32, %eax
	movq	%rax, 800(%rsp)                 # 8-byte Spill
	addq	%rcx, %rbx
	movq	%rbx, 224(%rsp)                 # 8-byte Spill
	movq	176(%rsp), %rax                 # 8-byte Reload
	shlq	$7, %rax
	movq	%rax, 816(%rsp)                 # 8-byte Spill
	addq	%rcx, %r14
	movq	%r14, 480(%rsp)                 # 8-byte Spill
	addq	%rcx, %r12
	movq	%r12, 288(%rsp)                 # 8-byte Spill
	addq	%rcx, %r8
	movq	%r8, 384(%rsp)                  # 8-byte Spill
	movq	640(%rsp), %r8                  # 8-byte Reload
	addq	%rcx, %r8
	addq	%rcx, 896(%rsp)                 # 8-byte Folded Spill
	addq	%rcx, %rdx
	addq	$96, 320(%rsp)                  # 8-byte Folded Spill
	addq	%rcx, %rsi
	movq	%rsi, 48(%rsp)                  # 8-byte Spill
	addq	%rcx, %r13
	addq	%rcx, %r11
	addq	%rcx, 112(%rsp)                 # 8-byte Folded Spill
	addq	%rcx, %r10
	addq	%rcx, %r9
	movq	%rcx, %rax
	subq	1000(%rsp), %rax                # 8-byte Folded Reload
	movq	%rax, 2496(%rsp)                # 8-byte Spill
	movq	%rcx, %rax
	subq	280(%rsp), %rax                 # 8-byte Folded Reload
	movq	1008(%rsp), %rdi                # 8-byte Reload
	leaq	128(%rdi), %rsi
	movq	%rsi, 2560(%rsp)                # 8-byte Spill
	imulq	%rsi, %rax
	leaq	(%r15,%rax,4), %rax
	addq	$32, %rax
	movq	%rax, 2464(%rsp)                # 8-byte Spill
	movq	%r11, 512(%rsp)                 # 8-byte Spill
	movq	%r13, %rax
	movq	%r13, 256(%rsp)                 # 8-byte Spill
	vbroadcastsd	.LCPI0_69(%rip), %ymm0  # ymm0 = [32,32,32,32]
	vmovaps	%ymm0, 2176(%rsp)               # 32-byte Spill
	vmovss	.LCPI0_48(%rip), %xmm12         # xmm12 = mem[0],zero,zero,zero
	movq	176(%rsp), %rsi                 # 8-byte Reload
	leaq	(,%rsi,4), %rsi
	movq	%rsi, 1600(%rsp)                # 8-byte Spill
	movq	%r8, %r13
	movq	%r8, 640(%rsp)                  # 8-byte Spill
	leaq	512(,%rdi,4), %rsi
	movq	%rsi, 2592(%rsp)                # 8-byte Spill
	movq	112(%rsp), %r8                  # 8-byte Reload
	movq	%r9, 1280(%rsp)                 # 8-byte Spill
	movq	%r9, 2432(%rsp)                 # 8-byte Spill
	movq	%r10, 928(%rsp)                 # 8-byte Spill
	movq	%r10, 1888(%rsp)                # 8-byte Spill
	movq	%r8, %rsi
	movq	%r11, 2400(%rsp)                # 8-byte Spill
	movq	%rax, %rdi
	movq	48(%rsp), %rax                  # 8-byte Reload
	movq	%rax, 2144(%rsp)                # 8-byte Spill
	movq	%rdx, 2112(%rsp)                # 8-byte Spill
	movq	896(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 2272(%rsp)                # 8-byte Spill
	movq	%r13, 2240(%rsp)                # 8-byte Spill
	movq	384(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 2080(%rsp)                # 8-byte Spill
	movq	%r12, 2048(%rsp)                # 8-byte Spill
	movq	%r14, 2208(%rsp)                # 8-byte Spill
	movq	%rbx, 976(%rsp)                 # 8-byte Spill
	movq	440(%rsp), %r12                 # 8-byte Reload
	movq	%rcx, %rbx
	movq	%rdx, 448(%rsp)                 # 8-byte Spill
	movq	%r8, 112(%rsp)                  # 8-byte Spill
	.p2align	4, 0x90
.LBB0_132:                              # %"for load_cost_output.s0.n"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_135 Depth 2
                                        #     Child Loop BB0_139 Depth 2
	cmpl	$0, 72(%rsp)                    # 4-byte Folded Reload
	movq	%rsi, 704(%rsp)                 # 8-byte Spill
	movq	%rdi, 1792(%rsp)                # 8-byte Spill
	movq	%r12, 1920(%rsp)                # 8-byte Spill
	movq	%rbx, 2304(%rsp)                # 8-byte Spill
	jle	.LBB0_142
# %bb.133:                              # %"for f1.s1.r77$x.preheader"
                                        #   in Loop: Header=BB0_132 Depth=1
	cmpb	$0, 960(%rsp)                   # 1-byte Folded Reload
	je	.LBB0_137
# %bb.134:                              # %vector.ph1033
                                        #   in Loop: Header=BB0_132 Depth=1
	movq	%rbx, %rax
	subq	280(%rsp), %rax                 # 8-byte Folded Reload
	imulq	2560(%rsp), %rax                # 8-byte Folded Reload
	vmovq	%rax, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 1568(%rsp)               # 32-byte Spill
	leaq	256(%rax), %rcx
	vmovq	%rcx, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 1632(%rsp)               # 32-byte Spill
	leaq	512(%rax), %rcx
	vmovq	%rcx, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 1856(%rsp)               # 32-byte Spill
	addq	$768, %rax                      # imm = 0x300
	vmovq	%rax, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 2528(%rsp)               # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 1248(%rsp)               # 32-byte Spill
	movq	800(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 456(%rsp)                 # 8-byte Spill
	vmovdqa	.LCPI0_62(%rip), %ymm1          # ymm1 = [0,1,2,3]
	vmovdqa	.LCPI0_61(%rip), %ymm2          # ymm2 = [4,5,6,7]
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 864(%rsp)                # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 1184(%rsp)               # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 832(%rsp)                # 32-byte Spill
	.p2align	4, 0x90
.LBB0_135:                              # %vector.body1028
                                        #   Parent Loop BB0_132 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovdqa	%ymm1, 1120(%rsp)               # 32-byte Spill
	vpsllq	$5, %ymm1, %ymm1
	vmovdqa	%ymm1, 1504(%rsp)               # 32-byte Spill
	vmovdqa	%ymm2, 1312(%rsp)               # 32-byte Spill
	vpsllq	$5, %ymm2, %ymm15
	vmovdqa	1568(%rsp), %ymm0               # 32-byte Reload
	vpaddq	%ymm0, %ymm15, %ymm2
	movq	112(%rsp), %rax                 # 8-byte Reload
	vmovups	(%r12,%rax,4), %ymm14
	vmovups	32(%r12,%rax,4), %ymm13
	vpaddq	%ymm0, %ymm1, %ymm5
	movq	256(%rsp), %rax                 # 8-byte Reload
	vmovups	(%r12,%rax,4), %ymm11
	vmovups	32(%r12,%rax,4), %ymm0
	vmovaps	%ymm0, 1472(%rsp)               # 32-byte Spill
	movq	48(%rsp), %rax                  # 8-byte Reload
	vmovups	(%r12,%rax,4), %ymm0
	vmovaps	%ymm0, 1440(%rsp)               # 32-byte Spill
	vmovdqa	1632(%rsp), %ymm0               # 32-byte Reload
	vpaddq	%ymm0, %ymm15, %ymm8
	vmovdqa	%ymm15, 1824(%rsp)              # 32-byte Spill
	vmovups	32(%r12,%rax,4), %ymm3
	vmovaps	%ymm3, 1216(%rsp)               # 32-byte Spill
	movq	320(%rsp), %rax                 # 8-byte Reload
	vmovups	-96(%r12,%rax), %ymm10
	vmovups	-64(%r12,%rax), %ymm12
	movq	%r12, %rbx
	vpaddq	%ymm1, %ymm0, %ymm6
	vmovq	%xmm5, %rcx
	vpextrq	$1, %xmm5, %r11
	vextracti128	$1, %ymm5, %xmm5
	vmovq	%xmm2, %r12
	vpextrq	$1, %xmm2, %rdi
	vmovq	%xmm5, %r10
	vextracti128	$1, %ymm2, %xmm2
	vmovq	%xmm6, 16(%rsp)                 # 8-byte Folded Spill
	vpextrq	$1, %xmm5, %rdx
	vpextrq	$1, %xmm6, 128(%rsp)            # 8-byte Folded Spill
	vextracti128	$1, %ymm6, %xmm3
	vmovq	%xmm2, %r14
	vpextrq	$1, %xmm8, 192(%rsp)            # 8-byte Folded Spill
	vpextrq	$1, %xmm2, %r8
	vmovss	40(%r15,%r12,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vmovss	20(%r15,%rcx,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmovss	24(%r15,%rcx,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	%rcx, %r13
	vmovss	20(%r15,%r12,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 20(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 20(%r15,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$16, 20(%r15,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 20(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 20(%r15,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 20(%r15,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm5, %ymm5
	vmovss	24(%r15,%r12,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%rdi,4), %xmm7, %xmm2 # xmm2 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$16, 24(%r15,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%rdi, %r9
	vinsertps	$32, 24(%r15,%r14,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$16, 24(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 24(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 24(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 24(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm6, %ymm4
	movq	224(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%rbx,%rcx,4), %ymm10, %ymm6
	vmulps	%ymm6, %ymm4, %ymm7
	movq	%r13, %rsi
	vmovss	40(%r15,%r13,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	480(%rsp), %r13                 # 8-byte Reload
	vmulps	(%rbx,%r13,4), %ymm10, %ymm6
	vfmadd231ps	%ymm6, %ymm5, %ymm7     # ymm7 = (ymm5 * ymm6) + ymm7
	vmovss	28(%r15,%rsi,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	%r12, %rdi
	vmovss	28(%r15,%r12,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$16, 28(%r15,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	%r14, 768(%rsp)                 # 8-byte Spill
	vinsertps	$32, 28(%r15,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$16, 28(%r15,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 28(%r15,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	movq	%r10, 1152(%rsp)                # 8-byte Spill
	vinsertps	$32, 28(%r15,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 28(%r15,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	movq	288(%rsp), %rax                 # 8-byte Reload
	vmulps	(%rbx,%rax,4), %ymm14, %ymm6
	vfmadd231ps	%ymm6, %ymm5, %ymm7     # ymm7 = (ymm5 * ymm6) + ymm7
	vmovss	32(%r15,%r12,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmovss	32(%r15,%rsi,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	%rsi, %rax
	movq	%rsi, 1664(%rsp)                # 8-byte Spill
	vinsertps	$16, 32(%r15,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	%r9, %rcx
	movq	%r9, 576(%rsp)                  # 8-byte Spill
	vinsertps	$16, 32(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	%r11, 368(%rsp)                 # 8-byte Spill
	vinsertps	$32, 32(%r15,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$32, 32(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 32(%r15,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovq	%xmm8, %r9
	vinsertps	$48, 32(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	movq	384(%rsp), %rsi                 # 8-byte Reload
	vmulps	(%rbx,%rsi,4), %ymm11, %ymm6
	movq	%rbx, %r12
	vinsertps	$32, 40(%r15,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vfmadd231ps	%ymm6, %ymm5, %ymm7     # ymm7 = (ymm5 * ymm6) + ymm7
	vmovss	36(%r15,%rdi,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$32, 40(%r15,%r10,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$16, 36(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 36(%r15,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vextracti128	$1, %ymm8, %xmm1
	vmovss	36(%r15,%rax,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	%r8, 1536(%rsp)                 # 8-byte Spill
	vinsertps	$48, 40(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 36(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	%rdx, 1344(%rsp)                # 8-byte Spill
	vinsertps	$48, 40(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 36(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 36(%r15,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovq	%xmm1, %rcx
	vinsertps	$48, 36(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	movq	640(%rsp), %r14                 # 8-byte Reload
	vmulps	(%rbx,%r14,4), %ymm14, %ymm6
	vfmadd231ps	%ymm6, %ymm5, %ymm7     # ymm7 = (ymm5 * ymm6) + ymm7
	vmovaps	1440(%rsp), %ymm9               # 32-byte Reload
	vmulps	%ymm9, %ymm11, %ymm5
	vmovss	28(%r15,%r9,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	movq	192(%rsp), %r8                  # 8-byte Reload
	vinsertps	$16, 28(%r15,%r8,4), %xmm6, %xmm0 # xmm0 = xmm6[0],mem[0],xmm6[2,3]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	movq	16(%rsp), %r11                  # 8-byte Reload
	vmovss	28(%r15,%r11,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	128(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, 28(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vfmadd231ps	%ymm5, %ymm2, %ymm7     # ymm7 = (ymm2 * ymm5) + ymm7
	vmovss	60(%r15,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovaps	%xmm2, 560(%rsp)                # 16-byte Spill
	vmovss	20(%r15,%r9,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$32, 28(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$16, 20(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrq	$1, %xmm1, %r10
	vmovss	20(%r15,%r11,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovq	%xmm3, %rax
	vinsertps	$32, 28(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$32, 20(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 28(%r15,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vpextrq	$1, %xmm3, %rsi
	vinsertps	$48, 28(%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$16, 20(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 20(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovss	24(%r15,%r11,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$32, 20(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vmulps	32(%rbx,%r13,4), %ymm12, %ymm8
	vinsertps	$48, 20(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vmovss	24(%r15,%r9,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	movq	%r8, 192(%rsp)                  # 8-byte Spill
	vinsertps	$16, 24(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%rdx, %r13
	vinsertps	$16, 24(%r15,%rdx,4), %xmm5, %xmm4 # xmm4 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 24(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 24(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 24(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 24(%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	movq	%rdi, %rdx
	vmovss	56(%r15,%rdi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmovaps	%xmm3, 608(%rsp)                # 16-byte Spill
	movq	224(%rsp), %rdi                 # 8-byte Reload
	vmulps	32(%rbx,%rdi,4), %ymm12, %ymm4
	vmulps	%ymm4, %ymm2, %ymm4
	vmovss	44(%r15,%rdx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vfmadd231ps	%ymm8, %ymm1, %ymm4     # ymm4 = (ymm1 * ymm8) + ymm4
	vmovaps	%ymm13, %ymm12
	movq	288(%rsp), %rdi                 # 8-byte Reload
	vmulps	32(%rbx,%rdi,4), %ymm13, %ymm1
	vmovss	32(%r15,%r9,4), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vfmadd231ps	%ymm1, %ymm0, %ymm4     # ymm4 = (ymm0 * ymm1) + ymm4
	vmovss	32(%r15,%r11,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r15,%r8,4), %xmm3, %xmm1 # xmm1 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$16, 32(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	%rcx, 32(%rsp)                  # 8-byte Spill
	vinsertps	$32, 32(%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%rax, 352(%rsp)                 # 8-byte Spill
	vinsertps	$32, 32(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%r10, 160(%rsp)                 # 8-byte Spill
	vinsertps	$48, 32(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovss	36(%r15,%r9,4), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vinsertps	$48, 32(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovaps	1472(%rsp), %ymm5               # 32-byte Reload
	movq	384(%rsp), %rbx                 # 8-byte Reload
	vmulps	32(%r12,%rbx,4), %ymm5, %ymm1
	movq	576(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, 44(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vfmadd231ps	%ymm1, %ymm0, %ymm4     # ymm4 = (ymm0 * ymm1) + ymm4
	vmovss	36(%r15,%r11,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r15,%r8,4), %xmm3, %xmm1 # xmm1 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$16, 36(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 36(%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 36(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 36(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovss	40(%r15,%r9,4), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vinsertps	$48, 36(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmulps	32(%r12,%r14,4), %ymm13, %ymm1
	vinsertps	$16, 40(%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vfmadd231ps	%ymm1, %ymm0, %ymm4     # ymm4 = (ymm0 * ymm1) + ymm4
	vmovss	40(%r15,%r11,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	%r11, %r8
	vinsertps	$32, 40(%r15,%rcx,4), %xmm3, %xmm1 # xmm1 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, 40(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	%r13, 128(%rsp)                 # 8-byte Spill
	vinsertps	$48, 40(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 40(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 40(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	%rsi, %r14
	movq	%rsi, 80(%rsp)                  # 8-byte Spill
	vmovss	52(%r15,%rdx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovaps	1216(%rsp), %ymm10              # 32-byte Reload
	vmulps	%ymm5, %ymm10, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm4     # ymm4 = (ymm0 * ymm1) + ymm4
	vmovss	48(%r15,%rdx,4), %xmm13         # xmm13 = mem[0],zero,zero,zero
	movq	1664(%rsp), %r11                # 8-byte Reload
	vmovss	44(%r15,%r11,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	768(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$32, 44(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	368(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 44(%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	1536(%rsp), %rax                # 8-byte Reload
	vinsertps	$48, 44(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	1152(%rsp), %rsi                # 8-byte Reload
	vinsertps	$32, 44(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	1344(%rsp), %rdx                # 8-byte Reload
	vinsertps	$48, 44(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vmovdqa	1856(%rsp), %ymm6               # 32-byte Reload
	vpaddq	%ymm6, %ymm15, %ymm2
	vmulps	%ymm9, %ymm14, %ymm8
	vfmadd231ps	%ymm8, %ymm1, %ymm7     # ymm7 = (ymm1 * ymm8) + ymm7
	movq	%r9, %r10
	vmovss	44(%r15,%r9,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movq	%r8, 16(%rsp)                   # 8-byte Spill
	vmovss	44(%r15,%r8,4), %xmm0           # xmm0 = mem[0],zero,zero,zero
	movq	192(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 44(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$16, 44(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	32(%rsp), %r13                  # 8-byte Reload
	vinsertps	$32, 44(%r15,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	352(%rsp), %r9                  # 8-byte Reload
	vinsertps	$32, 44(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	160(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 44(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 44(%r15,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovdqa	1504(%rsp), %ymm15              # 32-byte Reload
	vpaddq	%ymm6, %ymm15, %ymm1
	movq	576(%rsp), %r14                 # 8-byte Reload
	vinsertps	$16, 52(%r15,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmulps	%ymm10, %ymm12, %ymm8
	vfmadd231ps	%ymm8, %ymm0, %ymm4     # ymm4 = (ymm0 * ymm8) + ymm4
	movq	512(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%r12,%rdi,4), %ymm0
	vmovss	52(%r15,%r11,4), %xmm8          # xmm8 = mem[0],zero,zero,zero
	vinsertps	$32, 52(%r15,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, 52(%r15,%rcx,4), %xmm8, %xmm8 # xmm8 = xmm8[0],mem[0],xmm8[2,3]
	vinsertps	$48, 52(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 52(%r15,%rsi,4), %xmm8, %xmm6 # xmm6 = xmm8[0,1],mem[0],xmm8[3]
	vinsertps	$48, 52(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm6, %ymm8
	vmovss	48(%r15,%r11,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	%r11, %rdi
	vinsertps	$16, 48(%r15,%r14,4), %xmm13, %xmm3 # xmm3 = xmm13[0],mem[0],xmm13[2,3]
	vinsertps	$16, 48(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 48(%r15,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 48(%r15,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 48(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 48(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	vextracti128	$1, %ymm1, %xmm6
	vmulps	%ymm0, %ymm14, %ymm10
	vmulps	%ymm0, %ymm11, %ymm11
	vextracti128	$1, %ymm2, %xmm0
	vfmadd231ps	%ymm11, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm11) + ymm7
	vmovss	48(%r15,%r10,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	%r10, %rsi
	vmovss	48(%r15,%r8,4), %xmm11          # xmm11 = mem[0],zero,zero,zero
	movq	192(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 48(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	128(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 48(%r15,%rcx,4), %xmm11, %xmm11 # xmm11 = xmm11[0],mem[0],xmm11[2,3]
	vpextrq	$1, %xmm2, %r11
	vmovq	%xmm2, %r14
	vmovss	20(%r15,%r14,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$32, 48(%r15,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, 20(%r15,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 48(%r15,%r9,4), %xmm11, %xmm11 # xmm11 = xmm11[0,1],mem[0],xmm11[3]
	vmovq	%xmm0, %r8
	vinsertps	$32, 20(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vfmadd231ps	%ymm10, %ymm8, %ymm7    # ymm7 = (ymm8 * ymm10) + ymm7
	vmovss	60(%r15,%rdi,4), %xmm14         # xmm14 = mem[0],zero,zero,zero
	vmovq	%xmm1, %rdx
	vpextrq	$1, %xmm1, %rbx
	vmovss	20(%r15,%rdx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 20(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovq	%xmm6, %r13
	vinsertps	$32, 20(%r15,%r13,4), %xmm1, %xmm8 # xmm8 = xmm1[0,1],mem[0],xmm1[3]
	vpextrq	$1, %xmm0, %r10
	vmovss	24(%r15,%rdx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	24(%r15,%r14,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	160(%rsp), %r9                  # 8-byte Reload
	vinsertps	$48, 48(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$16, 24(%r15,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	80(%rsp), %rax                  # 8-byte Reload
	vinsertps	$48, 48(%r15,%rax,4), %xmm11, %xmm10 # xmm10 = xmm11[0,1,2],mem[0]
	vinsertps	$32, 24(%r15,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 20(%r15,%r10,4), %xmm2, %xmm13 # xmm13 = xmm2[0,1,2],mem[0]
	vpextrq	$1, %xmm6, %rcx
	vinsertps	$48, 20(%r15,%rcx,4), %xmm8, %xmm6 # xmm6 = xmm8[0,1,2],mem[0]
	vinsertps	$16, 24(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 24(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovss	56(%r15,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$32, 24(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertf128	$1, %xmm3, %ymm10, %ymm3
	movq	512(%rsp), %rax                 # 8-byte Reload
	vmovups	32(%r12,%rax,4), %ymm11
	vinsertps	$48, 24(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm13, %ymm6, %ymm10
	movq	320(%rsp), %rdi                 # 8-byte Reload
	vmovups	-32(%r12,%rdi), %ymm8
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	32(%r15,%r14,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vmulps	%ymm5, %ymm11, %ymm1
	vfmadd231ps	%ymm1, %ymm3, %ymm4     # ymm4 = (ymm3 * ymm1) + ymm4
	movq	224(%rsp), %rdi                 # 8-byte Reload
	vmulps	64(%r12,%rdi,4), %ymm8, %ymm1
	vmovss	28(%r15,%rdx,4), %xmm13         # xmm13 = mem[0],zero,zero,zero
	vmulps	%ymm1, %ymm0, %ymm1
	vmovss	32(%r15,%rdx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	480(%rsp), %rdi                 # 8-byte Reload
	vmulps	64(%r12,%rdi,4), %ymm8, %ymm8
	vinsertps	$16, 32(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vfmadd231ps	%ymm8, %ymm10, %ymm1    # ymm1 = (ymm10 * ymm8) + ymm1
	vmovss	28(%r15,%r14,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$32, 32(%r15,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$16, 28(%r15,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 32(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 28(%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 32(%r15,%r10,4), %xmm6, %xmm10 # xmm10 = xmm6[0,1,2],mem[0]
	vinsertps	$16, 28(%r15,%rbx,4), %xmm13, %xmm6 # xmm6 = xmm13[0],mem[0],xmm13[2,3]
	vinsertps	$48, 32(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 28(%r15,%r13,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 28(%r15,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	112(%rsp), %rdi                 # 8-byte Reload
	vmovups	64(%r12,%rdi,4), %ymm8
	vinsertps	$48, 28(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm10, %ymm0, %ymm0
	movq	256(%rsp), %rdi                 # 8-byte Reload
	vmovups	64(%r12,%rdi,4), %ymm10
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	movq	288(%rsp), %rdi                 # 8-byte Reload
	vmulps	64(%r12,%rdi,4), %ymm8, %ymm6
	vfmadd231ps	%ymm6, %ymm3, %ymm1     # ymm1 = (ymm3 * ymm6) + ymm1
	movq	384(%rsp), %rdi                 # 8-byte Reload
	vmulps	64(%r12,%rdi,4), %ymm10, %ymm3
	vmovss	36(%r15,%r14,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vfmadd231ps	%ymm3, %ymm0, %ymm1     # ymm1 = (ymm0 * ymm3) + ymm1
	vmovss	36(%r15,%rdx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r15,%r11,4), %xmm6, %xmm3 # xmm3 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$16, 36(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 36(%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 36(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 36(%r15,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovss	52(%r15,%rsi,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$48, 36(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm0, %ymm0
	movq	640(%rsp), %rdi                 # 8-byte Reload
	vmulps	64(%r12,%rdi,4), %ymm8, %ymm3
	movq	192(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 52(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vfmadd231ps	%ymm3, %ymm0, %ymm1     # ymm1 = (ymm0 * ymm3) + ymm1
	movq	16(%rsp), %rax                  # 8-byte Reload
	vmovss	52(%r15,%rax,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	32(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$32, 52(%r15,%rdi,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1],mem[0],xmm6[3]
	movq	128(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 52(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 52(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	48(%rsp), %rdi                  # 8-byte Reload
	vmovups	64(%r12,%rdi,4), %ymm6
	movq	352(%rsp), %r9                  # 8-byte Reload
	vinsertps	$32, 52(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmulps	%ymm11, %ymm12, %ymm11
	vmovss	60(%r15,%rsi,4), %xmm12         # xmm12 = mem[0],zero,zero,zero
	movq	%rsi, %rdi
	movq	80(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$48, 52(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm0, %ymm0
	vmovss	44(%r15,%r14,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r15,%r11,4), %xmm3, %xmm13 # xmm13 = xmm3[0],mem[0],xmm3[2,3]
	vfmadd231ps	%ymm11, %ymm0, %ymm4    # ymm4 = (ymm0 * ymm11) + ymm4
	vmovss	40(%r15,%r14,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	40(%r15,%rdx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$16, 40(%r15,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 40(%r15,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 40(%r15,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 40(%r15,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	%rcx, %rsi
	vinsertps	$48, 40(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmulps	%ymm6, %ymm10, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm1     # ymm1 = (ymm0 * ymm3) + ymm1
	vmovss	44(%r15,%rdx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$32, 44(%r15,%r8,4), %xmm13, %xmm3 # xmm3 = xmm13[0,1],mem[0],xmm13[3]
	vinsertps	$16, 44(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 44(%r15,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 44(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 44(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovss	56(%r15,%rdi,4), %xmm11         # xmm11 = mem[0],zero,zero,zero
	vinsertf128	$1, %xmm3, %ymm0, %ymm0
	vmovss	48(%r15,%r14,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmulps	%ymm6, %ymm8, %ymm6
	vfmadd231ps	%ymm6, %ymm0, %ymm1     # ymm1 = (ymm0 * ymm6) + ymm1
	vmovss	48(%r15,%rdx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r15,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$16, 48(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 48(%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 48(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 48(%r15,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	512(%rsp), %rcx                 # 8-byte Reload
	vmovups	64(%r12,%rcx,4), %ymm13
	vinsertps	$48, 48(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	%rsi, 1216(%rsp)                # 8-byte Spill
	vinsertf128	$1, %xmm3, %ymm0, %ymm0
	vmovss	60(%r15,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmulps	%ymm10, %ymm13, %ymm10
	vfmadd231ps	%ymm10, %ymm0, %ymm1    # ymm1 = (ymm0 * ymm10) + ymm1
	vmovss	56(%r15,%rax,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	576(%rsp), %rcx                 # 8-byte Reload
	vmovaps	560(%rsp), %xmm5                # 16-byte Reload
	vinsertps	$16, 60(%r15,%rcx,4), %xmm5, %xmm6 # xmm6 = xmm5[0],mem[0],xmm5[2,3]
	vmovaps	608(%rsp), %xmm5                # 16-byte Reload
	vinsertps	$16, 56(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	368(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 60(%r15,%rcx,4), %xmm14, %xmm9 # xmm9 = xmm14[0],mem[0],xmm14[2,3]
	vinsertps	$16, 56(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	192(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 60(%r15,%rax,4), %xmm12, %xmm10 # xmm10 = xmm12[0],mem[0],xmm12[2,3]
	vinsertps	$16, 56(%r15,%rax,4), %xmm11, %xmm11 # xmm11 = xmm11[0],mem[0],xmm11[2,3]
	movq	128(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 60(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$16, 56(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	768(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$32, 60(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$32, 56(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	1152(%rsp), %rcx                # 8-byte Reload
	vinsertps	$32, 60(%r15,%rcx,4), %xmm9, %xmm9 # xmm9 = xmm9[0,1],mem[0],xmm9[3]
	vinsertps	$32, 56(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	32(%rsp), %rax                  # 8-byte Reload
	vinsertps	$32, 60(%r15,%rax,4), %xmm10, %xmm12 # xmm12 = xmm10[0,1],mem[0],xmm10[3]
	vinsertps	$32, 56(%r15,%rax,4), %xmm11, %xmm10 # xmm10 = xmm11[0,1],mem[0],xmm11[3]
	vinsertps	$32, 60(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 56(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	1536(%rsp), %rax                # 8-byte Reload
	vinsertps	$48, 60(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmovaps	%xmm6, 576(%rsp)                # 16-byte Spill
	vinsertps	$48, 56(%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	1344(%rsp), %rax                # 8-byte Reload
	vinsertps	$48, 60(%r15,%rax,4), %xmm9, %xmm6 # xmm6 = xmm9[0,1,2],mem[0]
	vmovaps	%ymm6, 128(%rsp)                # 32-byte Spill
	vinsertps	$48, 56(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	1280(%rsp), %rax                # 8-byte Reload
	vmovups	(%r12,%rax,4), %ymm6
	movq	%rax, %rdi
	movq	928(%rsp), %rax                 # 8-byte Reload
	vmulps	(%r12,%rax,4), %ymm6, %ymm6
	movq	%rax, %rcx
	vmovaps	2720(%rsp), %ymm14              # 32-byte Reload
	vmaxps	%ymm14, %ymm6, %ymm11
	vinsertf128	$1, %xmm5, %ymm2, %ymm2
	movq	896(%rsp), %rax                 # 8-byte Reload
	vmulps	(%r12,%rax,4), %ymm11, %ymm5
	vfmadd231ps	%ymm5, %ymm2, %ymm7     # ymm7 = (ymm2 * ymm5) + ymm7
	vmovss	52(%r15,%r14,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovss	52(%r15,%rdx,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r15,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$16, 52(%r15,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 52(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 52(%r15,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 52(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 52(%r15,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm5, %ymm2
	vmovss	60(%r15,%r14,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmulps	%ymm13, %ymm8, %ymm6
	vmovss	56(%r15,%r14,4), %xmm8          # xmm8 = mem[0],zero,zero,zero
	vmovdqa	2528(%rsp), %ymm9               # 32-byte Reload
	vpaddq	1824(%rsp), %ymm9, %ymm13       # 32-byte Folded Reload
	vpaddq	%ymm15, %ymm9, %ymm15
	vfmadd231ps	%ymm6, %ymm2, %ymm1     # ymm1 = (ymm2 * ymm6) + ymm1
	vmovups	32(%r12,%rdi,4), %ymm2
	vmulps	32(%r12,%rcx,4), %ymm2, %ymm6
	vinsertps	$16, 60(%r15,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	160(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 60(%r15,%rax,4), %xmm12, %xmm2 # xmm2 = xmm12[0,1,2],mem[0]
	vmovaps	%xmm2, 16(%rsp)                 # 16-byte Spill
	vinsertps	$16, 56(%r15,%r11,4), %xmm8, %xmm8 # xmm8 = xmm8[0],mem[0],xmm8[2,3]
	vinsertps	$48, 56(%r15,%rax,4), %xmm10, %xmm12 # xmm12 = xmm10[0,1,2],mem[0]
	vmovq	%xmm15, %r14
	vpextrq	$1, %xmm15, %r11
	vextracti128	$1, %ymm15, %xmm15
	movq	80(%rsp), %rax                  # 8-byte Reload
	vinsertps	$48, 60(%r15,%rax,4), %xmm3, %xmm10 # xmm10 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 56(%r15,%rax,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1,2],mem[0]
	vmovq	%xmm15, %r9
	vmovss	60(%r15,%rdx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$32, 60(%r15,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$32, 56(%r15,%r8,4), %xmm8, %xmm8 # xmm8 = xmm8[0,1],mem[0],xmm8[3]
	movq	320(%rsp), %rsi                 # 8-byte Reload
	vpextrq	$1, %xmm15, %r8
	vmovss	56(%r15,%rdx,4), %xmm15         # xmm15 = mem[0],zero,zero,zero
	vmovq	%xmm13, %rax
	vinsertps	$16, 60(%r15,%rbx,4), %xmm3, %xmm0 # xmm0 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$16, 56(%r15,%rbx,4), %xmm15, %xmm15 # xmm15 = xmm15[0],mem[0],xmm15[2,3]
	vpextrq	$1, %xmm13, %rcx
	vinsertps	$48, 60(%r15,%r10,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vmovaps	%xmm3, 192(%rsp)                # 16-byte Spill
	vinsertps	$48, 56(%r15,%r10,4), %xmm8, %xmm8 # xmm8 = xmm8[0,1,2],mem[0]
	movq	%rsi, %rdx
	vextracti128	$1, %ymm13, %xmm5
	vmovq	%xmm5, %rbx
	vinsertps	$32, 60(%r15,%r13,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 56(%r15,%r13,4), %xmm15, %xmm0 # xmm0 = xmm15[0,1],mem[0],xmm15[3]
	vpextrq	$1, %xmm5, %rdi
	movq	1280(%rsp), %rsi                # 8-byte Reload
	vmovups	64(%r12,%rsi,4), %ymm5
	movq	928(%rsp), %rsi                 # 8-byte Reload
	vmulps	64(%r12,%rsi,4), %ymm5, %ymm15
	vmaxps	%ymm14, %ymm6, %ymm9
	vinsertf128	$1, %xmm12, %ymm2, %ymm2
	movq	896(%rsp), %r13                 # 8-byte Reload
	vmulps	32(%r12,%r13,4), %ymm9, %ymm12
	vfmadd231ps	%ymm12, %ymm2, %ymm4    # ymm4 = (ymm2 * ymm12) + ymm4
	vmovss	20(%r15,%r14,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovss	20(%r15,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 20(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$16, 20(%r15,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 20(%r15,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 20(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	1216(%rsp), %r10                # 8-byte Reload
	vinsertps	$48, 56(%r15,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm8, %ymm0, %ymm0
	vmovss	24(%r15,%rax,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$48, 20(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$16, 24(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 20(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 24(%r15,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 24(%r15,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovss	24(%r15,%r14,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmaxps	%ymm14, %ymm15, %ymm15
	vinsertps	$32, 24(%r15,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmulps	64(%r12,%r13,4), %ymm15, %ymm8
	vfmadd231ps	%ymm8, %ymm0, %ymm1     # ymm1 = (ymm0 * ymm8) + ymm1
	vinsertps	$48, 24(%r15,%r8,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	movq	%rdx, 320(%rsp)                 # 8-byte Spill
	vmovups	(%r12,%rdx), %ymm3
	vinsertf128	$1, %xmm5, %ymm0, %ymm0
	movq	224(%rsp), %rdx                 # 8-byte Reload
	vmulps	96(%r12,%rdx,4), %ymm3, %ymm5
	vmulps	%ymm5, %ymm0, %ymm0
	movq	112(%rsp), %rdx                 # 8-byte Reload
	vmovups	96(%r12,%rdx,4), %ymm8
	movq	480(%rsp), %rdx                 # 8-byte Reload
	vmulps	96(%r12,%rdx,4), %ymm3, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	vmovss	36(%r15,%rax,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 36(%r15,%rbx,4), %xmm2, %xmm12 # xmm12 = xmm2[0,1],mem[0],xmm2[3]
	movq	448(%rsp), %rsi                 # 8-byte Reload
	vmulps	(%r12,%rsi,4), %ymm11, %ymm3
	vmovss	36(%r15,%r14,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r15,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovaps	128(%rsp), %ymm2                # 32-byte Reload
	vinsertf128	$1, 576(%rsp), %ymm2, %ymm6 # 16-byte Folded Reload
	vmovss	32(%r15,%rax,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vfmadd231ps	%ymm3, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm3) + ymm7
	vmovss	28(%r15,%r14,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmovss	28(%r15,%rax,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$32, 36(%r15,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$16, 28(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 32(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 28(%r15,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 36(%r15,%rdi,4), %xmm12, %xmm11 # xmm11 = xmm12[0,1,2],mem[0]
	vinsertps	$16, 28(%r15,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 28(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 28(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 28(%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm3, %ymm3
	vmovss	32(%r15,%r14,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$48, 36(%r15,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$16, 32(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 32(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 32(%r15,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	288(%rsp), %rdx                 # 8-byte Reload
	vmulps	96(%r12,%rdx,4), %ymm8, %ymm12
	vfmadd231ps	%ymm12, %ymm3, %ymm0    # ymm0 = (ymm3 * ymm12) + ymm0
	vinsertps	$48, 32(%r15,%r8,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm11, %ymm5, %ymm5
	movq	256(%rsp), %rdx                 # 8-byte Reload
	vmovups	96(%r12,%rdx,4), %ymm6
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movq	384(%rsp), %rdx                 # 8-byte Reload
	vmulps	96(%r12,%rdx,4), %ymm6, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	movq	640(%rsp), %rdx                 # 8-byte Reload
	vmulps	96(%r12,%rdx,4), %ymm8, %ymm2
	vmovss	48(%r15,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vfmadd231ps	%ymm2, %ymm5, %ymm0     # ymm0 = (ymm5 * ymm2) + ymm0
	vmovss	48(%r15,%r14,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r15,%rcx,4), %xmm3, %xmm14 # xmm14 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$16, 48(%r15,%r11,4), %xmm2, %xmm11 # xmm11 = xmm2[0],mem[0],xmm2[2,3]
	vmulps	32(%r12,%rsi,4), %ymm9, %ymm5
	vmovss	44(%r15,%rax,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r15,%rcx,4), %xmm2, %xmm12 # xmm12 = xmm2[0],mem[0],xmm2[2,3]
	vinsertf128	$1, 16(%rsp), %ymm10, %ymm9 # 16-byte Folded Reload
	vmovss	40(%r15,%rax,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vfmadd231ps	%ymm5, %ymm9, %ymm4     # ymm4 = (ymm9 * ymm5) + ymm4
	movq	48(%rsp), %rdx                  # 8-byte Reload
	vmovups	96(%r12,%rdx,4), %ymm5
	vmovss	40(%r15,%r14,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$32, 48(%r15,%rbx,4), %xmm14, %xmm9 # xmm9 = xmm14[0,1],mem[0],xmm14[3]
	vinsertps	$16, 40(%r15,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 40(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 40(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 40(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 40(%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmulps	%ymm5, %ymm6, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	vmovss	44(%r15,%r14,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$32, 48(%r15,%r9,4), %xmm11, %xmm10 # xmm10 = xmm11[0,1],mem[0],xmm11[3]
	vinsertps	$16, 44(%r15,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 44(%r15,%rbx,4), %xmm12, %xmm3 # xmm3 = xmm12[0,1],mem[0],xmm12[3]
	vinsertps	$32, 44(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 44(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 44(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 48(%r15,%rdi,4), %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vinsertps	$48, 48(%r15,%r8,4), %xmm10, %xmm3 # xmm3 = xmm10[0,1,2],mem[0]
	vmulps	%ymm5, %ymm8, %ymm5
	vfmadd231ps	%ymm5, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm5) + ymm0
	vinsertf128	$1, %xmm9, %ymm3, %ymm2
	movq	512(%rsp), %rdx                 # 8-byte Reload
	vmovups	96(%r12,%rdx,4), %ymm3
	vmulps	%ymm6, %ymm3, %ymm5
	vfmadd231ps	%ymm5, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm5) + ymm0
	vinsertps	$48, 60(%r15,%r10,4), %xmm13, %xmm2 # xmm2 = xmm13[0,1,2],mem[0]
	vinsertf128	$1, 192(%rsp), %ymm2, %ymm2 # 16-byte Folded Reload
	vmovss	60(%r15,%rax,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmulps	64(%r12,%rsi,4), %ymm15, %ymm6
	vfmadd231ps	%ymm6, %ymm2, %ymm1     # ymm1 = (ymm2 * ymm6) + ymm1
	vmovss	52(%r15,%rax,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vmovss	52(%r15,%r14,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$16, 52(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 52(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 52(%r15,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 52(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 52(%r15,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vmovss	56(%r15,%rax,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 60(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$16, 56(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmulps	%ymm3, %ymm8, %ymm3
	movq	1280(%rsp), %rax                # 8-byte Reload
	vmovups	96(%r12,%rax,4), %ymm8
	movq	928(%rsp), %rax                 # 8-byte Reload
	vmulps	96(%r12,%rax,4), %ymm8, %ymm8
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	vmovss	56(%r15,%r14,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$32, 56(%r15,%rbx,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$16, 56(%r15,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 56(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 56(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 56(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmaxps	2720(%rsp), %ymm8, %ymm6        # 32-byte Folded Reload
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vmulps	96(%r12,%r13,4), %ymm6, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	vmovss	60(%r15,%r14,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$32, 60(%r15,%rbx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$16, 60(%r15,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 60(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 60(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 60(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vmulps	96(%r12,%rsi,4), %ymm6, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	vmovaps	1248(%rsp), %ymm2               # 32-byte Reload
	vaddps	%ymm7, %ymm2, %ymm2
	vmovaps	%ymm2, 1248(%rsp)               # 32-byte Spill
	vmovaps	864(%rsp), %ymm2                # 32-byte Reload
	vaddps	%ymm4, %ymm2, %ymm2
	vmovaps	%ymm2, 864(%rsp)                # 32-byte Spill
	vmovaps	1184(%rsp), %ymm2               # 32-byte Reload
	vaddps	%ymm1, %ymm2, %ymm2
	vmovaps	%ymm2, 1184(%rsp)               # 32-byte Spill
	vmovaps	832(%rsp), %ymm1                # 32-byte Reload
	vaddps	%ymm0, %ymm1, %ymm1
	vmovaps	%ymm1, 832(%rsp)                # 32-byte Spill
	vmovdqa	2176(%rsp), %ymm0               # 32-byte Reload
	vmovdqa	1120(%rsp), %ymm1               # 32-byte Reload
	vpaddq	%ymm0, %ymm1, %ymm1
	vmovdqa	1312(%rsp), %ymm2               # 32-byte Reload
	vpaddq	%ymm0, %ymm2, %ymm2
	addq	816(%rsp), %r12                 # 8-byte Folded Reload
	addq	$-32, 456(%rsp)                 # 8-byte Folded Spill
	jne	.LBB0_135
# %bb.136:                              # %middle.block1026
                                        #   in Loop: Header=BB0_132 Depth=1
	vmovaps	864(%rsp), %ymm0                # 32-byte Reload
	vaddps	1248(%rsp), %ymm0, %ymm0        # 32-byte Folded Reload
	vaddps	1184(%rsp), %ymm0, %ymm0        # 32-byte Folded Reload
	vaddps	832(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm11
	movq	800(%rsp), %rcx                 # 8-byte Reload
	movq	%rcx, %rax
	cmpq	104(%rsp), %rcx                 # 8-byte Folded Reload
	vmovss	.LCPI0_48(%rip), %xmm12         # xmm12 = mem[0],zero,zero,zero
	jne	.LBB0_138
	jmp	.LBB0_140
	.p2align	4, 0x90
.LBB0_137:                              #   in Loop: Header=BB0_132 Depth=1
	vxorps	%xmm11, %xmm11, %xmm11
	xorl	%eax, %eax
.LBB0_138:                              # %"for f1.s1.r77$x.preheader1316"
                                        #   in Loop: Header=BB0_132 Depth=1
	movq	104(%rsp), %rcx                 # 8-byte Reload
	subq	%rax, %rcx
	movq	176(%rsp), %rdx                 # 8-byte Reload
	imulq	%rax, %rdx
	movq	976(%rsp), %rsi                 # 8-byte Reload
	addq	%rdx, %rsi
	movq	%rsi, 192(%rsp)                 # 8-byte Spill
	movq	2208(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 128(%rsp)                 # 8-byte Spill
	movq	2048(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 16(%rsp)                  # 8-byte Spill
	movq	2080(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 576(%rsp)                 # 8-byte Spill
	movq	2240(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	%rsi, 32(%rsp)                  # 8-byte Spill
	movq	2272(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %r12
	movq	2112(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %r13
	movq	2496(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %r14
	movq	2144(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %rbx
	movq	1792(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %rsi
	movq	2400(%rsp), %rdi                # 8-byte Reload
	leaq	(%rdi,%rdx), %r8
	movq	704(%rsp), %rdi                 # 8-byte Reload
	leaq	(%rdi,%rdx), %r9
	movq	1888(%rsp), %rdi                # 8-byte Reload
	leaq	(%rdi,%rdx), %r10
	addq	2432(%rsp), %rdx                # 8-byte Folded Reload
	shlq	$7, %rax
	addq	2464(%rsp), %rax                # 8-byte Folded Reload
	movq	440(%rsp), %r11                 # 8-byte Reload
	.p2align	4, 0x90
.LBB0_139:                              # %"for f1.s1.r77$x"
                                        #   Parent Loop BB0_132 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%r11,%rdx,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmulss	(%r11,%r10,4), %xmm1, %xmm1
	vmovss	(%r11,%r9,4), %xmm2             # xmm2 = mem[0],zero,zero,zero
	vmovss	(%r11,%r8,4), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vmovss	(%r11,%rsi,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmovss	(%r11,%rbx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vmovss	(%r11,%r14,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vmaxss	%xmm12, %xmm1, %xmm1
	vmulss	(%r11,%r13,4), %xmm1, %xmm8
	vmulss	(%r11,%r12,4), %xmm1, %xmm9
	vmulss	%xmm3, %xmm2, %xmm10
	vmulss	%xmm4, %xmm3, %xmm3
	vmulss	%xmm5, %xmm2, %xmm1
	vmulss	%xmm5, %xmm4, %xmm5
	movq	32(%rsp), %rdi                  # 8-byte Reload
	vmulss	(%r11,%rdi,4), %xmm2, %xmm7
	movq	576(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%r11,%rdi,4), %xmm4, %xmm4
	movq	16(%rsp), %rdi                  # 8-byte Reload
	vmulss	(%r11,%rdi,4), %xmm2, %xmm2
	movq	128(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%r11,%rdi,4), %xmm6, %xmm0
	movq	192(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%r11,%rdi,4), %xmm6, %xmm6
	vmulss	-8(%rax), %xmm6, %xmm6
	vfmadd231ss	-12(%rax), %xmm0, %xmm6 # xmm6 = (xmm0 * mem) + xmm6
	vfmadd231ss	-4(%rax), %xmm2, %xmm6  # xmm6 = (xmm2 * mem) + xmm6
	vfmadd231ss	(%rax), %xmm4, %xmm6    # xmm6 = (xmm4 * mem) + xmm6
	vfmadd231ss	4(%rax), %xmm7, %xmm6   # xmm6 = (xmm7 * mem) + xmm6
	vfmadd231ss	8(%rax), %xmm5, %xmm6   # xmm6 = (xmm5 * mem) + xmm6
	vfmadd231ss	12(%rax), %xmm1, %xmm6  # xmm6 = (xmm1 * mem) + xmm6
	vfmadd231ss	16(%rax), %xmm3, %xmm6  # xmm6 = (xmm3 * mem) + xmm6
	vfmadd231ss	20(%rax), %xmm10, %xmm6 # xmm6 = (xmm10 * mem) + xmm6
	vfmadd231ss	24(%rax), %xmm9, %xmm6  # xmm6 = (xmm9 * mem) + xmm6
	vfmadd231ss	28(%rax), %xmm8, %xmm6  # xmm6 = (xmm8 * mem) + xmm6
	vaddss	%xmm6, %xmm11, %xmm11
	addq	1600(%rsp), %r11                # 8-byte Folded Reload
	subq	$-128, %rax
	decq	%rcx
	jne	.LBB0_139
.LBB0_140:                              # %"consume f1.loopexit"
                                        #   in Loop: Header=BB0_132 Depth=1
	vmovd	%xmm11, %eax
.LBB0_141:                              # %"consume f1"
                                        #   in Loop: Header=BB0_132 Depth=1
	movq	2304(%rsp), %rbx                # 8-byte Reload
	movq	%rbx, %rcx
	subq	1752(%rsp), %rcx                # 8-byte Folded Reload
	movq	2664(%rsp), %rdx                # 8-byte Reload
	movl	%eax, (%rdx,%rcx,4)
	incq	%rbx
	movq	1920(%rsp), %r12                # 8-byte Reload
	addq	$4, %r12
	incq	976(%rsp)                       # 8-byte Folded Spill
	incq	2208(%rsp)                      # 8-byte Folded Spill
	incq	2048(%rsp)                      # 8-byte Folded Spill
	incq	2080(%rsp)                      # 8-byte Folded Spill
	incq	2240(%rsp)                      # 8-byte Folded Spill
	incq	2272(%rsp)                      # 8-byte Folded Spill
	incq	2112(%rsp)                      # 8-byte Folded Spill
	incq	2496(%rsp)                      # 8-byte Folded Spill
	incq	2144(%rsp)                      # 8-byte Folded Spill
	movq	1792(%rsp), %rdi                # 8-byte Reload
	incq	%rdi
	incq	2400(%rsp)                      # 8-byte Folded Spill
	movq	704(%rsp), %rsi                 # 8-byte Reload
	incq	%rsi
	incq	1888(%rsp)                      # 8-byte Folded Spill
	incq	2432(%rsp)                      # 8-byte Folded Spill
	movq	2592(%rsp), %rax                # 8-byte Reload
	addq	%rax, 2464(%rsp)                # 8-byte Folded Spill
	cmpl	%ebx, 2036(%rsp)                # 4-byte Folded Reload
	jne	.LBB0_132
	jmp	.LBB0_143
.LBB0_142:                              #   in Loop: Header=BB0_132 Depth=1
	vxorps	%xmm11, %xmm11, %xmm11
	xorl	%eax, %eax
	jmp	.LBB0_141
.LBB0_143:                              # %"consume relu1218.loopexit"
	vmovss	%xmm11, 3424(%rsp)
.LBB0_144:                              # %"consume relu1218"
	cmpl	$0, 1392(%rsp)                  # 4-byte Folded Reload
	jle	.LBB0_175
# %bb.145:                              # %"for store_cost_output.s0.n.preheader"
	movq	632(%rsp), %rdi                 # 8-byte Reload
	leal	(%rdi,%rdi,4), %eax
	leal	(%rdi,%rax,2), %r8d
	leal	(,%rdi,4), %ecx
	leal	(%rcx,%rcx,2), %r9d
	leal	(%rdi,%rdi,8), %edx
	leal	(%rdx,%rdx,2), %edx
	addl	%edi, %edx
	addl	%edi, %edx
	movl	%edi, %esi
	shll	$5, %esi
	subl	%edi, %esi
	subl	%edi, %esi
	movq	472(%rsp), %rdi                 # 8-byte Reload
	subl	%edi, %r8d
	subl	%edi, %edx
	movl	2816(%rsp), %r10d               # 4-byte Reload
	subl	%edi, %r10d
	movl	2784(%rsp), %ebx                # 4-byte Reload
	subl	%edi, %ebx
	subl	%edi, %r9d
	movl	2368(%rsp), %eax                # 4-byte Reload
	subl	%edi, %eax
	movl	2752(%rsp), %ecx                # 4-byte Reload
	subl	%edi, %ecx
	subl	%edi, %esi
	movslq	%r8d, %r8
	movslq	%edx, %r11
	movslq	%r10d, %r10
	movslq	%ebx, %rbx
	movslq	%r9d, %r9
	movslq	%eax, %rdi
	movslq	%ecx, %r14
	movslq	%esi, %r12
	cmpl	$31, 72(%rsp)                   # 4-byte Folded Reload
	seta	%al
	cmpl	$1, 184(%rsp)                   # 4-byte Folded Reload
	sete	%cl
	andb	%al, %cl
	movb	%cl, 3136(%rsp)                 # 1-byte Spill
	movq	104(%rsp), %rax                 # 8-byte Reload
                                        # kill: def $eax killed $eax killed $rax def $rax
	andl	$-32, %eax
	movq	%rax, 2912(%rsp)                # 8-byte Spill
	movq	1760(%rsp), %rdx                # 8-byte Reload
	addq	%rdx, %r8
	movq	%r8, 1120(%rsp)                 # 8-byte Spill
	leaq	(,%rdx,4), %rcx
	movq	1000(%rsp), %rsi                # 8-byte Reload
	leaq	(,%rsi,4), %rax
	subq	%rax, %rcx
	movq	%rdx, %rax
	subq	%rsi, %rax
	movq	%rax, 2080(%rsp)                # 8-byte Spill
	movq	176(%rsp), %r13                 # 8-byte Reload
	movq	%r13, %rax
	shlq	$7, %rax
	movq	%rax, 1888(%rsp)                # 8-byte Spill
	addq	%rdx, %r11
	movq	%r11, 1504(%rsp)                # 8-byte Spill
	addq	$96, %rcx
	addq	%rdx, %r10
	addq	%rdx, %rbx
	addq	%rdx, %r9
	addq	%rdx, %rdi
	addq	%rdx, %r14
	addq	%rdx, %r12
	movq	%rdx, %rax
	subq	280(%rsp), %rax                 # 8-byte Folded Reload
	movq	1008(%rsp), %r8                 # 8-byte Reload
	leaq	128(%r8), %rsi
	movq	%rsi, 3072(%rsp)                # 8-byte Spill
	imulq	%rsi, %rax
	leaq	(%r15,%rax,4), %rax
	addq	$64, %rax
	movq	%rax, 2048(%rsp)                # 8-byte Spill
	vbroadcastsd	.LCPI0_63(%rip), %ymm1  # ymm1 = [8,8,8,8]
	vbroadcastsd	.LCPI0_64(%rip), %ymm0  # ymm0 = [16,16,16,16]
	vmovaps	%ymm0, 2144(%rsp)               # 32-byte Spill
	vbroadcastsd	.LCPI0_65(%rip), %ymm8  # ymm8 = [24,24,24,24]
	vmovss	.LCPI0_48(%rip), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vmovss	.LCPI0_66(%rip), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vmovss	.LCPI0_67(%rip), %xmm5          # xmm5 = mem[0],zero,zero,zero
	leaq	(,%r13,4), %rax
	movq	%rax, 2496(%rsp)                # 8-byte Spill
	movq	%r14, 1440(%rsp)                # 8-byte Spill
	movq	%rbx, 112(%rsp)                 # 8-byte Spill
	movq	%r10, 160(%rsp)                 # 8-byte Spill
	leaq	512(,%r8,4), %rax
	movq	%rax, 3104(%rsp)                # 8-byte Spill
	movq	%r12, 560(%rsp)                 # 8-byte Spill
	movq	%r9, 1216(%rsp)                 # 8-byte Spill
	movq	%r12, 2208(%rsp)                # 8-byte Spill
	movq	%r14, 976(%rsp)                 # 8-byte Spill
	movq	%rdi, 960(%rsp)                 # 8-byte Spill
	movq	%r9, 800(%rsp)                  # 8-byte Spill
	movq	%rbx, 2592(%rsp)                # 8-byte Spill
	movq	%r10, 2560(%rsp)                # 8-byte Spill
	movq	%r11, 2976(%rsp)                # 8-byte Spill
	movq	1120(%rsp), %rax                # 8-byte Reload
	movq	%rax, 2944(%rsp)                # 8-byte Spill
	movq	440(%rsp), %r8                  # 8-byte Reload
	movq	%rdi, 1472(%rsp)                # 8-byte Spill
	movq	%rcx, 608(%rsp)                 # 8-byte Spill
	vmovaps	%ymm1, 2400(%rsp)               # 32-byte Spill
	vmovaps	%ymm8, 2112(%rsp)               # 32-byte Spill
	jmp	.LBB0_164
.LBB0_41:                               # %true_bb43
	movl	%r11d, %eax
	movl	1856(%rsp), %r13d               # 4-byte Reload
	subl	%r13d, %eax
	movq	40(%rsi), %rcx
	movq	72(%rsp), %rdx                  # 8-byte Reload
	addl	$3, %edx
	andl	$-4, %edx
	imull	$39, %eax, %r8d
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%rsi)
	movq	$0, 16(%rsi)
	movabsq	$12884975618, %rdi              # imm = 0x300012002
	movq	%rdi, 32(%rsi)
	movl	%r13d, (%rcx)
	movq	88(%rbp), %r13
	movl	%eax, 4(%rcx)
	movq	$1, 8(%rcx)
	movq	40(%rsi), %rcx
	movabsq	$167503724544, %rdi             # imm = 0x2700000000
	movq	%rdi, 16(%rcx)
	movq	480(%rsp), %rdi                 # 8-byte Reload
	movl	%eax, 24(%rcx)
	movl	$0, 28(%rcx)
	movq	24(%rbp), %rcx
	movq	40(%rsi), %rax
	movl	$0, 32(%rax)
	movl	%edx, 36(%rax)
	movl	%r8d, 40(%rax)
	movq	96(%rbp), %r8
	movl	$0, 44(%rax)
	movq	128(%rsp), %rax                 # 8-byte Reload
	movq	$0, 24(%rsi)
	cmpq	$0, 16(%r9)
	jne	.LBB0_40
.LBB0_42:                               # %_halide_buffer_is_bounds_query.exit368
	cmpq	$0, (%r9)
	je	.LBB0_146
.LBB0_43:                               # %after_bb48.thread
	cmpq	$0, (%r9)
	sete	%dl
	movl	%edx, 480(%rsp)                 # 4-byte Spill
	cmpq	$0, 16(%rsi)
	jne	.LBB0_44
.LBB0_148:
	cmpq	$0, (%rsi)
	sete	%dl
	movl	%edx, 128(%rsp)                 # 4-byte Spill
	movq	32(%rbp), %rdx
	cmpq	$0, 16(%r10)
	jne	.LBB0_45
.LBB0_149:
	cmpq	$0, (%r10)
	sete	%sil
	movl	%esi, 1568(%rsp)                # 4-byte Spill
	movq	48(%rbp), %r10
	movl	%r11d, 2880(%rsp)               # 4-byte Spill
	cmpq	$0, 16(%rax)
	jne	.LBB0_46
.LBB0_150:
	cmpq	$0, (%rax)
	sete	%r15b
	cmpq	$0, 16(%r13)
	jne	.LBB0_47
.LBB0_151:
	cmpq	$0, (%r13)
	sete	%r13b
	cmpq	$0, 16(%r8)
	jne	.LBB0_48
.LBB0_152:
	cmpq	$0, (%r8)
	sete	%al
	movq	40(%rbp), %r8
	cmpq	$0, 16(%rcx)
	jne	.LBB0_49
.LBB0_153:
	cmpq	$0, (%rcx)
	sete	%cl
	cmpq	$0, 16(%rdx)
	jne	.LBB0_50
.LBB0_154:
	cmpq	$0, (%rdx)
	sete	%dl
	cmpq	$0, 16(%rdi)
	jne	.LBB0_51
.LBB0_155:
	cmpq	$0, (%rdi)
	sete	%sil
	movq	16(%rbp), %rdi
	cmpq	$0, 16(%rdi)
	jne	.LBB0_52
.LBB0_156:
	cmpq	$0, (%rdi)
	sete	%dil
	cmpq	$0, 16(%r8)
	jne	.LBB0_53
.LBB0_157:
	cmpq	$0, (%r8)
	sete	%r8b
	cmpq	$0, 16(%r10)
	jne	.LBB0_54
.LBB0_158:
	cmpq	$0, (%r10)
	sete	%r9b
	movq	112(%rbp), %r11
	cmpq	$0, 16(%r11)
	movl	$0, %r10d
	jne	.LBB0_56
	jmp	.LBB0_55
.LBB0_146:                              # %after_bb48
	movq	40(%r9), %rax
	vxorps	%xmm0, %xmm0, %xmm0
	vmovups	%xmm0, (%r9)
	movq	$0, 16(%r9)
	movabsq	$4295041026, %rcx               # imm = 0x100012002
	movq	%rcx, 32(%r9)
	movq	1072(%rsp), %rcx                # 8-byte Reload
	movl	%ecx, (%rax)
	movq	1392(%rsp), %rcx                # 8-byte Reload
	movl	%ecx, 4(%rax)
	movq	24(%rbp), %rcx
	movq	$1, 8(%rax)
	movq	128(%rsp), %rax                 # 8-byte Reload
	movq	$0, 24(%r9)
	cmpq	$0, 16(%r9)
	jne	.LBB0_40
	jmp	.LBB0_43
.LBB0_159:                              # %"assert failed204"
	vmovaps	480(%rsp), %ymm0                # 32-byte Reload
	vmovaps	%ymm0, 3424(%rsp)
	vmovaps	288(%rsp), %ymm0                # 32-byte Reload
	vmovaps	%ymm0, 3456(%rsp)
	vmovaps	384(%rsp), %ymm0                # 32-byte Reload
	vmovaps	%ymm0, 3488(%rsp)
	vmovaps	320(%rsp), %ymm0                # 32-byte Reload
	vmovaps	%ymm0, 3520(%rsp)
.LBB0_160:                              # %"assert failed208"
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_out_of_memory@PLT
	movl	%eax, %r12d
	testl	%r12d, %r12d
	je	.LBB0_225
.LBB0_161:
	xorl	%ebx, %ebx
	xorl	%edi, %edi
	movq	128(%rsp), %rsi                 # 8-byte Reload
	callq	halide_free@PLT
	movq	192(%rsp), %r14                 # 8-byte Reload
	jmp	.LBB0_191
.LBB0_162:                              #   in Loop: Header=BB0_164 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	.p2align	4, 0x90
.LBB0_163:                              # %"consume f3"
                                        #   in Loop: Header=BB0_164 Depth=1
	movq	2240(%rsp), %rdx                # 8-byte Reload
	movq	%rdx, %rax
	subq	1760(%rsp), %rax                # 8-byte Folded Reload
	movq	2672(%rsp), %rcx                # 8-byte Reload
	vmovss	%xmm0, (%rcx,%rax,4)
	incq	%rdx
	movq	2272(%rsp), %r8                 # 8-byte Reload
	addq	$4, %r8
	incq	2944(%rsp)                      # 8-byte Folded Spill
	incq	2976(%rsp)                      # 8-byte Folded Spill
	incq	2080(%rsp)                      # 8-byte Folded Spill
	incq	2560(%rsp)                      # 8-byte Folded Spill
	incq	2592(%rsp)                      # 8-byte Folded Spill
	incq	800(%rsp)                       # 8-byte Folded Spill
	incq	960(%rsp)                       # 8-byte Folded Spill
	incq	976(%rsp)                       # 8-byte Folded Spill
	incq	2208(%rsp)                      # 8-byte Folded Spill
	movq	3104(%rsp), %rax                # 8-byte Reload
	addq	%rax, 2048(%rsp)                # 8-byte Folded Spill
	cmpl	%edx, 2040(%rsp)                # 4-byte Folded Reload
	je	.LBB0_175
.LBB0_164:                              # %"for store_cost_output.s0.n"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_167 Depth 2
                                        #     Child Loop BB0_173 Depth 2
	cmpl	$0, 72(%rsp)                    # 4-byte Folded Reload
	movq	%r8, 2272(%rsp)                 # 8-byte Spill
	movq	%rdx, 2240(%rsp)                # 8-byte Spill
	jle	.LBB0_162
# %bb.165:                              # %"for f3.s1.r77$x.preheader"
                                        #   in Loop: Header=BB0_164 Depth=1
	cmpb	$0, 3136(%rsp)                  # 1-byte Folded Reload
	je	.LBB0_169
# %bb.166:                              # %vector.ph1125
                                        #   in Loop: Header=BB0_164 Depth=1
	movq	%rdx, %rax
	subq	280(%rsp), %rax                 # 8-byte Folded Reload
	imulq	3072(%rsp), %rax                # 8-byte Folded Reload
	vmovq	%rax, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 2464(%rsp)               # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 864(%rsp)                # 32-byte Spill
	movq	%r8, 1312(%rsp)                 # 8-byte Spill
	movq	2912(%rsp), %rax                # 8-byte Reload
	movq	%rax, 2432(%rsp)                # 8-byte Spill
	vmovdqa	.LCPI0_62(%rip), %ymm2          # ymm2 = [0,1,2,3]
	vmovdqa	.LCPI0_61(%rip), %ymm9          # ymm9 = [4,5,6,7]
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 1184(%rsp)               # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 832(%rsp)                # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 896(%rsp)                # 32-byte Spill
	.p2align	4, 0x90
.LBB0_167:                              # %vector.body1120
                                        #   Parent Loop BB0_164 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovdqa	2400(%rsp), %ymm1               # 32-byte Reload
	vpaddq	%ymm1, %ymm9, %ymm0
	vmovdqa	%ymm2, %ymm7
	vpaddq	%ymm1, %ymm2, %ymm4
	vmovdqa	2144(%rsp), %ymm2               # 32-byte Reload
	vpaddq	%ymm2, %ymm9, %ymm1
	vpaddq	%ymm2, %ymm7, %ymm5
	vmovdqa	2112(%rsp), %ymm3               # 32-byte Reload
	vpaddq	%ymm3, %ymm9, %ymm2
	vpaddq	%ymm3, %ymm7, %ymm6
	vpsllq	$5, %ymm7, %ymm3
	vmovdqa	%ymm3, 192(%rsp)                # 32-byte Spill
	vmovdqa	%ymm7, 1824(%rsp)               # 32-byte Spill
	vpxor	%xmm3, %xmm3, %xmm3
	vpcmpeqq	%ymm3, %ymm7, %ymm7
	vpcmpeqq	%ymm3, %ymm9, %ymm8
	vmovdqa	%ymm9, 1664(%rsp)               # 32-byte Spill
	vperm2i128	$49, %ymm8, %ymm7, %ymm10 # ymm10 = ymm7[2,3],ymm8[2,3]
	vmovdqa	%ymm10, 224(%rsp)               # 32-byte Spill
	vinserti128	$1, %xmm8, %ymm7, %ymm7
	vmovdqa	%ymm7, 384(%rsp)                # 32-byte Spill
	vpcmpeqq	%ymm3, %ymm4, %ymm8
	vpcmpeqq	%ymm3, %ymm0, %ymm12
	vperm2i128	$49, %ymm12, %ymm8, %ymm7 # ymm7 = ymm8[2,3],ymm12[2,3]
	vmovdqa	%ymm7, 320(%rsp)                # 32-byte Spill
	vinserti128	$1, %xmm12, %ymm8, %ymm13
	vpcmpeqq	%ymm3, %ymm5, %ymm12
	vpcmpeqq	%ymm3, %ymm1, %ymm14
	vperm2i128	$49, %ymm14, %ymm12, %ymm10 # ymm10 = ymm12[2,3],ymm14[2,3]
	vinserti128	$1, %xmm14, %ymm12, %ymm12
	vpcmpeqq	%ymm3, %ymm6, %ymm14
	vpcmpeqq	%ymm3, %ymm2, %ymm11
	vperm2i128	$49, %ymm11, %ymm14, %ymm15 # ymm15 = ymm14[2,3],ymm11[2,3]
	vinserti128	$1, %xmm11, %ymm14, %ymm11
	vpsllq	$5, %ymm9, %ymm14
	vpsllq	$5, %ymm4, %ymm7
	vpsllq	$5, %ymm0, %ymm3
	vpsllq	$5, %ymm5, %ymm8
	vpsllq	$5, %ymm1, %ymm4
	vpsllq	$5, %ymm6, %ymm6
	vpsllq	$5, %ymm2, %ymm5
	vmovdqa	2464(%rsp), %ymm0               # 32-byte Reload
	vpaddq	%ymm0, %ymm14, %ymm2
	vpaddq	192(%rsp), %ymm0, %ymm1         # 32-byte Folded Reload
	vpaddq	%ymm0, %ymm3, %ymm3
	vpaddq	%ymm0, %ymm7, %ymm7
	vpaddq	%ymm0, %ymm4, %ymm14
	vpaddq	%ymm0, %ymm8, %ymm4
	vpaddq	%ymm0, %ymm5, %ymm5
	vmovdqa	%ymm5, 128(%rsp)                # 32-byte Spill
	vpaddq	%ymm0, %ymm6, %ymm6
	vmovdqa	384(%rsp), %ymm0                # 32-byte Reload
	vpackssdw	224(%rsp), %ymm0, %ymm0 # 32-byte Folded Reload
	vmovdqa	%ymm0, 192(%rsp)                # 32-byte Spill
	vmovq	%xmm1, %rax
	vpextrq	$1, %xmm1, %r11
	vpackssdw	320(%rsp), %ymm13, %ymm0 # 32-byte Folded Reload
	vmovdqa	%ymm0, 1568(%rsp)               # 32-byte Spill
	vextracti128	$1, %ymm1, %xmm0
	vpextrq	$1, %xmm2, %r14
	vpackssdw	%ymm10, %ymm12, %ymm1
	vmovdqa	%ymm1, 1632(%rsp)               # 32-byte Spill
	vmovq	%xmm2, %rcx
	vextracti128	$1, %ymm2, %xmm9
	vmovq	%xmm7, %r10
	vpackssdw	%ymm15, %ymm11, %ymm1
	vmovdqa	%ymm1, 1600(%rsp)               # 32-byte Spill
	vpextrq	$1, %xmm7, %r9
	vextracti128	$1, %ymm7, %xmm11
	vmovq	%xmm0, %rsi
	vpextrq	$1, %xmm3, %rdx
	vpextrq	$1, %xmm0, %rdi
	vmovq	%xmm3, %rbx
	vmovss	68(%r15,%rcx,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vmovss	72(%r15,%rcx,4), %xmm8          # xmm8 = mem[0],zero,zero,zero
	vmovq	%xmm9, %r12
	vmovss	68(%r15,%rax,4), %xmm12         # xmm12 = mem[0],zero,zero,zero
	vmovss	72(%r15,%rax,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm9, %r13
	vinsertps	$16, 68(%r15,%r14,4), %xmm5, %xmm0 # xmm0 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 68(%r15,%r12,4), %xmm0, %xmm5 # xmm5 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$16, 68(%r15,%r11,4), %xmm12, %xmm0 # xmm0 = xmm12[0],mem[0],xmm12[2,3]
	vinsertps	$32, 68(%r15,%rsi,4), %xmm0, %xmm9 # xmm9 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$16, 72(%r15,%r14,4), %xmm8, %xmm0 # xmm0 = xmm8[0],mem[0],xmm8[2,3]
	vinsertps	$32, 72(%r15,%r12,4), %xmm0, %xmm8 # xmm8 = xmm0[0,1],mem[0],xmm0[3]
	movq	%r12, %r8
	vinsertps	$16, 72(%r15,%r11,4), %xmm7, %xmm0 # xmm0 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 68(%r15,%r13,4), %xmm5, %xmm7 # xmm7 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 72(%r15,%rsi,4), %xmm0, %xmm5 # xmm5 = xmm0[0,1],mem[0],xmm0[3]
	movq	%rsi, 448(%rsp)                 # 8-byte Spill
	vinsertps	$48, 68(%r15,%rdi,4), %xmm9, %xmm9 # xmm9 = xmm9[0,1,2],mem[0]
	vextracti128	$1, %ymm3, %xmm12
	movq	%rcx, 768(%rsp)                 # 8-byte Spill
	vmovss	80(%r15,%rcx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$48, 72(%r15,%r13,4), %xmm8, %xmm3 # xmm3 = xmm8[0,1,2],mem[0]
	movq	%r14, 1920(%rsp)                # 8-byte Spill
	vinsertps	$16, 80(%r15,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 72(%r15,%rdi,4), %xmm5, %xmm10 # xmm10 = xmm5[0,1,2],mem[0]
	vextracti128	$1, %ymm4, %xmm8
	movq	%r12, 1792(%rsp)                # 8-byte Spill
	vinsertps	$32, 80(%r15,%r12,4), %xmm0, %xmm5 # xmm5 = xmm0[0,1],mem[0],xmm0[3]
	vinsertf128	$1, %xmm7, %ymm9, %ymm7
	movq	%rax, 2176(%rsp)                # 8-byte Spill
	vmovss	80(%r15,%rax,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	%r11, 704(%rsp)                 # 8-byte Spill
	vinsertps	$16, 80(%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertf128	$1, %xmm3, %ymm10, %ymm3
	vinsertps	$32, 80(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovaps	192(%rsp), %ymm1                # 32-byte Reload
	vblendvps	%ymm1, %ymm7, %ymm3, %ymm2
	vmovaps	%ymm2, 1856(%rsp)               # 32-byte Spill
	vextracti128	$1, %ymm14, %xmm13
	vmovss	84(%r15,%rcx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$48, 80(%r15,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	%r13, 816(%rsp)                 # 8-byte Spill
	vinsertps	$16, 84(%r15,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	%rdi, 456(%rsp)                 # 8-byte Spill
	vinsertps	$48, 80(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm0, %ymm5
	vmovss	84(%r15,%rax,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$32, 84(%r15,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, 84(%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 84(%r15,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 84(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 84(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm0, %ymm0
	movq	%rbx, 80(%rsp)                  # 8-byte Spill
	vmovss	68(%r15,%rbx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vblendvps	%ymm1, %ymm5, %ymm0, %ymm10
	movq	%r10, 512(%rsp)                 # 8-byte Spill
	vmovss	68(%r15,%r10,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%r15,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovq	%xmm12, %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	%r9, %rdi
	movq	%r9, 368(%rsp)                  # 8-byte Spill
	vinsertps	$16, 68(%r15,%r9,4), %xmm0, %xmm1 # xmm1 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 68(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovdqa	128(%rsp), %ymm2                # 32-byte Reload
	vextracti128	$1, %ymm2, %xmm15
	vmovq	%xmm4, %rsi
	vpextrq	$1, %xmm4, %r8
	movq	%r8, 192(%rsp)                  # 8-byte Spill
	vmovss	72(%r15,%rbx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vmovq	%xmm14, %rax
	vpextrq	$1, %xmm14, %rcx
	vmovss	68(%r15,%rax,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	%rax, %rbx
	movq	%rax, 1248(%rsp)                # 8-byte Spill
	vmovq	%xmm11, %rax
	movq	%rax, 576(%rsp)                 # 8-byte Spill
	vinsertps	$32, 68(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$16, 68(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	%rcx, %r14
	vpextrq	$1, %xmm12, %rax
	vmovss	68(%r15,%rsi,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	%rsi, 320(%rsp)                 # 8-byte Spill
	vmovq	%xmm13, %rcx
	vinsertps	$32, 68(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	%rcx, %r12
	vinsertps	$16, 68(%r15,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 68(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rax, %rcx
	movq	%rax, 352(%rsp)                 # 8-byte Spill
	vpextrq	$1, %xmm11, %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	vinsertps	$48, 68(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovss	72(%r15,%r10,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovq	%xmm2, %rax
	movq	%rax, 384(%rsp)                 # 8-byte Spill
	vpextrq	$1, %xmm2, %r11
	vmovss	68(%r15,%rax,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%r15,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%r11, 1536(%rsp)                # 8-byte Spill
	vinsertf128	$1, %xmm3, %ymm1, %ymm11
	vmovss	72(%r15,%rbx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmovq	%xmm8, %r10
	vpextrq	$1, %xmm8, %r13
	vmovq	%xmm6, %r9
	vmovss	68(%r15,%r9,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movq	%r9, 1344(%rsp)                 # 8-byte Spill
	vpextrq	$1, %xmm6, %r8
	vinsertps	$16, 68(%r15,%r8,4), %xmm1, %xmm8 # xmm8 = xmm1[0],mem[0],xmm1[2,3]
	movq	%r8, %rax
	movq	%r8, 928(%rsp)                  # 8-byte Spill
	vinsertps	$32, 68(%r15,%r10,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	%r10, 48(%rsp)                  # 8-byte Spill
	vinsertps	$16, 72(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%rdx, 288(%rsp)                 # 8-byte Spill
	vpextrq	$1, %xmm13, %rbx
	vmovss	72(%r15,%rsi,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovq	%xmm15, %r8
	vinsertps	$32, 68(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	%r8, 1280(%rsp)                 # 8-byte Spill
	vinsertps	$16, 72(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	32(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$32, 72(%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	576(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$32, 72(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%rbx, %rdi
	vinsertps	$48, 68(%r15,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$16, 72(%r15,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	%r14, 1152(%rsp)                # 8-byte Spill
	movq	%r13, %rsi
	movq	%r13, 128(%rsp)                 # 8-byte Spill
	vinsertps	$48, 68(%r15,%r13,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$32, 72(%r15,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	%r12, %rbx
	movq	%r12, 256(%rsp)                 # 8-byte Spill
	vinsertps	$48, 72(%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	192(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 72(%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	16(%rsp), %rcx                  # 8-byte Reload
	vinsertps	$48, 72(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 72(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 72(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdi, %r13
	vextracti128	$1, %ymm6, %xmm6
	vinsertps	$48, 72(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm7, %ymm5
	movq	384(%rsp), %rcx                 # 8-byte Reload
	vmovss	72(%r15,%rcx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%r15,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertf128	$1, %xmm4, %ymm0, %ymm0
	vmovss	72(%r15,%r9,4), %xmm4           # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertf128	$1, %xmm3, %ymm1, %ymm1
	movq	768(%rsp), %rcx                 # 8-byte Reload
	vmovss	92(%r15,%rcx,4), %xmm12         # xmm12 = mem[0],zero,zero,zero
	vmovq	%xmm6, %rax
	movq	%rax, 480(%rsp)                 # 8-byte Spill
	vinsertps	$32, 68(%r15,%rax,4), %xmm8, %xmm3 # xmm3 = xmm8[0,1],mem[0],xmm8[3]
	vmovaps	1568(%rsp), %ymm9               # 32-byte Reload
	vblendvps	%ymm9, %ymm11, %ymm0, %ymm11
	vmovaps	1632(%rsp), %ymm8               # 32-byte Reload
	vblendvps	%ymm8, %ymm5, %ymm1, %ymm0
	vmovaps	%ymm0, 2528(%rsp)               # 32-byte Spill
	vmovss	88(%r15,%rcx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	80(%rsp), %r11                  # 8-byte Reload
	vmovss	80(%r15,%r11,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$32, 72(%r15,%r8,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$16, 80(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 72(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vpextrq	$1, %xmm6, %rdi
	movq	%rdi, 640(%rsp)                 # 8-byte Spill
	vpextrq	$1, %xmm15, %rax
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	vmovss	64(%r15,%rcx,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	512(%rsp), %rsi                 # 8-byte Reload
	vmovss	80(%r15,%rsi,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	32(%rsp), %r10                  # 8-byte Reload
	vinsertps	$32, 80(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	368(%rsp), %r9                  # 8-byte Reload
	vinsertps	$16, 80(%r15,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 68(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	576(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$32, 80(%r15,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 68(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm13
	movq	1248(%rsp), %r12                # 8-byte Reload
	vmovss	80(%r15,%r12,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$48, 72(%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$16, 80(%r15,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 72(%r15,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm4, %ymm14
	movq	320(%rsp), %r14                 # 8-byte Reload
	vmovss	80(%r15,%r14,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$32, 80(%r15,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	192(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, 80(%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	352(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 80(%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	48(%rsp), %r8                   # 8-byte Reload
	vinsertps	$32, 80(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	16(%rsp), %rax                  # 8-byte Reload
	vinsertps	$48, 80(%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm7, %ymm1
	vmovss	84(%r15,%r11,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	%r13, 2304(%rsp)                # 8-byte Spill
	vinsertps	$48, 80(%r15,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	288(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 84(%r15,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	128(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 80(%r15,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmovss	84(%r15,%rsi,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$32, 84(%r15,%r10,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$16, 84(%r15,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 84(%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$32, 84(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 84(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm4, %ymm4
	movq	768(%rsp), %rax                 # 8-byte Reload
	vmovss	76(%r15,%rax,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	1920(%rsp), %rcx                # 8-byte Reload
	vinsertps	$16, 92(%r15,%rcx,4), %xmm12, %xmm2 # xmm2 = xmm12[0],mem[0],xmm12[2,3]
	vinsertps	$16, 88(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$16, 64(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$16, 76(%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	1792(%rsp), %rcx                # 8-byte Reload
	vinsertps	$32, 92(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 88(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 64(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$32, 76(%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	816(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 92(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 1792(%rsp)               # 16-byte Spill
	vinsertps	$48, 88(%r15,%rcx,4), %xmm0, %xmm2 # xmm2 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 64(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$48, 76(%r15,%rcx,4), %xmm7, %xmm15 # xmm15 = xmm7[0,1,2],mem[0]
	vblendvps	%ymm9, %ymm1, %ymm4, %ymm9
	vmovss	84(%r15,%r12,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	84(%r15,%r14,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	1152(%rsp), %rax                # 8-byte Reload
	vinsertps	$16, 84(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$16, 84(%r15,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	256(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 84(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 84(%r15,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 84(%r15,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 84(%r15,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm7, %ymm1
	movq	2176(%rsp), %r12                # 8-byte Reload
	vmovss	88(%r15,%r12,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	704(%rsp), %r13                 # 8-byte Reload
	vinsertps	$16, 88(%r15,%r13,4), %xmm7, %xmm5 # xmm5 = xmm7[0],mem[0],xmm7[2,3]
	vblendvps	%ymm8, %ymm3, %ymm1, %ymm0
	vmovaps	%ymm0, 768(%rsp)                # 32-byte Spill
	movq	384(%rsp), %rax                 # 8-byte Reload
	vmovss	80(%r15,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	1344(%rsp), %rbx                # 8-byte Reload
	vmovss	80(%r15,%rbx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	1536(%rsp), %rcx                # 8-byte Reload
	vinsertps	$16, 80(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	928(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 80(%r15,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	1280(%rsp), %rdx                # 8-byte Reload
	vinsertps	$32, 80(%r15,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	480(%rsp), %r9                  # 8-byte Reload
	vinsertps	$32, 80(%r15,%r9,4), %xmm7, %xmm12 # xmm12 = xmm7[0,1],mem[0],xmm7[3]
	vmovaps	1600(%rsp), %ymm0               # 32-byte Reload
	vblendvps	%ymm0, %ymm13, %ymm14, %ymm1
	vmovaps	%ymm1, 1568(%rsp)               # 32-byte Spill
	vmovss	84(%r15,%rax,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	224(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$48, 80(%r15,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$16, 84(%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	640(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 80(%r15,%rax,4), %xmm12, %xmm12 # xmm12 = xmm12[0,1,2],mem[0]
	vinsertps	$32, 84(%r15,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	84(%r15,%rbx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 84(%r15,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$32, 84(%r15,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 84(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm12, %ymm3
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	vblendvps	%ymm0, %ymm3, %ymm1, %ymm0
	vmovaps	%ymm0, 1600(%rsp)               # 32-byte Spill
	movq	1440(%rsp), %rax                # 8-byte Reload
	movq	1312(%rsp), %rsi                # 8-byte Reload
	vmovups	(%rsi,%rax,4), %ymm12
	vmovss	64(%r15,%r12,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 64(%r15,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	448(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$32, 64(%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	456(%rsp), %r8                  # 8-byte Reload
	vinsertps	$48, 64(%r15,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	2720(%rsp), %ymm4               # 32-byte Reload
	vcmpltps	%ymm12, %ymm4, %ymm0
	vinsertf128	$1, %xmm6, %ymm1, %ymm1
	vmovaps	1856(%rsp), %ymm3               # 32-byte Reload
	vblendvps	%ymm0, %ymm1, %ymm3, %ymm1
	vmovups	32(%rsi,%rax,4), %ymm14
	movq	112(%rsp), %rax                 # 8-byte Reload
	vmovups	(%rsi,%rax,4), %ymm3
	vinsertps	$32, 88(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	160(%rsp), %rax                 # 8-byte Reload
	vaddps	(%rsi,%rax,4), %ymm3, %ymm3
	vinsertps	$48, 88(%r15,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm5, %ymm2
	movq	80(%rsp), %r10                  # 8-byte Reload
	vmovss	64(%r15,%r10,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	288(%rsp), %r11                 # 8-byte Reload
	vinsertps	$16, 64(%r15,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmulps	%ymm3, %ymm2, %ymm2
	movq	512(%rsp), %rbx                 # 8-byte Reload
	vmovss	64(%r15,%rbx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	32(%rsp), %rax                  # 8-byte Reload
	vinsertps	$32, 64(%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	368(%rsp), %r14                 # 8-byte Reload
	vinsertps	$16, 64(%r15,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	352(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 64(%r15,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	576(%rsp), %r9                  # 8-byte Reload
	vinsertps	$32, 64(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	16(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$48, 64(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm3, %ymm3
	vmovss	76(%r15,%r12,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r15,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 76(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 76(%r15,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm15, %ymm5, %ymm5
	vcmpltps	%ymm14, %ymm4, %ymm15
	vblendvps	%ymm15, %ymm3, %ymm11, %ymm7
	vmovss	76(%r15,%r10,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vblendvps	%ymm0, %ymm5, %ymm10, %ymm3
	vmovss	76(%r15,%rbx,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 76(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rax, %r10
	movq	%r14, %r11
	vinsertps	$16, 76(%r15,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 76(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	movq	%rdx, %r14
	vinsertps	$32, 76(%r15,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 76(%r15,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	%rdi, %rbx
	vinsertf128	$1, %xmm6, %ymm5, %ymm5
	movq	1504(%rsp), %rax                # 8-byte Reload
	vmulps	(%rsi,%rax,4), %ymm12, %ymm6
	vmulps	%ymm1, %ymm6, %ymm1
	vmovss	92(%r15,%r12,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vblendvps	%ymm15, %ymm5, %ymm9, %ymm9
	movq	560(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%rsi,%rdi,4), %ymm5
	vmaxps	%ymm4, %ymm5, %ymm10
	vdivps	%ymm10, %ymm1, %ymm1
	vinsertps	$16, 92(%r15,%r13,4), %xmm6, %xmm5 # xmm5 = xmm6[0],mem[0],xmm6[2,3]
	vmulps	32(%rsi,%rax,4), %ymm14, %ymm6
	vmulps	%ymm7, %ymm6, %ymm6
	movq	1120(%rsp), %rax                # 8-byte Reload
	vfmadd132ps	(%rsi,%rax,4), %ymm1, %ymm3 # ymm3 = (ymm3 * mem) + ymm1
	vmovups	32(%rsi,%rdi,4), %ymm1
	vmaxps	%ymm4, %ymm1, %ymm1
	vdivps	%ymm1, %ymm6, %ymm6
	vinsertps	$32, 92(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vfmadd132ps	32(%rsi,%rax,4), %ymm6, %ymm9 # ymm9 = (ymm9 * mem) + ymm6
	movq	80(%rsp), %rcx                  # 8-byte Reload
	vmovss	92(%r15,%rcx,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vdivps	%ymm10, %ymm2, %ymm2
	vandps	%ymm2, %ymm0, %ymm0
	vmovss	88(%r15,%rcx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	288(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 92(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovaps	%xmm6, 80(%rsp)                 # 16-byte Spill
	vinsertps	$16, 88(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovaps	%xmm2, 288(%rsp)                # 16-byte Spill
	movq	608(%rsp), %r12                 # 8-byte Reload
	vfmadd132ps	-96(%rsi,%r12), %ymm0, %ymm3 # ymm3 = (ymm3 * mem) + ymm0
	movq	512(%rsp), %rcx                 # 8-byte Reload
	vmovss	92(%r15,%rcx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	%r11, %rdi
	vinsertps	$16, 92(%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovaps	%xmm0, 1632(%rsp)               # 16-byte Spill
	vmovss	88(%r15,%rcx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r15,%r11,4), %xmm2, %xmm13 # xmm13 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 92(%r15,%r8,4), %xmm5, %xmm11 # xmm11 = xmm5[0,1,2],mem[0]
	vbroadcastss	.LCPI0_66(%rip), %ymm5  # ymm5 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vdivps	%ymm10, %ymm5, %ymm10
	vinsertf128	$1, 1792(%rsp), %ymm11, %ymm11 # 16-byte Folded Reload
	vminps	%ymm12, %ymm10, %ymm10
	movq	1216(%rsp), %rcx                # 8-byte Reload
	vmulps	(%rsi,%rcx,4), %ymm10, %ymm10
	vmulps	%ymm10, %ymm12, %ymm10
	movq	1248(%rsp), %rdi                # 8-byte Reload
	vmovss	92(%r15,%rdi,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	1152(%rsp), %rax                # 8-byte Reload
	vinsertps	$16, 92(%r15,%rax,4), %xmm6, %xmm12 # xmm12 = xmm6[0],mem[0],xmm6[2,3]
	vmovss	88(%r15,%rdi,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r15,%rax,4), %xmm0, %xmm8 # xmm8 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	64(%r15,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	%rdi, %rdx
	vinsertps	$16, 64(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%rax, %rdi
	movq	1472(%rsp), %r8                 # 8-byte Reload
	vmulps	(%rsi,%r8,4), %ymm10, %ymm10
	vfmadd231ps	%ymm10, %ymm11, %ymm3   # ymm3 = (ymm11 * ymm10) + ymm3
	vmovss	76(%r15,%rdx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovaps	80(%rsp), %xmm6                 # 16-byte Reload
	vinsertps	$32, 92(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovaps	288(%rsp), %xmm0                # 16-byte Reload
	vinsertps	$32, 88(%r15,%r10,4), %xmm0, %xmm10 # xmm10 = xmm0[0,1],mem[0],xmm0[3]
	vmovaps	1632(%rsp), %xmm0               # 16-byte Reload
	vinsertps	$32, 92(%r15,%r9,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 88(%r15,%r9,4), %xmm13, %xmm13 # xmm13 = xmm13[0,1],mem[0],xmm13[3]
	movq	256(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 92(%r15,%rax,4), %xmm12, %xmm12 # xmm12 = xmm12[0,1],mem[0],xmm12[3]
	vinsertps	$32, 88(%r15,%rax,4), %xmm8, %xmm0 # xmm0 = xmm8[0,1],mem[0],xmm8[3]
	vinsertps	$32, 64(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 76(%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 92(%r15,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$48, 88(%r15,%r14,4), %xmm10, %xmm10 # xmm10 = xmm10[0,1,2],mem[0]
	vinsertps	$48, 92(%r15,%rbx,4), %xmm11, %xmm11 # xmm11 = xmm11[0,1,2],mem[0]
	vinsertps	$48, 88(%r15,%rbx,4), %xmm13, %xmm13 # xmm13 = xmm13[0,1,2],mem[0]
	movq	2304(%rsp), %rax                # 8-byte Reload
	vinsertps	$48, 92(%r15,%rax,4), %xmm12, %xmm12 # xmm12 = xmm12[0,1,2],mem[0]
	vinsertps	$48, 88(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 64(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm2, 16(%rsp)                 # 16-byte Spill
	vinsertps	$48, 76(%r15,%rax,4), %xmm7, %xmm2 # xmm2 = xmm7[0,1,2],mem[0]
	vmovaps	%xmm2, 288(%rsp)                # 16-byte Spill
	vinsertf128	$1, %xmm6, %ymm11, %ymm11
	movq	112(%rsp), %rax                 # 8-byte Reload
	vmovups	32(%rsi,%rax,4), %ymm7
	movq	160(%rsp), %rax                 # 8-byte Reload
	vaddps	32(%rsi,%rax,4), %ymm7, %ymm7
	vinsertf128	$1, %xmm10, %ymm13, %ymm10
	movq	320(%rsp), %r11                 # 8-byte Reload
	vmovss	92(%r15,%r11,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	192(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, 92(%r15,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmulps	%ymm7, %ymm10, %ymm7
	vdivps	%ymm1, %ymm7, %ymm7
	vandps	%ymm7, %ymm15, %ymm7
	vdivps	%ymm1, %ymm5, %ymm1
	vfmadd132ps	-64(%rsi,%r12), %ymm7, %ymm9 # ymm9 = (ymm9 * mem) + ymm7
	vmovss	88(%r15,%r11,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vminps	%ymm14, %ymm1, %ymm1
	vinsertps	$16, 88(%r15,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmulps	32(%rsi,%rcx,4), %ymm1, %ymm1
	vmulps	%ymm1, %ymm14, %ymm1
	movq	384(%rsp), %r13                 # 8-byte Reload
	vmovss	88(%r15,%r13,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	48(%rsp), %r14                  # 8-byte Reload
	vinsertps	$32, 92(%r15,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	1536(%rsp), %rdx                # 8-byte Reload
	vinsertps	$16, 88(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 88(%r15,%r14,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	1280(%rsp), %r10                # 8-byte Reload
	vinsertps	$32, 88(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmulps	32(%rsi,%r8,4), %ymm1, %ymm1
	vfmadd231ps	%ymm1, %ymm11, %ymm9    # ymm9 = (ymm11 * ymm1) + ymm9
	movq	1344(%rsp), %r8                 # 8-byte Reload
	vmovss	88(%r15,%r8,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movq	128(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$48, 92(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	928(%rsp), %r9                  # 8-byte Reload
	vinsertps	$16, 88(%r15,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 88(%r15,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	movq	480(%rsp), %r12                 # 8-byte Reload
	vinsertps	$32, 88(%r15,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	224(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 88(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	movq	1440(%rsp), %rax                # 8-byte Reload
	vmovups	64(%rsi,%rax,4), %ymm8
	movq	640(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 88(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm12, %ymm2, %ymm2
	vmovaps	%ymm2, 512(%rsp)                # 32-byte Spill
	vmovss	64(%r15,%r11,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	192(%rsp), %r11                 # 8-byte Reload
	vinsertps	$16, 64(%r15,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertf128	$1, %xmm0, %ymm7, %ymm0
	vmovups	96(%rsi,%rax,4), %ymm7
	vinsertps	$32, 64(%r15,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertf128	$1, %xmm6, %ymm1, %ymm1
	vcmpltps	%ymm8, %ymm4, %ymm6
	vinsertps	$48, 64(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, 16(%rsp), %ymm2, %ymm2 # 16-byte Folded Reload
	movq	112(%rsp), %rax                 # 8-byte Reload
	vmovups	64(%rsi,%rax,4), %ymm10
	vmovaps	2528(%rsp), %ymm11              # 32-byte Reload
	vblendvps	%ymm6, %ymm2, %ymm11, %ymm12
	vmovups	96(%rsi,%rax,4), %ymm11
	movq	160(%rsp), %rax                 # 8-byte Reload
	vaddps	64(%rsi,%rax,4), %ymm10, %ymm10
	vaddps	96(%rsi,%rax,4), %ymm11, %ymm11
	vmulps	%ymm0, %ymm10, %ymm13
	vmovss	64(%r15,%r13,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 64(%r15,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%rdx, %rax
	vmulps	%ymm1, %ymm11, %ymm10
	vmovss	64(%r15,%r8,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$32, 64(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$16, 64(%r15,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 64(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 64(%r15,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%rdi, %rbx
	vinsertps	$48, 64(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vcmpltps	%ymm7, %ymm4, %ymm2
	vmovaps	1568(%rsp), %ymm0               # 32-byte Reload
	vblendvps	%ymm2, %ymm1, %ymm0, %ymm1
	movq	320(%rsp), %rcx                 # 8-byte Reload
	vmovss	76(%r15,%rcx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	560(%rsp), %rcx                 # 8-byte Reload
	vmovups	64(%rsi,%rcx,4), %ymm11
	vmaxps	%ymm4, %ymm11, %ymm11
	vdivps	%ymm11, %ymm13, %ymm13
	vinsertps	$32, 76(%r15,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	1504(%rsp), %rdx                # 8-byte Reload
	vmulps	64(%rsi,%rdx,4), %ymm8, %ymm15
	vmulps	%ymm12, %ymm15, %ymm12
	vdivps	%ymm11, %ymm12, %ymm12
	vdivps	%ymm11, %ymm5, %ymm11
	vmovups	96(%rsi,%rcx,4), %ymm15
	vmaxps	%ymm4, %ymm15, %ymm15
	vdivps	%ymm15, %ymm5, %ymm5
	movq	128(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 76(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	96(%rsi,%rdx,4), %ymm7, %ymm14
	vmulps	%ymm1, %ymm14, %ymm1
	vdivps	%ymm15, %ymm10, %ymm10
	vinsertf128	$1, 288(%rsp), %ymm0, %ymm0 # 16-byte Folded Reload
	vdivps	%ymm15, %ymm1, %ymm14
	vmovaps	768(%rsp), %ymm1                # 32-byte Reload
	vblendvps	%ymm6, %ymm0, %ymm1, %ymm0
	movq	1120(%rsp), %rdx                # 8-byte Reload
	vfmadd132ps	64(%rsi,%rdx,4), %ymm12, %ymm0 # ymm0 = (ymm0 * mem) + ymm12
	vminps	%ymm8, %ymm11, %ymm11
	movq	1216(%rsp), %r14                # 8-byte Reload
	vmulps	64(%rsi,%r14,4), %ymm11, %ymm11
	vmulps	%ymm11, %ymm8, %ymm8
	vandps	%ymm6, %ymm13, %ymm6
	movq	608(%rsp), %rdi                 # 8-byte Reload
	vfmadd132ps	-32(%rsi,%rdi), %ymm6, %ymm0 # ymm0 = (ymm0 * mem) + ymm6
	vmovss	76(%r15,%r13,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	1472(%rsp), %r11                # 8-byte Reload
	vmulps	64(%rsi,%r11,4), %ymm8, %ymm8
	vfmadd231ps	512(%rsp), %ymm8, %ymm0 # 32-byte Folded Reload
                                        # ymm0 = (ymm8 * mem) + ymm0
	vmovss	76(%r15,%r8,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$32, 76(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$16, 76(%r15,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	224(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 76(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 76(%r15,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 76(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm1, %ymm1
	vmovaps	1600(%rsp), %ymm4               # 32-byte Reload
	vblendvps	%ymm2, %ymm1, %ymm4, %ymm1
	vandps	%ymm2, %ymm10, %ymm2
	vfmadd132ps	96(%rsi,%rdx,4), %ymm14, %ymm1 # ymm1 = (ymm1 * mem) + ymm14
	vfmadd132ps	(%rsi,%rdi), %ymm2, %ymm1 # ymm1 = (ymm1 * mem) + ymm2
	vmovss	92(%r15,%r13,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vminps	%ymm7, %ymm5, %ymm5
	vmulps	96(%rsi,%r14,4), %ymm5, %ymm5
	vmulps	%ymm5, %ymm7, %ymm5
	vmovss	92(%r15,%r8,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$16, 92(%r15,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 92(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 92(%r15,%r12,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 92(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 92(%r15,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmulps	96(%rsi,%r11,4), %ymm5, %ymm5
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vfmadd231ps	%ymm5, %ymm2, %ymm1     # ymm1 = (ymm2 * ymm5) + ymm1
	vbroadcastss	.LCPI0_67(%rip), %ymm2  # ymm2 = [2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0]
	vmovaps	864(%rsp), %ymm5                # 32-byte Reload
	vfmadd231ps	%ymm3, %ymm2, %ymm5     # ymm5 = (ymm2 * ymm3) + ymm5
	vmovaps	%ymm5, 864(%rsp)                # 32-byte Spill
	vmovaps	1184(%rsp), %ymm3               # 32-byte Reload
	vfmadd231ps	%ymm9, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm9) + ymm3
	vmovaps	%ymm3, 1184(%rsp)               # 32-byte Spill
	vmovaps	832(%rsp), %ymm3                # 32-byte Reload
	vfmadd231ps	%ymm0, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm0) + ymm3
	vmovaps	%ymm3, 832(%rsp)                # 32-byte Spill
	vmovaps	896(%rsp), %ymm0                # 32-byte Reload
	vfmadd231ps	%ymm1, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm1) + ymm0
	vmovaps	%ymm0, 896(%rsp)                # 32-byte Spill
	vpbroadcastq	.LCPI0_69(%rip), %ymm0  # ymm0 = [32,32,32,32]
	vmovdqa	1824(%rsp), %ymm2               # 32-byte Reload
	vpaddq	%ymm0, %ymm2, %ymm2
	vmovdqa	1664(%rsp), %ymm9               # 32-byte Reload
	vpaddq	%ymm0, %ymm9, %ymm9
	addq	1888(%rsp), %rsi                # 8-byte Folded Reload
	movq	%rsi, 1312(%rsp)                # 8-byte Spill
	addq	$-32, 2432(%rsp)                # 8-byte Folded Spill
	jne	.LBB0_167
# %bb.168:                              # %middle.block1118
                                        #   in Loop: Header=BB0_164 Depth=1
	vmovaps	1184(%rsp), %ymm0               # 32-byte Reload
	vaddps	864(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vaddps	832(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vaddps	896(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	movq	2912(%rsp), %rax                # 8-byte Reload
	movq	%rax, %rdi
	cmpq	104(%rsp), %rax                 # 8-byte Folded Reload
	vmovss	.LCPI0_48(%rip), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vmovss	.LCPI0_66(%rip), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vmovss	.LCPI0_67(%rip), %xmm5          # xmm5 = mem[0],zero,zero,zero
	je	.LBB0_163
	jmp	.LBB0_170
	.p2align	4, 0x90
.LBB0_169:                              #   in Loop: Header=BB0_164 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%edi, %edi
.LBB0_170:                              # %"for f3.s1.r77$x.preheader1310"
                                        #   in Loop: Header=BB0_164 Depth=1
	movq	176(%rsp), %rcx                 # 8-byte Reload
	imulq	%rdi, %rcx
	movq	2944(%rsp), %rax                # 8-byte Reload
	addq	%rcx, %rax
	movq	%rax, 192(%rsp)                 # 8-byte Spill
	movq	2976(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movq	2080(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %r14
	movq	2560(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	movq	2592(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	movq	800(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %r12
	movq	960(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %r13
	movq	976(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %r10
	addq	2208(%rsp), %rcx                # 8-byte Folded Reload
	movq	%rdi, %rax
	shlq	$7, %rax
	addq	2048(%rsp), %rax                # 8-byte Folded Reload
	movq	440(%rsp), %rbx                 # 8-byte Reload
	jmp	.LBB0_173
	.p2align	4, 0x90
.LBB0_171:                              # %"for f3.s1.r77$x"
                                        #   in Loop: Header=BB0_173 Depth=2
	vxorps	%xmm4, %xmm4, %xmm4
.LBB0_172:                              # %"for f3.s1.r77$x"
                                        #   in Loop: Header=BB0_173 Depth=2
	cmovbq	%rax, %r8
	movq	128(%rsp), %rsi                 # 8-byte Reload
	vmulss	(%rbx,%rsi,4), %xmm1, %xmm1
	vmulss	(%r8), %xmm1, %xmm1
	vdivss	%xmm3, %xmm1, %xmm1
	movq	192(%rsp), %rsi                 # 8-byte Reload
	vmovss	(%rbx,%rsi,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	cmovbq	%rdx, %r9
	vfmadd132ss	(%r9), %xmm1, %xmm3     # xmm3 = (xmm3 * mem) + xmm1
	vfmadd132ss	(%rbx,%r14,4), %xmm4, %xmm3 # xmm3 = (xmm3 * mem) + xmm4
	vfmadd132ss	28(%rax), %xmm3, %xmm2  # xmm2 = (xmm2 * mem) + xmm3
	vfmadd231ss	%xmm5, %xmm2, %xmm0     # xmm0 = (xmm2 * xmm5) + xmm0
	incq	%rdi
	addq	2496(%rsp), %rbx                # 8-byte Folded Reload
	subq	$-128, %rax
	cmpq	%rdi, 104(%rsp)                 # 8-byte Folded Reload
	je	.LBB0_163
.LBB0_173:                              # %"for f3.s1.r77$x"
                                        #   Parent Loop BB0_164 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%rcx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmovss	(%rbx,%r10,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	leaq	4(%rax), %rsi
	leaq	8(%rax), %r8
	leaq	12(%rax), %rdx
	leaq	16(%rax), %r11
	testq	%rdi, %rdi
	cmoveq	%rsi, %r8
	leaq	20(%rax), %r9
	cmoveq	%r11, %r9
	vmaxss	%xmm6, %xmm2, %xmm3
	vdivss	%xmm3, %xmm7, %xmm2
	vminss	%xmm1, %xmm2, %xmm2
	vmulss	(%rbx,%r12,4), %xmm2, %xmm2
	vucomiss	%xmm1, %xmm6
	vmulss	%xmm2, %xmm1, %xmm2
	vmulss	(%rbx,%r13,4), %xmm2, %xmm2
	jae	.LBB0_171
# %bb.174:                              #   in Loop: Header=BB0_173 Depth=2
	movq	320(%rsp), %rsi                 # 8-byte Reload
	vmovss	(%rbx,%rsi,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	movq	224(%rsp), %rsi                 # 8-byte Reload
	vaddss	(%rbx,%rsi,4), %xmm4, %xmm4
	vmulss	24(%rax), %xmm4, %xmm4
	vdivss	%xmm3, %xmm4, %xmm4
	jmp	.LBB0_172
.LBB0_175:                              # %"consume relu1219"
	cmpl	$0, 1024(%rsp)                  # 4-byte Folded Reload
	jle	.LBB0_188
# %bb.176:                              # %"for compute_cost_output.s0.n.preheader"
	vxorps	%xmm13, %xmm13, %xmm13
	vcvtsi2ssl	760(%rsp), %xmm13, %xmm5 # 4-byte Folded Reload
	movq	632(%rsp), %rdx                 # 8-byte Reload
	movl	%edx, %eax
	shll	$4, %eax
	addl	%edx, %eax
	leal	(%rdx,%rdx,4), %ecx
	leal	(%rdx,%rcx,4), %ecx
	addl	%edx, %ecx
	movq	1008(%rsp), %r8                 # 8-byte Reload
	leaq	128(%r8), %r10
	movq	472(%rsp), %rdx                 # 8-byte Reload
	subl	%edx, %eax
	movl	2368(%rsp), %r9d                # 4-byte Reload
	subl	%edx, %r9d
	movl	2752(%rsp), %edi                # 4-byte Reload
	subl	%edx, %edi
	movl	2816(%rsp), %r14d               # 4-byte Reload
	subl	%edx, %r14d
	movl	2784(%rsp), %r12d               # 4-byte Reload
	subl	%edx, %r12d
	subl	%edx, %ecx
	movslq	%eax, %r11
	movslq	%r9d, %r9
	movslq	%edi, %rbx
	movslq	%r14d, %rdi
	movslq	%r12d, %rsi
	movslq	%ecx, %rdx
	cmpl	$31, 72(%rsp)                   # 4-byte Folded Reload
	seta	%al
	cmpl	$1, 184(%rsp)                   # 4-byte Folded Reload
	sete	%r14b
	andb	%al, %r14b
	movq	104(%rsp), %rax                 # 8-byte Reload
	movl	%eax, %r12d
	andl	$-32, %r12d
	vbroadcastss	%xmm5, %ymm8
	movq	1768(%rsp), %rcx                # 8-byte Reload
	addq	%rcx, %r11
	movq	176(%rsp), %rax                 # 8-byte Reload
	movq	%rax, %r13
	shlq	$7, %r13
	addq	%rcx, %r9
	addq	%rcx, %rbx
	addq	%rcx, %rdi
	addq	%rcx, %rsi
	addq	%rcx, %rdx
	leaq	(,%rax,4), %rax
	movq	%rax, 1440(%rsp)                # 8-byte Spill
	movq	%rcx, %rax
	subq	280(%rsp), %rax                 # 8-byte Folded Reload
	movq	%r10, 1824(%rsp)                # 8-byte Spill
	imulq	%r10, %rax
	leaq	(%r15,%rax,4), %rax
	movq	%rax, 832(%rsp)                 # 8-byte Spill
	leaq	512(,%r8,4), %rax
	movq	%rax, 560(%rsp)                 # 8-byte Spill
	movq	%rsi, %r8
	vmovss	.LCPI0_48(%rip), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vxorps	%xmm6, %xmm6, %xmm6
	movq	%rbx, 896(%rsp)                 # 8-byte Spill
	movq	%r9, 1312(%rsp)                 # 8-byte Spill
	movq	%r11, 1120(%rsp)                # 8-byte Spill
	movq	%rdi, 1504(%rsp)                # 8-byte Spill
	movq	%rsi, 1216(%rsp)                # 8-byte Spill
	movq	%rdx, 1472(%rsp)                # 8-byte Spill
	movq	440(%rsp), %rsi                 # 8-byte Reload
	movq	%rcx, 864(%rsp)                 # 8-byte Spill
	movq	%r8, 128(%rsp)                  # 8-byte Spill
	movq	%rdx, 16(%rsp)                  # 8-byte Spill
	vmovaps	%xmm5, 608(%rsp)                # 16-byte Spill
	movq	%r11, 512(%rsp)                 # 8-byte Spill
	movq	%r9, 768(%rsp)                  # 8-byte Spill
	movq	%rbx, 368(%rsp)                 # 8-byte Spill
	movq	%rdi, 192(%rsp)                 # 8-byte Spill
	movb	%r14b, 1664(%rsp)               # 1-byte Spill
	movq	%r12, 1600(%rsp)                # 8-byte Spill
	movq	%r13, 352(%rsp)                 # 8-byte Spill
	vmovaps	%ymm8, 1248(%rsp)               # 32-byte Spill
	vmovaps	2720(%rsp), %ymm14              # 32-byte Reload
	.p2align	4, 0x90
.LBB0_177:                              # %"for compute_cost_output.s0.n"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_180 Depth 2
                                        #     Child Loop BB0_184 Depth 2
	vxorps	%xmm0, %xmm0, %xmm0
	movl	$0, %eax
	cmpl	$0, 72(%rsp)                    # 4-byte Folded Reload
	jle	.LBB0_186
# %bb.178:                              # %"for f5.s1.r77$x.preheader"
                                        #   in Loop: Header=BB0_177 Depth=1
	testb	%r14b, %r14b
	movq	%rsi, 1184(%rsp)                # 8-byte Spill
	je	.LBB0_182
# %bb.179:                              # %vector.ph1197
                                        #   in Loop: Header=BB0_177 Depth=1
	movq	864(%rsp), %rax                 # 8-byte Reload
	subq	280(%rsp), %rax                 # 8-byte Folded Reload
	imulq	1824(%rsp), %rax                # 8-byte Folded Reload
	vmovq	%rax, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 928(%rsp)                # 32-byte Spill
	leaq	256(%rax), %rcx
	vmovq	%rcx, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 1280(%rsp)               # 32-byte Spill
	leaq	512(%rax), %rcx
	vmovq	%rcx, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 1344(%rsp)               # 32-byte Spill
	addq	$768, %rax                      # imm = 0x300
	vmovq	%rax, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 1536(%rsp)               # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 224(%rsp)                # 32-byte Spill
	movq	%rsi, %r11
	movq	%r12, 1152(%rsp)                # 8-byte Spill
	vmovdqa	.LCPI0_62(%rip), %ymm12         # ymm12 = [0,1,2,3]
	vmovdqa	.LCPI0_61(%rip), %ymm9          # ymm9 = [4,5,6,7]
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 320(%rsp)                # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 384(%rsp)                # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 288(%rsp)                # 32-byte Spill
	.p2align	4, 0x90
.LBB0_180:                              # %vector.body1192
                                        #   Parent Loop BB0_177 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	16(%rsp), %rax                  # 8-byte Reload
	vmovups	(%r11,%rax,4), %ymm0
	movq	%rax, %r9
	movq	128(%rsp), %rax                 # 8-byte Reload
	vmulps	(%r11,%rax,4), %ymm0, %ymm0
	vmovaps	%ymm0, 576(%rsp)                # 32-byte Spill
	vpsllq	$5, %ymm12, %ymm0
	vpsllq	$5, %ymm9, %ymm3
	vmovdqa	928(%rsp), %ymm2                # 32-byte Reload
	vpaddq	%ymm2, %ymm3, %ymm1
	vpaddq	%ymm2, %ymm0, %ymm2
	vmovdqa	1280(%rsp), %ymm4               # 32-byte Reload
	vpaddq	%ymm3, %ymm4, %ymm5
	vpaddq	%ymm0, %ymm4, %ymm4
	vpextrq	$1, %xmm2, %rcx
	vmovq	%xmm2, %rdx
	vextracti128	$1, %ymm2, %xmm7
	vpextrq	$1, %xmm1, %r8
	vmovq	%xmm1, %rax
	vextracti128	$1, %ymm1, %xmm1
	vpextrq	$1, %xmm4, 32(%rsp)             # 8-byte Folded Spill
	vmovq	%xmm4, %r12
	vextracti128	$1, %ymm4, %xmm4
	vpextrq	$1, %xmm5, %r13
	vmovq	%xmm5, %r10
	vextracti128	$1, %ymm5, %xmm5
	vmovss	(%r15,%rax,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmovss	4(%r15,%rax,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vmovss	8(%r15,%rax,4), %xmm11          # xmm11 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%r8,4), %xmm2, %xmm8 # xmm8 = xmm2[0],mem[0],xmm2[2,3]
	vmovq	%xmm7, %rdi
	vmovss	12(%r15,%rax,4), %xmm13         # xmm13 = mem[0],zero,zero,zero
	vmovss	(%r15,%rdx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm7, %rsi
	vmovss	4(%r15,%rdx,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rcx,4), %xmm2, %xmm15 # xmm15 = xmm2[0],mem[0],xmm2[2,3]
	vmovq	%xmm1, %rbx
	vinsertps	$16, 4(%r15,%r8,4), %xmm6, %xmm10 # xmm10 = xmm6[0],mem[0],xmm6[2,3]
	vpextrq	$1, %xmm1, %rax
	vmovss	8(%r15,%rdx,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r15,%rcx,4), %xmm7, %xmm1 # xmm1 = xmm7[0],mem[0],xmm7[2,3]
	vmovq	%xmm4, 80(%rsp)                 # 8-byte Folded Spill
	vinsertps	$16, 8(%r15,%r8,4), %xmm11, %xmm11 # xmm11 = xmm11[0],mem[0],xmm11[2,3]
	vpextrq	$1, %xmm4, 48(%rsp)             # 8-byte Folded Spill
	vmovss	12(%r15,%rdx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r15,%rcx,4), %xmm6, %xmm4 # xmm4 = xmm6[0],mem[0],xmm6[2,3]
	vmovq	%xmm5, %r14
	vinsertps	$16, 12(%r15,%r8,4), %xmm13, %xmm6 # xmm6 = xmm13[0],mem[0],xmm13[2,3]
	vpextrq	$1, %xmm5, %r8
	vmovdqa	1344(%rsp), %ymm13              # 32-byte Reload
	vpaddq	%ymm0, %ymm13, %ymm5
	vinsertps	$16, 12(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovq	%xmm5, 640(%rsp)                # 8-byte Folded Spill
	vinsertps	$32, (%r15,%rbx,4), %xmm8, %xmm7 # xmm7 = xmm8[0,1],mem[0],xmm8[3]
	vinsertps	$32, 4(%r15,%rbx,4), %xmm10, %xmm10 # xmm10 = xmm10[0,1],mem[0],xmm10[3]
	vinsertps	$32, 8(%r15,%rbx,4), %xmm11, %xmm11 # xmm11 = xmm11[0,1],mem[0],xmm11[3]
	vinsertps	$32, 12(%r15,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vpextrq	$1, %xmm5, 480(%rsp)            # 8-byte Folded Spill
	vinsertps	$32, (%r15,%rdi,4), %xmm15, %xmm8 # xmm8 = xmm15[0,1],mem[0],xmm15[3]
	vinsertps	$32, 4(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 8(%r15,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vextracti128	$1, %ymm5, %xmm5
	vinsertps	$32, 12(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovq	%xmm5, 160(%rsp)                # 8-byte Folded Spill
	vinsertps	$48, (%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$48, 4(%r15,%rax,4), %xmm10, %xmm10 # xmm10 = xmm10[0,1,2],mem[0]
	vinsertps	$48, 8(%r15,%rax,4), %xmm11, %xmm11 # xmm11 = xmm11[0,1,2],mem[0]
	vinsertps	$48, 12(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vpextrq	$1, %xmm5, 112(%rsp)            # 8-byte Folded Spill
	vinsertps	$48, (%r15,%rsi,4), %xmm8, %xmm5 # xmm5 = xmm8[0,1,2],mem[0]
	movq	192(%rsp), %rax                 # 8-byte Reload
	vmovups	(%r11,%rax,4), %ymm8
	vinsertf128	$1, %xmm7, %ymm5, %ymm5
	vxorps	%xmm15, %xmm15, %xmm15
	movq	512(%rsp), %rax                 # 8-byte Reload
	vcmpeqps	(%r11,%rax,4), %ymm15, %ymm7
	vinsertps	$48, 4(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm10, %ymm1, %ymm1
	vpaddq	%ymm3, %ymm13, %ymm10
	vinsertps	$48, 8(%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$48, 12(%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovq	%xmm10, 256(%rsp)               # 8-byte Folded Spill
	vmulps	%ymm1, %ymm8, %ymm1
	vmovaps	576(%rsp), %ymm13               # 32-byte Reload
	vfmadd231ps	%ymm5, %ymm13, %ymm1    # ymm1 = (ymm13 * ymm5) + ymm1
	vmovss	(%r15,%r10,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vinsertf128	$1, %xmm11, %ymm4, %ymm4
	vmovss	4(%r15,%r10,4), %xmm11          # xmm11 = mem[0],zero,zero,zero
	vinsertf128	$1, %xmm6, %ymm2, %ymm2
	vmovss	8(%r15,%r10,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vmulps	%ymm2, %ymm8, %ymm2
	vmovss	12(%r15,%r10,4), %xmm8          # xmm8 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm10, %rsi
	vextracti128	$1, %ymm10, %xmm10
	vinsertps	$16, (%r15,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vfmadd231ps	%ymm4, %ymm13, %ymm2    # ymm2 = (ymm13 * ymm4) + ymm2
	vmovss	(%r15,%r12,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmovss	4(%r15,%r12,4), %xmm13          # xmm13 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r15,%r13,4), %xmm11, %xmm11 # xmm11 = xmm11[0],mem[0],xmm11[2,3]
	vinsertps	$16, 8(%r15,%r13,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vblendvps	%ymm7, %ymm1, %ymm2, %ymm1
	vmovss	8(%r15,%r12,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r15,%r13,4), %xmm8, %xmm8 # xmm8 = xmm8[0],mem[0],xmm8[2,3]
	vmovq	%xmm10, %r13
	vmovss	12(%r15,%r12,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm10, %r12
	movq	%r9, %rcx
	vmovups	32(%r11,%r9,4), %ymm10
	movq	128(%rsp), %rdx                 # 8-byte Reload
	vmulps	32(%r11,%rdx,4), %ymm10, %ymm10
	vinsertps	$32, (%r15,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	32(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, (%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	80(%rsp), %r9                   # 8-byte Reload
	vinsertps	$32, (%r15,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$32, 4(%r15,%r14,4), %xmm11, %xmm11 # xmm11 = xmm11[0,1],mem[0],xmm11[3]
	vinsertps	$48, (%r15,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$16, 4(%r15,%rax,4), %xmm13, %xmm13 # xmm13 = xmm13[0],mem[0],xmm13[2,3]
	movq	48(%rsp), %r10                  # 8-byte Reload
	vinsertps	$48, (%r15,%r10,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 4(%r15,%r9,4), %xmm13, %xmm13 # xmm13 = xmm13[0,1],mem[0],xmm13[3]
	vinsertps	$48, 4(%r15,%r8,4), %xmm11, %xmm11 # xmm11 = xmm11[0,1,2],mem[0]
	vinsertps	$32, 8(%r15,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 4(%r15,%r10,4), %xmm13, %xmm13 # xmm13 = xmm13[0,1,2],mem[0]
	vinsertps	$16, 8(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 8(%r15,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm4, %ymm4
	vinsertps	$32, 8(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertf128	$1, %xmm11, %ymm13, %ymm5
	movq	192(%rsp), %rbx                 # 8-byte Reload
	vmovups	32(%r11,%rbx,4), %ymm11
	vinsertps	$48, 8(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmulps	%ymm5, %ymm11, %ymm5
	vfmadd231ps	%ymm4, %ymm10, %ymm5    # ymm5 = (ymm10 * ymm4) + ymm5
	vinsertf128	$1, %xmm6, %ymm2, %ymm2
	vinsertps	$32, 12(%r15,%r14,4), %xmm8, %xmm4 # xmm4 = xmm8[0,1],mem[0],xmm8[3]
	vinsertps	$16, 12(%r15,%rax,4), %xmm7, %xmm6 # xmm6 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 12(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 12(%r15,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 12(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm6, %ymm4
	vmovups	64(%r11,%rcx,4), %ymm6
	vmulps	64(%r11,%rdx,4), %ymm6, %ymm8
	vmulps	%ymm4, %ymm11, %ymm4
	movq	512(%rsp), %r10                 # 8-byte Reload
	vcmpeqps	32(%r11,%r10,4), %ymm15, %ymm7
	vmovdqa	1536(%rsp), %ymm11              # 32-byte Reload
	vpaddq	%ymm0, %ymm11, %ymm0
	vmovq	%xmm0, %r14
	vpextrq	$1, %xmm0, 576(%rsp)            # 8-byte Folded Spill
	vextracti128	$1, %ymm0, %xmm0
	vfmadd231ps	%ymm2, %ymm10, %ymm4    # ymm4 = (ymm10 * ymm2) + ymm4
	movq	256(%rsp), %rax                 # 8-byte Reload
	vmovss	(%r15,%rax,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmovss	4(%r15,%rax,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	movq	%rax, %rbx
	vmovq	%xmm0, %r8
	vinsertps	$16, (%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vblendvps	%ymm7, %ymm5, %ymm4, %ymm10
	movq	640(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%r15,%rdx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vmovss	4(%r15,%rdx,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm0, %rcx
	movq	480(%rsp), %r9                  # 8-byte Reload
	vinsertps	$16, (%r15,%r9,4), %xmm5, %xmm0 # xmm0 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%r15,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$16, 4(%r15,%rsi,4), %xmm6, %xmm5 # xmm5 = xmm6[0],mem[0],xmm6[2,3]
	movq	160(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$32, (%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 4(%r15,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, (%r15,%r12,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 4(%r15,%r9,4), %xmm7, %xmm6 # xmm6 = xmm7[0],mem[0],xmm7[2,3]
	movq	112(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, (%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 4(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 4(%r15,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovss	8(%r15,%rbx,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$48, 4(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vmovss	12(%r15,%rbx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	movq	192(%rsp), %rbx                 # 8-byte Reload
	vmovups	64(%r11,%rbx,4), %ymm6
	vmulps	%ymm5, %ymm6, %ymm5
	vfmadd231ps	%ymm0, %ymm8, %ymm5     # ymm5 = (ymm8 * ymm0) + ymm5
	vmovss	8(%r15,%rdx,4), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r15,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$16, 8(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 8(%r15,%r13,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$32, 8(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 8(%r15,%r12,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$48, 8(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm0, %ymm0
	vmovss	12(%r15,%rdx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 12(%r15,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	352(%rsp), %r13                 # 8-byte Reload
	vinsertps	$16, 12(%r15,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 12(%r15,%r12,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 12(%r15,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 12(%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm7, %ymm2
	movq	16(%rsp), %rax                  # 8-byte Reload
	vmovups	96(%r11,%rax,4), %ymm7
	movq	128(%rsp), %rax                 # 8-byte Reload
	vmulps	96(%r11,%rax,4), %ymm7, %ymm7
	vmulps	%ymm2, %ymm6, %ymm2
	vcmpeqps	64(%r11,%r10,4), %ymm15, %ymm6
	vpaddq	%ymm3, %ymm11, %ymm3
	vmovq	%xmm3, %rax
	vpextrq	$1, %xmm3, %rsi
	vextracti128	$1, %ymm3, %xmm3
	vfmadd231ps	%ymm0, %ymm8, %ymm2     # ymm2 = (ymm8 * ymm0) + ymm2
	vmovss	(%r15,%rax,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vmovss	4(%r15,%rax,4), %xmm4           # xmm4 = mem[0],zero,zero,zero
	vmovq	%xmm3, %rdi
	vinsertps	$16, (%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vblendvps	%ymm6, %ymm5, %ymm2, %ymm2
	vmovss	(%r15,%r14,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vmovss	4(%r15,%r14,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm3, %rbx
	movq	576(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, (%r15,%rdx,4), %xmm5, %xmm3 # xmm3 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, (%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$16, 4(%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, (%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 4(%r15,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, (%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 4(%r15,%rdx,4), %xmm6, %xmm5 # xmm5 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, (%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 4(%r15,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 4(%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vmovss	8(%r15,%rax,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vinsertps	$48, 4(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmovss	12(%r15,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertf128	$1, %xmm4, %ymm5, %ymm4
	movq	192(%rsp), %rax                 # 8-byte Reload
	vmovups	96(%r11,%rax,4), %ymm5
	vmulps	%ymm4, %ymm5, %ymm4
	vfmadd231ps	%ymm0, %ymm7, %ymm4     # ymm4 = (ymm7 * ymm0) + ymm4
	vmovss	8(%r15,%r14,4), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r15,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$16, 8(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 8(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$32, 8(%r15,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 8(%r15,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$48, 8(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm0, %ymm0
	vmovss	12(%r15,%r14,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r15,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 12(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, 12(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 12(%r15,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 12(%r15,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 12(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	movq	368(%rsp), %rcx                 # 8-byte Reload
	vmovups	(%r11,%rcx,4), %ymm6
	vmulps	%ymm3, %ymm5, %ymm3
	vmovups	32(%r11,%rcx,4), %ymm5
	vfmadd231ps	%ymm0, %ymm7, %ymm3     # ymm3 = (ymm7 * ymm0) + ymm3
	vmovups	64(%r11,%rcx,4), %ymm0
	movq	768(%rsp), %rax                 # 8-byte Reload
	vmulps	(%r11,%rax,4), %ymm6, %ymm6
	vmulps	32(%r11,%rax,4), %ymm5, %ymm5
	vmulps	64(%r11,%rax,4), %ymm0, %ymm0
	vcmpeqps	96(%r11,%r10,4), %ymm15, %ymm7
	vblendvps	%ymm7, %ymm4, %ymm3, %ymm3
	vmovups	96(%r11,%rcx,4), %ymm4
	vmulps	96(%r11,%rax,4), %ymm4, %ymm4
	vmaxps	%ymm14, %ymm6, %ymm6
	vmaxps	%ymm14, %ymm5, %ymm5
	vmovaps	1248(%rsp), %ymm7               # 32-byte Reload
	vdivps	%ymm7, %ymm6, %ymm6
	vmaxps	%ymm14, %ymm0, %ymm0
	vdivps	%ymm7, %ymm5, %ymm5
	vmaxps	%ymm14, %ymm4, %ymm4
	vdivps	%ymm7, %ymm0, %ymm0
	vdivps	%ymm7, %ymm4, %ymm4
	vroundps	$10, %ymm6, %ymm7
	vmulps	%ymm1, %ymm7, %ymm1
	vroundps	$10, %ymm5, %ymm7
	vmulps	%ymm7, %ymm10, %ymm7
	vroundps	$10, %ymm0, %ymm8
	vmulps	%ymm2, %ymm8, %ymm2
	vroundps	$10, %ymm4, %ymm8
	vmulps	%ymm3, %ymm8, %ymm3
	vmaxps	%ymm14, %ymm6, %ymm6
	vdivps	%ymm6, %ymm1, %ymm1
	vmaxps	%ymm14, %ymm5, %ymm5
	vdivps	%ymm5, %ymm7, %ymm5
	vmaxps	%ymm14, %ymm0, %ymm0
	vdivps	%ymm0, %ymm2, %ymm0
	vmaxps	%ymm14, %ymm4, %ymm2
	vdivps	%ymm2, %ymm3, %ymm2
	vmovaps	224(%rsp), %ymm3                # 32-byte Reload
	vaddps	%ymm1, %ymm3, %ymm3
	vmovaps	%ymm3, 224(%rsp)                # 32-byte Spill
	vmovaps	320(%rsp), %ymm1                # 32-byte Reload
	vaddps	%ymm5, %ymm1, %ymm1
	vmovaps	%ymm1, 320(%rsp)                # 32-byte Spill
	vmovaps	384(%rsp), %ymm1                # 32-byte Reload
	vaddps	%ymm0, %ymm1, %ymm1
	vmovaps	%ymm1, 384(%rsp)                # 32-byte Spill
	vmovaps	288(%rsp), %ymm0                # 32-byte Reload
	vaddps	%ymm2, %ymm0, %ymm0
	vmovaps	%ymm0, 288(%rsp)                # 32-byte Spill
	vpbroadcastq	.LCPI0_69(%rip), %ymm0  # ymm0 = [32,32,32,32]
	vpaddq	%ymm0, %ymm12, %ymm12
	vpaddq	%ymm0, %ymm9, %ymm9
	addq	%r13, %r11
	addq	$-32, 1152(%rsp)                # 8-byte Folded Spill
	jne	.LBB0_180
# %bb.181:                              # %middle.block1190
                                        #   in Loop: Header=BB0_177 Depth=1
	vmovaps	320(%rsp), %ymm0                # 32-byte Reload
	vaddps	224(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vaddps	384(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vaddps	288(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm0
	movq	1600(%rsp), %r12                # 8-byte Reload
	movq	%r12, %rax
	cmpq	104(%rsp), %r12                 # 8-byte Folded Reload
	vmovaps	608(%rsp), %xmm5                # 16-byte Reload
	movb	1664(%rsp), %r14b               # 1-byte Reload
	movq	1440(%rsp), %r11                # 8-byte Reload
	vmovss	.LCPI0_48(%rip), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vxorps	%xmm6, %xmm6, %xmm6
	jne	.LBB0_183
	jmp	.LBB0_185
	.p2align	4, 0x90
.LBB0_182:                              #   in Loop: Header=BB0_177 Depth=1
	vxorps	%xmm0, %xmm0, %xmm0
	xorl	%eax, %eax
	movq	1440(%rsp), %r11                # 8-byte Reload
.LBB0_183:                              # %"for f5.s1.r77$x.preheader1303"
                                        #   in Loop: Header=BB0_177 Depth=1
	movq	104(%rsp), %rcx                 # 8-byte Reload
	subq	%rax, %rcx
	movq	176(%rsp), %rdx                 # 8-byte Reload
	imulq	%rax, %rdx
	movq	1472(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %r8
	movq	1216(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %r9
	movq	1504(%rsp), %rsi                # 8-byte Reload
	leaq	(%rsi,%rdx), %r10
	shlq	$7, %rax
	addq	832(%rsp), %rax                 # 8-byte Folded Reload
	movq	1120(%rsp), %rsi                # 8-byte Reload
	addq	%rdx, %rsi
	movq	1312(%rsp), %rdi                # 8-byte Reload
	addq	%rdx, %rdi
	addq	896(%rsp), %rdx                 # 8-byte Folded Reload
	movq	440(%rsp), %rbx                 # 8-byte Reload
	.p2align	4, 0x90
.LBB0_184:                              # %"for f5.s1.r77$x"
                                        #   Parent Loop BB0_177 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	(%rbx,%r8,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	vmovss	(%rbx,%rdx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	vmulss	(%rbx,%rdi,4), %xmm2, %xmm2
	vmulss	(%rbx,%r9,4), %xmm1, %xmm1
	vmaxss	%xmm7, %xmm2, %xmm2
	vdivss	%xmm5, %xmm2, %xmm2
	vmovss	(%rbx,%r10,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vmulss	4(%rax), %xmm3, %xmm4
	vfmadd231ss	(%rax), %xmm1, %xmm4    # xmm4 = (xmm1 * mem) + xmm4
	vmulss	12(%rax), %xmm3, %xmm3
	vfmadd231ss	8(%rax), %xmm1, %xmm3   # xmm3 = (xmm1 * mem) + xmm3
	vcmpeqss	(%rbx,%rsi,4), %xmm6, %xmm1
	vblendvps	%xmm1, %xmm4, %xmm3, %xmm1
	vroundss	$10, %xmm2, %xmm2, %xmm3
	vmaxss	%xmm7, %xmm2, %xmm2
	vmulss	%xmm1, %xmm3, %xmm1
	vdivss	%xmm2, %xmm1, %xmm1
	vaddss	%xmm1, %xmm0, %xmm0
	addq	%r11, %rbx
	subq	$-128, %rax
	decq	%rcx
	jne	.LBB0_184
.LBB0_185:                              # %"consume f5.loopexit"
                                        #   in Loop: Header=BB0_177 Depth=1
	vmovd	%xmm0, %eax
	movq	1184(%rsp), %rsi                # 8-byte Reload
.LBB0_186:                              # %"consume f5"
                                        #   in Loop: Header=BB0_177 Depth=1
	movl	%r14d, %r10d
	movq	864(%rsp), %r14                 # 8-byte Reload
	movq	%r14, %rcx
	subq	1768(%rsp), %rcx                # 8-byte Folded Reload
	movq	2680(%rsp), %rdx                # 8-byte Reload
	movl	%eax, (%rdx,%rcx,4)
	incq	%r14
	addq	$4, %rsi
	incq	1472(%rsp)                      # 8-byte Folded Spill
	incq	1216(%rsp)                      # 8-byte Folded Spill
	incq	1504(%rsp)                      # 8-byte Folded Spill
	movq	560(%rsp), %rax                 # 8-byte Reload
	addq	%rax, 832(%rsp)                 # 8-byte Folded Spill
	incq	1120(%rsp)                      # 8-byte Folded Spill
	incq	1312(%rsp)                      # 8-byte Folded Spill
	incq	896(%rsp)                       # 8-byte Folded Spill
	movq	%r14, 864(%rsp)                 # 8-byte Spill
	cmpl	%r14d, 2044(%rsp)               # 4-byte Folded Reload
	movl	%r10d, %r14d
	jne	.LBB0_177
# %bb.187:                              # %call_destructor.exit386.loopexit
	vmovss	%xmm0, 3424(%rsp)
.LBB0_188:                              # %call_destructor.exit386
	xorl	%r14d, %r14d
	xorl	%edi, %edi
	movq	%r15, %rsi
	vzeroupper
	callq	halide_free@PLT
.LBB0_189:                              # %call_destructor.exit340.thread407
	xorl	%r12d, %r12d
.LBB0_190:                              # %call_destructor.exit340.thread407
	testl	%r12d, %r12d
	sete	%bl
.LBB0_191:                              # %call_destructor.exit341
	testq	%r14, %r14
	je	.LBB0_194
# %bb.192:                              # %call_destructor.exit341
	testb	%bl, %bl
	jne	.LBB0_194
# %bb.193:
	xorl	%edi, %edi
	movq	%r14, %rsi
	callq	halide_free@PLT
.LBB0_194:                              # %call_destructor.exit342
	movl	%r12d, %eax
.LBB0_195:                              # %call_destructor.exit342
	leaq	-40(%rbp), %rsp
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	.cfi_def_cfa %rsp, 8
	retq
.LBB0_196:                              #   in Loop: Header=BB0_198 Depth=1
	.cfi_def_cfa %rbp, 16
	vxorps	%xmm10, %xmm10, %xmm10
	.p2align	4, 0x90
.LBB0_197:                              # %"consume f7"
                                        #   in Loop: Header=BB0_198 Depth=1
	movq	1976(%rsp), %rax                # 8-byte Reload
	movq	1408(%rsp), %rcx                # 8-byte Reload
	vmovss	%xmm10, (%rax,%rcx,4)
	incq	%rcx
	movq	752(%rsp), %r12                 # 8-byte Reload
	addq	$4, %r12
	incq	688(%rsp)                       # 8-byte Folded Spill
	incq	1704(%rsp)                      # 8-byte Folded Spill
	incq	1712(%rsp)                      # 8-byte Folded Spill
	incq	1720(%rsp)                      # 8-byte Folded Spill
	incq	1728(%rsp)                      # 8-byte Folded Spill
	incq	1040(%rsp)                      # 8-byte Folded Spill
	incq	728(%rsp)                       # 8-byte Folded Spill
	incq	1736(%rsp)                      # 8-byte Folded Spill
	incq	736(%rsp)                       # 8-byte Folded Spill
	incq	1744(%rsp)                      # 8-byte Folded Spill
	incq	672(%rsp)                       # 8-byte Folded Spill
	incq	1048(%rsp)                      # 8-byte Folded Spill
	incq	1056(%rsp)                      # 8-byte Folded Spill
	incq	1064(%rsp)                      # 8-byte Folded Spill
	incq	1400(%rsp)                      # 8-byte Folded Spill
	incq	744(%rsp)                       # 8-byte Folded Spill
	incq	1072(%rsp)                      # 8-byte Folded Spill
	incq	1080(%rsp)                      # 8-byte Folded Spill
	incq	432(%rsp)                       # 8-byte Folded Spill
	incq	1088(%rsp)                      # 8-byte Folded Spill
	incq	1096(%rsp)                      # 8-byte Folded Spill
	incq	680(%rsp)                       # 8-byte Folded Spill
	movq	1960(%rsp), %rax                # 8-byte Reload
	addq	%rax, 696(%rsp)                 # 8-byte Folded Spill
	cmpq	1968(%rsp), %rcx                # 8-byte Folded Reload
	je	.LBB0_130
.LBB0_198:                              # %"for prediction_output.s0.n.v13"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_201 Depth 2
                                        #     Child Loop BB0_206 Depth 2
	cmpl	$0, 72(%rsp)                    # 4-byte Folded Reload
	movq	%rcx, 1408(%rsp)                # 8-byte Spill
	movq	%r12, 752(%rsp)                 # 8-byte Spill
	jle	.LBB0_196
# %bb.199:                              # %"for f7.s1.r77$x.preheader"
                                        #   in Loop: Header=BB0_198 Depth=1
	cmpb	$0, 764(%rsp)                   # 1-byte Folded Reload
	je	.LBB0_203
# %bb.200:                              # %vector.ph909
                                        #   in Loop: Header=BB0_198 Depth=1
	movq	%rcx, %rax
	subq	280(%rsp), %rax                 # 8-byte Folded Reload
	imulq	1952(%rsp), %rax                # 8-byte Folded Reload
	vmovq	%rax, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqa	%ymm0, 2208(%rsp)               # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 2272(%rsp)               # 32-byte Spill
	movq	%r12, 976(%rsp)                 # 8-byte Spill
	movq	1032(%rsp), %rax                # 8-byte Reload
	movq	%rax, 1784(%rsp)                # 8-byte Spill
	vmovdqa	.LCPI0_62(%rip), %ymm2          # ymm2 = [0,1,2,3]
	vmovdqa	.LCPI0_61(%rip), %ymm4          # ymm4 = [4,5,6,7]
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 2240(%rsp)               # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 2080(%rsp)               # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqa	%ymm0, 2048(%rsp)               # 32-byte Spill
	.p2align	4, 0x90
.LBB0_201:                              # %vector.body905
                                        #   Parent Loop BB0_198 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vpbroadcastq	.LCPI0_63(%rip), %ymm1  # ymm1 = [8,8,8,8]
	vpbroadcastq	.LCPI0_64(%rip), %ymm0  # ymm0 = [16,16,16,16]
	vpxor	%xmm7, %xmm7, %xmm7
	vpcmpeqq	%ymm7, %ymm2, %ymm3
	vmovdqa	%ymm2, %ymm5
	vmovdqa	%ymm4, %ymm6
	vpcmpeqq	%ymm7, %ymm4, %ymm4
	vperm2i128	$49, %ymm4, %ymm3, %ymm10 # ymm10 = ymm3[2,3],ymm4[2,3]
	vinserti128	$1, %xmm4, %ymm3, %ymm8
	vpsllq	$5, %ymm2, %ymm3
	vmovdqa	%ymm2, %ymm9
	vpsllq	$5, %ymm6, %ymm4
	vmovdqa	2208(%rsp), %ymm2               # 32-byte Reload
	vpaddq	%ymm2, %ymm4, %ymm5
	vpaddq	%ymm2, %ymm3, %ymm3
	vmovq	%xmm3, %rcx
	vpextrq	$1, %xmm3, %rdx
	vextracti128	$1, %ymm3, %xmm4
	vmovq	%xmm5, %rsi
	vpextrq	$1, %xmm5, %rdi
	vextracti128	$1, %ymm5, %xmm5
	vmovss	68(%r15,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vpextrq	$1, %xmm4, %rbx
	vinsertps	$16, 68(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vpextrq	$1, %xmm5, %r8
	vmovq	%xmm5, %rax
	vinsertps	$32, 68(%r15,%rax,4), %xmm3, %xmm7 # xmm7 = xmm3[0,1],mem[0],xmm3[3]
	vmovdqa	%ymm6, %ymm5
	vpaddq	%ymm1, %ymm6, %ymm11
	vmovss	68(%r15,%rcx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%r15,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovdqa	%ymm9, %ymm2
	vpaddq	%ymm1, %ymm9, %ymm6
	vmovq	%xmm4, %r9
	vinsertps	$32, 68(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vpaddq	%ymm0, %ymm5, %ymm12
	vmovdqa	%ymm5, %ymm1
	vmovdqa	%ymm5, 2592(%rsp)               # 32-byte Spill
	vmovss	72(%r15,%rsi,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%r15,%rdi,4), %xmm4, %xmm5 # xmm5 = xmm4[0],mem[0],xmm4[2,3]
	vpaddq	%ymm0, %ymm9, %ymm15
	vmovdqa	%ymm9, 2560(%rsp)               # 32-byte Spill
	vinsertps	$32, 72(%r15,%rax,4), %xmm5, %xmm0 # xmm0 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 68(%r15,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vpackssdw	%ymm10, %ymm8, %ymm4
	vmovdqa	%ymm4, 192(%rsp)                # 32-byte Spill
	vmovss	72(%r15,%rcx,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$48, 68(%r15,%rbx,4), %xmm3, %xmm8 # xmm8 = xmm3[0,1,2],mem[0]
	vinsertps	$16, 72(%r15,%rdx,4), %xmm5, %xmm3 # xmm3 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 72(%r15,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	%xmm0, 128(%rsp)                # 16-byte Spill
	vinsertps	$32, 72(%r15,%r9,4), %xmm3, %xmm0 # xmm0 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 72(%r15,%rbx,4), %xmm0, %xmm13 # xmm13 = xmm0[0,1,2],mem[0]
	vmovss	80(%r15,%rsi,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 80(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vxorps	%xmm3, %xmm3, %xmm3
	vpcmpeqq	%ymm3, %ymm6, %ymm5
	vmovdqa	%ymm11, %ymm4
	vpcmpeqq	%ymm3, %ymm11, %ymm10
	vinsertps	$32, 80(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpcmpeqq	%ymm3, %ymm15, %ymm11
	vpcmpeqq	%ymm3, %ymm12, %ymm14
	movq	%r8, 928(%rsp)                  # 8-byte Spill
	vinsertps	$48, 80(%r15,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vperm2i128	$49, %ymm10, %ymm5, %ymm3 # ymm3 = ymm5[2,3],ymm10[2,3]
	vmovdqa	%ymm3, 224(%rsp)                # 32-byte Spill
	vmovss	80(%r15,%rcx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 80(%r15,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinserti128	$1, %xmm10, %ymm5, %ymm9
	vperm2i128	$49, %ymm14, %ymm11, %ymm5 # ymm5 = ymm11[2,3],ymm14[2,3]
	vmovdqa	%ymm5, 320(%rsp)                # 32-byte Spill
	vinsertps	$32, 80(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinserti128	$1, %xmm14, %ymm11, %ymm5
	vmovdqa	%ymm5, 288(%rsp)                # 32-byte Spill
	vpbroadcastq	.LCPI0_65(%rip), %ymm14 # ymm14 = [24,24,24,24]
	movq	%rbx, 1344(%rsp)                # 8-byte Spill
	vinsertps	$48, 80(%r15,%rbx,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm8, %ymm10
	vmovss	84(%r15,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertf128	$1, 128(%rsp), %ymm13, %ymm13 # 16-byte Folded Reload
	vpaddq	%ymm1, %ymm14, %ymm8
	vinsertps	$32, 84(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertf128	$1, %xmm0, %ymm5, %ymm7
	vmovss	84(%r15,%rcx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	%rcx, 480(%rsp)                 # 8-byte Spill
	movq	%rdx, 640(%rsp)                 # 8-byte Spill
	vinsertps	$16, 84(%r15,%rdx,4), %xmm0, %xmm1 # xmm1 = xmm0[0],mem[0],xmm0[2,3]
	vpaddq	%ymm2, %ymm14, %ymm0
	vinsertps	$32, 84(%r15,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%r9, 1280(%rsp)                 # 8-byte Spill
	vinsertps	$48, 84(%r15,%r8,4), %xmm3, %xmm2 # xmm2 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 84(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovaps	192(%rsp), %ymm5                # 32-byte Reload
	vblendvps	%ymm5, %ymm10, %ymm13, %ymm3
	vmovaps	%ymm3, 1664(%rsp)               # 32-byte Spill
	vxorps	%xmm3, %xmm3, %xmm3
	vpcmpeqq	%ymm3, %ymm0, %ymm11
	vinsertf128	$1, %xmm2, %ymm1, %ymm14
	vpcmpeqq	%ymm3, %ymm8, %ymm10
	vpsllq	$5, %ymm6, %ymm2
	vpsllq	$5, %ymm4, %ymm6
	vpsllq	$5, %ymm15, %ymm4
	vpsllq	$5, %ymm12, %ymm15
	vpsllq	$5, %ymm0, %ymm13
	vmovdqa	2208(%rsp), %ymm3               # 32-byte Reload
	vpaddq	%ymm3, %ymm6, %ymm6
	vpaddq	%ymm3, %ymm2, %ymm2
	vpaddq	%ymm3, %ymm15, %ymm1
	vpaddq	%ymm3, %ymm4, %ymm15
	vblendvps	%ymm5, %ymm7, %ymm14, %ymm0
	vmovaps	%ymm0, 2144(%rsp)               # 32-byte Spill
	movq	%rsi, 864(%rsp)                 # 8-byte Spill
	vmovss	(%r15,%rsi,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vmovq	%xmm2, %r12
	movq	%r12, 128(%rsp)                 # 8-byte Spill
	vinsertps	$16, (%r15,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%rdi, 48(%rsp)                  # 8-byte Spill
	vpextrq	$1, %xmm2, %r10
	movq	%r10, 16(%rsp)                  # 8-byte Spill
	vextracti128	$1, %ymm2, %xmm2
	movq	%rax, 1120(%rsp)                # 8-byte Spill
	vinsertps	$32, (%r15,%rax,4), %xmm4, %xmm0 # xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovaps	%xmm0, 1792(%rsp)               # 16-byte Spill
	vmovq	%xmm6, %rbx
	vmovss	(%r15,%rcx,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, (%r15,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpackssdw	224(%rsp), %ymm9, %ymm12 # 32-byte Folded Reload
	vextracti128	$1, %ymm6, %xmm0
	vinsertps	$32, (%r15,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovdqa	288(%rsp), %ymm4                # 32-byte Reload
	vpackssdw	320(%rsp), %ymm4, %ymm9 # 32-byte Folded Reload
	vmovss	4(%r15,%rsi,4), %xmm4           # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r15,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovq	%xmm2, %rsi
	movq	%rsi, 320(%rsp)                 # 8-byte Spill
	vinsertps	$32, 4(%r15,%rax,4), %xmm4, %xmm14 # xmm14 = xmm4[0,1],mem[0],xmm4[3]
	vpextrq	$1, %xmm2, 576(%rsp)            # 8-byte Folded Spill
	vextracti128	$1, %ymm15, %xmm2
	vpextrq	$1, %xmm6, %rax
	vextracti128	$1, %ymm1, %xmm4
	vmovq	%xmm0, %rdi
	movq	%rdi, 112(%rsp)                 # 8-byte Spill
	vpextrq	$1, %xmm0, %r11
	vmovss	68(%r15,%rbx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	%rbx, %r9
	movq	%rbx, 384(%rsp)                 # 8-byte Spill
	vinsertps	$16, 68(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	%rax, %r8
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	vpextrq	$1, %xmm15, %rcx
	movq	%rcx, 160(%rsp)                 # 8-byte Spill
	vmovq	%xmm15, %r13
	vmovss	68(%r15,%r12,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$16, 68(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vpextrq	$1, %xmm1, %rdx
	movq	%rdx, 512(%rsp)                 # 8-byte Spill
	vmovq	%xmm1, %rax
	vmovss	68(%r15,%rax,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	%rax, %r14
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	vinsertps	$32, 68(%r15,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$16, 68(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpextrq	$1, %xmm4, %rsi
	vmovq	%xmm4, %rax
	vmovss	68(%r15,%r13,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%rax, %rdx
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	vinsertps	$16, 68(%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrq	$1, %xmm2, %rax
	vmovq	%xmm2, %rbx
	vmovss	72(%r15,%r9,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%rbx, 1248(%rsp)                # 8-byte Spill
	vinsertps	$16, 72(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%r11, 192(%rsp)                 # 8-byte Spill
	vinsertps	$48, 68(%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 72(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	576(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 68(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm6, %ymm6
	vmovss	72(%r15,%r12,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$48, 68(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$16, 72(%r15,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 68(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	320(%rsp), %r12                 # 8-byte Reload
	vinsertps	$32, 72(%r15,%r12,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 72(%r15,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm4, %ymm4
	vinsertps	$48, 72(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm1
	vmovss	72(%r15,%r14,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	72(%r15,%r13,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	%r13, %r8
	movq	512(%rsp), %r11                 # 8-byte Reload
	vinsertps	$16, 72(%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$16, 72(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 72(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 72(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 72(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	%rsi, %r13
	vpsllq	$5, %ymm8, %ymm8
	vinsertps	$48, 72(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	%rax, %r9
	vinsertf128	$1, %xmm0, %ymm2, %ymm2
	movq	480(%rsp), %rax                 # 8-byte Reload
	vmovss	4(%r15,%rax,4), %xmm0           # xmm0 = mem[0],zero,zero,zero
	movq	640(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 4(%r15,%rax,4), %xmm0, %xmm15 # xmm15 = xmm0[0],mem[0],xmm0[2,3]
	vpaddq	%ymm3, %ymm8, %ymm8
	vpaddq	%ymm3, %ymm13, %ymm13
	vblendvps	%ymm12, %ymm6, %ymm1, %ymm0
	vmovaps	%ymm0, 2368(%rsp)               # 32-byte Spill
	vextracti128	$1, %ymm13, %xmm6
	vmovq	%xmm13, %r10
	vpextrq	$1, %xmm13, %rdx
	vextracti128	$1, %ymm8, %xmm0
	vmovq	%xmm8, %rcx
	movq	%rcx, 1536(%rsp)                # 8-byte Spill
	vpextrq	$1, %xmm8, %r14
	movq	%r14, 288(%rsp)                 # 8-byte Spill
	vmovss	68(%r15,%rcx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%r15,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpextrq	$1, %xmm0, %rsi
	vmovq	%xmm0, %rax
	movq	%rax, 1184(%rsp)                # 8-byte Spill
	vmovss	68(%r15,%r10,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	%r10, 832(%rsp)                 # 8-byte Spill
	vinsertps	$32, 68(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$16, 68(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	%rdx, %rdi
	movq	%rdx, 896(%rsp)                 # 8-byte Spill
	vpextrq	$1, %xmm6, %rdx
	vmovq	%xmm6, %rbx
	movq	%rbx, 768(%rsp)                 # 8-byte Spill
	vmovss	72(%r15,%rcx,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$16, 72(%r15,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	%rsi, 1152(%rsp)                # 8-byte Spill
	vinsertps	$48, 68(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 72(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rdx, 368(%rsp)                 # 8-byte Spill
	vinsertps	$48, 68(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 72(%r15,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vblendvps	%ymm9, %ymm4, %ymm2, %ymm2
	vmovaps	%ymm2, 1600(%rsp)               # 32-byte Spill
	vmovss	72(%r15,%r10,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vperm2i128	$49, %ymm10, %ymm11, %ymm4 # ymm4 = ymm11[2,3],ymm10[2,3]
	vinserti128	$1, %xmm10, %ymm11, %ymm3
	vinsertps	$32, 72(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	384(%rsp), %rcx                 # 8-byte Reload
	vmovss	80(%r15,%rcx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	224(%rsp), %r10                 # 8-byte Reload
	vinsertps	$16, 80(%r15,%r10,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 72(%r15,%rdx,4), %xmm2, %xmm8 # xmm8 = xmm2[0,1,2],mem[0]
	vpackssdw	%ymm4, %ymm3, %ymm13
	movq	112(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$32, 80(%r15,%rdi,4), %xmm7, %xmm3 # xmm3 = xmm7[0,1],mem[0],xmm7[3]
	movq	128(%rsp), %rax                 # 8-byte Reload
	vmovss	80(%r15,%rax,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	16(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 80(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	192(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 80(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vinsertps	$32, 80(%r15,%r12,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	movq	80(%rsp), %r12                  # 8-byte Reload
	vmovss	80(%r15,%r12,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	%r11, %rdx
	vinsertps	$16, 80(%r15,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	576(%rsp), %r11                 # 8-byte Reload
	vinsertps	$48, 80(%r15,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm8, %ymm6
	movq	32(%rsp), %r14                  # 8-byte Reload
	vinsertps	$32, 80(%r15,%r14,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r8, %rbx
	movq	%r8, 1312(%rsp)                 # 8-byte Spill
	vmovss	80(%r15,%r8,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	movq	160(%rsp), %r8                  # 8-byte Reload
	vinsertps	$16, 80(%r15,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	%r13, 352(%rsp)                 # 8-byte Spill
	vinsertps	$48, 80(%r15,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	1248(%rsp), %rsi                # 8-byte Reload
	vinsertps	$32, 80(%r15,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	%r9, 256(%rsp)                  # 8-byte Spill
	vinsertps	$48, 80(%r15,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm1, %ymm1
	vinsertf128	$1, %xmm4, %ymm7, %ymm4
	vblendvps	%ymm13, %ymm0, %ymm6, %ymm0
	vmovaps	%ymm0, 2816(%rsp)               # 32-byte Spill
	vmovss	84(%r15,%rcx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	128(%rsp), %rcx                 # 8-byte Reload
	vmovss	84(%r15,%rcx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%r15,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	16(%rsp), %rcx                  # 8-byte Reload
	vinsertps	$16, 84(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 84(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	320(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$32, 84(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 84(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 84(%r15,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmovss	84(%r15,%r12,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%r15,%rdx,4), %xmm3, %xmm6 # xmm6 = xmm3[0],mem[0],xmm3[2,3]
	vblendvps	%ymm12, %ymm1, %ymm0, %ymm0
	vmovaps	%ymm0, 3040(%rsp)               # 32-byte Spill
	movq	1536(%rsp), %rcx                # 8-byte Reload
	vmovss	80(%r15,%rcx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	84(%r15,%rbx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$32, 84(%r15,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$16, 84(%r15,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 84(%r15,%r13,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 84(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 84(%r15,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm1, %ymm1
	movq	832(%rsp), %r10                 # 8-byte Reload
	vmovss	80(%r15,%r10,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	288(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 80(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	896(%rsp), %r11                 # 8-byte Reload
	vinsertps	$16, 80(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	1184(%rsp), %rsi                # 8-byte Reload
	vinsertps	$32, 80(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	768(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$32, 80(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vblendvps	%ymm9, %ymm4, %ymm1, %ymm1
	vmovaps	%ymm1, 2112(%rsp)               # 32-byte Spill
	movq	480(%rsp), %r9                  # 8-byte Reload
	vmovss	8(%r15,%r9,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmovss	84(%r15,%rcx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	1152(%rsp), %rcx                # 8-byte Reload
	vinsertps	$48, 80(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 84(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	368(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 80(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm6, %ymm0
	vmovss	84(%r15,%r10,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$32, 84(%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$16, 84(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 84(%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 84(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 84(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm6, %ymm4
	movq	864(%rsp), %r14                 # 8-byte Reload
	vmovss	8(%r15,%r14,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	movq	1280(%rsp), %rcx                # 8-byte Reload
	vinsertps	$32, 4(%r15,%rcx,4), %xmm15, %xmm7 # xmm7 = xmm15[0,1],mem[0],xmm15[3]
	movq	48(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, 8(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	928(%rsp), %rax                 # 8-byte Reload
	vmovaps	1792(%rsp), %xmm2               # 16-byte Reload
	vinsertps	$48, (%r15,%rax,4), %xmm2, %xmm3 # xmm3 = xmm2[0,1,2],mem[0]
	movq	1120(%rsp), %rsi                # 8-byte Reload
	vinsertps	$32, 8(%r15,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	1344(%rsp), %rbx                # 8-byte Reload
	vinsertps	$48, (%r15,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	640(%rsp), %r12                 # 8-byte Reload
	vinsertps	$16, 8(%r15,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 4(%r15,%rax,4), %xmm14, %xmm2 # xmm2 = xmm14[0,1,2],mem[0]
	vinsertps	$32, 8(%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vblendvps	%ymm13, %ymm0, %ymm4, %ymm0
	vmovaps	%ymm0, 2784(%rsp)               # 32-byte Spill
	vmovss	12(%r15,%r14,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$48, 4(%r15,%rbx,4), %xmm7, %xmm4 # xmm4 = xmm7[0,1,2],mem[0]
	vinsertps	$16, 12(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 8(%r15,%rax,4), %xmm6, %xmm7 # xmm7 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 12(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 8(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	976(%rsp), %r14                 # 8-byte Reload
	movq	960(%rsp), %rdx                 # 8-byte Reload
	vmovups	(%r14,%rdx,4), %ymm8
	vmovaps	%ymm8, 1888(%rsp)               # 32-byte Spill
	vinsertps	$48, 12(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm5, %ymm3
	vmovss	12(%r15,%r9,4), %xmm5           # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r15,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	movq	1112(%rsp), %r13                # 8-byte Reload
	vmovups	(%r14,%r13,4), %ymm6
	vinsertps	$32, 12(%r15,%rcx,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1],mem[0],xmm5[3]
	vmulps	%ymm2, %ymm6, %ymm2
	vinsertps	$48, 12(%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	1104(%rsp), %r12                # 8-byte Reload
	vmulps	(%r14,%r12,4), %ymm8, %ymm5
	vfmadd231ps	%ymm3, %ymm5, %ymm2     # ymm2 = (ymm5 * ymm3) + ymm2
	movq	384(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%r15,%rdx,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	movq	224(%rsp), %r8                  # 8-byte Reload
	vinsertps	$16, (%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	movq	128(%rsp), %rbx                 # 8-byte Reload
	vmovss	(%r15,%rbx,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movq	16(%rsp), %r9                   # 8-byte Reload
	vinsertps	$16, (%r15,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vmulps	%ymm0, %ymm6, %ymm0
	vmovaps	%ymm6, %ymm8
	vmovaps	%ymm6, 2336(%rsp)               # 32-byte Spill
	vxorps	%xmm4, %xmm4, %xmm4
	movq	800(%rsp), %rax                 # 8-byte Reload
	vcmpeqps	(%r14,%rax,4), %ymm4, %ymm4
	vfmadd231ps	%ymm1, %ymm5, %ymm0     # ymm0 = (ymm5 * ymm1) + ymm0
	vmovss	4(%r15,%rdx,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$32, (%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, 4(%r15,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vblendvps	%ymm4, %ymm2, %ymm0, %ymm0
	vmovaps	%ymm0, 3136(%rsp)               # 32-byte Spill
	vmovss	4(%r15,%rbx,4), %xmm0           # xmm0 = mem[0],zero,zero,zero
	movq	%rbx, %rax
	movq	320(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, (%r15,%rsi,4), %xmm7, %xmm2 # xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$16, 4(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 4(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 4(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%rsi, %r10
	movq	192(%rsp), %r11                 # 8-byte Reload
	vinsertps	$48, (%r15,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovss	8(%r15,%rdx,4), %xmm4           # xmm4 = mem[0],zero,zero,zero
	movq	%rdx, %rsi
	movq	576(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, (%r15,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 8(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 4(%r15,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 8(%r15,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	8(%r15,%rbx,4), %xmm5           # xmm5 = mem[0],zero,zero,zero
	movq	%r9, %rbx
	vinsertps	$16, 8(%r15,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 4(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 8(%r15,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 8(%r15,%r11,4), %xmm4, %xmm7 # xmm7 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vinsertps	$48, 8(%r15,%rdx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	12(%r15,%rsi,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r15,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovups	32(%r14,%r13,4), %ymm4
	vmovaps	%ymm4, 2400(%rsp)               # 32-byte Spill
	vmulps	%ymm0, %ymm4, %ymm10
	movq	960(%rsp), %rcx                 # 8-byte Reload
	vmovups	32(%r14,%rcx,4), %ymm0
	vmovaps	%ymm0, 2432(%rsp)               # 32-byte Spill
	vmulps	32(%r14,%r12,4), %ymm0, %ymm5
	vfmadd231ps	%ymm2, %ymm5, %ymm10    # ymm10 = (ymm5 * ymm2) + ymm10
	vinsertps	$32, 12(%r15,%rdi,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	12(%r15,%rax,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r15,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 12(%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 12(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 12(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm3, %ymm2
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmulps	%ymm0, %ymm4, %ymm0
	vfmadd231ps	%ymm2, %ymm5, %ymm0     # ymm0 = (ymm5 * ymm2) + ymm0
	movq	80(%rsp), %r8                   # 8-byte Reload
	vmovss	(%r15,%r8,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	movq	512(%rsp), %r10                 # 8-byte Reload
	vinsertps	$16, (%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	1312(%rsp), %rbx                # 8-byte Reload
	vmovss	(%r15,%rbx,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	movq	160(%rsp), %r9                  # 8-byte Reload
	vinsertps	$16, (%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$32, (%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	1248(%rsp), %rsi                # 8-byte Reload
	vinsertps	$32, (%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	352(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, (%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	1536(%rsp), %rax                # 8-byte Reload
	vmovss	(%r15,%rax,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	movq	288(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, (%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovss	4(%r15,%r8,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r15,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	256(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, (%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vinsertps	$32, 4(%r15,%rdx,4), %xmm5, %xmm2 # xmm2 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	4(%r15,%rbx,4), %xmm5           # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r15,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 4(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 4(%r15,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 4(%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm5, %ymm2
	vmovss	8(%r15,%r8,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r15,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 8(%r15,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 8(%r15,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmovss	8(%r15,%rbx,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r15,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovups	64(%r14,%r13,4), %ymm4
	vmulps	%ymm2, %ymm4, %ymm2
	vmovaps	%ymm4, %ymm11
	vmovaps	%ymm4, 2304(%rsp)               # 32-byte Spill
	vmovups	64(%r14,%rcx,4), %ymm4
	vmovaps	%ymm4, 1920(%rsp)               # 32-byte Spill
	vmulps	64(%r14,%r12,4), %ymm4, %ymm9
	vfmadd231ps	%ymm1, %ymm9, %ymm2     # ymm2 = (ymm9 * ymm1) + ymm2
	vinsertps	$32, 8(%r15,%rsi,4), %xmm7, %xmm1 # xmm1 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	12(%r15,%r8,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r15,%r10,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 8(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm1, %ymm1
	vinsertps	$32, 12(%r15,%rdx,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	12(%r15,%rbx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r15,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 12(%r15,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 12(%r15,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 12(%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm7, %ymm5
	movq	1184(%rsp), %r11                # 8-byte Reload
	vinsertps	$32, (%r15,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	832(%rsp), %r10                 # 8-byte Reload
	vmovss	(%r15,%r10,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movq	896(%rsp), %r9                  # 8-byte Reload
	vinsertps	$16, (%r15,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	1152(%rsp), %rsi                # 8-byte Reload
	vinsertps	$48, (%r15,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	768(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$32, (%r15,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	368(%rsp), %r8                  # 8-byte Reload
	vinsertps	$48, (%r15,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	movq	1536(%rsp), %rax                # 8-byte Reload
	vmovss	4(%r15,%rax,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	movq	288(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 4(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmulps	%ymm5, %ymm11, %ymm5
	vfmadd231ps	%ymm1, %ymm9, %ymm5     # ymm5 = (ymm9 * ymm1) + ymm5
	vinsertps	$32, 4(%r15,%r11,4), %xmm6, %xmm1 # xmm1 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	4(%r15,%r10,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%r15,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 4(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm7, %ymm3
	vinsertps	$32, 4(%r15,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	8(%r15,%rax,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r15,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 4(%r15,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm6, %ymm1
	vinsertps	$32, 8(%r15,%r11,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	8(%r15,%r10,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%r15,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 8(%r15,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 8(%r15,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 8(%r15,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmovss	12(%r15,%rax,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r15,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovups	96(%r14,%r13,4), %ymm14
	vmovaps	%ymm14, 1792(%rsp)              # 32-byte Spill
	vmulps	%ymm1, %ymm14, %ymm1
	vmovups	96(%r14,%rcx,4), %ymm9
	vmovaps	%ymm9, 2464(%rsp)               # 32-byte Spill
	vmulps	96(%r14,%r12,4), %ymm9, %ymm9
	vfmadd231ps	%ymm3, %ymm9, %ymm1     # ymm1 = (ymm9 * ymm3) + ymm1
	vinsertps	$32, 12(%r15,%r11,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	12(%r15,%r10,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%r15,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 12(%r15,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 12(%r15,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 12(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm7, %ymm6
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmulps	%ymm3, %ymm14, %ymm3
	vfmadd231ps	%ymm6, %ymm9, %ymm3     # ymm3 = (ymm9 * ymm6) + ymm3
	vxorps	%xmm6, %xmm6, %xmm6
	movq	800(%rsp), %rax                 # 8-byte Reload
	vcmpeqps	32(%r14,%rax,4), %ymm6, %ymm4
	vblendvps	%ymm4, %ymm10, %ymm0, %ymm0
	vmovaps	%ymm0, 3072(%rsp)               # 32-byte Spill
	vcmpeqps	64(%r14,%rax,4), %ymm6, %ymm0
	vxorps	%xmm4, %xmm4, %xmm4
	vblendvps	%ymm0, %ymm2, %ymm5, %ymm0
	vmovaps	%ymm0, 2912(%rsp)               # 32-byte Spill
	vcmpeqps	96(%r14,%rax,4), %ymm4, %ymm0
	vblendvps	%ymm0, %ymm1, %ymm3, %ymm0
	vmovaps	%ymm0, 3104(%rsp)               # 32-byte Spill
	movq	864(%rsp), %r10                 # 8-byte Reload
	vmovss	64(%r15,%r10,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	48(%rsp), %r8                   # 8-byte Reload
	vinsertps	$16, 64(%r15,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	480(%rsp), %rcx                 # 8-byte Reload
	vmovss	64(%r15,%rcx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	640(%rsp), %r12                 # 8-byte Reload
	vinsertps	$16, 64(%r15,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	1120(%rsp), %rbx                # 8-byte Reload
	vinsertps	$32, 64(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	1280(%rsp), %r13                # 8-byte Reload
	vinsertps	$32, 64(%r15,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	928(%rsp), %r9                  # 8-byte Reload
	vinsertps	$48, 64(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	1344(%rsp), %rax                # 8-byte Reload
	vinsertps	$48, 64(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	2528(%rsp), %rdi                # 8-byte Reload
	vmovups	(%r14,%rdi,4), %ymm1
	vmovaps	2720(%rsp), %ymm11              # 32-byte Reload
	vcmpltps	%ymm1, %ymm11, %ymm2
	vmovaps	%ymm1, %ymm9
	vmovaps	1664(%rsp), %ymm1               # 32-byte Reload
	vblendvps	%ymm2, %ymm0, %ymm1, %ymm0
	vmovaps	%ymm2, %ymm7
	movq	384(%rsp), %rdx                 # 8-byte Reload
	vmovss	64(%r15,%rdx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	224(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 64(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	128(%rsp), %rax                 # 8-byte Reload
	vmovss	64(%r15,%rax,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	16(%rsp), %r11                  # 8-byte Reload
	vinsertps	$16, 64(%r15,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	112(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 64(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	320(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 64(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	192(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$48, 64(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	576(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$48, 64(%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovups	32(%r14,%rdi,4), %ymm6
	vcmpltps	%ymm6, %ymm11, %ymm2
	vmovaps	2368(%rsp), %ymm3               # 32-byte Reload
	vblendvps	%ymm2, %ymm1, %ymm3, %ymm1
	vmovaps	%ymm2, %ymm5
	vmovaps	%ymm2, 2496(%rsp)               # 32-byte Spill
	vmovss	76(%r15,%r10,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	76(%r15,%rcx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	%rcx, %rdi
	vinsertps	$16, 76(%r15,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 76(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 76(%r15,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 76(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	%r9, %r13
	movq	1344(%rsp), %r9                 # 8-byte Reload
	vinsertps	$48, 76(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	1632(%rsp), %rcx                # 8-byte Reload
	vmulps	(%r14,%rcx,4), %ymm9, %ymm4
	vmulps	%ymm0, %ymm4, %ymm0
	vmovss	76(%r15,%rdx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	224(%rsp), %r8                  # 8-byte Reload
	vinsertps	$16, 76(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movq	112(%rsp), %r10                 # 8-byte Reload
	vinsertps	$32, 76(%r15,%r10,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1],mem[0],xmm4[3]
	movq	128(%rsp), %rdx                 # 8-byte Reload
	vmovss	76(%r15,%rdx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r15,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	192(%rsp), %r11                 # 8-byte Reload
	vinsertps	$48, 76(%r15,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 76(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 76(%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	%rsi, %rdx
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmovaps	2144(%rsp), %ymm4               # 32-byte Reload
	vblendvps	%ymm7, %ymm2, %ymm4, %ymm15
	vmovaps	%ymm7, %ymm12
	vmovaps	%ymm7, 2944(%rsp)               # 32-byte Spill
	movq	816(%rsp), %rsi                 # 8-byte Reload
	vmovups	(%r14,%rsi,4), %ymm2
	vmaxps	%ymm11, %ymm2, %ymm2
	vdivps	%ymm2, %ymm0, %ymm0
	vmovaps	3040(%rsp), %ymm4               # 32-byte Reload
	vblendvps	%ymm5, %ymm3, %ymm4, %ymm14
	vmulps	32(%r14,%rcx,4), %ymm6, %ymm3
	vmulps	%ymm1, %ymm3, %ymm1
	movq	2176(%rsp), %rax                # 8-byte Reload
	vfmadd132ps	(%r14,%rax,4), %ymm0, %ymm15 # ymm15 = (ymm15 * mem) + ymm0
	vmovups	32(%r14,%rsi,4), %ymm0
	vmaxps	%ymm11, %ymm0, %ymm0
	vdivps	%ymm0, %ymm1, %ymm1
	vfmadd132ps	32(%r14,%rax,4), %ymm1, %ymm14 # ymm14 = (ymm14 * mem) + ymm1
	movq	864(%rsp), %rax                 # 8-byte Reload
	vmovss	92(%r15,%rax,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	48(%rsp), %rcx                  # 8-byte Reload
	vinsertps	$16, 92(%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 92(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	92(%r15,%rdi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r15,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovss	88(%r15,%rax,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 92(%r15,%r13,4), %xmm1, %xmm5 # xmm5 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 88(%r15,%rbx,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	88(%r15,%rdi,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r15,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 88(%r15,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	1280(%rsp), %r13                # 8-byte Reload
	vinsertps	$32, 88(%r15,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 88(%r15,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm4, %ymm1
	vinsertps	$32, 92(%r15,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vaddps	1888(%rsp), %ymm8, %ymm4        # 32-byte Folded Reload
	vmulps	%ymm1, %ymm4, %ymm1
	vinsertps	$48, 92(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%r9, %r12
	movq	384(%rsp), %rdi                 # 8-byte Reload
	vmovss	92(%r15,%rdi,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%r10, %r9
	vinsertps	$32, 92(%r15,%r10,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vdivps	%ymm2, %ymm1, %ymm7
	vbroadcastss	.LCPI0_66(%rip), %ymm1  # ymm1 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vdivps	%ymm2, %ymm1, %ymm10
	vmovaps	%ymm1, %ymm8
	movq	128(%rsp), %rsi                 # 8-byte Reload
	vmovss	92(%r15,%rsi,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	16(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 92(%r15,%rax,4), %xmm1, %xmm2 # xmm2 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 92(%r15,%r11,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1,2],mem[0]
	movq	320(%rsp), %r10                 # 8-byte Reload
	vinsertps	$32, 92(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 92(%r15,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovss	88(%r15,%rdi,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovaps	%ymm9, 2976(%rsp)               # 32-byte Spill
	vminps	%ymm9, %ymm10, %ymm10
	vinsertps	$32, 88(%r15,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	1856(%rsp), %rdi                # 8-byte Reload
	vmulps	(%r14,%rdi,4), %ymm10, %ymm10
	vinsertf128	$1, %xmm5, %ymm3, %ymm5
	vinsertps	$48, 88(%r15,%r11,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1,2],mem[0]
	vmovss	88(%r15,%rsi,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vandps	%ymm7, %ymm12, %ymm7
	movq	456(%rsp), %rax                 # 8-byte Reload
	vmovups	-96(%r14,%rax), %ymm12
	vfmadd213ps	%ymm7, %ymm12, %ymm15   # ymm15 = (ymm12 * ymm15) + ymm7
	movq	704(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%r14,%rcx,4), %ymm9, %ymm7
	vmovaps	%ymm7, 2848(%rsp)               # 32-byte Spill
	vmulps	%ymm7, %ymm10, %ymm7
	vfmadd231ps	%ymm7, %ymm5, %ymm15    # ymm15 = (ymm5 * ymm7) + ymm15
	vmovaps	%ymm15, 2368(%rsp)              # 32-byte Spill
	vinsertps	$32, 88(%r15,%r10,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	864(%rsp), %r10                 # 8-byte Reload
	vmovss	20(%r15,%r10,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	48(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$16, 20(%r15,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 88(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vinsertps	$32, 20(%r15,%rbx,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1],mem[0],xmm5[3]
	vmovaps	2400(%rsp), %ymm5               # 32-byte Reload
	vaddps	2432(%rsp), %ymm5, %ymm5        # 32-byte Folded Reload
	vmulps	%ymm3, %ymm5, %ymm3
	vdivps	%ymm0, %ymm3, %ymm3
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovaps	%ymm8, %ymm15
	vdivps	%ymm0, %ymm8, %ymm0
	vmovaps	%ymm6, 2752(%rsp)               # 32-byte Spill
	vminps	%ymm6, %ymm0, %ymm0
	movq	928(%rsp), %r8                  # 8-byte Reload
	vinsertps	$48, 20(%r15,%r8,4), %xmm4, %xmm2 # xmm2 = xmm4[0,1,2],mem[0]
	movq	480(%rsp), %r11                 # 8-byte Reload
	vmovss	20(%r15,%r11,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	640(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, 20(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmulps	32(%r14,%rdi,4), %ymm0, %ymm0
	vandps	2496(%rsp), %ymm3, %ymm3        # 32-byte Folded Reload
	vinsertps	$32, 20(%r15,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovups	-64(%r14,%rax), %ymm5
	vmovaps	%ymm5, 2144(%rsp)               # 32-byte Spill
	vfmadd213ps	%ymm3, %ymm5, %ymm14    # ymm14 = (ymm5 * ymm14) + ymm3
	vmulps	32(%r14,%rcx,4), %ymm6, %ymm3
	vmovaps	%ymm3, 3008(%rsp)               # 32-byte Spill
	vmulps	%ymm3, %ymm0, %ymm0
	vfmadd231ps	%ymm0, %ymm1, %ymm14    # ymm14 = (ymm1 * ymm0) + ymm14
	vmovaps	%ymm14, 1664(%rsp)              # 32-byte Spill
	vinsertps	$48, 20(%r15,%r12,4), %xmm4, %xmm0 # xmm0 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vmovss	24(%r15,%r10,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	24(%r15,%r11,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r15,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%rdx, %rcx
	vinsertps	$32, 24(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 24(%r15,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 24(%r15,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	%r8, %rdx
	vinsertps	$48, 24(%r15,%r12,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovss	28(%r15,%r10,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovaps	%ymm12, %ymm4
	vmovaps	%ymm12, 3296(%rsp)              # 32-byte Spill
	movq	1824(%rsp), %rax                # 8-byte Reload
	vmulps	(%r14,%rax,4), %ymm12, %ymm3
	vmulps	%ymm3, %ymm1, %ymm12
	movq	%rbx, %rax
	vinsertps	$32, 28(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	28(%r15,%r11,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	%rcx, %rdi
	vinsertps	$16, 28(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	%r8, %rcx
	vinsertps	$48, 28(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 28(%r15,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	1440(%rsp), %rdx                # 8-byte Reload
	vmulps	(%r14,%rdx,4), %ymm4, %ymm4
	vfmadd231ps	%ymm4, %ymm0, %ymm12    # ymm12 = (ymm0 * ymm4) + ymm12
	movq	%r12, %rdx
	vinsertps	$48, 28(%r15,%r12,4), %xmm3, %xmm0 # xmm0 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vmovss	32(%r15,%r10,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	32(%r15,%r11,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	%r11, %rsi
	vinsertps	$16, 32(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 32(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 32(%r15,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 32(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	%r8, %r12
	vinsertps	$48, 32(%r15,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovaps	1888(%rsp), %ymm8               # 32-byte Reload
	movq	608(%rsp), %rax                 # 8-byte Reload
	vmulps	(%r14,%rax,4), %ymm8, %ymm4
	vfmadd231ps	%ymm4, %ymm0, %ymm12    # ymm12 = (ymm0 * ymm4) + ymm12
	movq	80(%rsp), %r11                  # 8-byte Reload
	vmovss	64(%r15,%r11,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	512(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 64(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movq	32(%rsp), %r9                   # 8-byte Reload
	vinsertps	$32, 64(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	1312(%rsp), %rbx                # 8-byte Reload
	vmovss	64(%r15,%rbx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	160(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 64(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	352(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 64(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	1248(%rsp), %r8                 # 8-byte Reload
	vinsertps	$32, 64(%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovaps	2336(%rsp), %ymm13              # 32-byte Reload
	movq	1216(%rsp), %rax                # 8-byte Reload
	vmulps	(%r14,%rax,4), %ymm13, %ymm4
	vfmadd231ps	%ymm4, %ymm2, %ymm12    # ymm12 = (ymm2 * ymm4) + ymm12
	movq	256(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 64(%r15,%rax,4), %xmm3, %xmm2 # xmm2 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	movq	2528(%rsp), %r13                # 8-byte Reload
	vmovups	64(%r14,%r13,4), %ymm14
	vcmpltps	%ymm14, %ymm11, %ymm2
	vmovaps	1600(%rsp), %ymm1               # 32-byte Reload
	vblendvps	%ymm2, %ymm0, %ymm1, %ymm9
	vmovaps	%ymm2, %ymm0
	movq	%r10, %r13
	vmovss	36(%r15,%r10,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	48(%rsp), %r10                  # 8-byte Reload
	vinsertps	$16, 36(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	36(%r15,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	640(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 36(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	1120(%rsp), %rsi                # 8-byte Reload
	vinsertps	$32, 36(%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	1280(%rsp), %rdx                # 8-byte Reload
	vinsertps	$32, 36(%r15,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 36(%r15,%r12,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	1344(%rsp), %r12                # 8-byte Reload
	vinsertps	$48, 36(%r15,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovss	40(%r15,%r13,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	1472(%rsp), %rax                # 8-byte Reload
	vmulps	(%r14,%rax,4), %ymm8, %ymm4
	vfmadd231ps	%ymm4, %ymm2, %ymm12    # ymm12 = (ymm2 * ymm4) + ymm12
	movq	%rsi, %rax
	vinsertps	$32, 40(%r15,%rsi,4), %xmm3, %xmm2 # xmm2 = xmm3[0,1],mem[0],xmm3[3]
	movq	480(%rsp), %rsi                 # 8-byte Reload
	vmovss	40(%r15,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	928(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 40(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 40(%r15,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	%r12, %rdx
	vinsertps	$48, 40(%r15,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovss	76(%r15,%r11,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	512(%rsp), %r12                 # 8-byte Reload
	vinsertps	$16, 76(%r15,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovss	76(%r15,%rbx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	160(%rsp), %r12                 # 8-byte Reload
	vinsertps	$16, 76(%r15,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	44(%r15,%r13,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r15,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 76(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 44(%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	%rax, %r13
	movq	1504(%rsp), %rax                # 8-byte Reload
	vmovups	(%r14,%rax,4), %ymm7
	vmulps	%ymm7, %ymm13, %ymm10
	vfmadd231ps	%ymm10, %ymm2, %ymm12   # ymm12 = (ymm2 * ymm10) + ymm12
	vinsertps	$48, 44(%r15,%rcx,4), %xmm5, %xmm2 # xmm2 = xmm5[0,1,2],mem[0]
	vmovss	44(%r15,%rsi,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	640(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 44(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 76(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	1280(%rsp), %r10                # 8-byte Reload
	vinsertps	$32, 44(%r15,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 76(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 44(%r15,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	256(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 76(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vinsertf128	$1, %xmm2, %ymm5, %ymm5
	vmovaps	2112(%rsp), %ymm1               # 32-byte Reload
	vmovaps	%ymm0, %ymm2
	vmovaps	%ymm0, 2880(%rsp)               # 32-byte Spill
	vblendvps	%ymm0, %ymm3, %ymm1, %ymm6
	vmulps	%ymm7, %ymm8, %ymm3
	vmovss	88(%r15,%r11,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	512(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 88(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vfmadd231ps	%ymm3, %ymm5, %ymm12    # ymm12 = (ymm5 * ymm3) + ymm12
	vinsertps	$32, 88(%r15,%r9,4), %xmm4, %xmm0 # xmm0 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	88(%r15,%rbx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	%r12, %rsi
	vinsertps	$16, 88(%r15,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 88(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 88(%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 88(%r15,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmovss	92(%r15,%r11,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 92(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 92(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovss	92(%r15,%rbx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%r15,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovaps	2304(%rsp), %ymm1               # 32-byte Reload
	vaddps	1920(%rsp), %ymm1, %ymm7        # 32-byte Folded Reload
	vmulps	%ymm0, %ymm7, %ymm0
	vinsertps	$32, 92(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	816(%rsp), %rax                 # 8-byte Reload
	vmovups	64(%r14,%rax,4), %ymm7
	vmaxps	%ymm11, %ymm7, %ymm7
	vdivps	%ymm7, %ymm0, %ymm1
	vinsertps	$48, 92(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	1632(%rsp), %rax                # 8-byte Reload
	vmovaps	%ymm14, 3040(%rsp)              # 32-byte Spill
	vmulps	64(%r14,%rax,4), %ymm14, %ymm10
	vmulps	%ymm9, %ymm10, %ymm0
	vdivps	%ymm7, %ymm0, %ymm0
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vdivps	%ymm7, %ymm15, %ymm4
	vmovaps	%ymm15, %ymm5
	movq	2176(%rsp), %rax                # 8-byte Reload
	vfmadd132ps	64(%r14,%rax,4), %ymm0, %ymm6 # ymm6 = (ymm6 * mem) + ymm0
	vandps	%ymm1, %ymm2, %ymm0
	movq	456(%rsp), %rax                 # 8-byte Reload
	vmovups	-32(%r14,%rax), %ymm1
	vmovaps	%ymm1, 2112(%rsp)               # 32-byte Spill
	vfmadd213ps	%ymm0, %ymm1, %ymm6     # ymm6 = (ymm1 * ymm6) + ymm0
	vminps	%ymm14, %ymm4, %ymm0
	movq	1856(%rsp), %rax                # 8-byte Reload
	vmulps	64(%r14,%rax,4), %ymm0, %ymm0
	movq	704(%rsp), %rax                 # 8-byte Reload
	vmulps	64(%r14,%rax,4), %ymm14, %ymm1
	vmovaps	%ymm1, 3264(%rsp)               # 32-byte Spill
	vmulps	%ymm1, %ymm0, %ymm0
	vfmadd231ps	%ymm0, %ymm3, %ymm6     # ymm6 = (ymm3 * ymm0) + ymm6
	vmovaps	%ymm6, 1600(%rsp)               # 32-byte Spill
	movq	864(%rsp), %rdx                 # 8-byte Reload
	vmovss	48(%r15,%rdx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	48(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 48(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	480(%rsp), %rsi                 # 8-byte Reload
	vmovss	48(%r15,%rsi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	%rcx, %r8
	vinsertps	$16, 48(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%r13, %rbx
	vinsertps	$32, 48(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%r10, %r9
	vinsertps	$32, 48(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	928(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 48(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	1344(%rsp), %r10                # 8-byte Reload
	vinsertps	$48, 48(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmovss	52(%r15,%rdx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	560(%rsp), %rdi                 # 8-byte Reload
	vmovups	(%r14,%rdi,4), %ymm3
	vmulps	%ymm3, %ymm13, %ymm4
	vfmadd231ps	%ymm4, %ymm0, %ymm12    # ymm12 = (ymm0 * ymm4) + ymm12
	vinsertps	$32, 52(%r15,%r13,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	52(%r15,%rsi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 52(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 52(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 52(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmovss	56(%r15,%rdx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmulps	%ymm3, %ymm8, %ymm3
	vinsertps	$32, 56(%r15,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	56(%r15,%rsi,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 56(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 56(%r15,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 56(%r15,%r10,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vfmadd231ps	%ymm3, %ymm0, %ymm12    # ymm12 = (ymm0 * ymm3) + ymm12
	movq	384(%rsp), %rdi                 # 8-byte Reload
	vmovss	20(%r15,%rdi,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	224(%rsp), %r9                  # 8-byte Reload
	vinsertps	$16, 20(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	movq	112(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, 20(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	128(%rsp), %rbx                 # 8-byte Reload
	vmovss	20(%r15,%rbx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	16(%rsp), %r13                  # 8-byte Reload
	vinsertps	$16, 20(%r15,%r13,4), %xmm3, %xmm4 # xmm4 = xmm3[0],mem[0],xmm3[2,3]
	movq	192(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 20(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmovaps	2848(%rsp), %ymm1               # 32-byte Reload
	vmaxps	%ymm11, %ymm1, %ymm10
	movq	320(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$32, 20(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	24(%r15,%rdi,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r15,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	%r9, %r12
	movq	448(%rsp), %rcx                 # 8-byte Reload
	vmulps	(%r14,%rcx,4), %ymm10, %ymm8
	vfmadd231ps	%ymm8, %ymm2, %ymm12    # ymm12 = (ymm2 * ymm8) + ymm12
	vinsertps	$32, 24(%r15,%rsi,4), %xmm6, %xmm2 # xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	24(%r15,%rbx,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r15,%r13,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	576(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 20(%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 24(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 24(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm4, %ymm4
	vinsertps	$48, 24(%r15,%rcx,4), %xmm6, %xmm0 # xmm0 = xmm6[0,1,2],mem[0]
	movq	%rcx, %r8
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vmovaps	2144(%rsp), %ymm1               # 32-byte Reload
	movq	1824(%rsp), %rcx                # 8-byte Reload
	vmulps	32(%r14,%rcx,4), %ymm1, %ymm2
	vmulps	%ymm2, %ymm0, %ymm9
	vmovss	28(%r15,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	1440(%rsp), %rcx                # 8-byte Reload
	vmulps	32(%r14,%rcx,4), %ymm1, %ymm6
	vfmadd231ps	%ymm6, %ymm4, %ymm9     # ymm9 = (ymm4 * ymm6) + ymm9
	movq	%rsi, %r10
	vinsertps	$32, 28(%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	28(%r15,%rbx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	%rbx, %r12
	vinsertps	$16, 28(%r15,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 28(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 28(%r15,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%rdx, %r11
	vinsertps	$48, 28(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	%r8, %rbx
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	movq	1536(%rsp), %rdx                # 8-byte Reload
	vmovss	92(%r15,%rdx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	288(%rsp), %r8                  # 8-byte Reload
	vinsertps	$16, 92(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	1184(%rsp), %r9                 # 8-byte Reload
	vinsertps	$32, 92(%r15,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	1152(%rsp), %rsi                # 8-byte Reload
	vinsertps	$48, 92(%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	832(%rsp), %rax                 # 8-byte Reload
	vmovss	92(%r15,%rax,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	896(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 92(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovss	32(%r15,%rdi,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	224(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 32(%r15,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovaps	2432(%rsp), %ymm1               # 32-byte Reload
	movq	608(%rsp), %rdi                 # 8-byte Reload
	vmulps	32(%r14,%rdi,4), %ymm1, %ymm8
	vfmadd231ps	%ymm8, %ymm2, %ymm9     # ymm9 = (ymm2 * ymm8) + ymm9
	vinsertps	$32, 32(%r15,%r10,4), %xmm7, %xmm2 # xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	32(%r15,%r12,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r15,%r13,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	192(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 32(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 32(%r15,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 32(%r15,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm7, %ymm2
	movq	768(%rsp), %r12                 # 8-byte Reload
	vinsertps	$32, 92(%r15,%r12,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	88(%r15,%rdx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r15,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovaps	2400(%rsp), %ymm15              # 32-byte Reload
	movq	1216(%rsp), %rdi                # 8-byte Reload
	vmulps	32(%r14,%rdi,4), %ymm15, %ymm8
	vfmadd231ps	%ymm8, %ymm2, %ymm9     # ymm9 = (ymm2 * ymm8) + ymm9
	vinsertps	$32, 88(%r15,%r9,4), %xmm7, %xmm2 # xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	88(%r15,%rax,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	368(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 92(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 88(%r15,%r12,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 88(%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm6, %ymm3
	vinsertps	$48, 88(%r15,%rdi,4), %xmm7, %xmm4 # xmm4 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	vmovss	64(%r15,%rdx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 64(%r15,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	64(%r15,%rax,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	%rax, %r10
	vinsertps	$16, 64(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 64(%r15,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$32, 64(%r15,%r12,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 64(%r15,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$48, 64(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm6, %ymm4
	movq	2528(%rsp), %rax                # 8-byte Reload
	vmovups	96(%r14,%rax,4), %ymm14
	vcmpltps	%ymm14, %ymm11, %ymm6
	vmovaps	2816(%rsp), %ymm0               # 32-byte Reload
	vblendvps	%ymm6, %ymm4, %ymm0, %ymm4
	vmovaps	%ymm6, %ymm0
	movq	816(%rsp), %rax                 # 8-byte Reload
	vmovups	96(%r14,%rax,4), %ymm6
	vmaxps	%ymm11, %ymm6, %ymm6
	vdivps	%ymm6, %ymm5, %ymm11
	vmovss	76(%r15,%rdx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r15,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovaps	2464(%rsp), %ymm8               # 32-byte Reload
	vaddps	1792(%rsp), %ymm8, %ymm13       # 32-byte Folded Reload
	vmulps	%ymm2, %ymm13, %ymm2
	vinsertps	$32, 76(%r15,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	76(%r15,%r10,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 76(%r15,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$32, 76(%r15,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 76(%r15,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm5, %ymm5
	vdivps	%ymm6, %ymm2, %ymm2
	vmovaps	2784(%rsp), %ymm7               # 32-byte Reload
	vmovaps	%ymm0, 2336(%rsp)               # 32-byte Spill
	vblendvps	%ymm0, %ymm5, %ymm7, %ymm13
	movq	1632(%rsp), %rax                # 8-byte Reload
	vmovaps	%ymm14, 2848(%rsp)              # 32-byte Spill
	vmulps	96(%r14,%rax,4), %ymm14, %ymm5
	vmulps	%ymm4, %ymm5, %ymm4
	vdivps	%ymm6, %ymm4, %ymm4
	movq	2176(%rsp), %rax                # 8-byte Reload
	vfmadd132ps	96(%r14,%rax,4), %ymm4, %ymm13 # ymm13 = (ymm13 * mem) + ymm4
	vandps	%ymm2, %ymm0, %ymm2
	movq	456(%rsp), %rax                 # 8-byte Reload
	vmovups	(%r14,%rax), %ymm0
	vmovaps	%ymm0, 1888(%rsp)               # 32-byte Spill
	vfmadd213ps	%ymm2, %ymm0, %ymm13    # ymm13 = (ymm0 * ymm13) + ymm2
	vminps	%ymm14, %ymm11, %ymm2
	movq	1856(%rsp), %rax                # 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm2, %ymm2
	movq	704(%rsp), %rax                 # 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm14, %ymm14
	vmulps	%ymm2, %ymm14, %ymm2
	vfmadd231ps	%ymm2, %ymm3, %ymm13    # ymm13 = (ymm3 * ymm2) + ymm13
	movq	384(%rsp), %rdx                 # 8-byte Reload
	vmovss	36(%r15,%rdx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	224(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 36(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	128(%rsp), %rax                 # 8-byte Reload
	vmovss	36(%r15,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	%r13, %rcx
	vinsertps	$16, 36(%r15,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	112(%rsp), %r13                 # 8-byte Reload
	vinsertps	$32, 36(%r15,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	320(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, 36(%r15,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	192(%rsp), %r10                 # 8-byte Reload
	vinsertps	$48, 36(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	%rbx, %r11
	vinsertps	$48, 36(%r15,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovss	48(%r15,%rdx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 48(%r15,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	48(%r15,%rax,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovss	40(%r15,%rdx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	1472(%rsp), %rbx                # 8-byte Reload
	vmulps	32(%r14,%rbx,4), %ymm1, %ymm11
	vfmadd231ps	%ymm11, %ymm2, %ymm9    # ymm9 = (ymm2 * ymm11) + ymm9
	vinsertps	$32, 40(%r15,%r13,4), %xmm7, %xmm2 # xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	40(%r15,%rax,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 40(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 40(%r15,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 40(%r15,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm7, %ymm2
	vinsertps	$32, 48(%r15,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	44(%r15,%rdx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r15,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	1504(%rsp), %rdx                # 8-byte Reload
	vmovups	32(%r14,%rdx,4), %ymm11
	vmovaps	%ymm15, %ymm4
	vmulps	%ymm11, %ymm15, %ymm15
	vfmadd231ps	%ymm15, %ymm2, %ymm9    # ymm9 = (ymm2 * ymm15) + ymm9
	vinsertps	$32, 44(%r15,%r13,4), %xmm7, %xmm2 # xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	44(%r15,%rax,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 44(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 44(%r15,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 44(%r15,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm7, %ymm2
	vmulps	%ymm1, %ymm11, %ymm7
	vmovaps	%ymm1, %ymm15
	vinsertps	$48, 48(%r15,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%r10, %r12
	vfmadd231ps	%ymm7, %ymm2, %ymm9     # ymm9 = (ymm2 * ymm7) + ymm9
	vinsertps	$48, 48(%r15,%r11,4), %xmm5, %xmm2 # xmm2 = xmm5[0,1,2],mem[0]
	movq	%r11, 576(%rsp)                 # 8-byte Spill
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	movq	80(%rsp), %r8                   # 8-byte Reload
	vmovss	20(%r15,%r8,4), %xmm3           # xmm3 = mem[0],zero,zero,zero
	movq	512(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$16, 20(%r15,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	560(%rsp), %rax                 # 8-byte Reload
	vmovups	32(%r14,%rax,4), %ymm5
	vmulps	%ymm5, %ymm4, %ymm7
	movq	32(%rsp), %rax                  # 8-byte Reload
	vinsertps	$32, 20(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	1312(%rsp), %r13                # 8-byte Reload
	vmovss	20(%r15,%r13,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	160(%rsp), %r10                 # 8-byte Reload
	vinsertps	$16, 20(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	352(%rsp), %r9                  # 8-byte Reload
	vinsertps	$48, 20(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vfmadd231ps	%ymm7, %ymm2, %ymm9     # ymm9 = (ymm2 * ymm7) + ymm9
	movq	1248(%rsp), %rdx                # 8-byte Reload
	vinsertps	$32, 20(%r15,%rdx,4), %xmm6, %xmm2 # xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	24(%r15,%r8,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r15,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	256(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 20(%r15,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vinsertps	$32, 24(%r15,%rax,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	24(%r15,%r13,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 24(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 24(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 24(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	vmovaps	2112(%rsp), %ymm0               # 32-byte Reload
	movq	1824(%rsp), %rbx                # 8-byte Reload
	vmulps	64(%r14,%rbx,4), %ymm0, %ymm6
	vmulps	%ymm6, %ymm3, %ymm11
	vmovss	32(%r15,%r8,4), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r15,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovss	28(%r15,%r8,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r15,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	1440(%rsp), %rcx                # 8-byte Reload
	vmulps	64(%r14,%rcx,4), %ymm0, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm11    # ymm11 = (ymm2 * ymm7) + ymm11
	vinsertps	$32, 28(%r15,%rax,4), %xmm6, %xmm2 # xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	28(%r15,%r13,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	%r10, %rcx
	vinsertps	$16, 28(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 28(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 28(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 28(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vinsertps	$32, 32(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	32(%r15,%r13,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 32(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 32(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovaps	1920(%rsp), %ymm1               # 32-byte Reload
	movq	608(%rsp), %rbx                 # 8-byte Reload
	vmulps	64(%r14,%rbx,4), %ymm1, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm11    # ymm11 = (ymm2 * ymm7) + ymm11
	vinsertps	$48, 32(%r15,%rdi,4), %xmm6, %xmm2 # xmm2 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	movq	384(%rsp), %rbx                 # 8-byte Reload
	vmovss	52(%r15,%rbx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	224(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, 52(%r15,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	128(%rsp), %rbx                 # 8-byte Reload
	vmovss	52(%r15,%rbx,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	16(%rsp), %rbx                  # 8-byte Reload
	vinsertps	$16, 52(%r15,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	112(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$32, 52(%r15,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	320(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$32, 52(%r15,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 52(%r15,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 52(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmovaps	2304(%rsp), %ymm4               # 32-byte Reload
	movq	1216(%rsp), %rbx                # 8-byte Reload
	vmulps	64(%r14,%rbx,4), %ymm4, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm11    # ymm11 = (ymm2 * ymm7) + ymm11
	vinsertf128	$1, %xmm3, %ymm6, %ymm2
	vmulps	%ymm5, %ymm15, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm9     # ymm9 = (ymm2 * ymm3) + ymm9
	vmovss	36(%r15,%r8,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	36(%r15,%r13,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r15,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 36(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 36(%r15,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 36(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 36(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vmovss	48(%r15,%r8,4), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r15,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 48(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	48(%r15,%r13,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r15,%r10,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovss	40(%r15,%r8,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	1472(%rsp), %rbx                # 8-byte Reload
	vmulps	64(%r14,%rbx,4), %ymm1, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm11    # ymm11 = (ymm2 * ymm7) + ymm11
	vinsertps	$32, 40(%r15,%rax,4), %xmm6, %xmm2 # xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	40(%r15,%r13,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 40(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 40(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 40(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vinsertps	$32, 48(%r15,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	44(%r15,%r8,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r15,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	1504(%rsp), %rsi                # 8-byte Reload
	vmovups	64(%r14,%rsi,4), %ymm7
	vmulps	%ymm7, %ymm4, %ymm15
	vfmadd231ps	%ymm15, %ymm2, %ymm11   # ymm11 = (ymm2 * ymm15) + ymm11
	vinsertps	$32, 44(%r15,%rax,4), %xmm6, %xmm2 # xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	44(%r15,%r13,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 44(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 44(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rdx, %r12
	vinsertps	$48, 44(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vmulps	%ymm7, %ymm1, %ymm6
	vinsertps	$48, 48(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%r9, %rbx
	vfmadd231ps	%ymm6, %ymm2, %ymm11    # ymm11 = (ymm2 * ymm6) + ymm11
	vinsertps	$48, 48(%r15,%rdi,4), %xmm5, %xmm2 # xmm2 = xmm5[0,1,2],mem[0]
	movq	%rdi, %r13
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	movq	1536(%rsp), %rdx                # 8-byte Reload
	vmovss	20(%r15,%rdx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	288(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 20(%r15,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	560(%rsp), %rax                 # 8-byte Reload
	vmovups	64(%r14,%rax,4), %ymm5
	vmulps	%ymm5, %ymm4, %ymm6
	movq	1184(%rsp), %r9                 # 8-byte Reload
	vinsertps	$32, 20(%r15,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	832(%rsp), %rsi                 # 8-byte Reload
	vmovss	20(%r15,%rsi,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	896(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 20(%r15,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	1152(%rsp), %rax                # 8-byte Reload
	vinsertps	$48, 20(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vfmadd231ps	%ymm6, %ymm2, %ymm11    # ymm11 = (ymm2 * ymm6) + ymm11
	movq	768(%rsp), %r11                 # 8-byte Reload
	vinsertps	$32, 20(%r15,%r11,4), %xmm7, %xmm2 # xmm2 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	24(%r15,%rdx,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	368(%rsp), %r10                 # 8-byte Reload
	vinsertps	$48, 20(%r15,%r10,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vinsertps	$32, 24(%r15,%r9,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	24(%r15,%rsi,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%r15,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 24(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rax, %rcx
	vinsertps	$32, 24(%r15,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%r11, %r8
	vinsertps	$48, 24(%r15,%r10,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	movq	1824(%rsp), %rax                # 8-byte Reload
	vmovaps	1888(%rsp), %ymm0               # 32-byte Reload
	vmulps	96(%r14,%rax,4), %ymm0, %ymm6
	vmulps	%ymm6, %ymm3, %ymm15
	movq	80(%rsp), %rax                  # 8-byte Reload
	vmovss	52(%r15,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	512(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 52(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	1312(%rsp), %rax                # 8-byte Reload
	vmovss	52(%r15,%rax,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	160(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 52(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	32(%rsp), %rax                  # 8-byte Reload
	vinsertps	$32, 52(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 52(%r15,%r12,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 52(%r15,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 52(%r15,%r13,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	movq	1440(%rsp), %rax                # 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm0, %ymm7
	vfmadd231ps	%ymm7, %ymm2, %ymm15    # ymm15 = (ymm2 * ymm7) + ymm15
	vinsertf128	$1, %xmm3, %ymm6, %ymm2
	vmulps	%ymm5, %ymm1, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm11    # ymm11 = (ymm2 * ymm3) + ymm11
	vmovss	28(%r15,%rdx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	288(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, 28(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	28(%r15,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 28(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 28(%r15,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 28(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$48, 28(%r15,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movq	864(%rsp), %rax                 # 8-byte Reload
	vmovss	60(%r15,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	48(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 60(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	608(%rsp), %rax                 # 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm8, %ymm5
	vfmadd231ps	%ymm5, %ymm2, %ymm15    # ymm15 = (ymm2 * ymm5) + ymm15
	movq	1120(%rsp), %rax                # 8-byte Reload
	vinsertps	$32, 60(%r15,%rax,4), %xmm3, %xmm2 # xmm2 = xmm3[0,1],mem[0],xmm3[3]
	movq	480(%rsp), %rax                 # 8-byte Reload
	vmovss	60(%r15,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	640(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 60(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	928(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 60(%r15,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	1280(%rsp), %rax                # 8-byte Reload
	vinsertps	$32, 60(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	1344(%rsp), %rax                # 8-byte Reload
	vinsertps	$48, 60(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	movq	1568(%rsp), %rax                # 8-byte Reload
	vmulps	(%r14,%rax,4), %ymm10, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm12    # ymm12 = (ymm2 * ymm3) + ymm12
	vmovaps	2720(%rsp), %ymm1               # 32-byte Reload
	vmovaps	3008(%rsp), %ymm0               # 32-byte Reload
	vmaxps	%ymm1, %ymm0, %ymm4
	vmovaps	2688(%rsp), %ymm8               # 32-byte Reload
	vdivps	%ymm8, %ymm10, %ymm5
	vdivps	%ymm8, %ymm4, %ymm6
	vroundps	$10, %ymm5, %ymm3
	vmulps	3136(%rsp), %ymm3, %ymm7        # 32-byte Folded Reload
	vroundps	$10, %ymm6, %ymm3
	vmulps	3072(%rsp), %ymm3, %ymm3        # 32-byte Folded Reload
	vmovaps	3264(%rsp), %ymm0               # 32-byte Reload
	vmaxps	%ymm1, %ymm0, %ymm10
	vmaxps	%ymm1, %ymm14, %ymm2
	vmovaps	%ymm2, 1920(%rsp)               # 32-byte Spill
	vdivps	%ymm8, %ymm10, %ymm0
	vdivps	%ymm8, %ymm2, %ymm2
	vroundps	$10, %ymm0, %ymm8
	vmulps	2912(%rsp), %ymm8, %ymm8        # 32-byte Folded Reload
	vroundps	$10, %ymm2, %ymm14
	vmulps	3104(%rsp), %ymm14, %ymm14      # 32-byte Folded Reload
	vmaxps	%ymm1, %ymm5, %ymm5
	vdivps	%ymm5, %ymm7, %ymm5
	vmaxps	%ymm1, %ymm6, %ymm6
	vdivps	%ymm6, %ymm3, %ymm3
	vmaxps	%ymm1, %ymm0, %ymm0
	vdivps	%ymm0, %ymm8, %ymm0
	vmaxps	%ymm1, %ymm2, %ymm1
	vdivps	%ymm1, %ymm14, %ymm1
	vbroadcastss	.LCPI0_67(%rip), %ymm6  # ymm6 = [2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0]
	vmovaps	2368(%rsp), %ymm14              # 32-byte Reload
	vfmadd213ps	%ymm5, %ymm6, %ymm14    # ymm14 = (ymm6 * ymm14) + ymm5
	vmovaps	1664(%rsp), %ymm2               # 32-byte Reload
	vfmadd213ps	%ymm3, %ymm6, %ymm2     # ymm2 = (ymm6 * ymm2) + ymm3
	vmovaps	%ymm2, 1664(%rsp)               # 32-byte Spill
	vmovaps	1600(%rsp), %ymm2               # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm6, %ymm2     # ymm2 = (ymm6 * ymm2) + ymm0
	vmovaps	%ymm2, 1600(%rsp)               # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm13    # ymm13 = (ymm6 * ymm13) + ymm1
	vmovss	32(%r15,%rdx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	32(%r15,%rsi,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 32(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 32(%r15,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 32(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 32(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	384(%rsp), %rax                 # 8-byte Reload
	vmovss	56(%r15,%rax,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	224(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 56(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovaps	1792(%rsp), %ymm7               # 32-byte Reload
	movq	1216(%rsp), %rax                # 8-byte Reload
	vmulps	96(%r14,%rax,4), %ymm7, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm3) + ymm15
	movq	112(%rsp), %r11                 # 8-byte Reload
	vinsertps	$32, 56(%r15,%r11,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1],mem[0],xmm1[3]
	movq	128(%rsp), %r13                 # 8-byte Reload
	vmovss	56(%r15,%r13,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	16(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 56(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	192(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 56(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	320(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 56(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	576(%rsp), %r12                 # 8-byte Reload
	vinsertps	$48, 56(%r15,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	448(%rsp), %rax                 # 8-byte Reload
	vmulps	32(%r14,%rax,4), %ymm4, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm9     # ymm9 = (ymm0 * ymm1) + ymm9
	vmovss	36(%r15,%rdx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	36(%r15,%rsi,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 36(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 36(%r15,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 36(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 36(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	80(%rsp), %rbx                  # 8-byte Reload
	vmovss	56(%r15,%rbx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	512(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, 56(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovaps	2464(%rsp), %ymm6               # 32-byte Reload
	movq	1472(%rsp), %rbx                # 8-byte Reload
	vmulps	96(%r14,%rbx,4), %ymm6, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm3) + ymm15
	movq	32(%rsp), %rbx                  # 8-byte Reload
	vinsertps	$32, 56(%r15,%rbx,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1],mem[0],xmm1[3]
	movq	1312(%rsp), %rbx                # 8-byte Reload
	vmovss	56(%r15,%rbx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	160(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, 56(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	352(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$48, 56(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	1248(%rsp), %rbx                # 8-byte Reload
	vinsertps	$32, 56(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	256(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$48, 56(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmulps	64(%r14,%rax,4), %ymm10, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm11    # ymm11 = (ymm0 * ymm1) + ymm11
	vmovss	40(%r15,%rdx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	288(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, 40(%r15,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	40(%r15,%rsi,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 40(%r15,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 40(%r15,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 40(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 40(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovss	44(%r15,%rdx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	1504(%rsp), %rax                # 8-byte Reload
	vmovups	96(%r14,%rax,4), %ymm3
	vmulps	%ymm3, %ymm7, %ymm5
	vmovaps	%ymm7, %ymm8
	vfmadd231ps	%ymm5, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm5) + ymm15
	vinsertps	$32, 44(%r15,%r9,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	44(%r15,%rsi,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 44(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 44(%r15,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 44(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovss	52(%r15,%rdx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmulps	%ymm3, %ymm6, %ymm3
	vmovaps	%ymm6, %ymm7
	vmovss	48(%r15,%rdx,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r15,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vfmadd231ps	%ymm3, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm3) + ymm15
	vinsertps	$32, 48(%r15,%r9,4), %xmm5, %xmm0 # xmm0 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	48(%r15,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 48(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 48(%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 48(%r15,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vinsertps	$32, 52(%r15,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	52(%r15,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	%rsi, %r9
	vinsertps	$16, 52(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 52(%r15,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	560(%rsp), %rsi                 # 8-byte Reload
	vmovups	96(%r14,%rsi,4), %ymm5
	vmulps	%ymm5, %ymm8, %ymm6
	vinsertps	$48, 52(%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	%rcx, %rsi
	vfmadd231ps	%ymm6, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm6) + ymm15
	vinsertps	$48, 52(%r15,%r10,4), %xmm3, %xmm0 # xmm0 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmulps	%ymm5, %ymm7, %ymm1
	movq	384(%rsp), %rax                 # 8-byte Reload
	vmovss	60(%r15,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	224(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 60(%r15,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vfmadd231ps	%ymm1, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm1) + ymm15
	vinsertps	$32, 60(%r15,%r11,4), %xmm3, %xmm0 # xmm0 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	60(%r15,%r13,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	16(%rsp), %r8                   # 8-byte Reload
	vinsertps	$16, 60(%r15,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	192(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 60(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	320(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$32, 60(%r15,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 60(%r15,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	1568(%rsp), %rax                # 8-byte Reload
	vmulps	32(%r14,%rax,4), %ymm4, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm9     # ymm9 = (ymm0 * ymm1) + ymm9
	movq	%rdx, %rax
	vmovss	56(%r15,%rdx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	288(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 56(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	56(%r15,%r9,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	1184(%rsp), %rdx                # 8-byte Reload
	vinsertps	$32, 56(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	768(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$32, 56(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 56(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 56(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	80(%rsp), %rsi                  # 8-byte Reload
	vmovss	60(%r15,%rsi,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	512(%rsp), %r11                 # 8-byte Reload
	vinsertps	$16, 60(%r15,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovaps	1920(%rsp), %ymm3               # 32-byte Reload
	movq	448(%rsp), %rsi                 # 8-byte Reload
	vmulps	96(%r14,%rsi,4), %ymm3, %ymm2
	vfmadd231ps	%ymm2, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm2) + ymm15
	movq	32(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$32, 60(%r15,%rsi,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1],mem[0],xmm1[3]
	movq	1312(%rsp), %r10                # 8-byte Reload
	vmovss	60(%r15,%r10,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	160(%rsp), %r12                 # 8-byte Reload
	vinsertps	$16, 60(%r15,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	352(%rsp), %r13                 # 8-byte Reload
	vinsertps	$48, 60(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	1248(%rsp), %rdi                # 8-byte Reload
	vinsertps	$32, 60(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	256(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$48, 60(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	1568(%rsp), %r9                 # 8-byte Reload
	vmulps	64(%r14,%r9,4), %ymm10, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm11    # ymm11 = (ymm0 * ymm1) + ymm11
	vmovss	60(%r15,%rax,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 60(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	832(%rsp), %rax                 # 8-byte Reload
	vmovss	60(%r15,%rax,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	896(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 60(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 60(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	768(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$32, 60(%r15,%rcx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	1152(%rsp), %rax                # 8-byte Reload
	vinsertps	$48, 60(%r15,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	368(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 60(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	864(%rsp), %rax                 # 8-byte Reload
	vmovss	96(%r15,%rax,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	48(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, 96(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmulps	96(%r14,%r9,4), %ymm3, %ymm2
	vfmadd231ps	%ymm2, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm2) + ymm15
	movq	1120(%rsp), %rdx                # 8-byte Reload
	vinsertps	$32, 96(%r15,%rdx,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1],mem[0],xmm1[3]
	movq	480(%rsp), %rdx                 # 8-byte Reload
	vmovss	96(%r15,%rdx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	640(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, 96(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	928(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 96(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	1280(%rsp), %rdx                # 8-byte Reload
	vinsertps	$32, 96(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	1344(%rsp), %rdx                # 8-byte Reload
	vinsertps	$48, 96(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	384(%rsp), %rdx                 # 8-byte Reload
	vmovss	96(%r15,%rdx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	224(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, 96(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vaddps	%ymm12, %ymm14, %ymm4
	movq	112(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$32, 96(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	128(%rsp), %r9                  # 8-byte Reload
	vmovss	96(%r15,%r9,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 96(%r15,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	192(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 96(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 96(%r15,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	576(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 96(%r15,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vfmadd231ps	3296(%rsp), %ymm0, %ymm4 # 32-byte Folded Reload
                                        # ymm4 = (ymm0 * mem) + ymm4
	vinsertf128	$1, %xmm1, %ymm2, %ymm0
	vaddps	1664(%rsp), %ymm9, %ymm14       # 32-byte Folded Reload
	vfmadd231ps	2144(%rsp), %ymm0, %ymm14 # 32-byte Folded Reload
                                        # ymm14 = (ymm0 * mem) + ymm14
	movq	80(%rsp), %rbx                  # 8-byte Reload
	vmovss	96(%r15,%rbx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 96(%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	96(%r15,%r10,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 96(%r15,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$32, 96(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 96(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 96(%r15,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 96(%r15,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vaddps	1600(%rsp), %ymm11, %ymm12      # 32-byte Folded Reload
	movq	1536(%rsp), %rsi                # 8-byte Reload
	vmovss	96(%r15,%rsi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	288(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$16, 96(%r15,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	832(%rsp), %r12                 # 8-byte Reload
	vmovss	96(%r15,%r12,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	896(%rsp), %r11                 # 8-byte Reload
	vinsertps	$16, 96(%r15,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	1184(%rsp), %r13                # 8-byte Reload
	vinsertps	$32, 96(%r15,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 96(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	1152(%rsp), %rcx                # 8-byte Reload
	vinsertps	$48, 96(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vfmadd231ps	2112(%rsp), %ymm0, %ymm12 # 32-byte Folded Reload
                                        # ymm12 = (ymm0 * mem) + ymm12
	movq	368(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 96(%r15,%rcx,4), %xmm5, %xmm0 # xmm0 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm5
	vmovss	108(%r15,%rax,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vaddps	%ymm15, %ymm13, %ymm11
	vmovss	104(%r15,%rax,4), %xmm6         # xmm6 = mem[0],zero,zero,zero
	vfmadd231ps	1888(%rsp), %ymm5, %ymm11 # 32-byte Folded Reload
                                        # ymm11 = (ymm5 * mem) + ymm11
	vmovss	100(%r15,%rax,4), %xmm5         # xmm5 = mem[0],zero,zero,zero
	movq	480(%rsp), %rdi                 # 8-byte Reload
	vmovss	104(%r15,%rdi,4), %xmm7         # xmm7 = mem[0],zero,zero,zero
	movq	48(%rsp), %r9                   # 8-byte Reload
	vinsertps	$16, 104(%r15,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	640(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 104(%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	1120(%rsp), %rdx                # 8-byte Reload
	vinsertps	$32, 104(%r15,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	1280(%rsp), %rcx                # 8-byte Reload
	vinsertps	$32, 104(%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	928(%rsp), %r8                  # 8-byte Reload
	vinsertps	$48, 104(%r15,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$16, 100(%r15,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	1344(%rsp), %rbx                # 8-byte Reload
	vinsertps	$48, 104(%r15,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm7, %ymm7
	vmovss	100(%r15,%rdi,4), %xmm6         # xmm6 = mem[0],zero,zero,zero
	vinsertps	$32, 100(%r15,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	%rdx, %rsi
	vinsertps	$16, 100(%r15,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 100(%r15,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 100(%r15,%rcx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 100(%r15,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm6, %ymm6
	vmovss	108(%r15,%rdi,4), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$16, 108(%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 108(%r15,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$32, 108(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 108(%r15,%r8,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	vmovaps	%xmm0, 640(%rsp)                # 16-byte Spill
	vinsertps	$48, 108(%r15,%rbx,4), %xmm5, %xmm10 # xmm10 = xmm5[0,1,2],mem[0]
	vandps	2944(%rsp), %ymm6, %ymm6        # 32-byte Folded Reload
	movq	1424(%rsp), %r8                 # 8-byte Reload
	vmovups	(%r14,%r8,4), %ymm8
	vfmadd213ps	%ymm4, %ymm8, %ymm6     # ymm6 = (ymm8 * ymm6) + ymm4
	movq	384(%rsp), %rdx                 # 8-byte Reload
	vmovss	108(%r15,%rdx,4), %xmm4         # xmm4 = mem[0],zero,zero,zero
	movq	224(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 108(%r15,%rcx,4), %xmm4, %xmm2 # xmm2 = xmm4[0],mem[0],xmm4[2,3]
	vmovaps	3232(%rsp), %ymm9               # 32-byte Reload
	vaddps	2976(%rsp), %ymm9, %ymm4        # 32-byte Folded Reload
	vmovss	104(%r15,%rdx,4), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmulps	%ymm4, %ymm8, %ymm4
	vmovss	100(%r15,%rdx,4), %xmm0         # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vfmadd231ps	%ymm4, %ymm7, %ymm6     # ymm6 = (ymm7 * ymm4) + ymm6
	movq	128(%rsp), %rbx                 # 8-byte Reload
	vmovss	100(%r15,%rbx,4), %xmm4         # xmm4 = mem[0],zero,zero,zero
	movq	112(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, 100(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	16(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 100(%r15,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	192(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 100(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	320(%rsp), %r9                  # 8-byte Reload
	vinsertps	$32, 100(%r15,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	576(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 100(%r15,%rcx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm4, %ymm0
	vmovss	108(%r15,%rbx,4), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%r15,%rax,4), %xmm4, %xmm13 # xmm13 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	104(%r15,%rbx,4), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%r15,%rax,4), %xmm4, %xmm1 # xmm1 = xmm4[0],mem[0],xmm4[2,3]
	movq	80(%rsp), %rbx                  # 8-byte Reload
	vmovss	108(%r15,%rbx,4), %xmm4         # xmm4 = mem[0],zero,zero,zero
	movq	512(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 108(%r15,%rax,4), %xmm4, %xmm3 # xmm3 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, 104(%r15,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vandps	2496(%rsp), %ymm0, %ymm4        # 32-byte Folded Reload
	vinsertps	$32, 104(%r15,%r9,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1],mem[0],xmm1[3]
	vmovups	32(%r14,%r8,4), %ymm1
	vfmadd213ps	%ymm14, %ymm1, %ymm4    # ymm4 = (ymm1 * ymm4) + ymm14
	vaddps	2752(%rsp), %ymm9, %ymm8        # 32-byte Folded Reload
	vinsertps	$48, 104(%r15,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vmulps	%ymm1, %ymm8, %ymm1
	vinsertps	$48, 104(%r15,%rcx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm0, %ymm0
	vmovss	104(%r15,%rbx,4), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovss	100(%r15,%rbx,4), %xmm7         # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r15,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, 108(%r15,%rsi,4), %xmm2, %xmm8 # xmm8 = xmm2[0,1],mem[0],xmm2[3]
	vfmadd231ps	%ymm1, %ymm0, %ymm4     # ymm4 = (ymm0 * ymm1) + ymm4
	movq	%r10, %rbx
	vmovss	104(%r15,%r10,4), %xmm0         # xmm0 = mem[0],zero,zero,zero
	movq	32(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$32, 104(%r15,%rsi,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1],mem[0],xmm5[3]
	movq	160(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 104(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	352(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 104(%r15,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	1248(%rsp), %r10                # 8-byte Reload
	vinsertps	$32, 104(%r15,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	256(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 104(%r15,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	100(%r15,%rbx,4), %xmm1         # xmm1 = mem[0],zero,zero,zero
	vinsertps	$32, 100(%r15,%rsi,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$16, 100(%r15,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 100(%r15,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 100(%r15,%r10,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 100(%r15,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm1, %ymm1
	vmovss	108(%r15,%rbx,4), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vinsertps	$32, 108(%r15,%r9,4), %xmm13, %xmm15 # xmm15 = xmm13[0,1],mem[0],xmm13[3]
	vinsertps	$16, 108(%r15,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 108(%r15,%rsi,4), %xmm3, %xmm13 # xmm13 = xmm3[0,1],mem[0],xmm3[3]
	vandps	2880(%rsp), %ymm1, %ymm1        # 32-byte Folded Reload
	movq	1536(%rsp), %rbx                # 8-byte Reload
	vmovss	104(%r15,%rbx,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	movq	288(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$16, 104(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	104(%r15,%r12,4), %xmm3         # xmm3 = mem[0],zero,zero,zero
	movq	%r11, %rax
	vinsertps	$16, 104(%r15,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovups	64(%r14,%r8,4), %ymm14
	vfmadd213ps	%ymm12, %ymm14, %ymm1   # ymm1 = (ymm14 * ymm1) + ymm12
	vaddps	3040(%rsp), %ymm9, %ymm12       # 32-byte Folded Reload
	vinsertps	$32, 104(%r15,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmulps	%ymm14, %ymm12, %ymm12
	movq	768(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, 104(%r15,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	100(%r15,%rbx,4), %xmm7         # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%r15,%rcx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$32, 100(%r15,%r13,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vfmadd231ps	%ymm12, %ymm0, %ymm1    # ymm1 = (ymm0 * ymm12) + ymm1
	vmovss	100(%r15,%r12,4), %xmm0         # xmm0 = mem[0],zero,zero,zero
	movq	1152(%rsp), %r9                 # 8-byte Reload
	vinsertps	$48, 104(%r15,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 100(%r15,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	368(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 104(%r15,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 100(%r15,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 100(%r15,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vinsertps	$48, 100(%r15,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm0, %ymm0
	vandps	2336(%rsp), %ymm0, %ymm0        # 32-byte Folded Reload
	vmovups	96(%r14,%r8,4), %ymm3
	vfmadd213ps	%ymm11, %ymm3, %ymm0    # ymm0 = (ymm3 * ymm0) + ymm11
	vaddps	2848(%rsp), %ymm9, %ymm7        # 32-byte Folded Reload
	vmulps	%ymm3, %ymm7, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm0     # ymm0 = (ymm2 * ymm3) + ymm0
	vinsertps	$32, 108(%r15,%r10,4), %xmm5, %xmm2 # xmm2 = xmm5[0,1],mem[0],xmm5[3]
	vinsertf128	$1, 640(%rsp), %ymm10, %ymm9 # 16-byte Folded Reload
	vmovss	108(%r15,%rbx,4), %xmm5         # xmm5 = mem[0],zero,zero,zero
	movq	192(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$48, 108(%r15,%rbx,4), %xmm8, %xmm7 # xmm7 = xmm8[0,1,2],mem[0]
	vinsertps	$16, 108(%r15,%rcx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	576(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 108(%r15,%rcx,4), %xmm15, %xmm8 # xmm8 = xmm15[0,1,2],mem[0]
	vinsertps	$32, 108(%r15,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	352(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 108(%r15,%rcx,4), %xmm13, %xmm3 # xmm3 = xmm13[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm8, %ymm8
	vmovss	108(%r15,%r12,4), %xmm7         # xmm7 = mem[0],zero,zero,zero
	movq	256(%rsp), %rcx                 # 8-byte Reload
	vinsertps	$48, 108(%r15,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 108(%r15,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 108(%r15,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 108(%r15,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 108(%r15,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vinsertf128	$1, %xmm5, %ymm7, %ymm3
	movq	1776(%rsp), %rax                # 8-byte Reload
	vfmadd231ps	(%r14,%rax,4), %ymm9, %ymm6 # ymm6 = (ymm9 * mem) + ymm6
	vfmadd231ps	32(%r14,%rax,4), %ymm8, %ymm4 # ymm4 = (ymm8 * mem) + ymm4
	vfmadd231ps	64(%r14,%rax,4), %ymm2, %ymm1 # ymm1 = (ymm2 * mem) + ymm1
	vfmadd231ps	96(%r14,%rax,4), %ymm3, %ymm0 # ymm0 = (ymm3 * mem) + ymm0
	vbroadcastss	.LCPI0_68(%rip), %ymm2  # ymm2 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vmovaps	2272(%rsp), %ymm3               # 32-byte Reload
	vfmadd231ps	%ymm6, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm6) + ymm3
	vmovaps	%ymm3, 2272(%rsp)               # 32-byte Spill
	vmovaps	2240(%rsp), %ymm3               # 32-byte Reload
	vfmadd231ps	%ymm4, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm4) + ymm3
	vmovaps	%ymm3, 2240(%rsp)               # 32-byte Spill
	vmovaps	2080(%rsp), %ymm3               # 32-byte Reload
	vfmadd231ps	%ymm1, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm1) + ymm3
	vmovaps	%ymm3, 2080(%rsp)               # 32-byte Spill
	vmovaps	2048(%rsp), %ymm1               # 32-byte Reload
	vfmadd231ps	%ymm0, %ymm2, %ymm1     # ymm1 = (ymm2 * ymm0) + ymm1
	vmovaps	%ymm1, 2048(%rsp)               # 32-byte Spill
	vpbroadcastq	.LCPI0_69(%rip), %ymm0  # ymm0 = [32,32,32,32]
	vmovdqa	2560(%rsp), %ymm2               # 32-byte Reload
	vpaddq	%ymm0, %ymm2, %ymm2
	vmovdqa	2592(%rsp), %ymm4               # 32-byte Reload
	vpaddq	%ymm0, %ymm4, %ymm4
	addq	1416(%rsp), %r14                # 8-byte Folded Reload
	movq	%r14, 976(%rsp)                 # 8-byte Spill
	addq	$-32, 1784(%rsp)                # 8-byte Folded Spill
	jne	.LBB0_201
# %bb.202:                              # %middle.block903
                                        #   in Loop: Header=BB0_198 Depth=1
	vmovaps	2240(%rsp), %ymm0               # 32-byte Reload
	vaddps	2272(%rsp), %ymm0, %ymm0        # 32-byte Folded Reload
	vaddps	2080(%rsp), %ymm0, %ymm0        # 32-byte Folded Reload
	vaddps	2048(%rsp), %ymm0, %ymm0        # 32-byte Folded Reload
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm10
	movq	1032(%rsp), %rax                # 8-byte Reload
	movq	%rax, %rdi
	cmpq	104(%rsp), %rax                 # 8-byte Folded Reload
	vmovss	.LCPI0_48(%rip), %xmm6          # xmm6 = mem[0],zero,zero,zero
	je	.LBB0_197
	jmp	.LBB0_204
	.p2align	4, 0x90
.LBB0_203:                              #   in Loop: Header=BB0_198 Depth=1
	vxorps	%xmm10, %xmm10, %xmm10
	xorl	%edi, %edi
.LBB0_204:                              # %"for f7.s1.r77$x.preheader1323"
                                        #   in Loop: Header=BB0_198 Depth=1
	movq	176(%rsp), %rcx                 # 8-byte Reload
	imulq	%rdi, %rcx
	movq	688(%rsp), %rax                 # 8-byte Reload
	addq	%rcx, %rax
	movq	%rax, 192(%rsp)                 # 8-byte Spill
	movq	1704(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movq	1712(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 224(%rsp)                 # 8-byte Spill
	movq	1720(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	movq	1728(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 384(%rsp)                 # 8-byte Spill
	movq	1040(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 288(%rsp)                 # 8-byte Spill
	movq	728(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 512(%rsp)                 # 8-byte Spill
	movq	1736(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 16(%rsp)                  # 8-byte Spill
	movq	736(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 576(%rsp)                 # 8-byte Spill
	movq	1744(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 32(%rsp)                  # 8-byte Spill
	movq	672(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 80(%rsp)                  # 8-byte Spill
	movq	1048(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 160(%rsp)                 # 8-byte Spill
	movq	1056(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	movq	1064(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 640(%rsp)                 # 8-byte Spill
	movq	1400(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 48(%rsp)                  # 8-byte Spill
	movq	744(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 480(%rsp)                 # 8-byte Spill
	movq	1072(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 256(%rsp)                 # 8-byte Spill
	movq	1080(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rax
	movq	%rax, 928(%rsp)                 # 8-byte Spill
	movq	432(%rsp), %rax                 # 8-byte Reload
	leaq	(%rax,%rcx), %r12
	movq	1088(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %r13
	movq	1096(%rsp), %rax                # 8-byte Reload
	leaq	(%rax,%rcx), %rsi
	addq	680(%rsp), %rcx                 # 8-byte Folded Reload
	movq	%rdi, %rax
	shlq	$7, %rax
	addq	696(%rsp), %rax                 # 8-byte Folded Reload
	movq	440(%rsp), %r8                  # 8-byte Reload
	jmp	.LBB0_206
	.p2align	4, 0x90
.LBB0_205:                              # %"for f7.s1.r77$x"
                                        #   in Loop: Header=BB0_206 Depth=2
	cmovbq	%r14, %r10
	movq	640(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%r8,%rdx,4), %xmm1, %xmm2
	vmulss	(%r10), %xmm2, %xmm2
	cmovbq	%r11, %r9
	vdivss	%xmm3, %xmm2, %xmm2
	movq	48(%rsp), %rdx                  # 8-byte Reload
	vmovss	(%r8,%rdx,4), %xmm3             # xmm3 = mem[0],zero,zero,zero
	vfmadd132ss	(%r9), %xmm2, %xmm3     # xmm3 = (xmm3 * mem) + xmm2
	vfmadd213ss	%xmm0, %xmm14, %xmm3    # xmm3 = (xmm14 * xmm3) + xmm0
	vfmadd132ss	92(%rax), %xmm3, %xmm6  # xmm6 = (xmm6 * mem) + xmm3
	vmulss	(%r8,%rsi,4), %xmm14, %xmm0
	vmulss	(%r8,%rcx,4), %xmm14, %xmm2
	vmulss	24(%rax), %xmm2, %xmm2
	vfmadd231ss	20(%rax), %xmm0, %xmm2  # xmm2 = (xmm0 * mem) + xmm2
	vmulss	(%r8,%r13,4), %xmm4, %xmm0
	vfmadd231ss	28(%rax), %xmm0, %xmm2  # xmm2 = (xmm0 * mem) + xmm2
	vmulss	(%r8,%r12,4), %xmm5, %xmm0
	vfmadd231ss	32(%rax), %xmm0, %xmm2  # xmm2 = (xmm0 * mem) + xmm2
	vmulss	%xmm4, %xmm12, %xmm15
	vmulss	%xmm5, %xmm12, %xmm3
	vmulss	%xmm5, %xmm13, %xmm5
	vmulss	%xmm4, %xmm13, %xmm0
	movq	928(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%r8,%rdx,4), %xmm4, %xmm4
	vfmadd231ss	36(%rax), %xmm4, %xmm2  # xmm2 = (xmm4 * mem) + xmm2
	vfmadd231ss	40(%rax), %xmm5, %xmm2  # xmm2 = (xmm5 * mem) + xmm2
	vfmadd231ss	44(%rax), %xmm0, %xmm2  # xmm2 = (xmm0 * mem) + xmm2
	vfmadd231ss	48(%rax), %xmm3, %xmm2  # xmm2 = (xmm3 * mem) + xmm2
	vfmadd231ss	52(%rax), %xmm15, %xmm2 # xmm2 = (xmm15 * mem) + xmm2
	movq	256(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%r8,%rdx,4), %xmm11, %xmm0
	vfmadd231ss	56(%rax), %xmm0, %xmm2  # xmm2 = (xmm0 * mem) + xmm2
	movq	480(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%r8,%rdx,4), %xmm11, %xmm0
	vfmadd231ss	60(%rax), %xmm0, %xmm2  # xmm2 = (xmm0 * mem) + xmm2
	vfmadd231ss	.LCPI0_67(%rip), %xmm6, %xmm7 # xmm7 = (xmm6 * mem) + xmm7
	vaddss	%xmm2, %xmm7, %xmm0
	vfmadd231ss	96(%rax), %xmm14, %xmm0 # xmm0 = (xmm14 * mem) + xmm0
	vmovss	.LCPI0_48(%rip), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vcmpltss	%xmm1, %xmm6, %xmm2
	vmovss	100(%rax), %xmm3                # xmm3 = mem[0],zero,zero,zero
	vandps	%xmm3, %xmm2, %xmm2
	vfmadd213ss	%xmm0, %xmm9, %xmm2     # xmm2 = (xmm9 * xmm2) + xmm0
	vaddss	.LCPI0_47(%rip), %xmm1, %xmm0
	vmulss	%xmm0, %xmm9, %xmm0
	vfmadd231ss	104(%rax), %xmm0, %xmm2 # xmm2 = (xmm0 * mem) + xmm2
	vfmadd231ss	108(%rax), %xmm8, %xmm2 # xmm2 = (xmm8 * mem) + xmm2
	vfmadd231ss	.LCPI0_68(%rip), %xmm2, %xmm10 # xmm10 = (xmm2 * mem) + xmm10
	incq	%rdi
	addq	1432(%rsp), %r8                 # 8-byte Folded Reload
	subq	$-128, %rax
	cmpq	%rdi, 104(%rsp)                 # 8-byte Folded Reload
	je	.LBB0_197
.LBB0_206:                              # %"for f7.s1.r77$x"
                                        #   Parent Loop BB0_198 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	movq	192(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%r8,%rdx,4), %xmm14            # xmm14 = mem[0],zero,zero,zero
	movq	128(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%r8,%rdx,4), %xmm4             # xmm4 = mem[0],zero,zero,zero
	movq	320(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%r8,%rdx,4), %xmm5             # xmm5 = mem[0],zero,zero,zero
	movq	384(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%r8,%rdx,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	movq	288(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%r8,%rdx,4), %xmm1, %xmm15
	vmaxss	%xmm6, %xmm15, %xmm11
	vdivss	2640(%rsp), %xmm11, %xmm7       # 16-byte Folded Reload
	movq	512(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%r8,%rdx,4), %xmm8             # xmm8 = mem[0],zero,zero,zero
	leaq	68(%rax), %rbx
	leaq	72(%rax), %r10
	leaq	80(%rax), %rdx
	leaq	84(%rax), %r9
	testq	%rdi, %rdi
	cmoveq	%rbx, %r10
	cmoveq	%rdx, %r9
	leaq	64(%rax), %r14
	leaq	76(%rax), %r11
	movq	224(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%r8,%rdx,4), %xmm4, %xmm0
	vmulss	4(%rax), %xmm5, %xmm3
	vfmadd231ss	(%rax), %xmm0, %xmm3    # xmm3 = (xmm0 * mem) + xmm3
	vmulss	12(%rax), %xmm5, %xmm2
	vfmadd231ss	8(%rax), %xmm0, %xmm2   # xmm2 = (xmm0 * mem) + xmm2
	vucomiss	%xmm1, %xmm6
	vxorps	%xmm0, %xmm0, %xmm0
	movq	160(%rsp), %rdx                 # 8-byte Reload
	vcmpeqss	(%r8,%rdx,4), %xmm0, %xmm9
	movq	16(%rsp), %rdx                  # 8-byte Reload
	vmovss	(%r8,%rdx,4), %xmm12            # xmm12 = mem[0],zero,zero,zero
	vblendvps	%xmm9, %xmm3, %xmm2, %xmm2
	movq	576(%rsp), %rdx                 # 8-byte Reload
	vmovss	(%r8,%rdx,4), %xmm13            # xmm13 = mem[0],zero,zero,zero
	vroundss	$10, %xmm7, %xmm7, %xmm3
	vmulss	%xmm2, %xmm3, %xmm2
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vmovss	(%r8,%rdx,4), %xmm9             # xmm9 = mem[0],zero,zero,zero
	vmaxss	%xmm6, %xmm8, %xmm3
	vmaxss	%xmm6, %xmm7, %xmm6
	vdivss	%xmm6, %xmm2, %xmm7
	vmovss	.LCPI0_66(%rip), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vdivss	%xmm3, %xmm2, %xmm2
	vminss	%xmm1, %xmm2, %xmm2
	movq	112(%rsp), %rdx                 # 8-byte Reload
	vmulss	(%r8,%rdx,4), %xmm2, %xmm2
	vmulss	%xmm2, %xmm15, %xmm6
	movq	80(%rsp), %rdx                  # 8-byte Reload
	vmovss	(%r8,%rdx,4), %xmm8             # xmm8 = mem[0],zero,zero,zero
	jae	.LBB0_205
# %bb.207:                              #   in Loop: Header=BB0_206 Depth=2
	vaddss	%xmm5, %xmm4, %xmm0
	vmulss	88(%rax), %xmm0, %xmm0
	vdivss	%xmm3, %xmm0, %xmm0
	jmp	.LBB0_205
.LBB0_208:                              # %entry
	leaq	.LJTI0_0(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB0_209:                              # %assert_failed
	leaq	.Lstr(%rip), %rsi
	jmp	.LBB0_242
.LBB0_210:                              # %"assert failed198"
	movl	2688(%rsp), %edx                # 4-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ecx
	imulq	%rcx, %rax
	shlq	$5, %rax
	leaq	(%rax,%rax,2), %rdx
	leaq	.Lstr.69(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_222
.LBB0_211:                              # %"assert failed206"
	movl	2688(%rsp), %edx                # 4-byte Reload
	movl	%edx, %ecx
	sarl	$31, %ecx
	andnl	%edx, %ecx, %ecx
	leal	1(%rax), %edx
	xorl	%esi, %esi
	testl	%eax, %eax
	cmovsl	%esi, %edx
	imulq	%rcx, %rdx
	shlq	$7, %rdx
	leaq	.Lstr.71(%rip), %rsi
	jmp	.LBB0_224
.LBB0_212:                              # %destructor_block
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.72(%rip), %rsi
	leaq	.Lstr.4(%rip), %rdx
	movl	$0, %edi
	xorl	%ecx, %ecx
	movl	1736(%rsp), %r8d                # 4-byte Reload
	movq	744(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	callq	halide_error_explicit_bounds_too_small@PLT
	movl	%eax, %ebx
	testl	%eax, %eax
	jne	.LBB0_227
	jmp	.LBB0_194
.LBB0_213:                              # %true_bb49
	movl	%r15d, 464(%rsp)                # 4-byte Spill
	movl	1984(%rsp), %r10d               # 4-byte Reload
	movl	1988(%rsp), %r9d                # 4-byte Reload
	movl	2004(%rsp), %r13d               # 4-byte Reload
	movl	2008(%rsp), %r12d               # 4-byte Reload
	movl	2012(%rsp), %r15d               # 4-byte Reload
	movl	2016(%rsp), %r14d               # 4-byte Reload
	movl	2020(%rsp), %ebx                # 4-byte Reload
	movl	2024(%rsp), %r11d               # 4-byte Reload
	movq	752(%rsp), %r8                  # 8-byte Reload
	movl	2028(%rsp), %edi                # 4-byte Reload
	movl	2032(%rsp), %esi                # 4-byte Reload
	movq	480(%rsp), %rcx                 # 8-byte Reload
	leaq	.LJTI0_1(%rip), %rdx
	movslq	(%rdx,%rax,4), %rax
	addq	%rdx, %rax
	jmpq	*%rax
.LBB0_214:                              # %assert_failed53
	leaq	.Lstr.16(%rip), %rsi
	xorl	%edi, %edi
	movl	2400(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_215:                              # %no_errors_bb52
	movq	752(%rsp), %r9                  # 8-byte Reload
	movq	2336(%rsp), %rbx                # 8-byte Reload
	leaq	.LJTI0_2(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB0_216:                              # %assert_failed117
	leaq	.Lstr.28(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	816(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_280
.LBB0_217:                              # %no_errors_bb116
	movq	1632(%rsp), %r13                # 8-byte Reload
	movq	1600(%rsp), %r12                # 8-byte Reload
	movq	1664(%rsp), %r15                # 8-byte Reload
	movq	1824(%rsp), %r14                # 8-byte Reload
	movq	608(%rsp), %r11                 # 8-byte Reload
	movq	560(%rsp), %r10                 # 8-byte Reload
	movq	128(%rsp), %r9                  # 8-byte Reload
	movq	1440(%rsp), %r8                 # 8-byte Reload
	movq	1472(%rsp), %rbx                # 8-byte Reload
	movq	1216(%rsp), %rdx                # 8-byte Reload
	leaq	.LJTI0_3(%rip), %rcx
	movslq	(%rcx,%rax,4), %rax
	addq	%rcx, %rax
	jmpq	*%rax
.LBB0_218:                              # %assert_failed150
	leaq	.Lstr.13(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r10, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_219:                              # %"assert failed194"
	movl	2688(%rsp), %ecx                # 4-byte Reload
	movl	%ecx, %eax
	sarl	$31, %eax
	andnl	%ecx, %eax, %edx
	shlq	$7, %rdx
	leaq	.Lstr.68(%rip), %rsi
	xorl	%r14d, %r14d
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_229
.LBB0_220:                              # %"assert failed196"
	xorl	%r14d, %r14d
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
	jmp	.LBB0_229
.LBB0_221:                              # %"assert failed200"
	xorl	%edi, %edi
	callq	halide_error_out_of_memory@PLT
.LBB0_222:                              # %call_destructor.exit340.thread407
	movl	%eax, %r12d
	movq	192(%rsp), %r14                 # 8-byte Reload
	jmp	.LBB0_190
.LBB0_223:                              # %"assert failed202"
	movl	%edx, %eax
	sarl	$31, %eax
	andnl	%edx, %eax, %eax
	imulq	$156, %rax, %rdx
	leaq	.Lstr.70(%rip), %rsi
.LBB0_224:                              # %call_destructor.exit340
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	movl	%eax, %r12d
	testl	%r12d, %r12d
	jne	.LBB0_161
.LBB0_225:
	xorl	%r12d, %r12d
	jmp	.LBB0_194
.LBB0_226:
	movl	%eax, %ebx
.LBB0_227:                              # %destructor_block.thread
	xorl	%edi, %edi
	movq	464(%rsp), %rsi                 # 8-byte Reload
	callq	halide_free@PLT
	movl	%ebx, %r12d
	jmp	.LBB0_194
.LBB0_228:                              # %"assert failed"
	movq	1632(%rsp), %rax                # 8-byte Reload
	addl	$23, %eax
	movl	%eax, (%rsp)
	leaq	.Lstr.15(%rip), %rsi
	xorl	%r14d, %r14d
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movl	176(%rsp), %r9d                 # 4-byte Reload
	callq	halide_error_constraints_make_required_region_smaller@PLT
.LBB0_229:                              # %call_destructor.exit340.thread407
	movl	%eax, %r12d
	jmp	.LBB0_190
.LBB0_230:                              # %assert_failed1
	leaq	.Lstr.3(%rip), %rsi
	jmp	.LBB0_242
.LBB0_231:                              # %assert_failed2
	leaq	.Lstr.4(%rip), %rsi
	jmp	.LBB0_242
.LBB0_232:                              # %assert_failed3
	leaq	.Lstr.5(%rip), %rsi
	jmp	.LBB0_242
.LBB0_233:                              # %assert_failed4
	leaq	.Lstr.6(%rip), %rsi
	jmp	.LBB0_242
.LBB0_234:                              # %assert_failed5
	leaq	.Lstr.7(%rip), %rsi
	jmp	.LBB0_242
.LBB0_235:                              # %assert_failed6
	leaq	.Lstr.8(%rip), %rsi
	jmp	.LBB0_242
.LBB0_236:                              # %assert_failed7
	leaq	.Lstr.9(%rip), %rsi
	jmp	.LBB0_242
.LBB0_237:                              # %assert_failed8
	leaq	.Lstr.10(%rip), %rsi
	jmp	.LBB0_242
.LBB0_238:                              # %assert_failed9
	leaq	.Lstr.11(%rip), %rsi
	jmp	.LBB0_242
.LBB0_239:                              # %assert_failed10
	leaq	.Lstr.12(%rip), %rsi
	jmp	.LBB0_242
.LBB0_240:                              # %assert_failed11
	leaq	.Lstr.13(%rip), %rsi
	jmp	.LBB0_242
.LBB0_241:                              # %assert_failed12
	leaq	.Lstr.14(%rip), %rsi
.LBB0_242:                              # %assert_failed
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_buffer_argument_is_null@PLT
	jmp	.LBB0_195
.LBB0_243:                              # %assert_failed118
	movq	%r11, %rdx
	leaq	.Lstr.30(%rip), %rsi
	leaq	.Lstr.31(%rip), %rcx
	xorl	%edi, %edi
	jmp	.LBB0_272
.LBB0_244:                              # %assert_failed119
	leaq	.Lstr.32(%rip), %rsi
	leaq	.Lstr.33(%rip), %rcx
	xorl	%edi, %edi
	movl	%r8d, %edx
	jmp	.LBB0_252
.LBB0_245:                              # %assert_failed120
	leaq	.Lstr.34(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	2176(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_280
.LBB0_246:                              # %assert_failed121
	leaq	.Lstr.35(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	704(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_280
.LBB0_247:                              # %assert_failed122
	leaq	.Lstr.36(%rip), %rsi
	leaq	.Lstr.31(%rip), %rcx
	xorl	%edi, %edi
	movq	1152(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_272
.LBB0_248:                              # %assert_failed123
	leaq	.Lstr.37(%rip), %rsi
	leaq	.Lstr.33(%rip), %rcx
	xorl	%edi, %edi
	movq	368(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_251
.LBB0_249:                              # %assert_failed124
	leaq	.Lstr.38(%rip), %rsi
	leaq	.Lstr.31(%rip), %rcx
	xorl	%edi, %edi
	movq	768(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_272
.LBB0_250:                              # %assert_failed125
	leaq	.Lstr.39(%rip), %rsi
	leaq	.Lstr.33(%rip), %rcx
	xorl	%edi, %edi
	movq	352(%rsp), %rdx                 # 8-byte Reload
.LBB0_251:                              # %assert_failed123
                                        # kill: def $edx killed $edx killed $rdx
.LBB0_252:                              # %assert_failed123
	movl	$32, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_195
.LBB0_253:                              # %assert_failed126
	leaq	.Lstr.40(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	1792(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_280
.LBB0_254:                              # %assert_failed127
	leaq	.Lstr.41(%rip), %rsi
	leaq	.Lstr.31(%rip), %rcx
	xorl	%edi, %edi
	movq	864(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_272
.LBB0_255:                              # %assert_failed128
	leaq	.Lstr.42(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movq	1184(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_259
.LBB0_256:                              # %assert_failed129
	leaq	.Lstr.44(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	1920(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_280
.LBB0_257:                              # %assert_failed130
	leaq	.Lstr.45(%rip), %rsi
	leaq	.Lstr.31(%rip), %rcx
	xorl	%edi, %edi
	movq	832(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_272
.LBB0_258:                              # %assert_failed131
	leaq	.Lstr.46(%rip), %rsi
	leaq	.Lstr.43(%rip), %rcx
	xorl	%edi, %edi
	movq	224(%rsp), %rdx                 # 8-byte Reload
.LBB0_259:                              # %assert_failed128
                                        # kill: def $edx killed $edx killed $rdx
	movl	$8, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_195
.LBB0_260:                              # %assert_failed132
	leaq	.Lstr.47(%rip), %rsi
	leaq	.Lstr.31(%rip), %rcx
	xorl	%edi, %edi
	movq	896(%rsp), %rdx                 # 8-byte Reload
	jmp	.LBB0_272
.LBB0_261:                              # %assert_failed133
	leaq	.Lstr.48(%rip), %rsi
	leaq	.Lstr.49(%rip), %rcx
	xorl	%edi, %edi
	movq	1312(%rsp), %rdx                # 8-byte Reload
                                        # kill: def $edx killed $edx killed $rdx
	movl	$40, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_195
.LBB0_262:                              # %assert_failed134
	leaq	.Lstr.50(%rip), %rsi
	leaq	.Lstr.31(%rip), %rcx
	xorl	%edi, %edi
	movl	%ebx, %edx
	jmp	.LBB0_273
.LBB0_263:                              # %assert_failed135
	leaq	.Lstr.51(%rip), %rsi
	leaq	.Lstr.52(%rip), %rcx
	xorl	%edi, %edi
	movl	%r9d, %edx
	movl	$7, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_195
.LBB0_264:                              # %assert_failed136
	leaq	.Lstr.53(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	2304(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_280
.LBB0_265:                              # %assert_failed137
	leaq	.Lstr.54(%rip), %rsi
	leaq	.Lstr.31(%rip), %rcx
	xorl	%edi, %edi
	movq	1120(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_272
.LBB0_266:                              # %assert_failed138
	leaq	.Lstr.55(%rip), %rsi
	leaq	.Lstr.56(%rip), %rcx
	xorl	%edi, %edi
	movq	1504(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_270
.LBB0_267:                              # %assert_failed139
	leaq	.Lstr.57(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	2496(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_280
.LBB0_268:                              # %assert_failed140
	leaq	.Lstr.58(%rip), %rsi
	leaq	.Lstr.31(%rip), %rcx
	xorl	%edi, %edi
	movq	1216(%rsp), %rdx                # 8-byte Reload
	jmp	.LBB0_272
.LBB0_269:                              # %assert_failed141
	leaq	.Lstr.59(%rip), %rsi
	leaq	.Lstr.56(%rip), %rcx
	xorl	%edi, %edi
	movq	1440(%rsp), %rdx                # 8-byte Reload
.LBB0_270:                              # %assert_failed138
                                        # kill: def $edx killed $edx killed $rdx
	movl	$24, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_195
.LBB0_271:                              # %assert_failed142
	leaq	.Lstr.60(%rip), %rsi
	leaq	.Lstr.31(%rip), %rcx
	xorl	%edi, %edi
	movq	1472(%rsp), %rdx                # 8-byte Reload
.LBB0_272:                              # %assert_failed118
                                        # kill: def $edx killed $edx killed $rdx
.LBB0_273:                              # %assert_failed118
	xorl	%r8d, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_195
.LBB0_274:                              # %assert_failed143
	leaq	.Lstr.61(%rip), %rsi
	leaq	.Lstr.62(%rip), %rcx
	xorl	%edi, %edi
	movq	560(%rsp), %rdx                 # 8-byte Reload
                                        # kill: def $edx killed $edx killed $rdx
	movl	$39, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_195
.LBB0_275:                              # %assert_failed144
	leaq	.Lstr.63(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	2464(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_280
.LBB0_276:                              # %assert_failed145
	leaq	.Lstr.64(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	2432(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_280
.LBB0_277:                              # %assert_failed146
	leaq	.Lstr.65(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	2144(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_280
.LBB0_278:                              # %assert_failed147
	leaq	.Lstr.66(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	2240(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_280
.LBB0_279:                              # %assert_failed148
	leaq	.Lstr.67(%rip), %rsi
	leaq	.Lstr.29(%rip), %rcx
	xorl	%edi, %edi
	movl	2912(%rsp), %edx                # 4-byte Reload
.LBB0_280:                              # %assert_failed117
	movl	$1, %r8d
	vzeroupper
	callq	halide_error_constraint_violated@PLT
	jmp	.LBB0_195
.LBB0_281:                              # %assert_failed151
	leaq	.Lstr.12(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r11, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_282:                              # %assert_failed152
	leaq	.Lstr.10(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r14, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_283:                              # %assert_failed153
	leaq	.Lstr.10(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r15, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_284:                              # %assert_failed154
	leaq	.Lstr.8(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r12, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_285:                              # %assert_failed155
	leaq	.Lstr.7(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r13, %rdx
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_286:                              # %assert_failed156
	leaq	.Lstr.5(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1568(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_287:                              # %assert_failed157
	leaq	.Lstr.5(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1856(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_288:                              # %assert_failed158
	leaq	.Lstr.5(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB0_195
.LBB0_289:                              # %assert_failed159
	leaq	.Lstr.5(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	816(%rsp), %rdx                 # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_290:                              # %assert_failed160
	leaq	.Lstr.5(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%rbx, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB0_195
.LBB0_291:                              # %assert_failed161
	leaq	.Lstr.4(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	2176(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_292:                              # %assert_failed162
	leaq	.Lstr.3(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	704(%rsp), %rdx                 # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_293:                              # %assert_failed163
	leaq	.Lstr.3(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1792(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_294:                              # %assert_failed164
	leaq	.Lstr.3(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r8, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB0_195
.LBB0_295:                              # %assert_failed165
	leaq	.Lstr.3(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	1920(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_296:                              # %assert_failed166
	leaq	.Lstr.3(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	%r9, %rdx
	vzeroupper
	callq	halide_error_buffer_extents_too_large@PLT
	jmp	.LBB0_195
.LBB0_297:                              # %assert_failed167
	leaq	.Lstr(%rip), %rsi
	movl	$2147483647, %ecx               # imm = 0x7FFFFFFF
	xorl	%edi, %edi
	movq	2304(%rsp), %rdx                # 8-byte Reload
	vzeroupper
	callq	halide_error_buffer_allocation_too_large@PLT
	jmp	.LBB0_195
.LBB0_298:                              # %assert_failed168
	leaq	.Lstr.16(%rip), %rsi
	jmp	.LBB0_311
.LBB0_299:                              # %assert_failed169
	leaq	.Lstr.17(%rip), %rsi
	jmp	.LBB0_311
.LBB0_300:                              # %assert_failed170
	leaq	.Lstr.15(%rip), %rsi
	jmp	.LBB0_311
.LBB0_301:                              # %assert_failed171
	leaq	.Lstr.18(%rip), %rsi
	jmp	.LBB0_311
.LBB0_302:                              # %assert_failed172
	leaq	.Lstr.19(%rip), %rsi
	jmp	.LBB0_311
.LBB0_303:                              # %assert_failed173
	leaq	.Lstr.20(%rip), %rsi
	jmp	.LBB0_311
.LBB0_304:                              # %assert_failed174
	leaq	.Lstr.21(%rip), %rsi
	jmp	.LBB0_311
.LBB0_305:                              # %assert_failed175
	leaq	.Lstr.22(%rip), %rsi
	jmp	.LBB0_311
.LBB0_306:                              # %assert_failed176
	leaq	.Lstr.23(%rip), %rsi
	jmp	.LBB0_311
.LBB0_307:                              # %assert_failed177
	leaq	.Lstr.24(%rip), %rsi
	jmp	.LBB0_311
.LBB0_308:                              # %assert_failed178
	leaq	.Lstr.25(%rip), %rsi
	jmp	.LBB0_311
.LBB0_309:                              # %assert_failed179
	leaq	.Lstr.26(%rip), %rsi
	jmp	.LBB0_311
.LBB0_310:                              # %assert_failed180
	leaq	.Lstr.27(%rip), %rsi
.LBB0_311:                              # %assert_failed168
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_device_dirty_with_no_device_support@PLT
	jmp	.LBB0_195
.LBB0_312:                              # %assert_failed181
	leaq	.Lstr.16(%rip), %rsi
	jmp	.LBB0_325
.LBB0_313:                              # %assert_failed182
	leaq	.Lstr.17(%rip), %rsi
	jmp	.LBB0_325
.LBB0_314:                              # %assert_failed183
	leaq	.Lstr.15(%rip), %rsi
	jmp	.LBB0_325
.LBB0_315:                              # %assert_failed184
	leaq	.Lstr.18(%rip), %rsi
	jmp	.LBB0_325
.LBB0_316:                              # %assert_failed185
	leaq	.Lstr.19(%rip), %rsi
	jmp	.LBB0_325
.LBB0_317:                              # %assert_failed186
	leaq	.Lstr.20(%rip), %rsi
	jmp	.LBB0_325
.LBB0_318:                              # %assert_failed187
	leaq	.Lstr.21(%rip), %rsi
	jmp	.LBB0_325
.LBB0_319:                              # %assert_failed188
	leaq	.Lstr.22(%rip), %rsi
	jmp	.LBB0_325
.LBB0_320:                              # %assert_failed189
	leaq	.Lstr.23(%rip), %rsi
	jmp	.LBB0_325
.LBB0_321:                              # %assert_failed190
	leaq	.Lstr.24(%rip), %rsi
	jmp	.LBB0_325
.LBB0_322:                              # %assert_failed191
	leaq	.Lstr.25(%rip), %rsi
	jmp	.LBB0_325
.LBB0_323:                              # %assert_failed192
	leaq	.Lstr.26(%rip), %rsi
	jmp	.LBB0_325
.LBB0_324:                              # %assert_failed193
	leaq	.Lstr.27(%rip), %rsi
.LBB0_325:                              # %assert_failed181
	xorl	%edi, %edi
	vzeroupper
	callq	halide_error_host_is_null@PLT
	jmp	.LBB0_195
.LBB0_326:                              # %assert_failed54
	leaq	.Lstr.16(%rip), %rsi
	xorl	%edi, %edi
	movl	1888(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_354
.LBB0_327:                              # %assert_failed55
	leaq	.Lstr.17(%rip), %rsi
	xorl	%edi, %edi
	movl	2112(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_328:                              # %assert_failed56
	leaq	.Lstr.17(%rip), %rsi
	xorl	%edi, %edi
	movl	2272(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_354
.LBB0_329:                              # %assert_failed57
	leaq	.Lstr.15(%rip), %rsi
	xorl	%edi, %edi
	movl	2080(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_330:                              # %assert_failed58
	leaq	.Lstr.15(%rip), %rsi
	xorl	%edi, %edi
	movl	2048(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_339
.LBB0_331:                              # %assert_failed59
	leaq	.Lstr.18(%rip), %rsi
	xorl	%edi, %edi
	movl	2208(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_332:                              # %assert_failed60
	leaq	.Lstr.18(%rip), %rsi
	xorl	%edi, %edi
	movl	976(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_354
.LBB0_333:                              # %assert_failed61
	leaq	.Lstr.19(%rip), %rsi
	xorl	%edi, %edi
	movl	960(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_352
.LBB0_334:                              # %assert_failed62
	leaq	.Lstr.19(%rip), %rsi
	xorl	%edi, %edi
	movl	800(%rsp), %edx                 # 4-byte Reload
	jmp	.LBB0_350
.LBB0_335:                              # %assert_failed63
	leaq	.Lstr.20(%rip), %rsi
	xorl	%edi, %edi
	movl	2592(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_336:                              # %assert_failed64
	leaq	.Lstr.20(%rip), %rsi
	xorl	%edi, %edi
	movl	2560(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_354
.LBB0_337:                              # %assert_failed65
	leaq	.Lstr.21(%rip), %rsi
	xorl	%edi, %edi
	movl	2976(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_338:                              # %assert_failed66
	leaq	.Lstr.21(%rip), %rsi
	xorl	%edi, %edi
	movl	2944(%rsp), %edx                # 4-byte Reload
.LBB0_339:                              # %assert_failed58
	movl	$2, %ecx
	vzeroupper
	callq	halide_error_bad_dimensions@PLT
	jmp	.LBB0_195
.LBB0_340:                              # %assert_failed67
	leaq	.Lstr.22(%rip), %rsi
	xorl	%edi, %edi
	movl	3136(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_341:                              # %assert_failed68
	leaq	.Lstr.22(%rip), %rsi
	xorl	%edi, %edi
	movl	3104(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_354
.LBB0_342:                              # %assert_failed69
	leaq	.Lstr.23(%rip), %rsi
	xorl	%edi, %edi
	movl	3072(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_343:                              # %assert_failed70
	leaq	.Lstr.23(%rip), %rsi
	xorl	%edi, %edi
	movl	2816(%rsp), %edx                # 4-byte Reload
	xorl	%ecx, %ecx
	vzeroupper
	callq	halide_error_bad_dimensions@PLT
	jmp	.LBB0_195
.LBB0_344:                              # %assert_failed71
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	movl	2784(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_345:                              # %assert_failed72
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	movl	2368(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_350
.LBB0_346:                              # %assert_failed73
	leaq	.Lstr.25(%rip), %rsi
	xorl	%edi, %edi
	movl	2752(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_347:                              # %assert_failed74
	leaq	.Lstr.25(%rip), %rsi
	xorl	%edi, %edi
	movl	3040(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_354
.LBB0_348:                              # %assert_failed75
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	2720(%rsp), %edx                # 4-byte Reload
	jmp	.LBB0_352
.LBB0_349:                              # %assert_failed76
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	3232(%rsp), %edx                # 4-byte Reload
.LBB0_350:                              # %assert_failed62
	movl	$3, %ecx
	vzeroupper
	callq	halide_error_bad_dimensions@PLT
	jmp	.LBB0_195
.LBB0_351:                              # %assert_failed77
	leaq	.Lstr.27(%rip), %rsi
	xorl	%edi, %edi
	movl	2848(%rsp), %edx                # 4-byte Reload
.LBB0_352:                              # %assert_failed53
	movl	$73730, %ecx                    # imm = 0x12002
	vzeroupper
	callq	halide_error_bad_type@PLT
	jmp	.LBB0_195
.LBB0_353:                              # %assert_failed78
	leaq	.Lstr.27(%rip), %rsi
	xorl	%edi, %edi
	movl	104(%rsp), %edx                 # 4-byte Reload
.LBB0_354:                              # %assert_failed54
	movl	$1, %ecx
	vzeroupper
	callq	halide_error_bad_dimensions@PLT
	jmp	.LBB0_195
.LBB0_355:                              # %assert_failed79
	decl	%esi
	movl	%esi, (%rsp)
	leaq	.Lstr.16(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	128(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_356:                              # %assert_failed80
	leaq	.Lstr.16(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_357:                              # %assert_failed81
	leaq	.Lstr.17(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1024(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_358:                              # %assert_failed82
	decl	%edi
	movl	%edi, (%rsp)
	leaq	.Lstr.15(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$31, %r8d
	movq	1152(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_359:                              # %assert_failed83
	leaq	.Lstr.15(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	368(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_360:                              # %assert_failed84
	movq	1632(%rsp), %r8                 # 8-byte Reload
	addl	$23, %r8d
	movl	%r11d, %ebx
	decl	%ebx
	movl	%ebx, (%rsp)
	leaq	.Lstr.15(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movl	176(%rsp), %ecx                 # 4-byte Reload
                                        # kill: def $r8d killed $r8d killed $r8
	movq	768(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_361:                              # %assert_failed85
	leaq	.Lstr.15(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	352(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_362:                              # %assert_failed86
	decl	%ebx
	movl	%ebx, (%rsp)
	leaq	.Lstr.18(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movq	864(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_363:                              # %assert_failed87
	leaq	.Lstr.18(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1184(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_364:                              # %assert_failed88
	decl	%r14d
	movl	%r14d, (%rsp)
	leaq	.Lstr.19(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$7, %r8d
	movq	832(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_365:                              # %assert_failed89
	leaq	.Lstr.19(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	224(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_366:                              # %assert_failed90
	decl	%r15d
	movl	%r15d, (%rsp)
	leaq	.Lstr.19(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$39, %r8d
	movq	896(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_367:                              # %assert_failed91
	leaq	.Lstr.19(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1312(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_368:                              # %assert_failed92
	decl	%r12d
	movl	%r12d, (%rsp)
	leaq	.Lstr.19(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movl	$6, %r8d
	movq	2336(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_369:                              # %assert_failed93
	leaq	.Lstr.19(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movl	%r8d, %ecx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_370:                              # %assert_failed94
	decl	%r13d
	movl	%r13d, (%rsp)
	leaq	.Lstr.20(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movq	1120(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_371:                              # %assert_failed95
	leaq	.Lstr.20(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1504(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_372:                              # %assert_failed96
	decl	%r9d
	movl	%r9d, (%rsp)
	leaq	.Lstr.21(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$23, %r8d
	movq	1216(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_373:                              # %assert_failed97
	leaq	.Lstr.21(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1440(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_374:                              # %assert_failed98
	decl	%r10d
	movl	%r10d, (%rsp)
	leaq	.Lstr.21(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$38, %r8d
	movq	1472(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_375:                              # %assert_failed99
	leaq	.Lstr.21(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	560(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_376:                              # %assert_failed100
	leaq	.Lstr.22(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1016(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_377:                              # %assert_failed101
	movl	2000(%rsp), %eax                # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movl	$39, %r8d
	movq	1112(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_378:                              # %assert_failed102
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	48(%rsp), %rcx                  # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_379:                              # %assert_failed103
	movl	1996(%rsp), %eax                # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$6, %r8d
	movq	928(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_380:                              # %assert_failed104
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1824(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_381:                              # %assert_failed105
	movq	72(%rsp), %r8                   # 8-byte Reload
	decl	%r8d
	movl	1992(%rsp), %eax                # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
                                        # kill: def $r8d killed $r8d killed $r8
	movq	1104(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_382:                              # %assert_failed106
	leaq	.Lstr.24(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	608(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_383:                              # %assert_failed107
	movl	728(%rsp), %eax                 # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.25(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	xorl	%ecx, %ecx
	movq	1064(%rsp), %r8                 # 8-byte Reload
                                        # kill: def $r8d killed $r8d killed $r8
	movq	744(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_384:                              # %assert_failed108
	leaq	.Lstr.25(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1248(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_385:                              # %assert_failed109
	movl	2880(%rsp), %r8d                # 4-byte Reload
	decl	%r8d
	movl	680(%rsp), %eax                 # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movl	1856(%rsp), %ecx                # 4-byte Reload
	movq	1056(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_386:                              # %assert_failed110
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	640(%rsp), %rcx                 # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_387:                              # %assert_failed111
	movl	432(%rsp), %eax                 # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	xorl	%ecx, %ecx
	movl	$38, %r8d
	movq	472(%rsp), %r9                  # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_388:                              # %assert_failed112
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$1, %edx
	movq	1600(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_389:                              # %assert_failed113
	movl	2688(%rsp), %r8d                # 4-byte Reload
	decl	%r8d
	movl	464(%rsp), %eax                 # 4-byte Reload
	decl	%eax
	movl	%eax, (%rsp)
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	xorl	%ecx, %ecx
	movq	1048(%rsp), %r9                 # 8-byte Reload
                                        # kill: def $r9d killed $r9d killed $r9
	vzeroupper
	callq	halide_error_access_out_of_bounds@PLT
	jmp	.LBB0_195
.LBB0_390:                              # %assert_failed114
	leaq	.Lstr.26(%rip), %rsi
	xorl	%edi, %edi
	movl	$2, %edx
	movq	1664(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.LBB0_391:                              # %assert_failed115
	leaq	.Lstr.27(%rip), %rsi
	xorl	%edi, %edi
	xorl	%edx, %edx
	movq	1392(%rsp), %rcx                # 8-byte Reload
                                        # kill: def $ecx killed $ecx killed $rcx
	vzeroupper
	callq	halide_error_buffer_extents_negative@PLT
	jmp	.LBB0_195
.Lfunc_end0:
	.size	cost_model, .Lfunc_end0-cost_model
	.cfi_endproc
	.section	.rodata.cost_model,"a",@progbits
	.p2align	2
.LJTI0_0:
	.long	.LBB0_209-.LJTI0_0
	.long	.LBB0_230-.LJTI0_0
	.long	.LBB0_231-.LJTI0_0
	.long	.LBB0_232-.LJTI0_0
	.long	.LBB0_233-.LJTI0_0
	.long	.LBB0_234-.LJTI0_0
	.long	.LBB0_235-.LJTI0_0
	.long	.LBB0_236-.LJTI0_0
	.long	.LBB0_237-.LJTI0_0
	.long	.LBB0_238-.LJTI0_0
	.long	.LBB0_239-.LJTI0_0
	.long	.LBB0_240-.LJTI0_0
	.long	.LBB0_241-.LJTI0_0
.LJTI0_1:
	.long	.LBB0_214-.LJTI0_1
	.long	.LBB0_326-.LJTI0_1
	.long	.LBB0_327-.LJTI0_1
	.long	.LBB0_328-.LJTI0_1
	.long	.LBB0_329-.LJTI0_1
	.long	.LBB0_330-.LJTI0_1
	.long	.LBB0_331-.LJTI0_1
	.long	.LBB0_332-.LJTI0_1
	.long	.LBB0_333-.LJTI0_1
	.long	.LBB0_334-.LJTI0_1
	.long	.LBB0_335-.LJTI0_1
	.long	.LBB0_336-.LJTI0_1
	.long	.LBB0_337-.LJTI0_1
	.long	.LBB0_338-.LJTI0_1
	.long	.LBB0_340-.LJTI0_1
	.long	.LBB0_341-.LJTI0_1
	.long	.LBB0_342-.LJTI0_1
	.long	.LBB0_343-.LJTI0_1
	.long	.LBB0_344-.LJTI0_1
	.long	.LBB0_345-.LJTI0_1
	.long	.LBB0_346-.LJTI0_1
	.long	.LBB0_347-.LJTI0_1
	.long	.LBB0_348-.LJTI0_1
	.long	.LBB0_349-.LJTI0_1
	.long	.LBB0_351-.LJTI0_1
	.long	.LBB0_353-.LJTI0_1
	.long	.LBB0_355-.LJTI0_1
	.long	.LBB0_356-.LJTI0_1
	.long	.LBB0_357-.LJTI0_1
	.long	.LBB0_358-.LJTI0_1
	.long	.LBB0_359-.LJTI0_1
	.long	.LBB0_360-.LJTI0_1
	.long	.LBB0_361-.LJTI0_1
	.long	.LBB0_362-.LJTI0_1
	.long	.LBB0_363-.LJTI0_1
	.long	.LBB0_364-.LJTI0_1
	.long	.LBB0_365-.LJTI0_1
	.long	.LBB0_366-.LJTI0_1
	.long	.LBB0_367-.LJTI0_1
	.long	.LBB0_368-.LJTI0_1
	.long	.LBB0_369-.LJTI0_1
	.long	.LBB0_370-.LJTI0_1
	.long	.LBB0_371-.LJTI0_1
	.long	.LBB0_372-.LJTI0_1
	.long	.LBB0_373-.LJTI0_1
	.long	.LBB0_374-.LJTI0_1
	.long	.LBB0_375-.LJTI0_1
	.long	.LBB0_376-.LJTI0_1
	.long	.LBB0_377-.LJTI0_1
	.long	.LBB0_378-.LJTI0_1
	.long	.LBB0_379-.LJTI0_1
	.long	.LBB0_380-.LJTI0_1
	.long	.LBB0_381-.LJTI0_1
	.long	.LBB0_382-.LJTI0_1
	.long	.LBB0_383-.LJTI0_1
	.long	.LBB0_384-.LJTI0_1
	.long	.LBB0_385-.LJTI0_1
	.long	.LBB0_386-.LJTI0_1
	.long	.LBB0_387-.LJTI0_1
	.long	.LBB0_388-.LJTI0_1
	.long	.LBB0_389-.LJTI0_1
	.long	.LBB0_390-.LJTI0_1
	.long	.LBB0_391-.LJTI0_1
.LJTI0_2:
	.long	.LBB0_216-.LJTI0_2
	.long	.LBB0_243-.LJTI0_2
	.long	.LBB0_244-.LJTI0_2
	.long	.LBB0_245-.LJTI0_2
	.long	.LBB0_246-.LJTI0_2
	.long	.LBB0_247-.LJTI0_2
	.long	.LBB0_248-.LJTI0_2
	.long	.LBB0_249-.LJTI0_2
	.long	.LBB0_250-.LJTI0_2
	.long	.LBB0_253-.LJTI0_2
	.long	.LBB0_254-.LJTI0_2
	.long	.LBB0_255-.LJTI0_2
	.long	.LBB0_256-.LJTI0_2
	.long	.LBB0_257-.LJTI0_2
	.long	.LBB0_258-.LJTI0_2
	.long	.LBB0_260-.LJTI0_2
	.long	.LBB0_261-.LJTI0_2
	.long	.LBB0_262-.LJTI0_2
	.long	.LBB0_263-.LJTI0_2
	.long	.LBB0_264-.LJTI0_2
	.long	.LBB0_265-.LJTI0_2
	.long	.LBB0_266-.LJTI0_2
	.long	.LBB0_267-.LJTI0_2
	.long	.LBB0_268-.LJTI0_2
	.long	.LBB0_269-.LJTI0_2
	.long	.LBB0_271-.LJTI0_2
	.long	.LBB0_274-.LJTI0_2
	.long	.LBB0_275-.LJTI0_2
	.long	.LBB0_276-.LJTI0_2
	.long	.LBB0_277-.LJTI0_2
	.long	.LBB0_278-.LJTI0_2
	.long	.LBB0_279-.LJTI0_2
.LJTI0_3:
	.long	.LBB0_218-.LJTI0_3
	.long	.LBB0_281-.LJTI0_3
	.long	.LBB0_282-.LJTI0_3
	.long	.LBB0_283-.LJTI0_3
	.long	.LBB0_284-.LJTI0_3
	.long	.LBB0_285-.LJTI0_3
	.long	.LBB0_286-.LJTI0_3
	.long	.LBB0_287-.LJTI0_3
	.long	.LBB0_288-.LJTI0_3
	.long	.LBB0_289-.LJTI0_3
	.long	.LBB0_290-.LJTI0_3
	.long	.LBB0_291-.LJTI0_3
	.long	.LBB0_292-.LJTI0_3
	.long	.LBB0_293-.LJTI0_3
	.long	.LBB0_294-.LJTI0_3
	.long	.LBB0_295-.LJTI0_3
	.long	.LBB0_296-.LJTI0_3
	.long	.LBB0_297-.LJTI0_3
	.long	.LBB0_298-.LJTI0_3
	.long	.LBB0_299-.LJTI0_3
	.long	.LBB0_300-.LJTI0_3
	.long	.LBB0_301-.LJTI0_3
	.long	.LBB0_302-.LJTI0_3
	.long	.LBB0_303-.LJTI0_3
	.long	.LBB0_304-.LJTI0_3
	.long	.LBB0_305-.LJTI0_3
	.long	.LBB0_306-.LJTI0_3
	.long	.LBB0_307-.LJTI0_3
	.long	.LBB0_308-.LJTI0_3
	.long	.LBB0_309-.LJTI0_3
	.long	.LBB0_310-.LJTI0_3
	.long	.LBB0_312-.LJTI0_3
	.long	.LBB0_313-.LJTI0_3
	.long	.LBB0_314-.LJTI0_3
	.long	.LBB0_315-.LJTI0_3
	.long	.LBB0_316-.LJTI0_3
	.long	.LBB0_317-.LJTI0_3
	.long	.LBB0_318-.LJTI0_3
	.long	.LBB0_319-.LJTI0_3
	.long	.LBB0_320-.LJTI0_3
	.long	.LBB0_321-.LJTI0_3
	.long	.LBB0_322-.LJTI0_3
	.long	.LBB0_323-.LJTI0_3
	.long	.LBB0_324-.LJTI0_3
                                        # -- End function
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5                               # -- Begin function cost_model.par_for.prediction_output.s0.n.v13
.LCPI1_0:
	.quad	4                               # 0x4
	.quad	5                               # 0x5
	.quad	6                               # 0x6
	.quad	7                               # 0x7
.LCPI1_1:
	.quad	0                               # 0x0
	.quad	1                               # 0x1
	.quad	2                               # 0x2
	.quad	3                               # 0x3
.LCPI1_11:
	.zero	32
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3
.LCPI1_2:
	.quad	8                               # 0x8
.LCPI1_3:
	.quad	16                              # 0x10
.LCPI1_4:
	.quad	24                              # 0x18
.LCPI1_10:
	.quad	32                              # 0x20
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2
.LCPI1_5:
	.long	0x3f800000                      # float 1
.LCPI1_6:
	.long	0x45800000                      # float 4096
.LCPI1_7:
	.long	0x40000000                      # float 2
.LCPI1_8:
	.long	0xbf800000                      # float -1
.LCPI1_9:
	.long	0x3089705f                      # float 9.99999971E-10
	.section	.text.cost_model.par_for.prediction_output.s0.n.v13,"ax",@progbits
	.p2align	4, 0x90
	.type	cost_model.par_for.prediction_output.s0.n.v13,@function
cost_model.par_for.prediction_output.s0.n.v13: # @cost_model.par_for.prediction_output.s0.n.v13
# %bb.0:                                # %entry
	pushq	%rbp
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rbx
	subq	$1768, %rsp                     # imm = 0x6E8
	movq	%rdx, %rbx
	movl	(%rdx), %eax
	movl	4(%rdx), %ecx
	movq	%rcx, 296(%rsp)                 # 8-byte Spill
	cmpq	$31, %rcx
	seta	416(%rsp)                       # 1-byte Folded Spill
	movl	12(%rdx), %r13d
	movslq	16(%rdx), %rcx
	movq	%rcx, 376(%rsp)                 # 8-byte Spill
	cmpq	$1, %rcx
	sete	-97(%rsp)                       # 1-byte Folded Spill
	movl	20(%rdx), %ecx
	shll	$3, %esi
	addl	$-8, %eax
	cmpl	%esi, %eax
	cmovgl	%esi, %eax
	movslq	%eax, %r9
	leal	(%r13,%r13,8), %r8d
	leal	(%r9,%r8,2), %edi
	subl	%ecx, %edi
	leal	(%r13,%r8,2), %edx
	addl	%r9d, %edx
	subl	%ecx, %edx
	movl	%edx, -96(%rsp)                 # 4-byte Spill
	leal	(%r13,%r13,4), %r14d
	leal	(%r14,%r14,4), %r15d
	leal	(%r15,%r13), %edx
	addl	%r9d, %edx
	subl	%ecx, %edx
	movl	%edx, -88(%rsp)                 # 4-byte Spill
	leal	(%r8,%r8,2), %esi
	leal	(%rsi,%r13), %edx
	leal	(%r9,%rdx), %ebp
	subl	%ecx, %ebp
	movl	%ebp, -56(%rsp)                 # 4-byte Spill
	movq	%r9, %rbp
	addl	%ebp, %esi
	subl	%ecx, %esi
	movq	%rsi, -48(%rsp)                 # 8-byte Spill
	movl	%r13d, %r9d
	shll	$5, %r9d
	leal	(%r9,%r13), %esi
	addl	%ebp, %esi
	subl	%ecx, %esi
	movl	%esi, -64(%rsp)                 # 4-byte Spill
	leal	(%r9,%r13,2), %esi
	addl	%ebp, %esi
	subl	%ecx, %esi
	movl	%esi, -72(%rsp)                 # 4-byte Spill
	leal	(%r13,%r14,2), %esi
	addl	%ebp, %esi
	subl	%ecx, %esi
	movl	%esi, -80(%rsp)                 # 4-byte Spill
	addl	%r13d, %edx
	addl	%ebp, %edx
	subl	%ecx, %edx
	movq	%rdx, 320(%rsp)                 # 8-byte Spill
	leal	(%r13,%r13,2), %r12d
	leal	(%rbp,%r12,4), %edx
	subl	%ecx, %edx
	movl	%edx, -24(%rsp)                 # 4-byte Spill
	movl	%r13d, %edx
	shll	$4, %edx
	addl	%r13d, %edx
	addl	%ebp, %edx
	subl	%ecx, %edx
	movl	%edx, -32(%rsp)                 # 4-byte Spill
	leal	(%r13,%r14,4), %r10d
	leal	(%r10,%rbp), %edx
	subl	%ecx, %edx
	leal	(%r9,%rbp), %r11d
	subl	%r13d, %r9d
	leal	(%r9,%rbp), %eax
	movq	%rbp, %rsi
	subl	%r13d, %r9d
	addl	%r13d, %r10d
	addl	%r13d, %ebp
	subl	%ecx, %ebp
	subl	%ecx, %eax
	movl	%eax, -40(%rsp)                 # 4-byte Spill
	subl	%ecx, %r11d
	addl	%esi, %r9d
	subl	%ecx, %r9d
	leal	(%rsi,%r14,2), %r14d
	subl	%ecx, %r14d
	addl	%esi, %r8d
	subl	%ecx, %r8d
	movq	%r8, %rax
	addl	%esi, %r15d
	subl	%ecx, %r15d
	movq	%r15, %r8
	addl	%esi, %r10d
	subl	%ecx, %r10d
	leal	(%rsi,%r12,8), %r15d
	subl	%ecx, %r15d
	movl	%esi, %r13d
	subl	%ecx, %r13d
	movb	-97(%rsp), %cl                  # 1-byte Reload
	andb	416(%rsp), %cl                  # 1-byte Folded Reload
	movb	%cl, -97(%rsp)                  # 1-byte Spill
	vbroadcastss	28(%rbx), %ymm3
	movl	8(%rbx), %ecx
	movl	%ecx, 288(%rsp)                 # 4-byte Spill
	movl	24(%rbx), %r12d
	movq	32(%rbx), %rcx
	movq	%rcx, 488(%rsp)                 # 8-byte Spill
	movq	48(%rbx), %rcx
	movq	64(%rbx), %rbx
	movq	%rbx, 472(%rsp)                 # 8-byte Spill
	subl	$-128, %r12d
	movq	296(%rsp), %rbx                 # 8-byte Reload
                                        # kill: def $ebx killed $ebx killed $rbx def $rbx
	andl	$-32, %ebx
	movq	%rbx, 464(%rsp)                 # 8-byte Spill
	movq	376(%rsp), %rbx                 # 8-byte Reload
	shlq	$7, %rbx
	movq	%rbx, 520(%rsp)                 # 8-byte Spill
	movq	%rsi, 496(%rsp)                 # 8-byte Spill
	movl	%esi, %ebx
	subl	288(%rsp), %ebx                 # 4-byte Folded Reload
	movl	%r12d, 292(%rsp)                # 4-byte Spill
	imull	%r12d, %ebx
	movl	%ebx, 92(%rsp)                  # 4-byte Spill
	leaq	108(%rcx), %rbx
	movq	%rbx, 480(%rsp)                 # 8-byte Spill
	movq	376(%rsp), %rsi                 # 8-byte Reload
	leaq	(,%rsi,4), %rbx
	movq	%rbx, 536(%rsp)                 # 8-byte Spill
	xorl	%esi, %esi
	vmovups	%ymm3, 608(%rsp)                # 32-byte Spill
	jmp	.LBB1_1
.LBB1_2:                                #   in Loop: Header=BB1_1 Depth=1
	vxorps	%xmm2, %xmm2, %xmm2
	movq	-8(%rsp), %rdi                  # 8-byte Reload
	movq	-16(%rsp), %rbx                 # 8-byte Reload
	.p2align	4, 0x90
.LBB1_15:                               # %"consume f7"
                                        #   in Loop: Header=BB1_1 Depth=1
	movq	488(%rsp), %rax                 # 8-byte Reload
	movq	504(%rsp), %rdx                 # 8-byte Reload
	vmovss	%xmm2, (%rax,%rdx,4)
	movq	512(%rsp), %rsi                 # 8-byte Reload
	incq	%rsi
	incl	%r12d
	leal	1(%rdi), %eax
	movl	%eax, -96(%rsp)                 # 4-byte Spill
	leal	1(%rbx), %eax
	movl	%eax, -88(%rsp)                 # 4-byte Spill
	movq	144(%rsp), %rax                 # 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, -56(%rsp)                 # 4-byte Spill
	movq	136(%rsp), %rax                 # 8-byte Reload
	leal	1(%rax), %eax
	movq	%rax, -48(%rsp)                 # 8-byte Spill
	movq	224(%rsp), %rax                 # 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, -64(%rsp)                 # 4-byte Spill
	movq	216(%rsp), %rax                 # 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, -72(%rsp)                 # 4-byte Spill
	movq	208(%rsp), %rax                 # 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, -80(%rsp)                 # 4-byte Spill
	movq	200(%rsp), %rax                 # 8-byte Reload
	leal	1(%rax), %eax
	movq	%rax, 320(%rsp)                 # 8-byte Spill
	movq	192(%rsp), %rax                 # 8-byte Reload
	leal	1(%rax), %eax
	movl	%eax, -24(%rsp)                 # 4-byte Spill
	movq	408(%rsp), %rdx                 # 8-byte Reload
	leal	1(%rdx), %eax
	movl	%eax, -32(%rsp)                 # 4-byte Spill
	movq	400(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %edx
	movq	392(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %ebp
	movq	128(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %eax
	movl	%eax, -40(%rsp)                 # 4-byte Spill
	movq	120(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %r11d
	movq	184(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %r9d
	movq	112(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %r14d
	movq	104(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %eax
	movq	176(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %r8d
	movq	384(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %r10d
	movq	304(%rsp), %rdi                 # 8-byte Reload
	leal	1(%rdi), %r15d
	movq	96(%rsp), %rdi                  # 8-byte Reload
	leal	1(%rdi), %r13d
	movl	%r12d, %edi
	movl	92(%rsp), %ebx                  # 4-byte Reload
	addl	292(%rsp), %ebx                 # 4-byte Folded Reload
	movl	%ebx, 92(%rsp)                  # 4-byte Spill
	cmpq	$8, %rsi
	je	.LBB1_16
.LBB1_1:                                # %"for prediction_output.s0.n.n"
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB1_6 Depth 2
                                        #     Child Loop BB1_9 Depth 2
	movq	%rax, %rbx
	movslq	%edi, %r12
	movslq	-96(%rsp), %rdi                 # 4-byte Folded Reload
	movq	%rdi, -8(%rsp)                  # 8-byte Spill
	movslq	-88(%rsp), %rdi                 # 4-byte Folded Reload
	movq	%rdi, -16(%rsp)                 # 8-byte Spill
	movslq	-56(%rsp), %rdi                 # 4-byte Folded Reload
	movq	%rdi, 144(%rsp)                 # 8-byte Spill
	movslq	-48(%rsp), %rdi                 # 4-byte Folded Reload
	movq	%rdi, 136(%rsp)                 # 8-byte Spill
	movslq	-64(%rsp), %rdi                 # 4-byte Folded Reload
	movq	%rdi, 224(%rsp)                 # 8-byte Spill
	movslq	-72(%rsp), %rdi                 # 4-byte Folded Reload
	movq	%rdi, 216(%rsp)                 # 8-byte Spill
	movslq	-80(%rsp), %rdi                 # 4-byte Folded Reload
	movq	%rdi, 208(%rsp)                 # 8-byte Spill
	movslq	320(%rsp), %rdi                 # 4-byte Folded Reload
	movq	%rdi, 200(%rsp)                 # 8-byte Spill
	movslq	-24(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 192(%rsp)                 # 8-byte Spill
	movslq	-32(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 408(%rsp)                 # 8-byte Spill
	movslq	%edx, %rdx
	movslq	%ebp, %rax
	movq	%rax, 392(%rsp)                 # 8-byte Spill
	movslq	-40(%rsp), %rax                 # 4-byte Folded Reload
	movq	%rax, 128(%rsp)                 # 8-byte Spill
	movslq	%r11d, %rax
	movq	%rax, 120(%rsp)                 # 8-byte Spill
	movslq	%r9d, %rax
	movq	%rax, 184(%rsp)                 # 8-byte Spill
	movslq	%r14d, %rax
	movq	%rax, 112(%rsp)                 # 8-byte Spill
	movslq	%ebx, %rax
	movq	%rax, 104(%rsp)                 # 8-byte Spill
	movslq	%r8d, %rax
	movq	%rax, 176(%rsp)                 # 8-byte Spill
	movslq	%r10d, %rax
	movq	%rax, 384(%rsp)                 # 8-byte Spill
	movslq	%r15d, %rax
	movq	%rax, 304(%rsp)                 # 8-byte Spill
	movslq	%r13d, %rax
	movq	%rax, 96(%rsp)                  # 8-byte Spill
	movq	496(%rsp), %rax                 # 8-byte Reload
	addq	%rsi, %rax
	cmpl	$0, 296(%rsp)                   # 4-byte Folded Reload
	movq	%rsi, 512(%rsp)                 # 8-byte Spill
	movq	%rdx, 400(%rsp)                 # 8-byte Spill
	movq	%rax, 504(%rsp)                 # 8-byte Spill
	jle	.LBB1_2
# %bb.3:                                # %"for f7.s1.r77$x.preheader"
                                        #   in Loop: Header=BB1_1 Depth=1
	cmpb	$0, -97(%rsp)                   # 1-byte Folded Reload
	je	.LBB1_4
# %bb.5:                                # %vector.ph
                                        #   in Loop: Header=BB1_1 Depth=1
                                        # kill: def $eax killed $eax killed $rax
	subl	288(%rsp), %eax                 # 4-byte Folded Reload
	imull	292(%rsp), %eax                 # 4-byte Folded Reload
	cltq
	vmovq	%rax, %xmm0
	vpbroadcastq	%xmm0, %ymm0
	vmovdqu	%ymm0, 1184(%rsp)               # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%ymm0, 960(%rsp)                # 32-byte Spill
	movq	472(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 240(%rsp)                 # 8-byte Spill
	movq	464(%rsp), %rax                 # 8-byte Reload
	movq	%rax, 528(%rsp)                 # 8-byte Spill
	vmovdqa	.LCPI1_1(%rip), %ymm2           # ymm2 = [0,1,2,3]
	vmovdqa	.LCPI1_0(%rip), %ymm4           # ymm4 = [4,5,6,7]
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%ymm0, 928(%rsp)                # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%ymm0, 896(%rsp)                # 32-byte Spill
	vpxor	%xmm0, %xmm0, %xmm0
	vmovdqu	%ymm0, 864(%rsp)                # 32-byte Spill
	movq	%r12, 232(%rsp)                 # 8-byte Spill
	.p2align	4, 0x90
.LBB1_6:                                # %vector.body
                                        #   Parent Loop BB1_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vpbroadcastq	.LCPI1_2(%rip), %ymm0   # ymm0 = [8,8,8,8]
	vpbroadcastq	.LCPI1_3(%rip), %ymm1   # ymm1 = [16,16,16,16]
	vpbroadcastq	.LCPI1_4(%rip), %ymm7   # ymm7 = [24,24,24,24]
	vpxor	%xmm6, %xmm6, %xmm6
	vpcmpeqq	%ymm6, %ymm2, %ymm3
	vmovdqa	%ymm2, %ymm10
	vmovdqa	%ymm4, %ymm5
	vpcmpeqq	%ymm6, %ymm4, %ymm4
	vperm2i128	$49, %ymm4, %ymm3, %ymm2 # ymm2 = ymm3[2,3],ymm4[2,3]
	vinserti128	$1, %xmm4, %ymm3, %ymm3
	vpsllq	$5, %ymm10, %ymm4
	vpackssdw	%ymm2, %ymm3, %ymm2
	vmovdqu	%ymm2, 320(%rsp)                # 32-byte Spill
	vpsllq	$5, %ymm5, %ymm2
	vmovdqu	1184(%rsp), %ymm13              # 32-byte Reload
	vpaddq	%ymm2, %ymm13, %ymm9
	vpaddq	%ymm4, %ymm13, %ymm2
	vpaddq	%ymm0, %ymm5, %ymm3
	vmovq	%xmm2, %r10
	vpextrq	$1, %xmm2, %rbx
	vpaddq	%ymm0, %ymm10, %ymm8
	vextracti128	$1, %ymm2, %xmm6
	vpextrq	$1, %xmm6, %rax
	vmovdqa	%ymm5, %ymm0
	vmovdqu	%ymm5, 1728(%rsp)               # 32-byte Spill
	vpaddq	%ymm1, %ymm5, %ymm5
	vmovq	%xmm9, %r9
	vpextrq	$1, %xmm9, %rdi
	vpaddq	%ymm1, %ymm10, %ymm2
	vmovdqa	%ymm10, %ymm14
	vmovdqu	%ymm10, 1696(%rsp)              # 32-byte Spill
	vextracti128	$1, %ymm9, %xmm4
	vpextrq	$1, %xmm4, %rbp
	vpaddq	%ymm7, %ymm0, %ymm0
	vmovdqu	%ymm0, 416(%rsp)                # 32-byte Spill
	vmovss	(%rcx,%r9,4), %xmm0             # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpxor	%xmm1, %xmm1, %xmm1
	vpcmpeqq	%ymm1, %ymm8, %ymm10
	vmovq	%xmm4, %rsi
	vinsertps	$32, (%rcx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovaps	%xmm0, 576(%rsp)                # 16-byte Spill
	movq	%rsi, %rdx
	vpcmpeqq	%ymm1, %ymm3, %ymm11
	vmovss	(%rcx,%r10,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, (%rcx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpcmpeqq	%ymm1, %ymm2, %ymm12
	vpxor	%xmm4, %xmm4, %xmm4
	vmovq	%xmm6, %rsi
	vinsertps	$32, (%rcx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovaps	%xmm0, 544(%rsp)                # 16-byte Spill
	vperm2i128	$49, %ymm11, %ymm10, %ymm9 # ymm9 = ymm10[2,3],ymm11[2,3]
	vmovss	68(%rcx,%r9,4), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%rcx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinserti128	$1, %xmm11, %ymm10, %ymm1
	vpcmpeqq	%ymm4, %ymm5, %ymm4
	vinsertps	$32, 68(%rcx,%rdx,4), %xmm0, %xmm11 # xmm11 = xmm0[0,1],mem[0],xmm0[3]
	vperm2i128	$49, %ymm4, %ymm12, %ymm15 # ymm15 = ymm12[2,3],ymm4[2,3]
	vmovss	68(%rcx,%r10,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%rcx,%rbx,4), %xmm0, %xmm6 # xmm6 = xmm0[0],mem[0],xmm0[2,3]
	vinserti128	$1, %xmm4, %ymm12, %ymm0
	vinsertps	$32, 68(%rcx,%rsi,4), %xmm6, %xmm4 # xmm4 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	72(%rcx,%r9,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vpaddq	%ymm7, %ymm14, %ymm10
	vinsertps	$16, 72(%rcx,%rdi,4), %xmm6, %xmm7 # xmm7 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 68(%rcx,%rbp,4), %xmm11, %xmm6 # xmm6 = xmm11[0,1,2],mem[0]
	vinsertps	$32, 72(%rcx,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 68(%rcx,%rax,4), %xmm4, %xmm11 # xmm11 = xmm4[0,1,2],mem[0]
	vpsllq	$5, %ymm8, %ymm12
	vinsertps	$48, 72(%rcx,%rbp,4), %xmm7, %xmm4 # xmm4 = xmm7[0,1,2],mem[0]
	vpackssdw	%ymm9, %ymm1, %ymm14
	vmovss	72(%rcx,%r10,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpaddq	%ymm13, %ymm12, %ymm12
	vinsertps	$32, 72(%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	%rsi, %r11
	vpextrq	$1, %xmm12, %r15
	vmovq	%xmm12, %r13
	vinsertps	$48, 72(%rcx,%rax,4), %xmm1, %xmm9 # xmm9 = xmm1[0,1,2],mem[0]
	movq	%rax, 72(%rsp)                  # 8-byte Spill
	vpackssdw	%ymm15, %ymm0, %ymm15
	movq	%r9, 80(%rsp)                   # 8-byte Spill
	vmovss	80(%rcx,%r9,4), %xmm0           # xmm0 = mem[0],zero,zero,zero
	movq	%rdi, (%rsp)                    # 8-byte Spill
	vinsertps	$16, 80(%rcx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vextracti128	$1, %ymm12, %xmm1
	vinsertps	$32, 80(%rcx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpextrq	$1, %xmm1, -96(%rsp)            # 8-byte Folded Spill
	vmovq	%xmm1, %r12
	movq	%rbp, -32(%rsp)                 # 8-byte Spill
	vinsertps	$48, 80(%rcx,%rbp,4), %xmm0, %xmm7 # xmm7 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm11, %ymm0
	movq	%r10, -72(%rsp)                 # 8-byte Spill
	vmovss	80(%rcx,%r10,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	%rbx, 64(%rsp)                  # 8-byte Spill
	vinsertps	$16, 80(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vpxor	%xmm8, %xmm8, %xmm8
	vpcmpeqq	%ymm8, %ymm10, %ymm11
	vinsertf128	$1, %xmm4, %ymm9, %ymm6
	movq	%rsi, 56(%rsp)                  # 8-byte Spill
	vinsertps	$32, 80(%rcx,%rsi,4), %xmm1, %xmm4 # xmm4 = xmm1[0,1],mem[0],xmm1[3]
	vmovdqu	416(%rsp), %ymm9                # 32-byte Reload
	vpcmpeqq	%ymm8, %ymm9, %ymm12
	vpsllq	$5, %ymm3, %ymm1
	vinsertps	$48, 80(%rcx,%rax,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm3, %ymm3
	vmovss	84(%rcx,%r9,4), %xmm4           # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%rcx,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpsllq	$5, %ymm2, %ymm7
	vpsllq	$5, %ymm5, %ymm2
	vinsertps	$32, 84(%rcx,%rdx,4), %xmm4, %xmm5 # xmm5 = xmm4[0,1],mem[0],xmm4[3]
	movq	%rdx, -80(%rsp)                 # 8-byte Spill
	vpsllq	$5, %ymm10, %ymm10
	vinsertps	$48, 84(%rcx,%rbp,4), %xmm5, %xmm4 # xmm4 = xmm5[0,1,2],mem[0]
	vmovups	320(%rsp), %ymm8                # 32-byte Reload
	vblendvps	%ymm8, %ymm0, %ymm6, %ymm0
	vmovups	%ymm0, 704(%rsp)                # 32-byte Spill
	vmovss	84(%rcx,%r10,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%rcx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vpaddq	%ymm1, %ymm13, %ymm1
	vpaddq	%ymm2, %ymm13, %ymm2
	vinsertps	$32, 84(%rcx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vpaddq	%ymm7, %ymm13, %ymm5
	vinsertps	$48, 84(%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vpextrq	$1, %xmm1, %r8
	vmovq	%xmm1, %rax
	vinsertf128	$1, %xmm4, %ymm0, %ymm0
	vmovss	4(%rcx,%r9,4), %xmm4            # xmm4 = mem[0],zero,zero,zero
	vextracti128	$1, %ymm1, %xmm6
	vinsertps	$16, 4(%rcx,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vblendvps	%ymm8, %ymm3, %ymm0, %ymm0
	vmovups	%ymm0, 832(%rsp)                # 32-byte Spill
	vextracti128	$1, %ymm5, %xmm3
	vinsertps	$32, 4(%rcx,%rdx,4), %xmm4, %xmm8 # xmm8 = xmm4[0,1],mem[0],xmm4[3]
	vmovq	%xmm6, %rdx
	vextracti128	$1, %ymm2, %xmm4
	vpextrq	$1, %xmm6, %r14
	movq	%rax, -88(%rsp)                 # 8-byte Spill
	vmovss	68(%rcx,%rax,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 68(%rcx,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	%r8, -56(%rsp)                  # 8-byte Spill
	vpextrq	$1, %xmm5, %r9
	vmovq	%xmm5, %r10
	movq	%r13, -64(%rsp)                 # 8-byte Spill
	vmovss	68(%rcx,%r13,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%rcx,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rdx, %rsi
	movq	%rdx, -48(%rsp)                 # 8-byte Spill
	vinsertps	$16, 68(%rcx,%r15,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpextrq	$1, %xmm2, %rdx
	movq	%rdx, 32(%rsp)                  # 8-byte Spill
	vmovq	%xmm2, %r11
	vmovss	68(%rcx,%r11,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	%r11, 24(%rsp)                  # 8-byte Spill
	movq	%r12, %rbp
	movq	%r12, -40(%rsp)                 # 8-byte Spill
	vinsertps	$32, 68(%rcx,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$16, 68(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vpextrq	$1, %xmm4, %rdi
	vmovq	%xmm4, %rdx
	movq	%r10, -24(%rsp)                 # 8-byte Spill
	vmovss	68(%rcx,%r10,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	%rdx, %rbx
	movq	%rdx, 40(%rsp)                  # 8-byte Spill
	vinsertps	$16, 68(%rcx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vpextrq	$1, %xmm3, %r12
	vmovq	%xmm3, %rdx
	vmovss	72(%rcx,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%rcx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%rdx, 160(%rsp)                 # 8-byte Spill
	vinsertps	$16, 72(%rcx,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	%r14, %r8
	vinsertps	$48, 68(%rcx,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 72(%rcx,%rsi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	-96(%rsp), %r14                 # 8-byte Reload
	vinsertps	$48, 68(%rcx,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm5, %ymm6
	vmovss	72(%rcx,%r13,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$48, 68(%rcx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 72(%rcx,%r15,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	%r15, %r13
	movq	%r12, %rax
	movq	%r12, 8(%rsp)                   # 8-byte Spill
	vinsertps	$48, 68(%rcx,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$32, 72(%rcx,%rbp,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 72(%rcx,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%r8, %r12
	movq	%r8, 320(%rsp)                  # 8-byte Spill
	vinsertf128	$1, %xmm2, %ymm4, %ymm4
	vinsertps	$48, 72(%rcx,%r14,4), %xmm5, %xmm2 # xmm2 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vmovss	72(%rcx,%r11,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vmovss	72(%rcx,%r10,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	32(%rsp), %r11                  # 8-byte Reload
	vinsertps	$16, 72(%rcx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$16, 72(%rcx,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	%r9, %r10
	movq	%r9, 16(%rsp)                   # 8-byte Spill
	vinsertps	$32, 72(%rcx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 72(%rcx,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 72(%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdi, %r9
	vpsllq	$5, %ymm9, %ymm7
	vinsertps	$48, 72(%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm5, %ymm3
	movq	-72(%rsp), %rax                 # 8-byte Reload
	vmovss	4(%rcx,%rax,4), %xmm5           # xmm5 = mem[0],zero,zero,zero
	movq	64(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 4(%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vpaddq	%ymm7, %ymm13, %ymm7
	vpaddq	%ymm13, %ymm10, %ymm10
	vblendvps	%ymm14, %ymm6, %ymm2, %ymm9
	vextracti128	$1, %ymm10, %xmm6
	vmovq	%xmm10, %r8
	vpextrq	$1, %xmm10, %rax
	vextracti128	$1, %ymm7, %xmm1
	vmovq	%xmm7, %rsi
	vpextrq	$1, %xmm7, %rbp
	movq	%rbp, 48(%rsp)                  # 8-byte Spill
	vmovss	68(%rcx,%rsi,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	%rsi, 416(%rsp)                 # 8-byte Spill
	vinsertps	$16, 68(%rcx,%rbp,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vpextrq	$1, %xmm1, %r15
	movq	%r15, 272(%rsp)                 # 8-byte Spill
	vmovq	%xmm1, %rdi
	movq	%rdi, 264(%rsp)                 # 8-byte Spill
	vmovss	68(%rcx,%r8,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movq	%r8, 312(%rsp)                  # 8-byte Spill
	vinsertps	$32, 68(%rcx,%rdi,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$16, 68(%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	%rax, %rdx
	movq	%rax, 168(%rsp)                 # 8-byte Spill
	vpextrq	$1, %xmm6, %rax
	vmovq	%xmm6, %rbx
	vmovss	72(%rcx,%rsi,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$32, 68(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$16, 72(%rcx,%rbp,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 68(%rcx,%r15,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$32, 72(%rcx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rax, 280(%rsp)                 # 8-byte Spill
	vinsertps	$48, 68(%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 72(%rcx,%r15,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vblendvps	%ymm15, %ymm4, %ymm3, %ymm0
	vmovups	%ymm0, 672(%rsp)                # 32-byte Spill
	vmovss	72(%rcx,%r8,4), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 72(%rcx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vperm2i128	$49, %ymm12, %ymm11, %ymm4 # ymm4 = ymm11[2,3],ymm12[2,3]
	vinserti128	$1, %xmm12, %ymm11, %ymm10
	vinsertps	$32, 72(%rcx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	-88(%rsp), %rdx                 # 8-byte Reload
	vmovss	80(%rcx,%rdx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	-56(%rsp), %rbp                 # 8-byte Reload
	vinsertps	$16, 80(%rcx,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 72(%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vpackssdw	%ymm4, %ymm10, %ymm4
	movq	-48(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 80(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	-64(%rsp), %rsi                 # 8-byte Reload
	vmovss	80(%rcx,%rsi,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 80(%rcx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 80(%rcx,%r12,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm1, %ymm1
	movq	-40(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 80(%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	24(%rsp), %r8                   # 8-byte Reload
	vmovss	80(%rcx,%r8,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	movq	%r11, %rdi
	vinsertps	$16, 80(%rcx,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 80(%rcx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm3, %ymm3
	movq	40(%rsp), %r11                  # 8-byte Reload
	vinsertps	$32, 80(%rcx,%r11,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1],mem[0],xmm7[3]
	movq	-24(%rsp), %r15                 # 8-byte Reload
	vmovss	80(%rcx,%r15,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 80(%rcx,%r10,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 80(%rcx,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	movq	%r9, %r12
	movq	%r9, 248(%rsp)                  # 8-byte Spill
	movq	160(%rsp), %r9                  # 8-byte Reload
	vinsertps	$32, 80(%rcx,%r9,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	8(%rsp), %rdx                   # 8-byte Reload
	vinsertps	$48, 80(%rcx,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	vinsertf128	$1, %xmm6, %ymm7, %ymm2
	vblendvps	%ymm4, %ymm1, %ymm3, %ymm1
	vmovups	%ymm1, 640(%rsp)                # 32-byte Spill
	movq	-88(%rsp), %r10                 # 8-byte Reload
	vmovss	84(%rcx,%r10,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vmovss	84(%rcx,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%rcx,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$16, 84(%rcx,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	-48(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, 84(%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 84(%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	320(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$48, 84(%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 84(%rcx,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	vmovss	84(%rcx,%r8,4), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 84(%rcx,%rdi,4), %xmm3, %xmm6 # xmm6 = xmm3[0],mem[0],xmm3[2,3]
	vblendvps	%ymm14, %ymm0, %ymm1, %ymm0
	vmovups	%ymm0, 768(%rsp)                # 32-byte Spill
	movq	416(%rsp), %r10                 # 8-byte Reload
	vmovss	80(%rcx,%r10,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vmovss	84(%rcx,%r15,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$32, 84(%rcx,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	16(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 84(%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 84(%rcx,%r12,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 84(%rcx,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 84(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm1, %ymm1
	movq	312(%rsp), %rdi                 # 8-byte Reload
	vmovss	80(%rcx,%rdi,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	48(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 80(%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	168(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$16, 80(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	264(%rsp), %r8                  # 8-byte Reload
	vinsertps	$32, 80(%rcx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%rbx, 256(%rsp)                 # 8-byte Spill
	vinsertps	$32, 80(%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vblendvps	%ymm15, %ymm2, %ymm1, %ymm1
	vmovups	%ymm1, 800(%rsp)                # 32-byte Spill
	movq	-72(%rsp), %r9                  # 8-byte Reload
	vmovss	8(%rcx,%r9,4), %xmm1            # xmm1 = mem[0],zero,zero,zero
	vmovss	84(%rcx,%r10,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	272(%rsp), %rbp                 # 8-byte Reload
	vinsertps	$48, 80(%rcx,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 84(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	280(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 80(%rcx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm6, %ymm0
	vmovss	84(%rcx,%rdi,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$32, 84(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$16, 84(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 84(%rcx,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 84(%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 84(%rcx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	movq	80(%rsp), %rsi                  # 8-byte Reload
	vmovss	8(%rcx,%rsi,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	movq	56(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$32, 4(%rcx,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	(%rsp), %rdi                    # 8-byte Reload
	vinsertps	$16, 8(%rcx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	-32(%rsp), %rbp                 # 8-byte Reload
	vmovaps	576(%rsp), %xmm3                # 16-byte Reload
	vinsertps	$48, (%rcx,%rbp,4), %xmm3, %xmm7 # xmm7 = xmm3[0,1,2],mem[0]
	movq	-80(%rsp), %r15                 # 8-byte Reload
	vinsertps	$32, 8(%rcx,%r15,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	72(%rsp), %rax                  # 8-byte Reload
	vmovaps	544(%rsp), %xmm3                # 16-byte Reload
	vinsertps	$48, (%rcx,%rax,4), %xmm3, %xmm10 # xmm10 = xmm3[0,1,2],mem[0]
	movq	64(%rsp), %rbx                  # 8-byte Reload
	vinsertps	$16, 8(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 4(%rcx,%rbp,4), %xmm8, %xmm3 # xmm3 = xmm8[0,1,2],mem[0]
	vinsertps	$32, 8(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vblendvps	%ymm4, %ymm0, %ymm2, %ymm0
	vmovups	%ymm0, 1568(%rsp)               # 32-byte Spill
	vmovss	12(%rcx,%rsi,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$48, 4(%rcx,%rax,4), %xmm5, %xmm2 # xmm2 = xmm5[0,1,2],mem[0]
	vinsertps	$16, 12(%rcx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 8(%rcx,%rbp,4), %xmm6, %xmm4 # xmm4 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 12(%rcx,%r15,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 8(%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	304(%rsp), %r14                 # 8-byte Reload
	movq	240(%rsp), %r15                 # 8-byte Reload
	vmovups	(%r15,%r14,4), %ymm13
	vinsertps	$48, 12(%rcx,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm10, %ymm5
	vmovss	12(%rcx,%r9,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	movq	176(%rsp), %rsi                 # 8-byte Reload
	vmovups	(%r15,%rsi,4), %ymm12
	movq	%rsi, %r10
	vinsertps	$32, 12(%rcx,%rdx,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1],mem[0],xmm6[3]
	vmulps	%ymm2, %ymm12, %ymm2
	vinsertps	$48, 12(%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	384(%rsp), %rbp                 # 8-byte Reload
	vmulps	(%r15,%rbp,4), %ymm13, %ymm6
	vmovups	%ymm13, 1312(%rsp)              # 32-byte Spill
	vfmadd231ps	%ymm5, %ymm6, %ymm2     # ymm2 = (ymm6 * ymm5) + ymm2
	movq	-88(%rsp), %rdi                 # 8-byte Reload
	vmovss	(%rcx,%rdi,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$16, (%rcx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertf128	$1, %xmm4, %ymm1, %ymm1
	movq	-64(%rsp), %r8                  # 8-byte Reload
	vmovss	(%rcx,%r8,4), %xmm4             # xmm4 = mem[0],zero,zero,zero
	movq	%r13, %rax
	movq	%r13, 152(%rsp)                 # 8-byte Spill
	vinsertps	$16, (%rcx,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmulps	%ymm0, %ymm12, %ymm0
	vmovups	%ymm12, 736(%rsp)               # 32-byte Spill
	movq	408(%rsp), %r13                 # 8-byte Reload
	vxorps	%xmm3, %xmm3, %xmm3
	vcmpeqps	(%r15,%r13,4), %ymm3, %ymm3
	vfmadd231ps	%ymm1, %ymm6, %ymm0     # ymm0 = (ymm6 * ymm1) + ymm0
	vmovss	4(%rcx,%rdi,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movq	-48(%rsp), %r12                 # 8-byte Reload
	vinsertps	$32, (%rcx,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$16, 4(%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	%rsi, %rbx
	vblendvps	%ymm3, %ymm2, %ymm0, %ymm0
	vmovups	%ymm0, 1600(%rsp)               # 32-byte Spill
	vmovss	4(%rcx,%r8,4), %xmm0            # xmm0 = mem[0],zero,zero,zero
	movq	-40(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, (%rcx,%rsi,4), %xmm4, %xmm2 # xmm2 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$16, 4(%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 4(%rcx,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 4(%rcx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	320(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rdx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vmovss	8(%rcx,%rdi,4), %xmm4           # xmm4 = mem[0],zero,zero,zero
	movq	%rdi, %r9
	movq	-96(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$16, 8(%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 4(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 8(%rcx,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	8(%rcx,%r8,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 4(%rcx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 8(%rcx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 8(%rcx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm2, %ymm2
	vinsertps	$48, 8(%rcx,%rdi,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	12(%rcx,%r9,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovups	32(%r15,%r10,4), %ymm5
	vmovups	%ymm5, 992(%rsp)                # 32-byte Spill
	vmulps	%ymm0, %ymm5, %ymm10
	vmovups	32(%r15,%r14,4), %ymm11
	vmulps	32(%r15,%rbp,4), %ymm11, %ymm6
	vmovups	%ymm11, 1024(%rsp)              # 32-byte Spill
	vfmadd231ps	%ymm2, %ymm6, %ymm10    # ymm10 = (ymm6 * ymm2) + ymm10
	vinsertps	$32, 12(%rcx,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	12(%rcx,%r8,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 12(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 12(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 12(%rcx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmulps	%ymm1, %ymm5, %ymm4
	vfmadd231ps	%ymm3, %ymm6, %ymm4     # ymm4 = (ymm6 * ymm3) + ymm4
	movq	24(%rsp), %r9                   # 8-byte Reload
	vmovss	(%rcx,%r9,4), %xmm1             # xmm1 = mem[0],zero,zero,zero
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, (%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	-24(%rsp), %r10                 # 8-byte Reload
	vmovss	(%rcx,%r10,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	movq	16(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, (%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	40(%rsp), %rbx                  # 8-byte Reload
	vinsertps	$32, (%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	160(%rsp), %r11                 # 8-byte Reload
	vinsertps	$32, (%rcx,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	248(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	416(%rsp), %r8                  # 8-byte Reload
	vmovss	(%rcx,%r8,4), %xmm3             # xmm3 = mem[0],zero,zero,zero
	movq	48(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$16, (%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovss	4(%rcx,%r9,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%rcx,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	8(%rsp), %r12                   # 8-byte Reload
	vinsertps	$48, (%rcx,%r12,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vinsertps	$32, 4(%rcx,%rbx,4), %xmm6, %xmm2 # xmm2 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	4(%rcx,%r10,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%rcx,%rax,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 4(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 4(%rcx,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 4(%rcx,%r12,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm6, %ymm2
	vmovss	8(%rcx,%r9,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%rcx,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 8(%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 8(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vmovss	8(%rcx,%r10,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	176(%rsp), %rdi                 # 8-byte Reload
	vmovups	64(%r15,%rdi,4), %ymm0
	vmulps	%ymm2, %ymm0, %ymm2
	vmovaps	%ymm0, %ymm14
	vmovups	%ymm0, 1056(%rsp)               # 32-byte Spill
	vmovups	64(%r15,%r14,4), %ymm0
	vmovups	%ymm0, 1088(%rsp)               # 32-byte Spill
	vmulps	64(%r15,%rbp,4), %ymm0, %ymm8
	vfmadd231ps	%ymm1, %ymm8, %ymm2     # ymm2 = (ymm8 * ymm1) + ymm2
	vinsertps	$32, 8(%rcx,%r11,4), %xmm7, %xmm1 # xmm1 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	12(%rcx,%r9,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%rcx,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 8(%rcx,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm1, %ymm1
	vinsertps	$32, 12(%rcx,%rbx,4), %xmm7, %xmm6 # xmm6 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	12(%rcx,%r10,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 12(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 12(%rcx,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 12(%rcx,%r12,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm6, %ymm7, %ymm6
	movq	264(%rsp), %r12                 # 8-byte Reload
	vinsertps	$32, (%rcx,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	312(%rsp), %rsi                 # 8-byte Reload
	vmovss	(%rcx,%rsi,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	movq	168(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, (%rcx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	272(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	256(%rsp), %r11                 # 8-byte Reload
	vinsertps	$32, (%rcx,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	280(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, (%rcx,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	movq	%r8, %rax
	vmovss	4(%rcx,%r8,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	movq	48(%rsp), %r14                  # 8-byte Reload
	vinsertps	$16, 4(%rcx,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmulps	%ymm6, %ymm14, %ymm6
	vfmadd231ps	%ymm1, %ymm8, %ymm6     # ymm6 = (ymm8 * ymm1) + ymm6
	vinsertps	$32, 4(%rcx,%r12,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	4(%rcx,%rsi,4), %xmm5           # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 4(%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 4(%rcx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm7, %ymm3
	vinsertps	$32, 4(%rcx,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	8(%rcx,%r8,4), %xmm7            # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%rcx,%r14,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 4(%rcx,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vinsertps	$32, 8(%rcx,%r12,4), %xmm7, %xmm5 # xmm5 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	8(%rcx,%rsi,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 8(%rcx,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 8(%rcx,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 8(%rcx,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 8(%rcx,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vmovss	12(%rcx,%r8,4), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%rcx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	176(%rsp), %rax                 # 8-byte Reload
	vmovups	96(%r15,%rax,4), %ymm15
	vmovups	%ymm15, 1152(%rsp)              # 32-byte Spill
	vmulps	%ymm1, %ymm15, %ymm1
	movq	304(%rsp), %rax                 # 8-byte Reload
	vmovups	96(%r15,%rax,4), %ymm8
	vmovups	%ymm8, 1120(%rsp)               # 32-byte Spill
	vmulps	96(%r15,%rbp,4), %ymm8, %ymm8
	vfmadd231ps	%ymm3, %ymm8, %ymm1     # ymm1 = (ymm8 * ymm3) + ymm1
	vinsertps	$32, 12(%rcx,%r12,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vmovss	12(%rcx,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 12(%rcx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 12(%rcx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 12(%rcx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 12(%rcx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm7, %ymm5
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vmulps	%ymm0, %ymm15, %ymm0
	vfmadd231ps	%ymm5, %ymm8, %ymm0     # ymm0 = (ymm8 * ymm5) + ymm0
	vxorps	%xmm5, %xmm5, %xmm5
	vcmpeqps	32(%r15,%r13,4), %ymm5, %ymm3
	vblendvps	%ymm3, %ymm10, %ymm4, %ymm3
	vmovups	%ymm3, 1472(%rsp)               # 32-byte Spill
	vcmpeqps	64(%r15,%r13,4), %ymm5, %ymm3
	vxorps	%xmm4, %xmm4, %xmm4
	vblendvps	%ymm3, %ymm2, %ymm6, %ymm2
	vmovups	%ymm2, 1536(%rsp)               # 32-byte Spill
	vcmpeqps	96(%r15,%r13,4), %ymm4, %ymm2
	movq	%r15, %rsi
	movq	%r15, 240(%rsp)                 # 8-byte Spill
	vblendvps	%ymm2, %ymm1, %ymm0, %ymm0
	vmovups	%ymm0, 1504(%rsp)               # 32-byte Spill
	movq	80(%rsp), %r15                  # 8-byte Reload
	vmovss	64(%rcx,%r15,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	(%rsp), %rax                    # 8-byte Reload
	vinsertps	$16, 64(%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	-72(%rsp), %r14                 # 8-byte Reload
	vmovss	64(%rcx,%r14,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	64(%rsp), %r11                  # 8-byte Reload
	vinsertps	$16, 64(%rcx,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	-80(%rsp), %r10                 # 8-byte Reload
	vinsertps	$32, 64(%rcx,%r10,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	56(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$32, 64(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	-32(%rsp), %r13                 # 8-byte Reload
	vinsertps	$48, 64(%rcx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	72(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$48, 64(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	104(%rsp), %rdx                 # 8-byte Reload
	vmovups	(%rsi,%rdx,4), %ymm1
	vbroadcastss	.LCPI1_5(%rip), %ymm10  # ymm10 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vcmpltps	%ymm1, %ymm10, %ymm2
	vmovaps	%ymm1, %ymm7
	vmovups	704(%rsp), %ymm1                # 32-byte Reload
	vblendvps	%ymm2, %ymm0, %ymm1, %ymm0
	vmovaps	%ymm2, %ymm8
	movq	-88(%rsp), %rbx                 # 8-byte Reload
	vmovss	64(%rcx,%rbx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	-56(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$16, 64(%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	-48(%rsp), %r8                  # 8-byte Reload
	vinsertps	$32, 64(%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	-64(%rsp), %r12                 # 8-byte Reload
	vmovss	64(%rcx,%r12,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	152(%rsp), %r9                  # 8-byte Reload
	vinsertps	$16, 64(%rcx,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	320(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 64(%rcx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	-40(%rsp), %rbp                 # 8-byte Reload
	vinsertps	$32, 64(%rcx,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	76(%rcx,%r15,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	-96(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 64(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vinsertps	$32, 76(%rcx,%r10,4), %xmm3, %xmm2 # xmm2 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	76(%rcx,%r14,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%rcx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 76(%rcx,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	56(%rsp), %r13                  # 8-byte Reload
	vinsertps	$32, 76(%rcx,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	72(%rsp), %r10                  # 8-byte Reload
	vinsertps	$48, 76(%rcx,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovss	76(%rcx,%rbx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%rcx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%rsi, %rbx
	vmovss	76(%rcx,%r12,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%rcx,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 76(%rcx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$32, 76(%rcx,%rbp,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	%rbp, %r8
	vinsertps	$48, 76(%rcx,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vinsertps	$48, 76(%rcx,%rdx,4), %xmm5, %xmm3 # xmm3 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm3, %ymm3
	movq	240(%rsp), %rdi                 # 8-byte Reload
	movq	104(%rsp), %rdx                 # 8-byte Reload
	vmovups	32(%rdi,%rdx,4), %ymm4
	vcmpltps	%ymm4, %ymm10, %ymm5
	vmovups	%ymm5, 704(%rsp)                # 32-byte Spill
	vmovaps	%ymm4, %ymm6
	vblendvps	%ymm5, %ymm1, %ymm9, %ymm1
	vmovups	832(%rsp), %ymm4                # 32-byte Reload
	vblendvps	%ymm8, %ymm2, %ymm4, %ymm14
	vmovaps	%ymm8, %ymm9
	vmovups	%ymm8, 1664(%rsp)               # 32-byte Spill
	vmovups	768(%rsp), %ymm2                # 32-byte Reload
	vblendvps	%ymm5, %ymm3, %ymm2, %ymm8
	vmovss	92(%rcx,%r15,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	-80(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, 92(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	92(%rcx,%r14,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%rcx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	-32(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 92(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 92(%rcx,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	88(%rcx,%r15,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%r10, %rax
	vinsertps	$48, 92(%rcx,%r10,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm2
	vinsertps	$32, 88(%rcx,%rsi,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	88(%rcx,%r14,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%rcx,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 88(%rcx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 88(%rcx,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r13, %r11
	movq	200(%rsp), %rdx                 # 8-byte Reload
	movq	%rdi, %r10
	vmulps	(%rdi,%rdx,4), %ymm7, %ymm5
	movq	%rdx, %r13
	vmulps	%ymm0, %ymm5, %ymm0
	vinsertps	$48, 88(%rcx,%rax,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	184(%rsp), %r12                 # 8-byte Reload
	vmovups	(%rdi,%r12,4), %ymm5
	vmaxps	%ymm10, %ymm5, %ymm5
	vdivps	%ymm5, %ymm0, %ymm0
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	208(%rsp), %rdi                 # 8-byte Reload
	vfmadd132ps	(%r10,%rdi,4), %ymm0, %ymm14 # ymm14 = (ymm14 * mem) + ymm0
	vaddps	%ymm12, %ymm13, %ymm0
	vmulps	%ymm3, %ymm0, %ymm0
	vdivps	%ymm5, %ymm0, %ymm0
	vbroadcastss	.LCPI1_6(%rip), %ymm4   # ymm4 = [4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3,4.096E+3]
	vdivps	%ymm5, %ymm4, %ymm3
	vmovaps	%ymm4, %ymm12
	vmovups	%ymm4, 1344(%rsp)               # 32-byte Spill
	vmovups	%ymm7, 1632(%rsp)               # 32-byte Spill
	vminps	%ymm7, %ymm3, %ymm3
	movq	192(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%r10,%rdx,4), %ymm3, %ymm3
	movq	%rdx, %r14
	movq	-88(%rsp), %rax                 # 8-byte Reload
	vmovss	88(%rcx,%rax,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	%rbx, %rsi
	vinsertps	$16, 88(%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vandps	%ymm0, %ymm9, %ymm0
	movq	96(%rsp), %rdx                  # 8-byte Reload
	vmovups	(%r10,%rdx,4), %ymm9
	vfmadd213ps	%ymm0, %ymm9, %ymm14    # ymm14 = (ymm9 * ymm14) + ymm0
	movq	112(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%r10,%rdx,4), %ymm7, %ymm13
	vmulps	%ymm3, %ymm13, %ymm0
	vfmadd231ps	%ymm0, %ymm2, %ymm14    # ymm14 = (ymm2 * ymm0) + ymm14
	vmovups	%ymm14, 832(%rsp)               # 32-byte Spill
	movq	-48(%rsp), %rbp                 # 8-byte Reload
	vinsertps	$32, 88(%rcx,%rbp,4), %xmm4, %xmm0 # xmm0 = xmm4[0,1],mem[0],xmm4[3]
	movq	-64(%rsp), %rbx                 # 8-byte Reload
	vmovss	88(%rcx,%rbx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	%r9, %rdx
	vinsertps	$16, 88(%rcx,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	320(%rsp), %r15                 # 8-byte Reload
	vinsertps	$48, 88(%rcx,%r15,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 88(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmulps	32(%r10,%r13,4), %ymm6, %ymm3
	vmulps	%ymm1, %ymm3, %ymm1
	movq	-96(%rsp), %r9                  # 8-byte Reload
	vinsertps	$48, 88(%rcx,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vmovups	32(%r10,%r12,4), %ymm3
	vmaxps	%ymm10, %ymm3, %ymm5
	vdivps	%ymm5, %ymm1, %ymm1
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vfmadd132ps	32(%r10,%rdi,4), %ymm1, %ymm8 # ymm8 = (ymm8 * mem) + ymm1
	vmovss	92(%rcx,%rax,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 92(%rcx,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	92(%rcx,%rbx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%rcx,%rdx,4), %xmm2, %xmm3 # xmm3 = xmm2[0],mem[0],xmm2[2,3]
	vaddps	992(%rsp), %ymm11, %ymm2        # 32-byte Folded Reload
	vmulps	%ymm0, %ymm2, %ymm0
	vdivps	%ymm5, %ymm0, %ymm4
	vdivps	%ymm5, %ymm12, %ymm2
	movq	80(%rsp), %rax                  # 8-byte Reload
	vmovss	20(%rcx,%rax,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	(%rsp), %rbp                    # 8-byte Reload
	vinsertps	$16, 20(%rcx,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 92(%rcx,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	-80(%rsp), %r8                  # 8-byte Reload
	vinsertps	$32, 20(%rcx,%r8,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 92(%rcx,%r15,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	-32(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 20(%rcx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 92(%rcx,%r9,4), %xmm3, %xmm5 # xmm5 = xmm3[0,1,2],mem[0]
	movq	-72(%rsp), %rsi                 # 8-byte Reload
	vmovss	20(%rcx,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	64(%rsp), %r12                  # 8-byte Reload
	vinsertps	$16, 20(%rcx,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovaps	%ymm6, %ymm7
	vmovups	%ymm6, 1440(%rsp)               # 32-byte Spill
	vminps	%ymm6, %ymm2, %ymm2
	vinsertps	$32, 20(%rcx,%r11,4), %xmm3, %xmm6 # xmm6 = xmm3[0,1],mem[0],xmm3[3]
	vmulps	32(%r10,%r14,4), %ymm2, %ymm3
	vinsertf128	$1, %xmm1, %ymm5, %ymm2
	movq	72(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$48, 20(%rcx,%rdx,4), %xmm6, %xmm1 # xmm1 = xmm6[0,1,2],mem[0]
	vandps	704(%rsp), %ymm4, %ymm4         # 32-byte Folded Reload
	vmovss	24(%rcx,%rax,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	%rax, %rbx
	movq	%rbp, %rax
	vinsertps	$16, 24(%rcx,%rbp,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	96(%rsp), %rbp                  # 8-byte Reload
	vmovups	32(%r10,%rbp,4), %ymm11
	vfmadd213ps	%ymm4, %ymm11, %ymm8    # ymm8 = (ymm11 * ymm8) + ymm4
	movq	112(%rsp), %rbp                 # 8-byte Reload
	vmulps	32(%r10,%rbp,4), %ymm7, %ymm4
	vmovups	%ymm4, 1248(%rsp)               # 32-byte Spill
	vmulps	%ymm4, %ymm3, %ymm3
	vfmadd231ps	%ymm3, %ymm2, %ymm8     # ymm8 = (ymm2 * ymm3) + ymm8
	vmovups	%ymm8, 576(%rsp)                # 32-byte Spill
	movq	%r8, %rbp
	vinsertps	$32, 24(%rcx,%r8,4), %xmm5, %xmm2 # xmm2 = xmm5[0,1],mem[0],xmm5[3]
	movq	%rsi, %r9
	vmovss	24(%rcx,%rsi,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%rcx,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 24(%rcx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 24(%rcx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 24(%rcx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	%rdx, %r15
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vinsertf128	$1, %xmm2, %ymm3, %ymm1
	vmovups	%ymm9, 1376(%rsp)               # 32-byte Spill
	movq	232(%rsp), %rsi                 # 8-byte Reload
	vmulps	(%r10,%rsi,4), %ymm9, %ymm2
	vmulps	%ymm2, %ymm1, %ymm15
	vmovss	32(%rcx,%rbx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	28(%rcx,%rbx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovss	28(%rcx,%r9,4), %xmm4           # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%rcx,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, 28(%rcx,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 28(%rcx,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r11, %rbx
	vinsertps	$48, 28(%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 28(%rcx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	-8(%rsp), %rax                  # 8-byte Reload
	vmulps	(%r10,%rax,4), %ymm9, %ymm5
	vfmadd231ps	%ymm5, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm5) + ymm15
	movq	24(%rsp), %r13                  # 8-byte Reload
	vmovss	64(%rcx,%r13,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	32(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 64(%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	-24(%rsp), %rdx                 # 8-byte Reload
	vmovss	64(%rcx,%rdx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	16(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$16, 64(%rcx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, 32(%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovups	1312(%rsp), %ymm2               # 32-byte Reload
	movq	-16(%rsp), %rbp                 # 8-byte Reload
	vmulps	(%r10,%rbp,4), %ymm2, %ymm5
	vfmadd231ps	%ymm5, %ymm3, %ymm15    # ymm15 = (ymm3 * ymm5) + ymm15
	vinsertps	$48, 32(%rcx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmovss	32(%rcx,%r9,4), %xmm3           # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%rcx,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	40(%rsp), %r14                  # 8-byte Reload
	vinsertps	$32, 64(%rcx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 32(%rcx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	160(%rsp), %r9                  # 8-byte Reload
	vinsertps	$32, 64(%rcx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 32(%rcx,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	248(%rsp), %rbp                 # 8-byte Reload
	vinsertps	$48, 64(%rcx,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	movq	8(%rsp), %r8                    # 8-byte Reload
	vinsertps	$48, 64(%rcx,%r8,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	movq	104(%rsp), %rdi                 # 8-byte Reload
	vmovups	64(%r10,%rdi,4), %ymm7
	vcmpltps	%ymm7, %ymm10, %ymm6
	vmovups	672(%rsp), %ymm3                # 32-byte Reload
	vblendvps	%ymm6, %ymm0, %ymm3, %ymm0
	vmovss	76(%rcx,%r13,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	%rax, %rdi
	vmovss	76(%rcx,%rdx,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%rcx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	%rsi, %rax
	vinsertps	$32, 76(%rcx,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 76(%rcx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 76(%rcx,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 76(%rcx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	%r8, %r11
	vmovups	736(%rsp), %ymm8                # 32-byte Reload
	movq	144(%rsp), %rdx                 # 8-byte Reload
	vmulps	(%r10,%rdx,4), %ymm8, %ymm5
	vfmadd231ps	%ymm5, %ymm1, %ymm15    # ymm15 = (ymm1 * ymm5) + ymm15
	vinsertf128	$1, %xmm3, %ymm4, %ymm1
	vmovups	800(%rsp), %ymm3                # 32-byte Reload
	vblendvps	%ymm6, %ymm1, %ymm3, %ymm9
	vmovaps	%ymm6, %ymm12
	vmovups	%ymm6, 1408(%rsp)               # 32-byte Spill
	movq	80(%rsp), %rdx                  # 8-byte Reload
	vmovss	36(%rcx,%rdx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	(%rsp), %rdx                    # 8-byte Reload
	vinsertps	$16, 36(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	-72(%rsp), %rdx                 # 8-byte Reload
	vmovss	36(%rcx,%rdx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%rcx,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	-80(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$32, 36(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 36(%rcx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	-32(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 36(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 36(%rcx,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	movq	%r13, %r8
	vmovss	88(%rcx,%r13,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	%rdi, %rdx
	vinsertps	$16, 88(%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	movq	136(%rsp), %rsi                 # 8-byte Reload
	vmulps	(%r10,%rsi,4), %ymm2, %ymm5
	vmovaps	%ymm2, %ymm4
	vfmadd231ps	%ymm5, %ymm1, %ymm15    # ymm15 = (ymm1 * ymm5) + ymm15
	vinsertps	$32, 88(%rcx,%r14,4), %xmm3, %xmm1 # xmm1 = xmm3[0,1],mem[0],xmm3[3]
	movq	-24(%rsp), %rbx                 # 8-byte Reload
	vmovss	88(%rcx,%rbx,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%rcx,%rax,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 88(%rcx,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	%r9, %rsi
	vinsertps	$32, 88(%rcx,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 88(%rcx,%r11,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	vmovss	92(%rcx,%r13,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$32, 92(%rcx,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 92(%rcx,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vmovss	92(%rcx,%rbx,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 92(%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovups	1056(%rsp), %ymm2               # 32-byte Reload
	vaddps	1088(%rsp), %ymm2, %ymm6        # 32-byte Folded Reload
	vmulps	%ymm1, %ymm6, %ymm1
	vinsertps	$32, 92(%rcx,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	184(%rsp), %rax                 # 8-byte Reload
	vmovups	64(%r10,%rax,4), %ymm6
	vmaxps	%ymm10, %ymm6, %ymm6
	vdivps	%ymm6, %ymm1, %ymm1
	vinsertps	$48, 92(%rcx,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	200(%rsp), %rdx                 # 8-byte Reload
	vmovaps	%ymm7, %ymm2
	vmovups	%ymm7, 768(%rsp)                # 32-byte Spill
	vmulps	64(%r10,%rdx,4), %ymm7, %ymm7
	vmulps	%ymm0, %ymm7, %ymm0
	vdivps	%ymm6, %ymm0, %ymm0
	vinsertf128	$1, %xmm3, %ymm5, %ymm3
	vmovups	1344(%rsp), %ymm14              # 32-byte Reload
	vdivps	%ymm6, %ymm14, %ymm5
	movq	208(%rsp), %rdx                 # 8-byte Reload
	vfmadd132ps	64(%r10,%rdx,4), %ymm0, %ymm9 # ymm9 = (ymm9 * mem) + ymm0
	vandps	%ymm1, %ymm12, %ymm0
	movq	96(%rsp), %rax                  # 8-byte Reload
	vmovups	64(%r10,%rax,4), %ymm1
	vmovups	%ymm1, 672(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm0, %ymm1, %ymm9     # ymm9 = (ymm1 * ymm9) + ymm0
	vminps	%ymm2, %ymm5, %ymm0
	movq	192(%rsp), %rax                 # 8-byte Reload
	vmulps	64(%r10,%rax,4), %ymm0, %ymm0
	movq	112(%rsp), %rax                 # 8-byte Reload
	vmulps	64(%r10,%rax,4), %ymm2, %ymm1
	vmovups	%ymm1, 1216(%rsp)               # 32-byte Spill
	vmulps	%ymm1, %ymm0, %ymm0
	vfmadd231ps	%ymm0, %ymm3, %ymm9     # ymm9 = (ymm3 * ymm0) + ymm9
	vmovups	%ymm9, 544(%rsp)                # 32-byte Spill
	movq	80(%rsp), %rax                  # 8-byte Reload
	vmovss	40(%rcx,%rax,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	(%rsp), %rbx                    # 8-byte Reload
	vinsertps	$16, 40(%rcx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	movq	-72(%rsp), %r11                 # 8-byte Reload
	vmovss	40(%rcx,%r11,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%rcx,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	-80(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$32, 40(%rcx,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	56(%rsp), %r8                   # 8-byte Reload
	vinsertps	$32, 40(%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	-32(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 40(%rcx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 40(%rcx,%r15,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovss	44(%rcx,%rax,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	128(%rsp), %rbp                 # 8-byte Reload
	vmovups	(%r10,%rbp,4), %ymm3
	vmovaps	%ymm8, %ymm2
	vmulps	%ymm3, %ymm8, %ymm5
	vfmadd231ps	%ymm5, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm5) + ymm15
	vinsertps	$32, 44(%rcx,%rdi,4), %xmm1, %xmm0 # xmm0 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	44(%rcx,%r11,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%rcx,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 44(%rcx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 44(%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 44(%rcx,%r15,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	vmovss	52(%rcx,%rax,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	48(%rcx,%rax,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmulps	%ymm3, %ymm4, %ymm3
	vfmadd231ps	%ymm3, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm3) + ymm15
	vinsertps	$32, 48(%rcx,%rdi,4), %xmm5, %xmm0 # xmm0 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	48(%rcx,%r11,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%rcx,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 48(%rcx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 48(%rcx,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$48, 48(%rcx,%r15,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm3, %ymm0
	vinsertps	$32, 52(%rcx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	52(%rcx,%r11,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%rcx,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vinsertps	$48, 52(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 52(%rcx,%r8,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	120(%rsp), %rbp                 # 8-byte Reload
	vmovups	(%r10,%rbp,4), %ymm5
	vmulps	%ymm5, %ymm8, %ymm6
	vfmadd231ps	%ymm6, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm6) + ymm15
	vinsertps	$48, 52(%rcx,%r15,4), %xmm3, %xmm0 # xmm0 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmulps	%ymm5, %ymm4, %ymm1
	vmovss	56(%rcx,%rax,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%rcx,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vfmadd231ps	%ymm1, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm1) + ymm15
	vinsertps	$32, 56(%rcx,%rdi,4), %xmm3, %xmm0 # xmm0 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	56(%rcx,%r11,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%rcx,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 56(%rcx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 56(%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 56(%rcx,%r15,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm1
	movq	-88(%rsp), %r9                  # 8-byte Reload
	vmovss	20(%rcx,%r9,4), %xmm0           # xmm0 = mem[0],zero,zero,zero
	movq	-56(%rsp), %r13                 # 8-byte Reload
	vinsertps	$16, 20(%rcx,%r13,4), %xmm0, %xmm3 # xmm3 = xmm0[0],mem[0],xmm0[2,3]
	vmaxps	%ymm10, %ymm13, %ymm8
	movq	-48(%rsp), %rbp                 # 8-byte Reload
	vinsertps	$32, 20(%rcx,%rbp,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	movq	-64(%rsp), %r15                 # 8-byte Reload
	vmovss	20(%rcx,%r15,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	152(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, 20(%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	320(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 20(%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	-40(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, 20(%rcx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	224(%rsp), %rax                 # 8-byte Reload
	vmulps	(%r10,%rax,4), %ymm8, %ymm6
	vfmadd231ps	%ymm6, %ymm1, %ymm15    # ymm15 = (ymm1 * ymm6) + ymm15
	movq	-96(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 20(%rcx,%rdx,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm1, %ymm3
	vmovss	24(%rcx,%r9,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%rcx,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	24(%rcx,%r15,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 24(%rcx,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 24(%rcx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 24(%rcx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 24(%rcx,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vmovss	28(%rcx,%r9,4), %xmm5           # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%rcx,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovups	%ymm11, 1280(%rsp)              # 32-byte Spill
	movq	232(%rsp), %rax                 # 8-byte Reload
	vmulps	32(%r10,%rax,4), %ymm11, %ymm6
	vmulps	%ymm6, %ymm1, %ymm13
	vinsertps	$32, 28(%rcx,%rbp,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	28(%rcx,%r15,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 28(%rcx,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 28(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	-8(%rsp), %rax                  # 8-byte Reload
	vmulps	32(%r10,%rax,4), %ymm11, %ymm11
	movq	%r10, %r12
	vfmadd231ps	%ymm11, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm11) + ymm13
	vinsertps	$48, 28(%rcx,%rdx,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm3, %ymm3
	vmovss	36(%rcx,%r9,4), %xmm5           # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%rcx,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovss	32(%rcx,%r9,4), %xmm6           # xmm6 = mem[0],zero,zero,zero
	movq	%r9, %r10
	vinsertps	$16, 32(%rcx,%r13,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vmovups	1024(%rsp), %ymm0               # 32-byte Reload
	movq	-16(%rsp), %rax                 # 8-byte Reload
	vmulps	32(%r12,%rax,4), %ymm0, %ymm11
	vfmadd231ps	%ymm11, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm11) + ymm13
	vinsertps	$32, 32(%rcx,%rbp,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	32(%rcx,%r15,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	%rbx, %r8
	vinsertps	$16, 32(%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 32(%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 32(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 32(%rcx,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	vinsertps	$32, 36(%rcx,%rbp,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	36(%rcx,%r15,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 36(%rcx,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertps	$32, 36(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%rsi, %r9
	vmovups	992(%rsp), %ymm2                # 32-byte Reload
	movq	144(%rsp), %rax                 # 8-byte Reload
	vmulps	32(%r12,%rax,4), %ymm2, %ymm11
	vfmadd231ps	%ymm11, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm11) + ymm13
	vinsertps	$48, 36(%rcx,%rdx,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm3, %ymm3
	movq	416(%rsp), %rax                 # 8-byte Reload
	vmovss	92(%rcx,%rax,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	48(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$16, 92(%rcx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovss	40(%rcx,%r10,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%rcx,%r13,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	136(%rsp), %rbx                 # 8-byte Reload
	vmulps	32(%r12,%rbx,4), %ymm0, %ymm11
	vfmadd231ps	%ymm11, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm11) + ymm13
	vinsertps	$32, 40(%rcx,%rbp,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	40(%rcx,%r15,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%rcx,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 40(%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 40(%rcx,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	%r9, %r12
	vinsertps	$48, 40(%rcx,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm6, %ymm3
	movq	264(%rsp), %r14                 # 8-byte Reload
	vinsertps	$32, 92(%rcx,%r14,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	312(%rsp), %r11                 # 8-byte Reload
	vmovss	92(%rcx,%r11,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	168(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$16, 92(%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	movq	256(%rsp), %r9                  # 8-byte Reload
	vinsertps	$32, 92(%rcx,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	88(%rcx,%rax,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%rcx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovss	44(%rcx,%r10,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%rcx,%r13,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	128(%rsp), %rax                 # 8-byte Reload
	movq	240(%rsp), %r10                 # 8-byte Reload
	vmovups	32(%r10,%rax,4), %ymm11
	vmulps	%ymm2, %ymm11, %ymm12
	vfmadd231ps	%ymm12, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm12) + ymm13
	vinsertps	$32, 44(%rcx,%rbp,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	44(%rcx,%r15,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%rcx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 44(%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 44(%rcx,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	movq	%r12, %rdi
	vinsertps	$48, 44(%rcx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	vmulps	%ymm0, %ymm11, %ymm4
	movq	272(%rsp), %r12                 # 8-byte Reload
	vinsertps	$48, 92(%rcx,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vfmadd231ps	%ymm4, %ymm3, %ymm13    # ymm13 = (ymm3 * ymm4) + ymm13
	vinsertps	$32, 88(%rcx,%r14,4), %xmm7, %xmm3 # xmm3 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	88(%rcx,%r11,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 88(%rcx,%rbx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	280(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$48, 92(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertps	$32, 88(%rcx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 88(%rcx,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm6, %ymm5
	vinsertps	$48, 88(%rcx,%rsi,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm4, %ymm3
	movq	416(%rsp), %r13                 # 8-byte Reload
	vmovss	64(%rcx,%r13,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	movq	48(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, 64(%rcx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vmovss	64(%rcx,%r11,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 64(%rcx,%rbx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 64(%rcx,%r14,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$32, 64(%rcx,%r9,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 64(%rcx,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertps	$48, 64(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm6, %ymm4
	movq	104(%rsp), %rax                 # 8-byte Reload
	vmovups	96(%r10,%rax,4), %ymm12
	vcmpltps	%ymm12, %ymm10, %ymm11
	vmovups	640(%rsp), %ymm0                # 32-byte Reload
	vblendvps	%ymm11, %ymm4, %ymm0, %ymm4
	movq	184(%rsp), %rax                 # 8-byte Reload
	vmovups	96(%r10,%rax,4), %ymm6
	vmaxps	%ymm10, %ymm6, %ymm6
	vdivps	%ymm6, %ymm14, %ymm7
	vmovss	76(%rcx,%r13,4), %xmm9          # xmm9 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%rcx,%rdx,4), %xmm9, %xmm9 # xmm9 = xmm9[0],mem[0],xmm9[2,3]
	vmovups	1120(%rsp), %ymm0               # 32-byte Reload
	vaddps	1152(%rsp), %ymm0, %ymm14       # 32-byte Folded Reload
	vmulps	%ymm3, %ymm14, %ymm14
	vinsertps	$32, 76(%rcx,%r14,4), %xmm9, %xmm3 # xmm3 = xmm9[0,1],mem[0],xmm9[3]
	vmovss	76(%rcx,%r11,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 76(%rcx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$48, 76(%rcx,%r12,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 76(%rcx,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$48, 76(%rcx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm0, %ymm0
	vdivps	%ymm6, %ymm14, %ymm9
	vmovups	1568(%rsp), %ymm1               # 32-byte Reload
	vmovups	%ymm11, 800(%rsp)               # 32-byte Spill
	vblendvps	%ymm11, %ymm0, %ymm1, %ymm3
	movq	200(%rsp), %rdx                 # 8-byte Reload
	vmovups	%ymm12, 736(%rsp)               # 32-byte Spill
	vmulps	96(%r10,%rdx,4), %ymm12, %ymm0
	vmulps	%ymm4, %ymm0, %ymm0
	vdivps	%ymm6, %ymm0, %ymm0
	movq	208(%rsp), %rdx                 # 8-byte Reload
	vfmadd132ps	96(%r10,%rdx,4), %ymm0, %ymm3 # ymm3 = (ymm3 * mem) + ymm0
	vandps	%ymm9, %ymm11, %ymm0
	movq	96(%rsp), %rax                  # 8-byte Reload
	vmovups	96(%r10,%rax,4), %ymm1
	vmovups	%ymm1, 640(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm0, %ymm1, %ymm3     # ymm3 = (ymm1 * ymm3) + ymm0
	vminps	%ymm12, %ymm7, %ymm0
	movq	192(%rsp), %rax                 # 8-byte Reload
	vmulps	96(%r10,%rax,4), %ymm0, %ymm0
	movq	112(%rsp), %rax                 # 8-byte Reload
	vmulps	96(%r10,%rax,4), %ymm12, %ymm4
	vmulps	%ymm4, %ymm0, %ymm0
	vfmadd231ps	%ymm0, %ymm5, %ymm3     # ymm3 = (ymm5 * ymm0) + ymm3
	movq	-88(%rsp), %rbx                 # 8-byte Reload
	vmovss	48(%rcx,%rbx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	movq	-56(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 48(%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	48(%rcx,%r15,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%rcx,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 48(%rcx,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 48(%rcx,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	320(%rsp), %r14                 # 8-byte Reload
	vinsertps	$48, 48(%rcx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	-96(%rsp), %r9                  # 8-byte Reload
	vinsertps	$48, 48(%rcx,%r9,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm5, %ymm0
	movq	80(%rsp), %rsi                  # 8-byte Reload
	vmovss	60(%rcx,%rsi,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	(%rsp), %rsi                    # 8-byte Reload
	vinsertps	$16, 60(%rcx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	120(%rsp), %rsi                 # 8-byte Reload
	vmovups	32(%r10,%rsi,4), %ymm7
	vmulps	%ymm7, %ymm2, %ymm9
	vfmadd231ps	%ymm9, %ymm0, %ymm13    # ymm13 = (ymm0 * ymm9) + ymm13
	movq	-80(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, 60(%rcx,%rsi,4), %xmm5, %xmm0 # xmm0 = xmm5[0,1],mem[0],xmm5[3]
	movq	-72(%rsp), %rsi                 # 8-byte Reload
	vmovss	60(%rcx,%rsi,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	64(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$16, 60(%rcx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	-32(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$48, 60(%rcx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	56(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$32, 60(%rcx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	72(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$48, 60(%rcx,%rsi,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm5, %ymm0
	movq	216(%rsp), %rsi                 # 8-byte Reload
	vmulps	(%r10,%rsi,4), %ymm8, %ymm5
	vfmadd231ps	%ymm5, %ymm0, %ymm15    # ymm15 = (ymm0 * ymm5) + ymm15
	vmovups	1248(%rsp), %ymm0               # 32-byte Reload
	vmaxps	%ymm10, %ymm0, %ymm5
	vmovups	608(%rsp), %ymm1                # 32-byte Reload
	vdivps	%ymm1, %ymm8, %ymm2
	vdivps	%ymm1, %ymm5, %ymm8
	vroundps	$10, %ymm2, %ymm9
	vmulps	1600(%rsp), %ymm9, %ymm14       # 32-byte Folded Reload
	vroundps	$10, %ymm8, %ymm9
	vmulps	1472(%rsp), %ymm9, %ymm9        # 32-byte Folded Reload
	vmovups	1216(%rsp), %ymm0               # 32-byte Reload
	vmaxps	%ymm10, %ymm0, %ymm6
	vmaxps	%ymm10, %ymm4, %ymm4
	vdivps	%ymm1, %ymm6, %ymm0
	vdivps	%ymm1, %ymm4, %ymm1
	vroundps	$10, %ymm0, %ymm12
	vmulps	1536(%rsp), %ymm12, %ymm12      # 32-byte Folded Reload
	vroundps	$10, %ymm1, %ymm11
	vmulps	1504(%rsp), %ymm11, %ymm11      # 32-byte Folded Reload
	vmaxps	%ymm10, %ymm2, %ymm2
	vdivps	%ymm2, %ymm14, %ymm2
	vmaxps	%ymm10, %ymm8, %ymm8
	vdivps	%ymm8, %ymm9, %ymm8
	vmaxps	%ymm10, %ymm0, %ymm0
	vmaxps	%ymm10, %ymm1, %ymm1
	vdivps	%ymm0, %ymm12, %ymm0
	vdivps	%ymm1, %ymm11, %ymm1
	vbroadcastss	.LCPI1_7(%rip), %ymm9   # ymm9 = [2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0,2.0E+0]
	vmovups	832(%rsp), %ymm14               # 32-byte Reload
	vfmadd213ps	%ymm2, %ymm9, %ymm14    # ymm14 = (ymm9 * ymm14) + ymm2
	vmovups	576(%rsp), %ymm2                # 32-byte Reload
	vfmadd213ps	%ymm8, %ymm9, %ymm2     # ymm2 = (ymm9 * ymm2) + ymm8
	vmovups	%ymm2, 576(%rsp)                # 32-byte Spill
	vmovups	544(%rsp), %ymm2                # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm9, %ymm2     # ymm2 = (ymm9 * ymm2) + ymm0
	vmovups	%ymm2, 544(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm9, %ymm3     # ymm3 = (ymm9 * ymm3) + ymm1
	vmovss	52(%rcx,%rbx,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	52(%rcx,%r15,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 52(%rcx,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	movq	%rbp, %r15
	vinsertps	$32, 52(%rcx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 52(%rcx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vmulps	1024(%rsp), %ymm7, %ymm2        # 32-byte Folded Reload
	vinsertps	$48, 52(%rcx,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	24(%rsp), %r9                   # 8-byte Reload
	vmovss	20(%rcx,%r9,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	movq	32(%rsp), %r11                  # 8-byte Reload
	vinsertps	$16, 20(%rcx,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	-24(%rsp), %rdi                 # 8-byte Reload
	vmovss	20(%rcx,%rdi,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	16(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$16, 20(%rcx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	40(%rsp), %rbp                  # 8-byte Reload
	vinsertps	$32, 20(%rcx,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	160(%rsp), %r8                  # 8-byte Reload
	vinsertps	$32, 20(%rcx,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	248(%rsp), %r13                 # 8-byte Reload
	vinsertps	$48, 20(%rcx,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vfmadd231ps	%ymm2, %ymm0, %ymm13    # ymm13 = (ymm0 * ymm2) + ymm13
	movq	8(%rsp), %rax                   # 8-byte Reload
	vinsertps	$48, 20(%rcx,%rax,4), %xmm7, %xmm0 # xmm0 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	24(%rcx,%r9,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%rcx,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	24(%rcx,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 24(%rcx,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 24(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 24(%rcx,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 24(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovss	28(%rcx,%r9,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%rcx,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovups	672(%rsp), %ymm8                # 32-byte Reload
	movq	232(%rsp), %rbx                 # 8-byte Reload
	vmulps	64(%r10,%rbx,4), %ymm8, %ymm7
	vmulps	%ymm7, %ymm1, %ymm10
	vinsertps	$32, 28(%rcx,%rbp,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	28(%rcx,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 28(%rcx,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 28(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	-8(%rsp), %rbx                  # 8-byte Reload
	vmulps	64(%r10,%rbx,4), %ymm8, %ymm7
	vfmadd231ps	%ymm7, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm7) + ymm10
	vinsertps	$48, 28(%rcx,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	36(%rcx,%r9,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%rcx,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	32(%rcx,%r9,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%rcx,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovups	1088(%rsp), %ymm9               # 32-byte Reload
	movq	-16(%rsp), %rbx                 # 8-byte Reload
	vmulps	64(%r10,%rbx,4), %ymm9, %ymm7
	vfmadd231ps	%ymm7, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm7) + ymm10
	vinsertps	$32, 32(%rcx,%rbp,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	32(%rcx,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 32(%rcx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 32(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 32(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vinsertps	$32, 36(%rcx,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	36(%rcx,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	%rdi, %rbx
	vinsertps	$16, 36(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 36(%rcx,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 36(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovups	1056(%rsp), %ymm11              # 32-byte Reload
	movq	144(%rsp), %rdi                 # 8-byte Reload
	vmulps	64(%r10,%rdi,4), %ymm11, %ymm7
	vfmadd231ps	%ymm7, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm7) + ymm10
	vinsertps	$48, 36(%rcx,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	movq	%rax, %rdi
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmovss	44(%rcx,%r9,4), %xmm1           # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%rcx,%r11,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	40(%rcx,%r9,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%rcx,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	136(%rsp), %rax                 # 8-byte Reload
	vmulps	64(%r10,%rax,4), %ymm9, %ymm7
	vfmadd231ps	%ymm7, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm7) + ymm10
	vinsertps	$32, 40(%rcx,%rbp,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	40(%rcx,%rbx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	%rsi, %rax
	vinsertps	$16, 40(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 40(%rcx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 40(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 40(%rcx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vinsertps	$32, 44(%rcx,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vmovss	44(%rcx,%rbx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 44(%rcx,%r13,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 44(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	128(%rsp), %rsi                 # 8-byte Reload
	vmovups	64(%r10,%rsi,4), %ymm7
	vmulps	%ymm7, %ymm11, %ymm8
	vfmadd231ps	%ymm8, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm8) + ymm10
	vinsertps	$48, 44(%rcx,%rdi,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	movq	%rdi, %rsi
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vmulps	%ymm7, %ymm9, %ymm1
	movq	-88(%rsp), %r12                 # 8-byte Reload
	vmovss	56(%rcx,%r12,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	-56(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 56(%rcx,%rdi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vfmadd231ps	%ymm1, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm1) + ymm10
	vinsertps	$32, 56(%rcx,%r15,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1],mem[0],xmm2[3]
	movq	-64(%rsp), %rdi                 # 8-byte Reload
	vmovss	56(%rcx,%rdi,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	152(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 56(%rcx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 56(%rcx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	movq	-40(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$32, 56(%rcx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	-96(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 56(%rcx,%rdi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	224(%rsp), %r15                 # 8-byte Reload
	vmulps	32(%r10,%r15,4), %ymm5, %ymm1
	vfmadd231ps	%ymm1, %ymm0, %ymm13    # ymm13 = (ymm0 * ymm1) + ymm13
	vmovss	48(%rcx,%r9,4), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%rcx,%r11,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	48(%rcx,%rbx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$32, 48(%rcx,%rbp,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 48(%rcx,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$48, 48(%rcx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 48(%rcx,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm1, %ymm0
	movq	120(%rsp), %rdi                 # 8-byte Reload
	vmovups	64(%r10,%rdi,4), %ymm1
	vmulps	%ymm1, %ymm11, %ymm2
	vmovss	52(%rcx,%r9,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%rcx,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vfmadd231ps	%ymm2, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm2) + ymm10
	vinsertps	$32, 52(%rcx,%rbp,4), %xmm7, %xmm0 # xmm0 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	52(%rcx,%rbx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$48, 52(%rcx,%r13,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$32, 52(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 52(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmovss	56(%rcx,%r9,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%rcx,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmulps	%ymm1, %ymm9, %ymm1
	vinsertps	$32, 56(%rcx,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	56(%rcx,%rbx,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 56(%rcx,%r13,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vfmadd231ps	%ymm1, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm1) + ymm10
	vinsertps	$32, 56(%rcx,%r8,4), %xmm7, %xmm0 # xmm0 = xmm7[0,1],mem[0],xmm7[3]
	movq	416(%rsp), %rdi                 # 8-byte Reload
	vmovss	20(%rcx,%rdi,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	movq	48(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, 20(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vinsertps	$48, 56(%rcx,%rsi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm0, %ymm0
	movq	264(%rsp), %r14                 # 8-byte Reload
	vinsertps	$32, 20(%rcx,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	312(%rsp), %r9                  # 8-byte Reload
	vmovss	20(%rcx,%r9,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	movq	168(%rsp), %r11                 # 8-byte Reload
	vinsertps	$16, 20(%rcx,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	272(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$48, 20(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	256(%rsp), %r8                  # 8-byte Reload
	vinsertps	$32, 20(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmulps	64(%r10,%r15,4), %ymm6, %ymm7
	vfmadd231ps	%ymm7, %ymm0, %ymm10    # ymm10 = (ymm0 * ymm7) + ymm10
	movq	280(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 20(%rcx,%rax,4), %xmm2, %xmm0 # xmm0 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm1
	vmovss	24(%rcx,%rdi,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%rcx,%rdx,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vmovss	24(%rcx,%r9,4), %xmm2           # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 24(%rcx,%r11,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vinsertps	$32, 24(%rcx,%r14,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertps	$32, 24(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 24(%rcx,%rbx,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$48, 24(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm0, %ymm2, %ymm0
	vmovss	28(%rcx,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	232(%rsp), %rbp                 # 8-byte Reload
	vmovups	640(%rsp), %ymm8                # 32-byte Reload
	vmulps	96(%r10,%rbp,4), %ymm8, %ymm7
	vmulps	%ymm7, %ymm0, %ymm0
	vinsertps	$32, 28(%rcx,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	28(%rcx,%r9,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 28(%rcx,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 28(%rcx,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 28(%rcx,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	-8(%rsp), %rbp                  # 8-byte Reload
	vmulps	96(%r10,%rbp,4), %ymm8, %ymm8
	vfmadd231ps	%ymm8, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm8) + ymm0
	vinsertps	$48, 28(%rcx,%rax,4), %xmm7, %xmm1 # xmm1 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vmovss	36(%rcx,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 36(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	32(%rcx,%rdi,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%rcx,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vmovups	1120(%rsp), %ymm11              # 32-byte Reload
	movq	-16(%rsp), %rbp                 # 8-byte Reload
	vmulps	96(%r10,%rbp,4), %ymm11, %ymm8
	vfmadd231ps	%ymm8, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm8) + ymm0
	vinsertps	$32, 32(%rcx,%r14,4), %xmm7, %xmm1 # xmm1 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	32(%rcx,%r9,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 32(%rcx,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 32(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 32(%rcx,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 32(%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm7, %ymm1
	vinsertps	$32, 36(%rcx,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	%r14, %r15
	vmovss	36(%rcx,%r9,4), %xmm7           # xmm7 = mem[0],zero,zero,zero
	movq	%r9, %r14
	vinsertps	$16, 36(%rcx,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 36(%rcx,%rbx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	%rbx, %r9
	vinsertps	$32, 36(%rcx,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vmovups	1152(%rsp), %ymm12              # 32-byte Reload
	movq	144(%rsp), %rbx                 # 8-byte Reload
	vmulps	96(%r10,%rbx,4), %ymm12, %ymm8
	vfmadd231ps	%ymm8, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm8) + ymm0
	vinsertps	$48, 36(%rcx,%rax,4), %xmm7, %xmm1 # xmm1 = xmm7[0,1,2],mem[0]
	movq	%rax, %rbp
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vmovss	44(%rcx,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	40(%rcx,%rdi,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	%rdi, %r13
	vinsertps	$16, 40(%rcx,%rdx,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	%rdx, %rbx
	movq	136(%rsp), %rax                 # 8-byte Reload
	vmulps	96(%r10,%rax,4), %ymm11, %ymm8
	vfmadd231ps	%ymm8, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm8) + ymm0
	vinsertps	$32, 40(%rcx,%r15,4), %xmm7, %xmm1 # xmm1 = xmm7[0,1],mem[0],xmm7[3]
	movq	%r14, %rdi
	vmovss	40(%rcx,%r14,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 40(%rcx,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 40(%rcx,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 40(%rcx,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 40(%rcx,%rbp,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm7, %ymm1
	vinsertps	$32, 44(%rcx,%r15,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmovss	44(%rcx,%r14,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 44(%rcx,%r11,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	%r11, %r14
	vinsertps	$48, 44(%rcx,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 44(%rcx,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	%r8, %r11
	movq	128(%rsp), %rax                 # 8-byte Reload
	vmovups	96(%r10,%rax,4), %ymm8
	vmulps	%ymm8, %ymm12, %ymm9
	vfmadd231ps	%ymm9, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm9) + ymm0
	vinsertps	$48, 44(%rcx,%rbp,4), %xmm7, %xmm1 # xmm1 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vmulps	%ymm8, %ymm11, %ymm2
	movq	%r12, %rsi
	vmovss	60(%rcx,%r12,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	-56(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 60(%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vfmadd231ps	%ymm2, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm2) + ymm0
	movq	-48(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 60(%rcx,%rax,4), %xmm7, %xmm1 # xmm1 = xmm7[0,1],mem[0],xmm7[3]
	movq	-64(%rsp), %rax                 # 8-byte Reload
	vmovss	60(%rcx,%rax,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	152(%rsp), %rax                 # 8-byte Reload
	vinsertps	$16, 60(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	320(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 60(%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	-40(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 60(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	-96(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 60(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	movq	216(%rsp), %rsi                 # 8-byte Reload
	vmulps	32(%r10,%rsi,4), %ymm5, %ymm2
	vfmadd231ps	%ymm2, %ymm1, %ymm13    # ymm13 = (ymm1 * ymm2) + ymm13
	movq	%r13, %rdx
	vmovss	48(%rcx,%r13,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	48(%rcx,%rdi,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 48(%rcx,%r14,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	%r14, %r13
	vinsertps	$32, 48(%rcx,%r15,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 48(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vinsertps	$48, 48(%rcx,%r9,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	%r9, %r12
	vinsertps	$48, 48(%rcx,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	movq	120(%rsp), %rax                 # 8-byte Reload
	vmovups	96(%r10,%rax,4), %ymm2
	vmulps	%ymm2, %ymm12, %ymm5
	movq	24(%rsp), %rax                  # 8-byte Reload
	vmovss	60(%rcx,%rax,4), %xmm7          # xmm7 = mem[0],zero,zero,zero
	movq	32(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 60(%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vfmadd231ps	%ymm5, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm5) + ymm0
	movq	40(%rsp), %r9                   # 8-byte Reload
	vinsertps	$32, 60(%rcx,%r9,4), %xmm7, %xmm1 # xmm1 = xmm7[0,1],mem[0],xmm7[3]
	movq	-24(%rsp), %rax                 # 8-byte Reload
	vmovss	60(%rcx,%rax,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	16(%rsp), %rax                  # 8-byte Reload
	vinsertps	$16, 60(%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	248(%rsp), %r14                 # 8-byte Reload
	vinsertps	$48, 60(%rcx,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	160(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 60(%rcx,%rax,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	8(%rsp), %r8                    # 8-byte Reload
	vinsertps	$48, 60(%rcx,%r8,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vmulps	64(%r10,%rsi,4), %ymm6, %ymm5
	vfmadd231ps	%ymm5, %ymm1, %ymm10    # ymm10 = (ymm1 * ymm5) + ymm10
	vmovss	52(%rcx,%rdx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%rcx,%rbx,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	vmovss	52(%rcx,%rdi,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 52(%rcx,%r13,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$32, 52(%rcx,%r15,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	vinsertps	$32, 52(%rcx,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 52(%rcx,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vmulps	%ymm2, %ymm11, %ymm2
	vinsertps	$48, 52(%rcx,%rbp,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	vmovss	56(%rcx,%rdx,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vmovss	56(%rcx,%rdi,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 56(%rcx,%r13,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 56(%rcx,%r15,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$32, 56(%rcx,%r11,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 56(%rcx,%r12,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	vfmadd231ps	%ymm2, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm2) + ymm0
	vinsertps	$48, 56(%rcx,%rbp,4), %xmm6, %xmm1 # xmm1 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm5, %ymm1, %ymm1
	movq	80(%rsp), %rax                  # 8-byte Reload
	vmovss	96(%rcx,%rax,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	(%rsp), %rsi                    # 8-byte Reload
	vinsertps	$16, 96(%rcx,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmovss	60(%rcx,%rdx,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 60(%rcx,%rbx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	224(%rsp), %rdx                 # 8-byte Reload
	vmulps	96(%r10,%rdx,4), %ymm4, %ymm6
	vfmadd231ps	%ymm6, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm6) + ymm0
	vinsertps	$32, 60(%rcx,%r15,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1],mem[0],xmm5[3]
	movq	%rdi, %r13
	vmovss	60(%rcx,%rdi,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	168(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$16, 60(%rcx,%rdi,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vinsertps	$48, 60(%rcx,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertps	$32, 60(%rcx,%r11,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	vinsertps	$48, 60(%rcx,%rbp,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1,2],mem[0]
	movq	%rbp, %r11
	vinsertf128	$1, %xmm1, %ymm5, %ymm1
	movq	-80(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$32, 96(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	-72(%rsp), %rdx                 # 8-byte Reload
	vmovss	96(%rcx,%rdx,4), %xmm5          # xmm5 = mem[0],zero,zero,zero
	movq	64(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, 96(%rcx,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	movq	-32(%rsp), %rbp                 # 8-byte Reload
	vinsertps	$48, 96(%rcx,%rbp,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	movq	56(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$32, 96(%rcx,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0,1],mem[0],xmm5[3]
	movq	216(%rsp), %rdx                 # 8-byte Reload
	vmulps	96(%r10,%rdx,4), %ymm4, %ymm4
	vfmadd231ps	%ymm4, %ymm1, %ymm0     # ymm0 = (ymm1 * ymm4) + ymm0
	movq	72(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$48, 96(%rcx,%rdx,4), %xmm5, %xmm1 # xmm1 = xmm5[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm1, %ymm1
	vaddps	%ymm15, %ymm14, %ymm5
	movq	-88(%rsp), %rdx                 # 8-byte Reload
	vmovss	96(%rcx,%rdx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, 96(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vfmadd231ps	1376(%rsp), %ymm1, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm1 * mem) + ymm5
	movq	-48(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$32, 96(%rcx,%rdx,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1],mem[0],xmm2[3]
	movq	-64(%rsp), %rdx                 # 8-byte Reload
	vmovss	96(%rcx,%rdx,4), %xmm2          # xmm2 = mem[0],zero,zero,zero
	movq	152(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, 96(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	320(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 96(%rcx,%rdx,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	-40(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$32, 96(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	-96(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 96(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovss	108(%rcx,%rax,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%rcx,%rsi,4), %xmm2, %xmm8 # xmm8 = xmm2[0],mem[0],xmm2[2,3]
	vaddps	576(%rsp), %ymm13, %ymm11       # 32-byte Folded Reload
	movq	24(%rsp), %rdx                  # 8-byte Reload
	vmovss	96(%rcx,%rdx,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, 96(%rcx,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vfmadd231ps	1280(%rsp), %ymm1, %ymm11 # 32-byte Folded Reload
                                        # ymm11 = (ymm1 * mem) + ymm11
	vinsertps	$32, 96(%rcx,%r9,4), %xmm6, %xmm1 # xmm1 = xmm6[0,1],mem[0],xmm6[3]
	movq	-24(%rsp), %rdx                 # 8-byte Reload
	vmovss	96(%rcx,%rdx,4), %xmm6          # xmm6 = mem[0],zero,zero,zero
	movq	16(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, 96(%rcx,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 96(%rcx,%r14,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	160(%rsp), %r14                 # 8-byte Reload
	vinsertps	$32, 96(%rcx,%r14,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 96(%rcx,%r8,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm6, %ymm6
	movq	416(%rsp), %rdx                 # 8-byte Reload
	vmovss	96(%rcx,%rdx,4), %xmm1          # xmm1 = mem[0],zero,zero,zero
	vinsertps	$16, 96(%rcx,%rbx,4), %xmm1, %xmm7 # xmm7 = xmm1[0],mem[0],xmm1[2,3]
	vaddps	544(%rsp), %ymm10, %ymm10       # 32-byte Folded Reload
	vinsertps	$32, 96(%rcx,%r15,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vmovss	96(%rcx,%r13,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 96(%rcx,%rdi,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 96(%rcx,%r12,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vfmadd231ps	672(%rsp), %ymm6, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm6 * mem) + ymm10
	movq	256(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$32, 96(%rcx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	104(%rcx,%rax,4), %xmm6         # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 96(%rcx,%r11,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm7, %ymm4, %ymm4
	vmovss	100(%rcx,%rax,4), %xmm7         # xmm7 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%rcx,%rsi,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vaddps	%ymm0, %ymm3, %ymm15
	movq	-72(%rsp), %rsi                 # 8-byte Reload
	vmovss	108(%rcx,%rsi,4), %xmm3         # xmm3 = mem[0],zero,zero,zero
	movq	64(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, 108(%rcx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vfmadd231ps	640(%rsp), %ymm4, %ymm15 # 32-byte Folded Reload
                                        # ymm15 = (ymm4 * mem) + ymm15
	movq	-80(%rsp), %rax                 # 8-byte Reload
	vinsertps	$32, 104(%rcx,%rax,4), %xmm6, %xmm4 # xmm4 = xmm6[0,1],mem[0],xmm6[3]
	vmovss	104(%rcx,%rsi,4), %xmm6         # xmm6 = mem[0],zero,zero,zero
	movq	%rsi, %r11
	vinsertps	$16, 104(%rcx,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$48, 104(%rcx,%rbp,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	movq	56(%rsp), %rdi                  # 8-byte Reload
	vinsertps	$32, 104(%rcx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	movq	72(%rsp), %rsi                  # 8-byte Reload
	vinsertps	$48, 104(%rcx,%rsi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm6, %ymm4
	vmovss	100(%rcx,%r11,4), %xmm6         # xmm6 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%rcx,%rdx,4), %xmm6, %xmm6 # xmm6 = xmm6[0],mem[0],xmm6[2,3]
	vinsertps	$32, 108(%rcx,%rax,4), %xmm8, %xmm1 # xmm1 = xmm8[0,1],mem[0],xmm8[3]
	vinsertps	$32, 100(%rcx,%rax,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	movq	%rdi, %rax
	vinsertps	$32, 108(%rcx,%rdi,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$32, 100(%rcx,%rdi,4), %xmm6, %xmm6 # xmm6 = xmm6[0,1],mem[0],xmm6[3]
	vinsertps	$48, 108(%rcx,%rbp,4), %xmm1, %xmm8 # xmm8 = xmm1[0,1,2],mem[0]
	vinsertps	$48, 100(%rcx,%rbp,4), %xmm7, %xmm1 # xmm1 = xmm7[0,1,2],mem[0]
	vinsertps	$48, 108(%rcx,%rsi,4), %xmm3, %xmm9 # xmm9 = xmm3[0,1,2],mem[0]
	vinsertps	$48, 100(%rcx,%rsi,4), %xmm6, %xmm3 # xmm3 = xmm6[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm3, %ymm1
	vandps	1664(%rsp), %ymm1, %ymm6        # 32-byte Folded Reload
	movq	392(%rsp), %r11                 # 8-byte Reload
	vmovups	(%r10,%r11,4), %ymm1
	vfmadd213ps	%ymm5, %ymm1, %ymm6     # ymm6 = (ymm1 * ymm6) + ymm5
	movq	-88(%rsp), %rax                 # 8-byte Reload
	vmovss	108(%rcx,%rax,4), %xmm3         # xmm3 = mem[0],zero,zero,zero
	movq	-56(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$16, 108(%rcx,%rdx,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vbroadcastss	.LCPI1_8(%rip), %ymm7   # ymm7 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
	vaddps	1632(%rsp), %ymm7, %ymm5        # 32-byte Folded Reload
	vmovss	104(%rcx,%rax,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 104(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	vmulps	%ymm1, %ymm5, %ymm1
	vmovss	100(%rcx,%rax,4), %xmm5         # xmm5 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%rcx,%rdx,4), %xmm5, %xmm5 # xmm5 = xmm5[0],mem[0],xmm5[2,3]
	vfmadd231ps	%ymm1, %ymm4, %ymm6     # ymm6 = (ymm4 * ymm1) + ymm6
	movq	-48(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$32, 104(%rcx,%rsi,4), %xmm2, %xmm1 # xmm1 = xmm2[0,1],mem[0],xmm2[3]
	movq	-64(%rsp), %rdi                 # 8-byte Reload
	vmovss	104(%rcx,%rdi,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	movq	152(%rsp), %r8                  # 8-byte Reload
	vinsertps	$16, 104(%rcx,%r8,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	320(%rsp), %rax                 # 8-byte Reload
	vinsertps	$48, 104(%rcx,%rax,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	movq	-40(%rsp), %r9                  # 8-byte Reload
	vinsertps	$32, 104(%rcx,%r9,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	movq	-96(%rsp), %rdx                 # 8-byte Reload
	vinsertps	$48, 104(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vinsertps	$32, 100(%rcx,%rsi,4), %xmm5, %xmm2 # xmm2 = xmm5[0,1],mem[0],xmm5[3]
	vmovss	100(%rcx,%rdi,4), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%rcx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 100(%rcx,%rax,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vinsertps	$32, 100(%rcx,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 100(%rcx,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm4, %ymm2
	vinsertps	$32, 108(%rcx,%rsi,4), %xmm3, %xmm12 # xmm12 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	108(%rcx,%rdi,4), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%rcx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$32, 108(%rcx,%r9,4), %xmm4, %xmm14 # xmm14 = xmm4[0,1],mem[0],xmm4[3]
	vandps	704(%rsp), %ymm2, %ymm5         # 32-byte Folded Reload
	movq	24(%rsp), %rbx                  # 8-byte Reload
	vmovss	104(%rcx,%rbx,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	movq	32(%rsp), %rdx                  # 8-byte Reload
	vinsertps	$16, 104(%rcx,%rdx,4), %xmm2, %xmm2 # xmm2 = xmm2[0],mem[0],xmm2[2,3]
	movq	-24(%rsp), %r8                  # 8-byte Reload
	vmovss	104(%rcx,%r8,4), %xmm3          # xmm3 = mem[0],zero,zero,zero
	movq	16(%rsp), %r9                   # 8-byte Reload
	vinsertps	$16, 104(%rcx,%r9,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vmovups	32(%r10,%r11,4), %ymm13
	vfmadd213ps	%ymm11, %ymm13, %ymm5   # ymm5 = (ymm13 * ymm5) + ymm11
	vaddps	1440(%rsp), %ymm7, %ymm11       # 32-byte Folded Reload
	movq	%rcx, %rax
	movq	40(%rsp), %rcx                  # 8-byte Reload
	vinsertps	$32, 104(%rax,%rcx,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1],mem[0],xmm2[3]
	vmulps	%ymm13, %ymm11, %ymm11
	movq	%r14, %rdi
	vinsertps	$32, 104(%rax,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vmovss	100(%rax,%rbx,4), %xmm4         # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%rax,%rdx,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	248(%rsp), %rsi                 # 8-byte Reload
	vinsertps	$48, 104(%rax,%rsi,4), %xmm2, %xmm2 # xmm2 = xmm2[0,1,2],mem[0]
	vfmadd231ps	%ymm11, %ymm1, %ymm5    # ymm5 = (ymm1 * ymm11) + ymm5
	vinsertps	$32, 100(%rax,%rcx,4), %xmm4, %xmm1 # xmm1 = xmm4[0,1],mem[0],xmm4[3]
	vmovss	100(%rax,%r8,4), %xmm4          # xmm4 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%rax,%r9,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	movq	8(%rsp), %rcx                   # 8-byte Reload
	vinsertps	$48, 104(%rax,%rcx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 100(%rax,%r14,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 100(%rax,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm2, %ymm3, %ymm3
	vinsertps	$48, 100(%rax,%rcx,4), %xmm4, %xmm2 # xmm2 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm2, %ymm1
	vmovss	108(%rax,%rbx,4), %xmm2         # xmm2 = mem[0],zero,zero,zero
	vinsertps	$16, 108(%rax,%rdx,4), %xmm2, %xmm11 # xmm11 = xmm2[0],mem[0],xmm2[2,3]
	vandps	1408(%rsp), %ymm1, %ymm2        # 32-byte Folded Reload
	vmovups	64(%r10,%r11,4), %ymm1
	vfmadd213ps	%ymm10, %ymm1, %ymm2    # ymm2 = (ymm1 * ymm2) + ymm10
	vmovss	108(%rax,%r8,4), %xmm0          # xmm0 = mem[0],zero,zero,zero
	vaddps	768(%rsp), %ymm7, %ymm10        # 32-byte Folded Reload
	vmulps	%ymm1, %ymm10, %ymm1
	movq	416(%rsp), %r15                 # 8-byte Reload
	vmovss	104(%rax,%r15,4), %xmm4         # xmm4 = mem[0],zero,zero,zero
	movq	48(%rsp), %r14                  # 8-byte Reload
	vinsertps	$16, 104(%rax,%r14,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vfmadd231ps	%ymm1, %ymm3, %ymm2     # ymm2 = (ymm3 * ymm1) + ymm2
	movq	%r13, %rdx
	vmovss	104(%rax,%r13,4), %xmm1         # xmm1 = mem[0],zero,zero,zero
	movq	264(%rsp), %rbx                 # 8-byte Reload
	vinsertps	$32, 104(%rax,%rbx,4), %xmm4, %xmm3 # xmm3 = xmm4[0,1],mem[0],xmm4[3]
	movq	168(%rsp), %r12                 # 8-byte Reload
	vinsertps	$16, 104(%rax,%r12,4), %xmm1, %xmm1 # xmm1 = xmm1[0],mem[0],xmm1[2,3]
	movq	272(%rsp), %r13                 # 8-byte Reload
	vinsertps	$48, 104(%rax,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	movq	256(%rsp), %r8                  # 8-byte Reload
	vinsertps	$32, 104(%rax,%r8,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1],mem[0],xmm1[3]
	movq	280(%rsp), %rbp                 # 8-byte Reload
	vinsertps	$48, 104(%rax,%rbp,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm1, %ymm1
	vmovss	100(%rax,%r15,4), %xmm3         # xmm3 = mem[0],zero,zero,zero
	vinsertps	$16, 100(%rax,%r14,4), %xmm3, %xmm3 # xmm3 = xmm3[0],mem[0],xmm3[2,3]
	vaddps	736(%rsp), %ymm7, %ymm4         # 32-byte Folded Reload
	vmovss	100(%rax,%rdx,4), %xmm7         # xmm7 = mem[0],zero,zero,zero
	vinsertps	$32, 100(%rax,%rbx,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1],mem[0],xmm3[3]
	vinsertps	$16, 100(%rax,%r12,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	vinsertps	$48, 100(%rax,%r13,4), %xmm3, %xmm3 # xmm3 = xmm3[0,1,2],mem[0]
	vinsertps	$32, 100(%rax,%r8,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 100(%rax,%rbp,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertf128	$1, %xmm3, %ymm7, %ymm3
	vandps	800(%rsp), %ymm3, %ymm3         # 32-byte Folded Reload
	vmovups	96(%r10,%r11,4), %ymm7
	vfmadd213ps	%ymm15, %ymm7, %ymm3    # ymm3 = (ymm7 * ymm3) + ymm15
	vmulps	%ymm7, %ymm4, %ymm4
	vfmadd231ps	%ymm4, %ymm1, %ymm3     # ymm3 = (ymm1 * ymm4) + ymm3
	movq	40(%rsp), %rcx                  # 8-byte Reload
	vinsertps	$32, 108(%rax,%rcx,4), %xmm11, %xmm1 # xmm1 = xmm11[0,1],mem[0],xmm11[3]
	movq	%rax, %rcx
	vinsertps	$16, 108(%rax,%r9,4), %xmm0, %xmm0 # xmm0 = xmm0[0],mem[0],xmm0[2,3]
	vinsertps	$32, 108(%rax,%rdi,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1],mem[0],xmm0[3]
	vinsertf128	$1, %xmm8, %ymm9, %ymm8
	vmovss	108(%rax,%r15,4), %xmm7         # xmm7 = mem[0],zero,zero,zero
	movq	320(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 108(%rax,%rdi,4), %xmm12, %xmm4 # xmm4 = xmm12[0,1,2],mem[0]
	vinsertps	$16, 108(%rax,%r14,4), %xmm7, %xmm7 # xmm7 = xmm7[0],mem[0],xmm7[2,3]
	movq	-96(%rsp), %rdi                 # 8-byte Reload
	vinsertps	$48, 108(%rax,%rdi,4), %xmm14, %xmm9 # xmm9 = xmm14[0,1,2],mem[0]
	vinsertps	$32, 108(%rax,%rbx,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1],mem[0],xmm7[3]
	vinsertps	$48, 108(%rax,%rsi,4), %xmm1, %xmm1 # xmm1 = xmm1[0,1,2],mem[0]
	vinsertf128	$1, %xmm4, %ymm9, %ymm9
	vmovss	108(%rax,%rdx,4), %xmm4         # xmm4 = mem[0],zero,zero,zero
	movq	8(%rsp), %rax                   # 8-byte Reload
	vinsertps	$48, 108(%rcx,%rax,4), %xmm0, %xmm0 # xmm0 = xmm0[0,1,2],mem[0]
	vinsertps	$16, 108(%rcx,%r12,4), %xmm4, %xmm4 # xmm4 = xmm4[0],mem[0],xmm4[2,3]
	vinsertps	$48, 108(%rcx,%r13,4), %xmm7, %xmm7 # xmm7 = xmm7[0,1,2],mem[0]
	vinsertps	$32, 108(%rcx,%r8,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1],mem[0],xmm4[3]
	vinsertps	$48, 108(%rcx,%rbp,4), %xmm4, %xmm4 # xmm4 = xmm4[0,1,2],mem[0]
	vinsertf128	$1, %xmm1, %ymm0, %ymm0
	vinsertf128	$1, %xmm7, %ymm4, %ymm1
	movq	400(%rsp), %rax                 # 8-byte Reload
	vfmadd231ps	(%r10,%rax,4), %ymm8, %ymm6 # ymm6 = (ymm8 * mem) + ymm6
	vfmadd231ps	32(%r10,%rax,4), %ymm9, %ymm5 # ymm5 = (ymm9 * mem) + ymm5
	vfmadd231ps	64(%r10,%rax,4), %ymm0, %ymm2 # ymm2 = (ymm0 * mem) + ymm2
	vfmadd231ps	96(%r10,%rax,4), %ymm1, %ymm3 # ymm3 = (ymm1 * mem) + ymm3
	vbroadcastss	.LCPI1_9(%rip), %ymm0   # ymm0 = [9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10,9.99999971E-10]
	vmovups	960(%rsp), %ymm1                # 32-byte Reload
	vfmadd231ps	%ymm6, %ymm0, %ymm1     # ymm1 = (ymm0 * ymm6) + ymm1
	vmovups	%ymm1, 960(%rsp)                # 32-byte Spill
	vmovups	928(%rsp), %ymm1                # 32-byte Reload
	vfmadd231ps	%ymm5, %ymm0, %ymm1     # ymm1 = (ymm0 * ymm5) + ymm1
	vmovups	%ymm1, 928(%rsp)                # 32-byte Spill
	vmovups	896(%rsp), %ymm1                # 32-byte Reload
	vfmadd231ps	%ymm2, %ymm0, %ymm1     # ymm1 = (ymm0 * ymm2) + ymm1
	vmovups	%ymm1, 896(%rsp)                # 32-byte Spill
	vmovups	864(%rsp), %ymm1                # 32-byte Reload
	vfmadd231ps	%ymm3, %ymm0, %ymm1     # ymm1 = (ymm0 * ymm3) + ymm1
	vmovups	%ymm1, 864(%rsp)                # 32-byte Spill
	vpbroadcastq	.LCPI1_10(%rip), %ymm0  # ymm0 = [32,32,32,32]
	vmovdqu	1696(%rsp), %ymm2               # 32-byte Reload
	vpaddq	%ymm0, %ymm2, %ymm2
	vmovdqu	1728(%rsp), %ymm4               # 32-byte Reload
	vpaddq	%ymm0, %ymm4, %ymm4
	addq	520(%rsp), %r10                 # 8-byte Folded Reload
	movq	%r10, 240(%rsp)                 # 8-byte Spill
	addq	$-32, 528(%rsp)                 # 8-byte Folded Spill
	jne	.LBB1_6
# %bb.7:                                # %middle.block
                                        #   in Loop: Header=BB1_1 Depth=1
	vmovups	928(%rsp), %ymm0                # 32-byte Reload
	vaddps	960(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vaddps	896(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vaddps	864(%rsp), %ymm0, %ymm0         # 32-byte Folded Reload
	vextractf128	$1, %ymm0, %xmm1
	vaddps	%xmm1, %xmm0, %xmm0
	vpermilpd	$1, %xmm0, %xmm1        # xmm1 = xmm0[1,0]
	vaddps	%xmm1, %xmm0, %xmm0
	vmovshdup	%xmm0, %xmm1            # xmm1 = xmm0[1,1,3,3]
	vaddss	%xmm1, %xmm0, %xmm2
	movq	464(%rsp), %rdx                 # 8-byte Reload
	movq	%rdx, %rax
	cmpq	296(%rsp), %rdx                 # 8-byte Folded Reload
	vmovups	608(%rsp), %ymm3                # 32-byte Reload
	movq	232(%rsp), %r12                 # 8-byte Reload
	movq	-8(%rsp), %rdi                  # 8-byte Reload
	movq	-16(%rsp), %rbx                 # 8-byte Reload
	je	.LBB1_15
	jmp	.LBB1_8
	.p2align	4, 0x90
.LBB1_4:                                #   in Loop: Header=BB1_1 Depth=1
	vxorps	%xmm2, %xmm2, %xmm2
	xorl	%eax, %eax
	movq	-8(%rsp), %rdi                  # 8-byte Reload
	movq	-16(%rsp), %rbx                 # 8-byte Reload
.LBB1_8:                                # %"for f7.s1.r77$x.preheader118"
                                        #   in Loop: Header=BB1_1 Depth=1
	movslq	92(%rsp), %rdx                  # 4-byte Folded Reload
	movq	%rax, %rsi
	shlq	$5, %rsi
	addq	%rdx, %rsi
	movq	480(%rsp), %rdx                 # 8-byte Reload
	leaq	(%rdx,%rsi,4), %rdx
	movq	376(%rsp), %rsi                 # 8-byte Reload
	imulq	%rax, %rsi
	movq	%r12, %rbp
	addq	%rsi, %rbp
	movq	%rbp, -88(%rsp)                 # 8-byte Spill
	addq	%rsi, %rdi
	movq	%rdi, 320(%rsp)                 # 8-byte Spill
	addq	%rsi, %rbx
	movq	%rbx, -48(%rsp)                 # 8-byte Spill
	movq	144(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, -56(%rsp)                 # 8-byte Spill
	movq	136(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, -64(%rsp)                 # 8-byte Spill
	movq	224(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, -72(%rsp)                 # 8-byte Spill
	movq	216(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, -80(%rsp)                 # 8-byte Spill
	movq	208(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, -24(%rsp)                 # 8-byte Spill
	movq	200(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, -32(%rsp)                 # 8-byte Spill
	movq	192(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, -40(%rsp)                 # 8-byte Spill
	movq	408(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, 416(%rsp)                 # 8-byte Spill
	movq	400(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, 40(%rsp)                  # 8-byte Spill
	movq	392(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, 32(%rsp)                  # 8-byte Spill
	movq	128(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, 24(%rsp)                  # 8-byte Spill
	movq	120(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, 16(%rsp)                  # 8-byte Spill
	movq	184(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, 8(%rsp)                   # 8-byte Spill
	movq	112(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, (%rsp)                    # 8-byte Spill
	movq	104(%rsp), %rdi                 # 8-byte Reload
	addq	%rsi, %rdi
	movq	%rdi, 48(%rsp)                  # 8-byte Spill
	movq	176(%rsp), %r13                 # 8-byte Reload
	addq	%rsi, %r13
	movq	384(%rsp), %r14                 # 8-byte Reload
	addq	%rsi, %r14
	movq	304(%rsp), %r15                 # 8-byte Reload
	addq	%rsi, %r15
	addq	96(%rsp), %rsi                  # 8-byte Folded Reload
	movq	472(%rsp), %rbp                 # 8-byte Reload
	jmp	.LBB1_9
	.p2align	4, 0x90
.LBB1_14:                               # %"for f7.s1.r77$x"
                                        #   in Loop: Header=BB1_9 Depth=2
	vmovups	608(%rsp), %ymm3                # 32-byte Reload
	movq	-8(%rsp), %rdi                  # 8-byte Reload
	movq	-16(%rsp), %rbx                 # 8-byte Reload
	vmovss	-96(%rsp), %xmm2                # 4-byte Reload
                                        # xmm2 = mem[0],zero,zero,zero
	vfmadd213ss	%xmm0, %xmm10, %xmm4    # xmm4 = (xmm10 * xmm4) + xmm0
	vfmadd231ss	-4(%rdx), %xmm1, %xmm4  # xmm4 = (xmm1 * mem) + xmm4
	vfmadd231ss	(%rdx), %xmm9, %xmm4    # xmm4 = (xmm9 * mem) + xmm4
	vfmadd231ss	.LCPI1_9(%rip), %xmm4, %xmm2 # xmm2 = (xmm4 * mem) + xmm2
	incq	%rax
	subq	$-128, %rdx
	addq	536(%rsp), %rbp                 # 8-byte Folded Reload
	cmpq	%rax, 296(%rsp)                 # 8-byte Folded Reload
	je	.LBB1_15
.LBB1_9:                                # %"for f7.s1.r77$x"
                                        #   Parent Loop BB1_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vmovss	%xmm2, -96(%rsp)                # 4-byte Spill
	vmovss	(%rbp,%rsi,4), %xmm14           # xmm14 = mem[0],zero,zero,zero
	vmovss	(%rbp,%r15,4), %xmm5            # xmm5 = mem[0],zero,zero,zero
	vmovss	(%rbp,%r13,4), %xmm6            # xmm6 = mem[0],zero,zero,zero
	movq	48(%rsp), %rdi                  # 8-byte Reload
	vmovss	(%rbp,%rdi,4), %xmm2            # xmm2 = mem[0],zero,zero,zero
	movq	(%rsp), %rdi                    # 8-byte Reload
	vmulss	(%rbp,%rdi,4), %xmm2, %xmm15
	vmovss	.LCPI1_5(%rip), %xmm9           # xmm9 = mem[0],zero,zero,zero
	vmaxss	%xmm9, %xmm15, %xmm11
	vdivss	%xmm3, %xmm11, %xmm0
	movq	8(%rsp), %rdi                   # 8-byte Reload
	vmovss	(%rbp,%rdi,4), %xmm8            # xmm8 = mem[0],zero,zero,zero
	leaq	-40(%rdx), %rbx
	leaq	-36(%rdx), %r9
	leaq	-28(%rdx), %rdi
	leaq	-24(%rdx), %r8
	testq	%rax, %rax
	cmoveq	%rbx, %r9
	cmoveq	%rdi, %r8
	leaq	-44(%rdx), %r11
	leaq	-32(%rdx), %r10
	vmulss	(%rbp,%r14,4), %xmm5, %xmm4
	vmulss	-104(%rdx), %xmm6, %xmm7
	vfmadd231ss	-108(%rdx), %xmm4, %xmm7 # xmm7 = (xmm4 * mem) + xmm7
	vmulss	-96(%rdx), %xmm6, %xmm3
	vfmadd231ss	-100(%rdx), %xmm4, %xmm3 # xmm3 = (xmm4 * mem) + xmm3
	vucomiss	%xmm2, %xmm9
	vxorps	%xmm4, %xmm4, %xmm4
	movq	416(%rsp), %rdi                 # 8-byte Reload
	vcmpeqss	(%rbp,%rdi,4), %xmm4, %xmm1
	movq	16(%rsp), %rdi                  # 8-byte Reload
	vmovss	(%rbp,%rdi,4), %xmm13           # xmm13 = mem[0],zero,zero,zero
	vblendvps	%xmm1, %xmm7, %xmm3, %xmm1
	movq	24(%rsp), %rdi                  # 8-byte Reload
	vmovss	(%rbp,%rdi,4), %xmm12           # xmm12 = mem[0],zero,zero,zero
	vroundss	$10, %xmm0, %xmm0, %xmm3
	vmulss	%xmm1, %xmm3, %xmm1
	movq	32(%rsp), %rdi                  # 8-byte Reload
	vmovss	(%rbp,%rdi,4), %xmm10           # xmm10 = mem[0],zero,zero,zero
	vmaxss	%xmm9, %xmm8, %xmm3
	vmaxss	%xmm9, %xmm0, %xmm0
	vdivss	%xmm0, %xmm1, %xmm8
	vmovss	.LCPI1_6(%rip), %xmm0           # xmm0 = mem[0],zero,zero,zero
	vdivss	%xmm3, %xmm0, %xmm1
	vminss	%xmm2, %xmm1, %xmm1
	movq	-40(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%rbp,%rdi,4), %xmm1, %xmm1
	vmulss	%xmm1, %xmm15, %xmm7
	movq	40(%rsp), %rdi                  # 8-byte Reload
	vmovss	(%rbp,%rdi,4), %xmm9            # xmm9 = mem[0],zero,zero,zero
	jb	.LBB1_10
# %bb.11:                               # %"for f7.s1.r77$x"
                                        #   in Loop: Header=BB1_9 Depth=2
	vxorps	%xmm15, %xmm15, %xmm15
	jmp	.LBB1_12
	.p2align	4, 0x90
.LBB1_10:                               #   in Loop: Header=BB1_9 Depth=2
	vaddss	%xmm6, %xmm5, %xmm1
	vmulss	-20(%rdx), %xmm1, %xmm1
	vdivss	%xmm3, %xmm1, %xmm15
.LBB1_12:                               # %"for f7.s1.r77$x"
                                        #   in Loop: Header=BB1_9 Depth=2
	cmovbq	%r11, %r9
	movq	-32(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%rbp,%rdi,4), %xmm2, %xmm1
	vmulss	(%r9), %xmm1, %xmm1
	vdivss	%xmm3, %xmm1, %xmm1
	cmovbq	%r10, %r8
	movq	-24(%rsp), %rdi                 # 8-byte Reload
	vmovss	(%rbp,%rdi,4), %xmm3            # xmm3 = mem[0],zero,zero,zero
	vfmadd132ss	(%r8), %xmm1, %xmm3     # xmm3 = (xmm3 * mem) + xmm1
	vfmadd213ss	%xmm15, %xmm14, %xmm3   # xmm3 = (xmm14 * xmm3) + xmm15
	vfmadd132ss	-16(%rdx), %xmm3, %xmm7 # xmm7 = (xmm7 * mem) + xmm3
	movq	320(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%rbp,%rdi,4), %xmm14, %xmm1
	movq	-88(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%rbp,%rdi,4), %xmm14, %xmm3
	vmulss	-84(%rdx), %xmm3, %xmm3
	vfmadd231ss	-88(%rdx), %xmm1, %xmm3 # xmm3 = (xmm1 * mem) + xmm3
	movq	-48(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%rbp,%rdi,4), %xmm5, %xmm1
	vfmadd231ss	-80(%rdx), %xmm1, %xmm3 # xmm3 = (xmm1 * mem) + xmm3
	movq	-56(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%rbp,%rdi,4), %xmm6, %xmm1
	vfmadd231ss	-76(%rdx), %xmm1, %xmm3 # xmm3 = (xmm1 * mem) + xmm3
	vmulss	%xmm5, %xmm13, %xmm15
	vmulss	%xmm6, %xmm13, %xmm1
	vmulss	%xmm6, %xmm12, %xmm6
	vmulss	%xmm5, %xmm12, %xmm0
	movq	-64(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%rbp,%rdi,4), %xmm5, %xmm5
	vfmadd231ss	-72(%rdx), %xmm5, %xmm3 # xmm3 = (xmm5 * mem) + xmm3
	vfmadd231ss	-68(%rdx), %xmm6, %xmm3 # xmm3 = (xmm6 * mem) + xmm3
	vfmadd231ss	-64(%rdx), %xmm0, %xmm3 # xmm3 = (xmm0 * mem) + xmm3
	vfmadd231ss	-60(%rdx), %xmm1, %xmm3 # xmm3 = (xmm1 * mem) + xmm3
	vfmadd231ss	-56(%rdx), %xmm15, %xmm3 # xmm3 = (xmm15 * mem) + xmm3
	movq	-72(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%rbp,%rdi,4), %xmm11, %xmm0
	vfmadd231ss	-52(%rdx), %xmm0, %xmm3 # xmm3 = (xmm0 * mem) + xmm3
	movq	-80(%rsp), %rdi                 # 8-byte Reload
	vmulss	(%rbp,%rdi,4), %xmm11, %xmm0
	vfmadd231ss	.LCPI1_7(%rip), %xmm7, %xmm8 # xmm8 = (xmm7 * mem) + xmm8
	vfmadd231ss	-48(%rdx), %xmm0, %xmm3 # xmm3 = (xmm0 * mem) + xmm3
	vaddss	%xmm3, %xmm8, %xmm0
	vfmadd231ss	-12(%rdx), %xmm14, %xmm0 # xmm0 = (xmm14 * mem) + xmm0
	vaddss	.LCPI1_8(%rip), %xmm2, %xmm1
	vmulss	%xmm1, %xmm10, %xmm1
	jae	.LBB1_14
# %bb.13:                               #   in Loop: Header=BB1_9 Depth=2
	vmovss	-8(%rdx), %xmm4                 # xmm4 = mem[0],zero,zero,zero
	jmp	.LBB1_14
.LBB1_16:                               # %destructor_block
	xorl	%eax, %eax
	addq	$1768, %rsp                     # imm = 0x6E8
	popq	%rbx
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	popq	%rbp
	vzeroupper
	retq
.Lfunc_end1:
	.size	cost_model.par_for.prediction_output.s0.n.v13, .Lfunc_end1-cost_model.par_for.prediction_output.s0.n.v13
                                        # -- End function
	.section	.text.cost_model_argv,"ax",@progbits
	.globl	cost_model_argv                 # -- Begin function cost_model_argv
	.p2align	4, 0x90
	.type	cost_model_argv,@function
cost_model_argv:                        # @cost_model_argv
	.cfi_startproc
# %bb.0:                                # %entry
	subq	$104, %rsp
	.cfi_def_cfa_offset 112
	movq	(%rdi), %rax
	movq	8(%rdi), %rcx
	movl	(%rax), %r10d
	movl	(%rcx), %esi
	movq	16(%rdi), %rcx
	movl	(%rcx), %edx
	movq	24(%rdi), %rcx
	movq	32(%rdi), %r8
	movq	40(%rdi), %r9
	vmovups	48(%rdi), %ymm0
	movq	80(%rdi), %rax
	vmovups	120(%rdi), %ymm1
	movq	152(%rdi), %rdi
	vmovups	%ymm1, 64(%rsp)
	movq	%rdi, 96(%rsp)
	movq	%rax, 32(%rsp)
	vmovups	%ymm0, (%rsp)
	movl	%r10d, %edi
	vzeroupper
	callq	cost_model@PLT
	addq	$104, %rsp
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end2:
	.size	cost_model_argv, .Lfunc_end2-cost_model_argv
	.cfi_endproc
                                        # -- End function
	.section	.text.cost_model_metadata,"ax",@progbits
	.globl	cost_model_metadata             # -- Begin function cost_model_metadata
	.p2align	4, 0x90
	.type	cost_model_metadata,@function
cost_model_metadata:                    # @cost_model_metadata
# %bb.0:                                # %entry
	leaq	.Lcost_model_metadata_storage(%rip), %rax
	retq
.Lfunc_end3:
	.size	cost_model_metadata, .Lfunc_end3-cost_model_metadata
                                        # -- End function
	.type	.Lstr,@object                   # @str
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr:
	.asciz	"store_cost_output"
	.size	.Lstr, 18

	.type	.Lstr.3,@object                 # @str.3
	.p2align	5
.Lstr.3:
	.asciz	"schedule_features"
	.size	.Lstr.3, 18

	.type	.Lstr.4,@object                 # @str.4
	.p2align	5
.Lstr.4:
	.asciz	"prediction_output"
	.size	.Lstr.4, 18

	.type	.Lstr.5,@object                 # @str.5
	.p2align	5
.Lstr.5:
	.asciz	"pipeline_features"
	.size	.Lstr.5, 18

	.type	.Lstr.6,@object                 # @str.6
	.p2align	5
.Lstr.6:
	.asciz	"loss_output"
	.size	.Lstr.6, 12

	.type	.Lstr.7,@object                 # @str.7
	.p2align	5
.Lstr.7:
	.asciz	"load_cost_output"
	.size	.Lstr.7, 17

	.type	.Lstr.8,@object                 # @str.8
	.p2align	5
.Lstr.8:
	.asciz	"head2_filter"
	.size	.Lstr.8, 13

	.type	.Lstr.9,@object                 # @str.9
	.p2align	5
.Lstr.9:
	.asciz	"head2_bias"
	.size	.Lstr.9, 11

	.type	.Lstr.10,@object                # @str.10
	.p2align	5
.Lstr.10:
	.asciz	"head1_filter"
	.size	.Lstr.10, 13

	.type	.Lstr.11,@object                # @str.11
	.p2align	5
.Lstr.11:
	.asciz	"head1_bias"
	.size	.Lstr.11, 11

	.type	.Lstr.12,@object                # @str.12
	.p2align	5
.Lstr.12:
	.asciz	"filter1"
	.size	.Lstr.12, 8

	.type	.Lstr.13,@object                # @str.13
	.p2align	5
.Lstr.13:
	.asciz	"compute_cost_output"
	.size	.Lstr.13, 20

	.type	.Lstr.14,@object                # @str.14
	.p2align	5
.Lstr.14:
	.asciz	"bias1"
	.size	.Lstr.14, 6

	.type	.Lstr.15,@object                # @str.15
	.p2align	5
.Lstr.15:
	.asciz	"Input buffer filter1"
	.size	.Lstr.15, 21

	.type	.Lstr.16,@object                # @str.16
	.p2align	5
.Lstr.16:
	.asciz	"Input buffer bias1"
	.size	.Lstr.16, 19

	.type	.Lstr.17,@object                # @str.17
	.p2align	5
.Lstr.17:
	.asciz	"Output buffer compute_cost_output"
	.size	.Lstr.17, 34

	.type	.Lstr.18,@object                # @str.18
	.p2align	5
.Lstr.18:
	.asciz	"Input buffer head1_bias"
	.size	.Lstr.18, 24

	.type	.Lstr.19,@object                # @str.19
	.p2align	5
.Lstr.19:
	.asciz	"Input buffer head1_filter"
	.size	.Lstr.19, 26

	.type	.Lstr.20,@object                # @str.20
	.p2align	5
.Lstr.20:
	.asciz	"Input buffer head2_bias"
	.size	.Lstr.20, 24

	.type	.Lstr.21,@object                # @str.21
	.p2align	5
.Lstr.21:
	.asciz	"Input buffer head2_filter"
	.size	.Lstr.21, 26

	.type	.Lstr.22,@object                # @str.22
	.p2align	5
.Lstr.22:
	.asciz	"Output buffer load_cost_output"
	.size	.Lstr.22, 31

	.type	.Lstr.23,@object                # @str.23
	.p2align	5
.Lstr.23:
	.asciz	"Output buffer loss_output"
	.size	.Lstr.23, 26

	.type	.Lstr.24,@object                # @str.24
	.p2align	5
.Lstr.24:
	.asciz	"Input buffer pipeline_features"
	.size	.Lstr.24, 31

	.type	.Lstr.25,@object                # @str.25
	.p2align	5
.Lstr.25:
	.asciz	"Output buffer prediction_output"
	.size	.Lstr.25, 32

	.type	.Lstr.26,@object                # @str.26
	.p2align	5
.Lstr.26:
	.asciz	"Input buffer schedule_features"
	.size	.Lstr.26, 31

	.type	.Lstr.27,@object                # @str.27
	.p2align	5
.Lstr.27:
	.asciz	"Output buffer store_cost_output"
	.size	.Lstr.27, 32

	.type	.Lstr.28,@object                # @str.28
	.p2align	5
.Lstr.28:
	.asciz	"bias1.stride.0"
	.size	.Lstr.28, 15

	.type	.Lstr.29,@object                # @str.29
	.p2align	5
.Lstr.29:
	.asciz	"1"
	.size	.Lstr.29, 2

	.type	.Lstr.30,@object                # @str.30
	.p2align	5
.Lstr.30:
	.asciz	"bias1.min.0"
	.size	.Lstr.30, 12

	.type	.Lstr.31,@object                # @str.31
	.p2align	5
.Lstr.31:
	.asciz	"0"
	.size	.Lstr.31, 2

	.type	.Lstr.32,@object                # @str.32
	.p2align	5
.Lstr.32:
	.asciz	"bias1.extent.0"
	.size	.Lstr.32, 15

	.type	.Lstr.33,@object                # @str.33
	.p2align	5
.Lstr.33:
	.asciz	"32"
	.size	.Lstr.33, 3

	.type	.Lstr.34,@object                # @str.34
	.p2align	5
.Lstr.34:
	.asciz	"compute_cost_output.stride.0"
	.size	.Lstr.34, 29

	.type	.Lstr.35,@object                # @str.35
	.p2align	5
.Lstr.35:
	.asciz	"filter1.stride.0"
	.size	.Lstr.35, 17

	.type	.Lstr.36,@object                # @str.36
	.p2align	5
.Lstr.36:
	.asciz	"filter1.min.0"
	.size	.Lstr.36, 14

	.type	.Lstr.37,@object                # @str.37
	.p2align	5
.Lstr.37:
	.asciz	"filter1.extent.0"
	.size	.Lstr.37, 17

	.type	.Lstr.38,@object                # @str.38
	.p2align	5
.Lstr.38:
	.asciz	"filter1.min.1"
	.size	.Lstr.38, 14

	.type	.Lstr.39,@object                # @str.39
	.p2align	5
.Lstr.39:
	.asciz	"filter1.extent.1"
	.size	.Lstr.39, 17

	.type	.Lstr.40,@object                # @str.40
	.p2align	5
.Lstr.40:
	.asciz	"head1_bias.stride.0"
	.size	.Lstr.40, 20

	.type	.Lstr.41,@object                # @str.41
	.p2align	5
.Lstr.41:
	.asciz	"head1_bias.min.0"
	.size	.Lstr.41, 17

	.type	.Lstr.42,@object                # @str.42
	.p2align	5
.Lstr.42:
	.asciz	"head1_bias.extent.0"
	.size	.Lstr.42, 20

	.type	.Lstr.43,@object                # @str.43
	.p2align	5
.Lstr.43:
	.asciz	"8"
	.size	.Lstr.43, 2

	.type	.Lstr.44,@object                # @str.44
	.p2align	5
.Lstr.44:
	.asciz	"head1_filter.stride.0"
	.size	.Lstr.44, 22

	.type	.Lstr.45,@object                # @str.45
	.p2align	5
.Lstr.45:
	.asciz	"head1_filter.min.0"
	.size	.Lstr.45, 19

	.type	.Lstr.46,@object                # @str.46
	.p2align	5
.Lstr.46:
	.asciz	"head1_filter.extent.0"
	.size	.Lstr.46, 22

	.type	.Lstr.47,@object                # @str.47
	.p2align	5
.Lstr.47:
	.asciz	"head1_filter.min.1"
	.size	.Lstr.47, 19

	.type	.Lstr.48,@object                # @str.48
	.p2align	5
.Lstr.48:
	.asciz	"head1_filter.extent.1"
	.size	.Lstr.48, 22

	.type	.Lstr.49,@object                # @str.49
	.p2align	5
.Lstr.49:
	.asciz	"40"
	.size	.Lstr.49, 3

	.type	.Lstr.50,@object                # @str.50
	.p2align	5
.Lstr.50:
	.asciz	"head1_filter.min.2"
	.size	.Lstr.50, 19

	.type	.Lstr.51,@object                # @str.51
	.p2align	5
.Lstr.51:
	.asciz	"head1_filter.extent.2"
	.size	.Lstr.51, 22

	.type	.Lstr.52,@object                # @str.52
	.p2align	5
.Lstr.52:
	.asciz	"7"
	.size	.Lstr.52, 2

	.type	.Lstr.53,@object                # @str.53
	.p2align	5
.Lstr.53:
	.asciz	"head2_bias.stride.0"
	.size	.Lstr.53, 20

	.type	.Lstr.54,@object                # @str.54
	.p2align	5
.Lstr.54:
	.asciz	"head2_bias.min.0"
	.size	.Lstr.54, 17

	.type	.Lstr.55,@object                # @str.55
	.p2align	5
.Lstr.55:
	.asciz	"head2_bias.extent.0"
	.size	.Lstr.55, 20

	.type	.Lstr.56,@object                # @str.56
	.p2align	5
.Lstr.56:
	.asciz	"24"
	.size	.Lstr.56, 3

	.type	.Lstr.57,@object                # @str.57
	.p2align	5
.Lstr.57:
	.asciz	"head2_filter.stride.0"
	.size	.Lstr.57, 22

	.type	.Lstr.58,@object                # @str.58
	.p2align	5
.Lstr.58:
	.asciz	"head2_filter.min.0"
	.size	.Lstr.58, 19

	.type	.Lstr.59,@object                # @str.59
	.p2align	5
.Lstr.59:
	.asciz	"head2_filter.extent.0"
	.size	.Lstr.59, 22

	.type	.Lstr.60,@object                # @str.60
	.p2align	5
.Lstr.60:
	.asciz	"head2_filter.min.1"
	.size	.Lstr.60, 19

	.type	.Lstr.61,@object                # @str.61
	.p2align	5
.Lstr.61:
	.asciz	"head2_filter.extent.1"
	.size	.Lstr.61, 22

	.type	.Lstr.62,@object                # @str.62
	.p2align	5
.Lstr.62:
	.asciz	"39"
	.size	.Lstr.62, 3

	.type	.Lstr.63,@object                # @str.63
	.p2align	5
.Lstr.63:
	.asciz	"load_cost_output.stride.0"
	.size	.Lstr.63, 26

	.type	.Lstr.64,@object                # @str.64
	.p2align	5
.Lstr.64:
	.asciz	"pipeline_features.stride.0"
	.size	.Lstr.64, 27

	.type	.Lstr.65,@object                # @str.65
	.p2align	5
.Lstr.65:
	.asciz	"prediction_output.stride.0"
	.size	.Lstr.65, 27

	.type	.Lstr.66,@object                # @str.66
	.p2align	5
.Lstr.66:
	.asciz	"schedule_features.stride.0"
	.size	.Lstr.66, 27

	.type	.Lstr.67,@object                # @str.67
	.p2align	5
.Lstr.67:
	.asciz	"store_cost_output.stride.0"
	.size	.Lstr.67, 27

	.type	.Lstr.68,@object                # @str.68
	.p2align	5
.Lstr.68:
	.asciz	"conv1_stage1"
	.size	.Lstr.68, 13

	.type	.Lstr.69,@object                # @str.69
	.p2align	5
.Lstr.69:
	.asciz	"head2_relu"
	.size	.Lstr.69, 11

	.type	.Lstr.70,@object                # @str.70
	.p2align	5
.Lstr.70:
	.asciz	"normalized_schedule_features"
	.size	.Lstr.70, 29

	.type	.Lstr.71,@object                # @str.71
	.p2align	5
.Lstr.71:
	.asciz	"relu1"
	.size	.Lstr.71, 6

	.type	.Lstr.72,@object                # @str.72
	.p2align	5
.Lstr.72:
	.asciz	"n"
	.size	.Lstr.72, 2

	.type	.Lstr.73,@object                # @str.73
	.p2align	5
.Lstr.73:
	.asciz	"num_stages"
	.size	.Lstr.73, 11

	.type	.L__unnamed_1,@object           # @0
	.p2align	3
.L__unnamed_1:
	.long	1                               # 0x1
	.long	0                               # 0x0
	.size	.L__unnamed_1, 8

	.type	.L__unnamed_2,@object           # @1
	.p2align	3
.L__unnamed_2:
	.long	13                              # 0xd
	.long	0                               # 0x0
	.size	.L__unnamed_2, 8

	.type	.Lstr.74,@object                # @str.74
	.p2align	5
.Lstr.74:
	.asciz	"batch_size"
	.size	.Lstr.74, 11

	.type	.L__unnamed_3,@object           # @2
	.p2align	3
.L__unnamed_3:
	.long	1                               # 0x1
	.long	0                               # 0x0
	.size	.L__unnamed_3, 8

	.type	.L__unnamed_4,@object           # @3
	.p2align	3
.L__unnamed_4:
	.long	80                              # 0x50
	.long	0                               # 0x0
	.size	.L__unnamed_4, 8

	.type	.Lstr.75,@object                # @str.75
	.p2align	5
.Lstr.75:
	.asciz	"num_cores"
	.size	.Lstr.75, 10

	.type	.L__unnamed_5,@object           # @4
	.p2align	3
.L__unnamed_5:
	.long	1                               # 0x1
	.long	0                               # 0x0
	.size	.L__unnamed_5, 8

	.type	.L__unnamed_6,@object           # @5
	.p2align	3
.L__unnamed_6:
	.long	32                              # 0x20
	.long	0                               # 0x0
	.size	.L__unnamed_6, 8

	.type	.L__unnamed_7,@object           # @6
	.p2align	3
.L__unnamed_7:
	.quad	0                               # 0x0
	.size	.L__unnamed_7, 8

	.type	.L__unnamed_8,@object           # @7
	.p2align	3
.L__unnamed_8:
	.quad	40                              # 0x28
	.size	.L__unnamed_8, 8

	.type	.L__unnamed_9,@object           # @8
	.p2align	3
.L__unnamed_9:
	.quad	0                               # 0x0
	.size	.L__unnamed_9, 8

	.type	.L__unnamed_10,@object          # @9
	.p2align	3
.L__unnamed_10:
	.quad	7                               # 0x7
	.size	.L__unnamed_10, 8

	.type	.L__unnamed_11,@object          # @10
	.p2align	3
.L__unnamed_11:
	.quad	0                               # 0x0
	.size	.L__unnamed_11, 8

	.type	.L__unnamed_12,@object          # @11
	.p2align	3
.L__unnamed_12:
	.quad	13                              # 0xd
	.size	.L__unnamed_12, 8

	.type	.L__unnamed_13,@object          # @12
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_13:
	.quad	.L__unnamed_7
	.quad	.L__unnamed_8
	.quad	.L__unnamed_9
	.quad	.L__unnamed_10
	.quad	.L__unnamed_11
	.quad	.L__unnamed_12
	.size	.L__unnamed_13, 48

	.type	.L__unnamed_14,@object          # @13
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_14:
	.quad	0                               # 0x0
	.size	.L__unnamed_14, 8

	.type	.L__unnamed_15,@object          # @14
	.p2align	3
.L__unnamed_15:
	.quad	80                              # 0x50
	.size	.L__unnamed_15, 8

	.type	.L__unnamed_16,@object          # @15
	.p2align	3
.L__unnamed_16:
	.quad	0                               # 0x0
	.size	.L__unnamed_16, 8

	.type	.L__unnamed_17,@object          # @16
	.p2align	3
.L__unnamed_17:
	.quad	39                              # 0x27
	.size	.L__unnamed_17, 8

	.type	.L__unnamed_18,@object          # @17
	.p2align	3
.L__unnamed_18:
	.quad	0                               # 0x0
	.size	.L__unnamed_18, 8

	.type	.L__unnamed_19,@object          # @18
	.p2align	3
.L__unnamed_19:
	.quad	13                              # 0xd
	.size	.L__unnamed_19, 8

	.type	.L__unnamed_20,@object          # @19
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_20:
	.quad	.L__unnamed_14
	.quad	.L__unnamed_15
	.quad	.L__unnamed_16
	.quad	.L__unnamed_17
	.quad	.L__unnamed_18
	.quad	.L__unnamed_19
	.size	.L__unnamed_20, 48

	.type	.L__unnamed_21,@object          # @20
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_21:
	.quad	0                               # 0x0
	.size	.L__unnamed_21, 8

	.type	.L__unnamed_22,@object          # @21
	.p2align	3
.L__unnamed_22:
	.quad	8                               # 0x8
	.size	.L__unnamed_22, 8

	.type	.L__unnamed_23,@object          # @22
	.p2align	3
.L__unnamed_23:
	.quad	0                               # 0x0
	.size	.L__unnamed_23, 8

	.type	.L__unnamed_24,@object          # @23
	.p2align	3
.L__unnamed_24:
	.quad	40                              # 0x28
	.size	.L__unnamed_24, 8

	.type	.L__unnamed_25,@object          # @24
	.p2align	3
.L__unnamed_25:
	.quad	0                               # 0x0
	.size	.L__unnamed_25, 8

	.type	.L__unnamed_26,@object          # @25
	.p2align	3
.L__unnamed_26:
	.quad	7                               # 0x7
	.size	.L__unnamed_26, 8

	.type	.L__unnamed_27,@object          # @26
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_27:
	.quad	.L__unnamed_21
	.quad	.L__unnamed_22
	.quad	.L__unnamed_23
	.quad	.L__unnamed_24
	.quad	.L__unnamed_25
	.quad	.L__unnamed_26
	.size	.L__unnamed_27, 48

	.type	.L__unnamed_28,@object          # @27
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_28:
	.quad	0                               # 0x0
	.size	.L__unnamed_28, 8

	.type	.L__unnamed_29,@object          # @28
	.p2align	3
.L__unnamed_29:
	.quad	8                               # 0x8
	.size	.L__unnamed_29, 8

	.type	.L__unnamed_30,@object          # @29
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__unnamed_30:
	.quad	.L__unnamed_28
	.quad	.L__unnamed_29
	.size	.L__unnamed_30, 16

	.type	.L__unnamed_31,@object          # @30
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_31:
	.quad	0                               # 0x0
	.size	.L__unnamed_31, 8

	.type	.L__unnamed_32,@object          # @31
	.p2align	3
.L__unnamed_32:
	.quad	24                              # 0x18
	.size	.L__unnamed_32, 8

	.type	.L__unnamed_33,@object          # @32
	.p2align	3
.L__unnamed_33:
	.quad	0                               # 0x0
	.size	.L__unnamed_33, 8

	.type	.L__unnamed_34,@object          # @33
	.p2align	3
.L__unnamed_34:
	.quad	39                              # 0x27
	.size	.L__unnamed_34, 8

	.type	.L__unnamed_35,@object          # @34
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_35:
	.quad	.L__unnamed_31
	.quad	.L__unnamed_32
	.quad	.L__unnamed_33
	.quad	.L__unnamed_34
	.size	.L__unnamed_35, 32

	.type	.L__unnamed_36,@object          # @35
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_36:
	.quad	0                               # 0x0
	.size	.L__unnamed_36, 8

	.type	.L__unnamed_37,@object          # @36
	.p2align	3
.L__unnamed_37:
	.quad	24                              # 0x18
	.size	.L__unnamed_37, 8

	.type	.L__unnamed_38,@object          # @37
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__unnamed_38:
	.quad	.L__unnamed_36
	.quad	.L__unnamed_37
	.size	.L__unnamed_38, 16

	.type	.L__unnamed_39,@object          # @38
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_39:
	.quad	0                               # 0x0
	.size	.L__unnamed_39, 8

	.type	.L__unnamed_40,@object          # @39
	.p2align	3
.L__unnamed_40:
	.quad	32                              # 0x20
	.size	.L__unnamed_40, 8

	.type	.L__unnamed_41,@object          # @40
	.p2align	3
.L__unnamed_41:
	.quad	0                               # 0x0
	.size	.L__unnamed_41, 8

	.type	.L__unnamed_42,@object          # @41
	.p2align	3
.L__unnamed_42:
	.quad	32                              # 0x20
	.size	.L__unnamed_42, 8

	.type	.L__unnamed_43,@object          # @42
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_43:
	.quad	.L__unnamed_39
	.quad	.L__unnamed_40
	.quad	.L__unnamed_41
	.quad	.L__unnamed_42
	.size	.L__unnamed_43, 32

	.type	.L__unnamed_44,@object          # @43
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_44:
	.quad	0                               # 0x0
	.size	.L__unnamed_44, 8

	.type	.L__unnamed_45,@object          # @44
	.p2align	3
.L__unnamed_45:
	.quad	32                              # 0x20
	.size	.L__unnamed_45, 8

	.type	.L__unnamed_46,@object          # @45
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__unnamed_46:
	.quad	.L__unnamed_44
	.quad	.L__unnamed_45
	.size	.L__unnamed_46, 16

	.type	.Lstr.76,@object                # @str.76
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr.76:
	.asciz	"learning_rate"
	.size	.Lstr.76, 14

	.type	.L__unnamed_47,@object          # @46
	.p2align	3
.L__unnamed_47:
	.long	0x3f800000                      # float 1
	.long	0x00000000                      # float 0
	.size	.L__unnamed_47, 8

	.type	.L__unnamed_48,@object          # @47
	.p2align	3
.L__unnamed_48:
	.long	0x3a83126f                      # float 0.00100000005
	.long	0x00000000                      # float 0
	.size	.L__unnamed_48, 8

	.type	.Lstr.77,@object                # @str.77
	.p2align	5
.Lstr.77:
	.asciz	"timestep"
	.size	.Lstr.77, 9

	.type	.L__unnamed_49,@object          # @48
	.p2align	3
.L__unnamed_49:
	.zero	8
	.size	.L__unnamed_49, 8

	.type	.L__unnamed_50,@object          # @49
	.p2align	3
.L__unnamed_50:
	.long	37                              # 0x25
	.long	0                               # 0x0
	.size	.L__unnamed_50, 8

	.type	.Lstr.78,@object                # @str.78
	.p2align	5
.Lstr.78:
	.asciz	"reference"
	.size	.Lstr.78, 10

	.type	.L__unnamed_51,@object          # @50
	.p2align	3
.L__unnamed_51:
	.zero	8
	.size	.L__unnamed_51, 8

	.type	.L__unnamed_52,@object          # @51
	.p2align	3
.L__unnamed_52:
	.zero	8
	.size	.L__unnamed_52, 8

	.type	.L__unnamed_53,@object          # @52
	.p2align	3
.L__unnamed_53:
	.quad	0                               # 0x0
	.size	.L__unnamed_53, 8

	.type	.L__unnamed_54,@object          # @53
	.p2align	3
.L__unnamed_54:
	.quad	80                              # 0x50
	.size	.L__unnamed_54, 8

	.type	.L__unnamed_55,@object          # @54
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__unnamed_55:
	.quad	.L__unnamed_53
	.quad	.L__unnamed_54
	.size	.L__unnamed_55, 16

	.type	.Lstr.79,@object                # @str.79
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr.79:
	.asciz	"true_runtime"
	.size	.Lstr.79, 13

	.type	.L__unnamed_56,@object          # @55
	.p2align	3
.L__unnamed_56:
	.quad	0                               # 0x0
	.size	.L__unnamed_56, 8

	.type	.L__unnamed_57,@object          # @56
	.p2align	3
.L__unnamed_57:
	.quad	80                              # 0x50
	.size	.L__unnamed_57, 8

	.type	.L__unnamed_58,@object          # @57
	.section	.data.rel.ro,"aw",@progbits
	.p2align	3
.L__unnamed_58:
	.quad	.L__unnamed_56
	.quad	.L__unnamed_57
	.size	.L__unnamed_58, 16

	.type	.L__unnamed_59,@object          # @58
	.section	.rodata,"a",@progbits
	.p2align	3
.L__unnamed_59:
	.zero	16
	.size	.L__unnamed_59, 16

	.type	.L__unnamed_60,@object          # @59
	.p2align	3
.L__unnamed_60:
	.zero	16
	.size	.L__unnamed_60, 16

	.type	.L__unnamed_61,@object          # @60
	.p2align	3
.L__unnamed_61:
	.zero	16
	.size	.L__unnamed_61, 16

	.type	.L__unnamed_62,@object          # @61
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.L__unnamed_62:
	.quad	.Lstr.73
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	0                               # 0x0
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_1
	.quad	0
	.quad	0
	.quad	.L__unnamed_2
	.quad	0
	.quad	.Lstr.74
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	0                               # 0x0
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_3
	.quad	0
	.quad	0
	.quad	.L__unnamed_4
	.quad	0
	.quad	.Lstr.75
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	0                               # 0x0
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_5
	.quad	0
	.quad	0
	.quad	.L__unnamed_6
	.quad	0
	.quad	.Lstr.5
	.long	1                               # 0x1
	.long	3                               # 0x3
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_13
	.quad	.Lstr.3
	.long	1                               # 0x1
	.long	3                               # 0x3
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_20
	.quad	.Lstr.10
	.long	1                               # 0x1
	.long	3                               # 0x3
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_27
	.quad	.Lstr.11
	.long	1                               # 0x1
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_30
	.quad	.Lstr.8
	.long	1                               # 0x1
	.long	2                               # 0x2
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_35
	.quad	.Lstr.9
	.long	1                               # 0x1
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_38
	.quad	.Lstr.12
	.long	1                               # 0x1
	.long	2                               # 0x2
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_43
	.quad	.Lstr.14
	.long	1                               # 0x1
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_46
	.quad	.Lstr.76
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_47
	.quad	0
	.quad	0
	.quad	.L__unnamed_48
	.quad	0
	.quad	.Lstr.77
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	0                               # 0x0
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_49
	.quad	0
	.quad	0
	.quad	.L__unnamed_50
	.quad	0
	.quad	.Lstr.78
	.long	0                               # 0x0
	.long	0                               # 0x0
	.byte	0                               # 0x0
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	.L__unnamed_51
	.quad	0
	.quad	0
	.quad	.L__unnamed_52
	.quad	0
	.quad	.Lstr.79
	.long	1                               # 0x1
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_55
	.quad	.Lstr.4
	.long	2                               # 0x2
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_58
	.quad	.Lstr.6
	.long	2                               # 0x2
	.long	0                               # 0x0
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.Lstr.7
	.long	2                               # 0x2
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_59
	.quad	.Lstr
	.long	2                               # 0x2
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_60
	.quad	.Lstr.13
	.long	2                               # 0x2
	.long	1                               # 0x1
	.byte	2                               # 0x2
	.byte	32                              # 0x20
	.short	1                               # 0x1
	.zero	4
	.quad	0
	.quad	0
	.quad	0
	.quad	0
	.quad	.L__unnamed_61
	.size	.L__unnamed_62, 1280

	.type	.Lstr.80,@object                # @str.80
	.section	.rodata,"a",@progbits
	.p2align	5
.Lstr.80:
	.asciz	"x86-64-linux-avx-avx2-f16c-fma-no_runtime-sse41"
	.size	.Lstr.80, 48

	.type	.Lstr.81,@object                # @str.81
	.p2align	5
.Lstr.81:
	.asciz	"cost_model"
	.size	.Lstr.81, 11

	.type	.Lcost_model_metadata_storage,@object # @cost_model_metadata_storage
	.section	.data.rel.ro,"aw",@progbits
	.p2align	4
.Lcost_model_metadata_storage:
	.long	1                               # 0x1
	.long	20                              # 0x14
	.quad	.L__unnamed_62
	.quad	.Lstr.80
	.quad	.Lstr.81
	.size	.Lcost_model_metadata_storage, 32

	.ident	"clang version 11.0.0"
	.ident	"clang version 11.0.0"
	.ident	"clang version 11.0.0"
	.ident	"clang version 11.0.0"
	.ident	"clang version 11.0.0"
	.section	".note.GNU-stack","",@progbits
